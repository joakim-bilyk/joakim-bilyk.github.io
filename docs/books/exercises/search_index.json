[["index.html", "Exercises Msc in Actuarial Mathematics Chapter 1 Introduction ", " Exercises Msc in Actuarial Mathematics Joakim Bilyk March 02, 2023 Abstract This document contain exercises in probability theory and mathematical statistics applied in finance, life insurance and non-life insurance. Chapter 1 Introduction "],["abbreviations.html", "1.1 Abbreviations", " 1.1 Abbreviations Below is given the abbreviations used when referencing to books: Chapter Abbreviation Source Basic Life Insurance Mathematics Stochastic Processes in Life Insurance Mathematics Life Insurance Mathematics Asmussen Risk and Insurance: A Graduate Text by Soren Asmussen and Mogens Steffensen (2020). Bladt Notes from lectures in Liv2. Topics in Life Insurance Mathematics Asmussen Risk and Insurance: A Graduate Text by Soren Asmussen and Mogens Steffensen (2020). Continuous Time Finance Bjork Arbitrage Theory in Continuous Time (Fourth edition) by Thomas Bjork, Oxford University Press (2019). Basic Non-Life Insurance Mathematics Stochastic Processes in Life Insurance Mathematics Topics in Non-Life Insurance Mathematics Probabilistic Machine Learning None Slides from lectures. Quantative Risk Management Measure Theory Bjork Arbitrage Theory in Continuous Time (Fourth edition) by Thomas Bjork, Oxford University Press (2019). Protter Probability Essentials (2. edition) by Jean Jacod and Philip Protter (2004). Random Variables Bjork Arbitrage Theory in Continuous Time (Fourth edition) by Thomas Bjork, Oxford University Press (2019). Hansen Stochastic Processes (2. edition) by Ernst Hansen (2021). Discrete Time Stochastic Processes Hansen Stochastic Processes (2. edition) by Ernst Hansen (2021). Continuous Time Stochastic Processes Bjork Arbitrage Theory in Continuous Time (Fourth edition) by Thomas Bjork, Oxford University Press (2019). Stochastic Calculus Bjork Arbitrage Theory in Continuous Time (Fourth edition) by Thomas Bjork, Oxford University Press (2019). Bladt Notes from lectures in Liv2. Linear Algebra Wiki Wikipedia "],["to-do-work.html", "1.2 To-do work", " 1.2 To-do work Chapter Note Progress ML Exercises week 1 "],["continuous-time-finance.html", "Chapter 2 Continuous Time Finance ", " Chapter 2 Continuous Time Finance "],["week-1.html", "2.1 Week 1", " 2.1 Week 1 Probability exercises Let \\((W(t))_{t\\ge}\\) be a Brownian motion (Bjork, Definition 4.1). Exercise 1. Show that the following processes also are Brownian motions. \\((-W(t))_{t\\ge 0}\\) (symmetry) For any \\(s\\ge 0\\), \\((W(t+s)-W(s))_{t\\ge 0}\\) (time-homogeneity). For every \\(c&gt;0\\), \\((cW(t/c^2))_{t\\ge 0}\\) (scaling). Solution (i). By assumption \\(W\\) is a Brownian motion and so it follows that \\[-W_0=-1\\cdot0=0\\] Furthermore, for \\(r&lt;s\\le t&lt; u\\) it holds that \\(W_u-W_t\\) and \\(W_s-W_r\\) is independent. By seperate transformations the independence property is preserved and \\(-(W_u-W_t)\\) and \\(-(W_s-W_r)\\) is independent. Next, for a normal distributed random variable \\(N\\sim\\mathcal{N}(\\mu,\\sigma^2)\\) it holds, that for a scaler \\(c\\in\\mathbb{R}\\) we have \\(c N\\sim\\mathcal{N}(c\\mu,c^2\\sigma ^2)\\). Then obviously; \\[-(W_t)=(-1)W_t\\stackrel{d}{=}\\mathcal{N}((-1)\\cdot0,(-1)^2(t-s))\\stackrel{d}{=}\\mathcal{N}( 0,t-s).\\] Lastly, let \\(\\omega \\in \\Omega\\) and consider the sample path \\(s\\mapsto (-W_s)(\\omega)\\). Clearly for two continuous functions \\(f\\) and \\(g\\) it holds that \\((g\\circ f)\\) is continuous. Then with \\(g(f)=-f\\) and \\(f(t)=W_t(\\omega)&quot;/&gt;\\) it follows that \\((-W_t)=(g\\circ W)(t)\\) is also continuous. Solution (ii). Much like the previous exercise we define a new process and show the properties hold. Let \\(s\\ge 0\\) be chosen arbitrary. Now define \\(X_t=W(t+s)-W(s)\\). First, we let \\(t=0\\) and see \\[X_0=W(0+s)-W(s)=W(s)-W(s)=0.\\] Secondly, we have that for \\(r&lt;u\\): \\[X_u-X_r=W(u+s)-W(s)-(W(r+s)-W(s))=W(u+s)-W(r+s)\\sim \\mathcal{N}(0,u+s-(r+s))=\\mathcal{N}(0,u-r).\\] and since for \\(r&lt;u\\le k&lt;l\\) the translation \\(r+s&lt;u+s\\le k+s&lt;l+s\\) still holds and \\(X_l-X_k=W(l+s)-W(k+s)\\) and \\(X_u-X_r=W(u+s)-W(k+s)\\) are independent. Finally since \\(W_t(\\omega)\\) is continuous in \\(t\\) hence the translation \\(W_{t+s}\\) is continouos. Adding a constant yields a function that is also continuous, hence \\(X_t\\) is continuous. Solution (iii). Let \\(c&gt;0\\) be given. We show that \\[X_t=cW\\left(\\frac{t}{c^2}\\right)\\] is a Brownian motion. We simply show the four properties. Let \\(t=0\\) and notice \\[X_0=cW\\left(\\frac{0}{c^2}\\right)=cW(0)=0.\\] The second property follows from seperate transformation and that for \\(r&lt;u\\le s&lt;t\\) we consider \\[X_u-X_r=c\\left(W\\left(\\frac{u}{c^2}\\right)-W\\left(\\frac{r}{c^2}\\right)\\right)\\hspace{20pt}\\text{and}\\hspace{20pt}X_t-X_s=c\\left(W\\left(\\frac{t}{c^2}\\right)-W\\left(\\frac{s}{c^2}\\right)\\right)\\] and since \\(c,r,u,t,s&gt;0\\) we have the same order for the scaled version of \\(r,u,t,s\\) and hence we have two independent RV scaled by \\(c\\). Then by seperate transformations the variables is still independent. Next for the third property: \\[X_t-X_s=c\\left(W\\left(\\frac{t}{c^2}\\right)-W\\left(\\frac{s}{c^2}\\right)\\right)\\sim\\mathcal{N}\\left(c\\cdot 0,c^2\\left(\\frac{t}{c^2}-\\frac{s}{c^2}\\right)\\right)=\\mathcal{N}(0,t-s).\\] Where we use the properties of scaling a normal distributed random variable i.e. for \\(c&gt;0\\) and \\(N\\sim\\mathcal{N}(\\mu,\\sigma ^2)\\) it follows that \\(c N\\sim\\mathcal{N}(c\\mu,c^2\\sigma ^2)\\). Finally, the forth property follows since \\(g(f)=cf\\) is continuous and \\(h(t)=t/c^2\\) is continuous, then for any continuous function \\(f(s)\\) it follows that \\((g \\circ f\\circ h)=g(f(h(t)))\\) is continuous.   Proposition B.37. Let \\((\\Omega,\\mathcal{F},P)\\) be a given probability space, let \\(\\mathcal{G}\\) be a sub-sigma-algebra of \\(\\mathcal{F}\\), and let \\(X\\) be a square integrable random variable. Consider the problem of minimizing \\[E\\left[(X-Z)^2\\right]\\] where \\(Z\\) is allowed to vary over the class of all square integrable \\(\\mathcal{G}\\) measurable random variables. The optimal solution \\(\\hat{Z}\\) is then given by. \\[\\hat{Z}=E[X\\vert\\mathcal{G}].\\] Exercise 2. (Bjork, exercise B.11.) Prove proposition B.37 by going along the following lines. Prove that the “estimation error” \\(X-E[X\\vert\\mathcal{G}]\\) is orthogonal to \\(L^2(\\Omega,\\mathcal{G},P)\\) in the sence that for any \\(Z\\in L^2(\\Omega,\\mathcal{G},P)\\) we have \\[E[Z\\cdot(X-E[X\\vert\\mathcal{G}])]=0\\] Now prove the proposition by writing \\[X-Z=(X-E[X\\vert\\mathcal{G}])+(E[X\\vert\\mathcal{G}]-Z)\\] and use the result just proved. Solution (a). Let \\(X\\in L^2(\\Omega,\\mathcal{F},P)\\) be a random variable. Now consider an arbitrary \\(Z\\in L^2(\\Omega,\\mathcal{G},P)\\). Recall that \\(\\mathcal{G}\\subset \\mathcal{F}\\) and so \\(X\\) is also in \\(Z\\in L^2(\\Omega,\\mathcal{G},P)\\), as it is bothe square integrable and \\(\\mathcal{G}\\)-measurable. Then \\[E\\left[Z\\cdot(X-E[X\\vert\\mathcal{G}])\\right]=E\\left[Z\\cdot X\\right]-E\\left[Z\\cdot E[X\\vert\\mathcal{G}]\\right].\\] Then by using the law of total expectation and secondly that \\(Z\\) is \\(\\mathcal{G}\\)-measurable we have that \\[E\\left[Z\\cdot X\\right]=E\\left[E[Z\\cdot X\\vert\\mathcal{G}]\\right]=E\\left[Z\\cdot E[ X\\vert\\mathcal{G}]\\right].\\] Combining the two equations gives the desired result. Solution (b). Obviously, we have that \\[X-Z=X-Z+E[X\\vert\\mathcal{G}]-E[X\\vert\\mathcal{G}]=(X-E[X\\vert\\mathcal{G}])+(E[X\\vert\\mathcal{G}]-Z).\\] Then squaring the terms gives \\[(X-Z)^2=(X-E[X\\vert\\mathcal{G}])^2+(E[X\\vert\\mathcal{G}]-Z)^2+2(X-E[X\\vert\\mathcal{G}])(E[X\\vert\\mathcal{G}]-Z)\\] Taking expectation on each side and using linearity of the expectation we have that \\[E[(X-Z)^2]=E\\left[(X-E[X\\vert\\mathcal{G}])^2\\right]+E\\left[(E[X\\vert\\mathcal{G}]-Z)^2\\right]+2E\\left[(X-E[X\\vert\\mathcal{G}])(E[X\\vert\\mathcal{G}]-Z)\\right].\\] We can now use that \\(E[X\\vert\\mathcal{G}]-Z\\) is \\(\\mathcal{G}\\)-measurable with the above result on the last term. \\[E[(X-Z)^2]=E\\left[(X-E[X\\vert\\mathcal{G}])^2\\right]+E\\left[(E[X\\vert\\mathcal{G}]-Z)^2\\right].\\] Now since \\(X\\) is given the term \\(E\\left[(X-E[X\\vert\\mathcal{G}])^2\\right]\\) is simply a constant not depending on the choice og \\(Z\\). The optimal choice of \\(Z\\) is then \\(E[X\\vert\\mathcal{G}]\\) since this minimizes the second term. The statement is then proved.   Exercise 3. Discuss the following theory/results of Moment generating functions (Laplace transform). Let \\(X\\) be a random variable with distribution function \\(F(x)=P(X\\le x)\\) and \\(Y\\) be a random variable with distribution function \\(G(y)=P(Y\\le y)\\). Definition. The moment generating function or Laplace transform of \\(X\\) is \\[\\psi_X(\\lambda)=E\\left[e^{\\lambda X}\\right]=\\int_{-\\infty}^\\infty e^{\\lambda x}dF(x)\\] provided the expectation is finite for \\(\\vert\\lambda\\vert&lt;h\\) for some \\(h&gt;0\\). The MGF uniquely determine the distribution of a random variable, due to the following result. Theorem 1. (Uniqueness) If \\(\\psi_X(\\lambda)=\\psi_Y(\\lambda)\\) when \\(\\vert\\lambda\\vert&lt;h\\) for some \\(h&gt;0\\), then \\(X\\) and \\(Y\\) has the same distribution, that is, \\(F=G\\). There is also the following result of independence for Moment generating functions. Theorem 1. (Independence) If \\[E\\left[e^{\\lambda_1X+\\lambda_2Y}\\right]=\\psi_X(\\lambda_1)\\psi_Y(\\lambda_2)\\] for \\(\\vert\\lambda_i\\vert&lt;h\\) for \\(i=1,2\\) for some \\(h&gt;0\\), then \\(X\\) and \\(Y\\) are independent random variables. Example. Recall that the Moment generating function of a normal (Gaussian) distribution is given by \\[\\psi_X(\\lambda)=E\\left[e^{\\lambda X}\\right]=\\exp\\left(\\lambda \\mu + \\frac{\\lambda^2}{2}\\sigma^2\\right)\\] where \\(X\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\) and \\(\\lambda\\in\\mathbb{R}\\) is a constant. Since a Brownian motion \\(W(t)\\) is normally distributed with zero mean and variance \\(t\\), we have that \\[E[\\exp(\\lambda W(t))]=\\exp\\left(\\frac{\\lambda^2}{2}t\\right).\\] Discussion.   Exercise 4. (Bjork, exercise C.8.(a-c)) Let \\(W\\) be a Brownian motion. Notice that for the natural filtration \\(\\mathcal{F}_s=\\sigma(W_t\\vert t\\le s)\\) \\(W_t-W_s\\) is independent of \\(\\mathcal{F}_s\\) Show that \\(W_t\\) is a martingale. Show that \\(W^2_t-t\\) is a martingale. Show that \\(\\exp(\\lambda W_t-\\frac{\\lambda^2}{2}t)\\) is a martingale. Solution (a). We show that for the natural filtration that \\(W_t\\) is a martingale. This include showing integrability and the martingale property. For the first we note that for a normal distributed random variable with mean 0 we have \\[E[\\vert N\\vert]=\\int_{-\\infty}^\\infty \\vert x\\vert dF_N(x)=2\\int_{0}^\\infty xdF_N(x)\\] since the distribution is symmetric. Substituting the distribution function \\(\\Phi(x)=P(N\\le x)\\) in we see that \\[E[\\vert N\\vert]=2\\int_{0}^\\infty xd\\Phi(x)=2\\int_{0}^\\infty x\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-x^2/(2\\sigma^2)}dx=(*)\\] by substituting \\(u=x^2/(2\\sigma^2)\\) (\\(x=\\sqrt{2\\sigma^2u}\\)) we have that \\[\\frac{dx}{du}=\\frac{1}{2}\\sqrt{2\\sigma^2u}2\\sigma^2=(\\sigma^2)^{3/2}\\sqrt{2}u\\iff dx=(\\sigma^2)^{3/2}\\sqrt{2}u\\ du\\] hence \\[(*)=\\frac{2}{\\sqrt{2\\pi\\sigma^2}}\\int_0^{\\infty}\\sqrt{2\\sigma^2u}e^{-u}(\\sigma^2)^{3/2}\\sqrt{2}u\\ du=\\frac{2\\sqrt{2\\sigma^2}(\\sigma^2)^{3/2}\\sqrt{2}}{\\sqrt{2\\pi\\sigma^2}}\\int_0^{\\infty}\\sqrt{u}e^{-u}u\\ du.\\] This then simplify to \\[(*)=\\frac{(2\\sigma^2)^{3/2}}{\\sqrt{\\pi}}\\int_0^{\\infty}u^{3/2}e^{-u}\\ du=(2\\sigma^2)^{1/2}\\sqrt{\\frac{2\\sigma^2}{\\pi}}\\int_0^{\\infty}u^{3/2}e^{-u}\\ du=\\sqrt{\\frac{2\\sigma^2}{\\pi}}&lt;\\infty.\\] (Obviously the above is not derived correctly, but the end expression is valid, source: link) However since \\[W_t=W_t-0=W_t-W_0\\sim\\mathcal{N}(0,t)\\] we have that \\(E\\vert W_t\\vert&lt;\\infty\\) as desired. Next, we have that \\[E[W_t\\vert \\mathcal{F}_s]=E[W_t-W_s\\vert\\mathcal{F}_s]+W_s=0+W_s=W_s.\\] In the above we used that \\(W_t-W_s\\) is \\(\\mathcal{F}_s\\)-measurable with mean 0. Then it follows that \\(W_t\\) is a martingale. Solution (b). Let \\(M_t=W_t^2-t\\). First, we observe that two measurable functions composed is still a measurable function. Hence we know that \\(M_t\\) is measurable wrt. the filtration since \\(W_t\\) is measurable and \\(w\\mapsto w^2+t\\) is measurable. Secondly, we have that \\[E[\\vert W_t^2-t\\vert]\\le E\\vert W_t^2\\vert +E\\vert t\\vert=t+t=2t&lt;\\infty\\] where we use the triangle inequality. Thirdly, for the martingale property we have that for \\(t&gt;s\\): \\[E[M_t\\vert \\mathcal{F}_s]=E[W_t^2-t\\vert \\mathcal{F}_s]=E[W_t^2+W_s^2-2W_tW_s-W_s^2+2W_tW_s-t\\vert \\mathcal{F}_s]\\] which by linearity and independence of increments to the filtration gives \\[E[M_t\\vert \\mathcal{F}_s]=E[(W_t-W_s)^2-W_s^2+2W_tW_s-t\\vert \\mathcal{F}_s]=t-s-t+E[2W_tW_s-W_s^2\\vert \\mathcal{F}_s]\\] However since \\(W_s\\) is measurable wrt. the filtration at time \\(s\\) the above is \\[E[M_t\\vert \\mathcal{F}_s]=2W_sE[W_t\\vert \\mathcal{F}_s]-W_s^2-s=2W_s^2-W_s^2-s=W_s^2-s=M_s.\\] Since from (a) we know that \\(W_t\\) is a martingale. Then we arrive at the desired result. Solution (c). Let \\(M_t=\\exp\\left(\\lambda W_t-\\frac{\\lambda^2}{2}t\\right)\\). First, by composition of measurable functions \\(M_t\\) is \\(\\mathcal{F}_t\\)-measurable. Secondly, we have using the MGF for a normal distributed random variable: \\[E\\vert M_t=E\\left(\\exp\\left(\\lambda W_t-\\frac{\\lambda^2}{2}t\\right)\\right)\\le E\\left(\\exp\\left(\\lambda W_t\\right)\\right)=\\exp\\left(\\frac{\\lambda^2}{2}t\\right)&lt;\\infty.\\] Thirdly, we consider \\[E[M_t\\vert\\mathcal{F}_s]=E\\left.\\left[\\left(\\exp\\left(\\lambda W_t-\\frac{\\lambda^2}{2}t\\right)\\right)\\right\\vert\\mathcal{F}_s\\right]=\\exp\\left(-\\frac{\\lambda^2}{2}t\\right)E\\left.\\left[\\left(\\exp\\left(\\lambda W_t\\right)\\right)\\right\\vert\\mathcal{F}_s\\right].\\] By adding and subtracting \\(W_s\\) in the exponent we get \\[\\begin{align*} E[M_t\\vert\\mathcal{F}_s]&amp;=\\exp\\left(-\\frac{\\lambda^2}{2}t\\right)E\\left.\\left[\\left(\\exp\\left(\\lambda (W_t-W_s)+\\lambda W_s\\right)\\right)\\right\\vert\\mathcal{F}_s\\right]\\\\ &amp;=\\exp\\left(-\\frac{\\lambda^2}{2}t\\right)\\exp\\left(\\frac{\\lambda^2}{2}(t-s)\\right)E\\left.\\left[\\left(\\exp\\left(\\lambda W_s\\right)\\right)\\right\\vert\\mathcal{F}_s\\right]. \\end{align*}\\] Using that \\(E\\left.\\left[\\left(\\exp\\left(\\lambda W_s\\right)\\right)\\right\\vert\\mathcal{F}_s\\right]=\\exp\\left(\\lambda W_s\\right)\\) and combining the exponents gives the desired: \\[E[M_t\\vert\\mathcal{F}_s]=\\exp\\left(\\lambda W_s-\\frac{\\lambda^2}{2}s\\right)=M_s.\\] "],["week-2.html", "2.2 Week 2", " 2.2 Week 2 Exercise 1 (Bjork 4.1) Compute the stochastic differential \\(dZ_t\\) when \\(Z_t=e^{\\alpha t}\\). \\(Z_t=\\int_0^t g_s\\ dW_s\\), where \\(g\\) is an adapted stochastic process. \\(Z_t=e^{\\alpha W_t}\\). \\(Z_t=e^{\\alpha X_t}\\), where \\(X\\) has stochastic differential \\(dX_t=\\mu\\ dt + \\sigma\\ dW_t\\) and \\(\\mu,\\sigma\\) is constants. \\(Z_t=X_t^2\\), where \\(X\\) has stochastic differential \\(dX_t=\\alpha X_t\\ dt+\\sigma X_t\\ dW_t\\). Solution (a). Let \\(Z_t=e^{\\alpha t}\\), then we see that \\(f(t,x)=e^{\\alpha t}\\) and the the following relevant derivatives is \\[ \\frac{\\partial f}{\\partial t}(t,x)=\\alpha e^{\\alpha t},\\hspace{10pt}\\frac{\\partial f}{\\partial x}(t,x) =0,\\hspace{10pt}\\frac{\\partial f}{\\partial x^2}(t,x) =0. \\] Since \\(Z\\) does not depend on any stochastic process, we will content with \\(X_t=0\\), that is \\(\\mu_t=\\sigma_t=0\\). Then by theorem 4.11 (Ito’s formula) we have \\[ dZ_t=\\left(\\alpha e^{\\alpha t} +0+0\\right)\\ dt + 0=\\alpha e^{\\alpha t}\\ dt, \\] as expected. \\(\\square\\) Solution (b). Let \\(Z_t=\\int_0^t g_s\\ dW_s\\), where \\(g\\) is an adapted stochastic process. We see that if we set \\(X_t=\\int_0^t g_s\\ dW_s\\) then \\[ dX_t=0\\ dt+g_t\\ dW_t. \\] Then we have the function \\(f(t,x)=x\\) and the relevant derivatives are: \\[ \\frac{\\partial f}{\\partial t}(t,x)=0,\\hspace{10pt}\\frac{\\partial f}{\\partial x}(t,x) =1,\\hspace{10pt}\\frac{\\partial f}{\\partial x^2}(t,x) =0. \\] This then gives \\[ dZ_t=\\left(0+0+\\frac{1}{2}g_t\\cdot 0\\right)\\ dt + g_t\\cdot 1\\ dW_t=g_t\\ dW_t, \\] as expected. \\(\\square\\) Solution (c). Let \\(Z_t=e^{\\alpha W_t}\\). Then we may set \\(X_t=W_t\\) and we then have \\(\\mu_t=0\\) and \\(\\sigma_t=1\\). The function \\(f(t,x)=e^{\\alpha x}\\) and the relevant derivatives are: \\[ \\frac{\\partial f}{\\partial t}(t,x)=0,\\hspace{10pt}\\frac{\\partial f}{\\partial x}(t,x) =\\alpha e^{\\alpha x},\\hspace{10pt}\\frac{\\partial f}{\\partial x^2}(t,x) =\\alpha^2 e^{\\alpha x}. \\] Then the dynamics of \\(Z_t\\) is as follows \\[\\begin{align*} dZ_t&amp;=\\left(0+0+\\frac{1}{2}1^2\\alpha^2e^{\\alpha X_t}\\right)\\ dt + 1\\alpha e^{\\alpha X_t}\\ dW_t\\\\ &amp;=\\frac{\\alpha^2}{2}e^{\\alpha X_t}\\ dt +\\alpha e^{\\alpha X_t}\\ dW_t\\\\ &amp;=\\frac{\\alpha^2}{2}Z_t\\ dt +\\alpha Z_t\\ dW_t. \\end{align*}\\] As desired. \\(\\square\\). Solution (d). Let \\(Z_t=e^{\\alpha X_t}\\), where \\(X\\) has stochastic differential \\(dX_t=\\mu\\ dt + \\sigma\\ dW_t\\) and \\(\\mu,\\sigma\\) is constants. Then we have been given the definition of \\(X_t\\) and we set \\(f(t,x)=e^{\\alpha x}\\). The relevant derivatives are then: \\[ \\frac{\\partial f}{\\partial t}(t,x)=0,\\hspace{10pt}\\frac{\\partial f}{\\partial x}(t,x) =\\alpha e^{\\alpha x},\\hspace{10pt}\\frac{\\partial f}{\\partial x^2}(t,x) =\\alpha^2 e^{\\alpha x}. \\] We may now derive the dynamics of \\(Z_t\\): \\[\\begin{align*} dZ_t&amp;=\\left(0+\\mu \\alpha e^{\\alpha X_t}+\\frac{1}{2} \\sigma^2\\alpha^2 e^{\\alpha X_t}\\right)\\ dt+\\sigma \\alpha e^{\\alpha X_t}\\ dW_t\\\\ &amp;=\\left(\\mu+\\frac{1}{2}\\sigma^2\\alpha\\right)\\alpha e^{\\alpha X_t}\\ dt+\\sigma \\alpha e^{\\alpha X_t}\\ dW_t\\\\ &amp;=\\left(\\mu+\\frac{1}{2}\\sigma^2\\alpha\\right)\\alpha Z_t\\ dt+\\sigma \\alpha Z_t\\ dW_t. \\end{align*}\\] As desired. \\(\\square\\). Solution (e). Let \\(Z_t=X_t^2\\), where \\(X\\) has stochastic differential \\(dX_t=\\alpha X_t\\ dt+\\sigma X_t\\ dW_t\\). Then we set \\(f(t,x)=x^2\\) and the relevant derivatives are: \\[ \\frac{\\partial f}{\\partial t}(t,x)=0,\\hspace{10pt}\\frac{\\partial f}{\\partial x}(t,x) =2x,\\hspace{10pt}\\frac{\\partial f}{\\partial x^2}(t,x) =2. \\] Given this we have the dynamics of \\(Z_t\\) as follows \\[\\begin{align*} dZ_t&amp;=\\left(0 + \\alpha X_t2X_t+\\frac{1}{2}(\\sigma X_t)^22\\right)\\ dt+\\sigma X_t 2 X_t\\ dW_t\\\\ &amp;=\\left(2\\alpha +\\sigma^2\\right) X_t^2\\ dt + 2\\sigma X_t^2\\ dW_t\\\\ &amp;=\\left(2\\alpha +\\sigma^2\\right) Z_t\\ dt + 2\\sigma Z_t\\ dW_t. \\end{align*}\\] As desired. \\(\\square\\). Exercise 2 (Bjork 4.2) Compute the stochastic differential for \\(Z\\) when \\(Z_t=(X_t)^{-1}\\) and \\(X\\) has the stochastic differential \\[ dX_t=\\alpha X_t\\ dt + \\sigma X_t\\ dW_t. \\] Furthermore, by using the definition \\(Z=X^{-1}\\) you can in fact express the right-hand side of \\(dZ\\) entirely in terms of \\(Z\\) itself (rather then in terms of \\(X\\)). Thus \\(Z\\) satisfies a stochastic differential equation. Which one? Solution. We see that \\(f(t,x)=1/x\\) and so the relevant derivatives is \\[ \\frac{\\partial f}{\\partial t}(t,x)=0,\\hspace{10pt}\\frac{\\partial f}{\\partial x}(t,x) =-\\frac{1}{x^2},\\hspace{10pt}\\frac{\\partial f}{\\partial x^2}(t,x) =\\frac{2}{x^3}. \\] Then we by Ito’s formula we have \\[\\begin{align*} dZ_t&amp;=\\left(0-\\alpha X_t\\frac{1}{X_t^2}+\\frac{1}{2} \\sigma^2 X_t^2\\frac{2}{X_t^3}\\right)\\ dt-\\sigma X_t\\frac{1}{X_t^2}\\ dW_t\\\\ &amp;=\\left(-\\alpha \\frac{1}{X_t}+ \\sigma^2 \\frac{1}{X_t}\\right)\\ dt-\\sigma \\frac{1}{X_t}\\ dW_t\\\\ &amp;=(\\sigma^2-\\alpha)Z_t\\ dt-\\sigma Z_t\\ dW_t. \\end{align*}\\] We also notice that \\[ Z_t=\\frac{1}{X_t}\\Rightarrow dZ_t=d\\left(\\frac{1}{X_t}\\right)=-\\left(\\frac{1}{X_t}\\right)^2\\ dX_t=-Z_t^2(\\alpha X_t\\ dt+\\sigma X_t\\ dW_t) \\] Hence we may insert \\(X_t=Z_t^{-1}\\) and optain \\[ dZ_t=-Z_t^2\\left(\\alpha\\frac{1}{Z_t}\\ dt + \\sigma \\frac{1}{Z_t}\\ dW_t\\right)=-\\alpha Z_t\\ dt-\\sigma Z_t\\ dW_t. \\] Which clearly is faulty.. \\(\\square\\) Exercise 3. (Bjork 4.3) Let \\(\\sigma(t)\\) be a given deterministic function of time and define the process \\(X\\) by \\[ X_t=\\int_0^t\\sigma(s)\\ dW_s. \\] Use the technique discribed in example 4.17 in order to show that the characteristic function of \\(X_t\\) (for a fixed \\(t\\)) is given by \\[ E[e^{iuX_t}]=\\exp\\left\\{-\\frac{u^2}{2}\\int_0^t\\sigma^2(s)\\ ds\\right\\},\\ \\ u\\in\\mathbb{R}, \\] thus showing that \\(X_t\\) is normally distributed with zero mean and a variance given by \\[ Var[X_t]=\\int_0^t\\sigma^2(s)\\ ds. \\] Solution. We follow along the lines of Determine the dynamics of \\(Z_t=e^{iuX_t}\\) (for fixed \\(u\\)). Write the integral form of \\(Z_t\\). Take expectation. Solve ODE. “1)” Set \\(f(t,x)=e^{iuX_t}\\) then the relevant derivatives are \\[ \\frac{\\partial f}{\\partial t}(t,x)=0,\\hspace{10pt}\\frac{\\partial f}{\\partial x}(t,x) =iue^{iuX_t}=iuZ_t,\\hspace{10pt}\\frac{\\partial f}{\\partial x^2}(t,x) =i^2u^2e^{iuX_t}=-u^2Z_t. \\] Recall that \\(dX_t=\\sigma(t)\\ dW_t\\), then by Ito’s formula we have \\[ dZ_t=\\left(-\\sigma(t)^2\\frac{1}{2}u^2Z_t\\right)\\ dt+\\sigma(t)iuZ_t\\ dW_t.\\tag{*} \\] “2)” We can now write (*) on integral form as below \\[ Z_t=Z_0-\\frac{u^2}{2}\\int_0^t\\sigma^2(s)Z_s\\ ds+iu\\int_0^t\\sigma (s)Z_s\\ dW_s, \\] where \\(Z_0=e^{iuX_0}=1\\). “3)” Taking expectation now yields \\[ E[Z_t]=1-\\frac{u^2}{2}\\int_0^t\\sigma^2(s)E[Z_s]\\ ds+iuE\\left[\\int_0^t \\sigma(s)Z_s\\ dW_s\\right]=1-\\frac{u^2}{2}\\int_0^t\\sigma^2(s)E[Z_s]\\ ds, \\] since any expectaion of an integral wrt. a Brownian motion is 0 (proposition 4.5). “4)” Now we see that the \\(t\\)-derivative gives \\[ dE[Z_t]=-\\frac{u^2}{2}\\sigma^2(t)E[Z_t]\\ dt,\\ \\ E[Z_0]=1. \\] This is a ordinary differential equation with solution \\(y(t)=\\exp\\{-u^2/2\\int_0^t\\sigma^2(s)\\ ds\\}\\) (check by differentiating) hence \\[ E[e^{iuX_t}]=E[Z_t]=\\exp\\left\\{-\\frac{u^2}{2}\\int_0^t\\sigma^2(s)\\ ds\\right\\}. \\] We recognize this as the characteristic function of a normally distributed random variable with variance \\(\\int_0^t\\sigma^2(s)\\ ds\\) as desired. (\\(X_t\\) follows this distributions since characteristic functions determine the distribution) \\(\\square\\) Exercise 4 (Bjork 4.4) Suppose that \\(X\\) has the stochastic differential \\[ dX_t=\\alpha X_t\\ dt+\\sigma_t\\ dW_t, \\] where \\(\\alpha\\) is a real number and \\(\\sigma_t\\) is a integrable adapted stochastic process. Use the technique in example 4.17 in order to determine the function \\(m(t)=E[X_t]\\). Solution. We follow the same steps as the previous exercise. We have been given the dynamics of \\(X\\) hence we may write it on integral form. \\[ X_t=X_0+\\alpha\\int_0^tX_s\\ ds+\\int_0^t\\sigma(s)\\ dW_s. \\] Then taking expectation now gives \\[ E[X_t]=X_0+\\alpha\\int_0^tE[X_s]\\ ds. \\] Hence \\(E[X_t]\\) follows from the solution to the ODE below \\[ dE[X_t]=\\alpha E[X_t]\\Rightarrow E[X_t]=C\\cdot\\exp\\{\\alpha t\\}. \\] Then obviously \\(C=X_0\\) and we arrive at the solution \\(E[X_t]=X_0e^{\\alpha t}\\), where \\(X_0\\) is some deterministic value. \\(\\square\\) Exercise 5 (Bjork 4.5) Suppose that the process \\(X\\) has a stochastic differential \\[ dX_t=\\mu_t\\ dt+\\sigma_t\\ dW_t, \\] and that \\(\\mu_t\\ge 0\\) with probability one for all \\(t\\ge 0\\). Show that this implies that \\(X\\) is a sub-martingale. Solution. Note that we are (strictly speaking) supposed to show adaptation and integrability, we will however only fokus on the submartingale property. “\\(E[X_t\\vert \\mathcal{F}_s]\\ge X_s\\)” Intuitively speaking, the statement is obvious since we have with probability one a positive upwards drift with Brownian distortion (i.e. martingale). Formally, we will show the statement by first writing \\(X_t\\) on integral form \\[ X_t=x_0+\\int_0^t\\mu_s\\ ds+\\int_0^t\\sigma_s\\ dW_s. \\] And so \\[ X_t-X_s=\\int_s^t\\mu_u\\ du+\\int_s^t\\sigma_u\\ dW_u. \\] We then have \\[\\begin{align*} E[X_t\\ \\vert\\ \\mathcal{F}_s]-X_s&amp;=E[X_t-X_s\\ \\vert\\ \\mathcal{F}_s]\\\\ &amp;=E\\left[\\left.\\int_s^t\\mu_u\\ du+\\int_s^t\\sigma_u\\ dW_u\\ \\right\\vert\\ \\mathcal{F}_s\\right]\\\\ &amp;=E\\left[\\left.\\int_s^t\\mu_u\\ du\\ \\right\\vert\\ \\mathcal{F}_s\\right]+E\\left[\\left.\\int_s^t\\sigma_u\\ dW_u\\ \\right\\vert\\ \\mathcal{F}_s\\right]\\\\ &amp;=E\\left[\\left.\\int_s^t\\mu_u\\ du\\ \\right\\vert\\ \\mathcal{F}_s\\right]\\ge 0. \\end{align*}\\] Then adding \\(X_s\\) to the above inequality yields the result. \\(\\square\\) Exercise 6 (Bjork 4.7) The objective of this exercise is to give an argument for the formal identity \\[ dW_1(t)\\cdot dW_2(t)=0, \\] when \\(W_1\\) and \\(W_2\\) are independent Brownian motions. Let us therefore fix a time \\(t\\), and divide the inerval \\([0,t]\\) into equidistant points \\(0=t_0&lt;t_1&lt;\\cdots &lt; t_n=t\\), where \\(t_i=\\frac{i}{n}\\cdot t\\). We use the notation \\[ \\Delta W_i(t_k)=W_i(t_k)-W_i(t_{k-1}),\\ i=1,2. \\] Now define \\(Q_n\\) by \\[ Q_n=\\sum_{k=1}^n \\Delta W_1(t_k)\\cdot \\Delta W_2(t_k). \\] Show that \\(Q_n\\to 0\\) in \\(L^2\\), i.e. show that \\[ E[Q_n]=0,\\\\ Var[Q_n]\\to 0. \\] Solution. We wish to show the statement \\[ E[(Q_n-0)^2]=E[Q_n^2]\\to 0, \\] as \\(n\\to \\infty\\). Recall that \\[ Var[Q_n]=E[Q_n^2]-E[Q_n]^2, \\] hence if \\(Q_n\\) has mean 0, then showing convergence in \\(L^2\\) is equivalent to showing variance going to 0. Let us start by showing the mean is 0. We have that \\[\\begin{align*} Q_n&amp;=\\sum_{k=1}^n \\Delta W_1(t_k)\\cdot \\Delta W_2(t_k)\\\\ &amp;=\\sum_{k=1}^n(W_1(t_k)-W_1(t_{k-1}))\\cdot (W_2(t_k)-W_2(t_{k-1}))\\\\ &amp;\\stackrel{\\mathcal{D}}{=}\\sum_{k=1}^nXY, \\end{align*}\\] where \\(X,Y\\sim\\mathcal{N}(0,t_k-t_{k-1})=\\mathcal{N}(0,1/n)\\) and independent random variable. This is justified since the increments of the Brownian motion has mean 0 and variance equal to the increment size. Now this implies, that we need to show that \\(E[XY]=0\\) and that \\(Var[XY]\\) is sufficiently small in terms of \\(n\\) such that it is summable. We see that \\[ E[XY]=E[X]E[Y]=0^2=0. \\] Here we use independence. We now know that the mean is \\[ E[Q_n]=\\sum_{k=1}^nE[XY]=0. \\] We know from basic properties of variance that \\[\\begin{align*} Var(Q_n)&amp;=\\sum_{k=1}^n Var(XY)=\\sum_{k=1}^n E[(XY)^2]\\\\ &amp;=\\sum_{k=1}^n\\frac{1}{n^2}=\\frac{1}{n^2}n\\\\ &amp;=\\frac{1}{n}\\to0,\\ n\\to\\infty. \\end{align*}\\] And so the result follows. \\(\\square\\) Exercise 7 (Bjork 4.8) Let \\(X\\) and \\(Y\\) be given as the solutions to the following system of stochastic differential equations. \\[\\begin{align*} &amp;dX_t=\\alpha X_t\\ dt-Y_t\\ dW_t,\\ &amp;X_0=x_0,\\\\ &amp;dY_t=\\alpha Y_t\\ dt + X_t\\ dW_t,\\ &amp;Y_0=y_0. \\end{align*}\\] Note that the initial values \\(x_0\\) and \\(y_0\\) are deterministic constants. Prove that the process \\(R\\) defined by \\(R_t=X_t^2+Y_t^2\\) is deterministic. Compute \\(E[X_t]\\). Solution (a). We see that \\[ dR_t=d(X_t^2+Y_t^2)=d(X_t^2)+d(Y_t^2) \\] Hence we may start by considering de dynamics of the processes \\(X_t^2\\) and \\(Y_t^2\\). We see that for the process \\(Z_t=X_t^2\\) we may set \\(f(t,x)=x^2\\) and the relevant derivatives are \\[ \\frac{\\partial f}{\\partial t}(t,x)=0,\\ \\frac{\\partial f}{\\partial x}(t,x)=2x,\\ \\frac{\\partial^2 f}{\\partial x^2}(t,x)=2. \\] By Ito’s formula we have \\[ d(X_t^2)=\\left(\\alpha X_t2X_t+Y_t^22\\right)\\ dt-Y_t2X_t\\ dW_t=2(\\alpha X_t^2+Y_t^2)\\ dt-2X_tY_t\\ dW_t. \\] By the same concept we have \\[ d(Y_t^2)=\\left(\\alpha Y_t2Y_t+X_t^22\\right)\\ dt+X_t2Y_t\\ dW_t=2(\\alpha Y_t^2+X_t^2)\\ dt+2X_tY_t\\ dW_t. \\] Combining we get the dynamics \\[\\begin{align*} dR_t&amp;=2(\\alpha X_t^2+Y_t^2)\\ dt-2X_tY_t\\ dW_t\\\\ &amp;+2(\\alpha Y_t^2+X_t^2)\\ dt+2X_tY_t\\ dW_t\\\\ &amp;=(2\\alpha +1)(X_t^2 + Y_t^2)\\ dt\\\\ &amp;=(2\\alpha +1)R_t\\ dt \\end{align*}\\] Hence \\(R_t\\) has deterministic derivative and therefore a deterministic process. In fact, the solution to above is \\[ R_t=R_0\\exp\\left\\{(2\\alpha + 1)t\\right\\}=(x_0^2+y_0^2)e^{(2\\alpha + 1)t}, \\] which is clearly deterministic. \\(\\square\\) Solution (b). We start by acknowledging that the differential form of \\(X\\) may be written on integral form: \\[ X_t=x_0+\\alpha\\int_0^tX_s\\ ds-\\int_0^tY_s\\ dW_s. \\] Taking expectation we see that \\[ E[X_t]=x_0+\\int_0^tE[X_s]\\ ds \\] as the last term has mean 0 according to proposition 4.5. Then the above may be written on the differential form \\[ dE[X_t]=E[X_t]\\ dt \\] Hence we have that \\[ E[X_t]=x_0e^{t}. \\] Hence \\(X_t\\) has mean not depending on the tragetory of the sister-process \\(Y_t\\). \\(\\square\\) "],["week-3.html", "2.3 Week 3", " 2.3 Week 3 Exercise 1. (Bjork 5.1) Show that the scalar SDE \\[ \\left\\{ \\begin{matrix} dX_t=\\alpha X_t\\ dt + \\sigma\\ dW_t,\\\\ X_0 = x_0, \\end{matrix}\\right. \\] has the solution \\[ X(t)=e^{\\alpha t}x_0+ \\sigma\\int_0^te^{\\alpha(t-s)}\\ dW_s, \\] by differentiating \\(X\\) as defined by the equation above and showing that \\(X\\) so defined satisfies the SDE. Solution. We move forward by rewriting the solution in terms of three processes \\(Z\\), \\(Y\\) and \\(R\\) as \\[ X_t=\\underbrace{x_0e^{\\alpha t}}_{:=Y_t}+\\underbrace{\\sigma e^{\\alpha t}}_{:=Z_t} \\underbrace{\\int_0^t e^{-\\alpha s}\\ dW_s}_{R_t}=Y_t+Z_t\\cdot R_t. \\] We furthermore see easily that the dynamics of the processes individually has dynamics \\[\\begin{align*} d Y_t&amp;=\\alpha x_0e^{\\alpha t}\\ dt=\\alpha Y_t\\ dt,\\ &amp;Y_0=x_0,\\\\ d Z_t&amp;=\\alpha \\sigma^{\\alpha t}\\ dt=\\alpha Z_t\\ dt,\\ &amp;Z_0=\\sigma,\\\\ d R_t&amp;=e^{-\\alpha t}\\ dW_s,\\ &amp;R_0=0. \\end{align*}\\] We then have the following function \\[ f\\left(t,y,z,r\\right)=y+zr. \\] With the following multi-dimensional process \\[ dM_t=\\begin{bmatrix}\\alpha Y_t\\\\ \\alpha Z_t\\\\ 0\\end{bmatrix}dt+\\begin{bmatrix}0 &amp;0 &amp;0 \\\\ 0 &amp; 0 &amp;0 \\\\ 0 &amp; 0 &amp; e^{-\\alpha t}\\end{bmatrix}\\begin{bmatrix}dW_t\\\\ dW_t\\\\ dW_t\\end{bmatrix}, \\] with \\[ C=\\sigma \\sigma^\\top =\\begin{bmatrix}0 &amp;0 &amp;0 \\\\ 0 &amp; 0 &amp;0 \\\\ 0 &amp; 0 &amp; e^{-\\alpha t}\\end{bmatrix}^2=\\begin{bmatrix}0 &amp;0 &amp;0 \\\\ 0 &amp; 0 &amp;0 \\\\ 0 &amp; 0 &amp; e^{-2\\alpha t}\\end{bmatrix}. \\] That is \\(X_t=f(t,M_t)\\). We can then use the multidimensional version of Ito’s formula. \\[\\begin{align*} dX_t&amp;=df(t,M_t)\\\\ &amp;=\\left(\\frac{\\partial f}{\\partial t}(t,M_t)+\\sum_{i=1}^3\\mu_i \\frac{\\partial f}{\\partial x^i}(t,M_t)+\\frac{1}{2}\\sum_{i,j=1}^3 C^{ij}_t\\frac{\\partial^2 f}{\\partial x^i\\partial x^j}(t,M_t)\\right)\\ dt + \\sum_{i=1}^3 \\frac{\\partial f}{\\partial x^i}(t,M_t) \\sigma^i_t\\ dW_t\\\\ &amp;=\\left(0+\\alpha Y_t+\\alpha Z_t R_t\\right)\\ dt + Z_te^{-\\alpha t}\\ dW_t\\\\ &amp;=\\left(\\alpha x_0e^{\\alpha t}+\\alpha \\sigma e^{\\alpha t} \\int_0^t e^{-\\alpha \\sigma}\\ dW_s\\right)\\ dt + \\sigma e^{\\alpha t}e^{-\\alpha t}\\ dW_t\\\\ &amp;=\\left(\\alpha x_0e^{\\alpha t}+\\alpha \\sigma \\int_0^t e^{(t-s)\\alpha }\\ dW_s\\right)\\ dt + \\sigma \\ dW_t\\\\ &amp;=\\alpha X_t\\ dt + \\sigma \\ dW_t. \\end{align*}\\] Then this solution does in fact satisfies the differential form. We furthermore have that \\(X_0=x_0\\) and the desired result follows. \\(\\square\\) Exercise 2. (Bjork 5.5) Suppose that \\(X\\) satisfies the SDE \\[ dX_t = \\alpha X_t\\ dt + \\sigma X_t\\ dW_t. \\] Now define \\(Y\\) by \\(Y_t = X^\\beta_t\\), where \\(\\beta\\) is a real number. Then \\(Y\\) is also a GBM process. Compute \\(dY_t\\) and find out which SDE \\(Y\\) satisfies. Solution. If we set \\(f(t,x)=x^\\beta\\), we have the relevant derivatives as follows \\[ \\frac{\\partial f}{\\partial t}(t,x)=0,\\ \\frac{\\partial f}{\\partial x}(t,x)=\\beta x^{\\beta -1},\\ \\frac{\\partial^2 f}{\\partial x^2}(t,x)=\\beta (\\beta -1) x^{\\beta -2}. \\] Then by applying Ito’s formula we have \\[\\begin{align*} dY_t&amp;=df(t,X_t)\\\\ &amp;=\\left(0 + \\beta X_t^{\\beta -1}\\alpha X_t+\\frac{1}{2}\\sigma ^2X_t^2\\beta (\\beta -1) X_t^{\\beta -2}\\right)\\ dt+\\sigma X_t\\beta X_t^{\\beta -1} \\ dW_t\\\\ &amp;=\\left(\\alpha \\beta+\\frac{1}{2}\\sigma ^2\\beta (\\beta -1)\\right) X_t^{\\beta}\\ dt+\\sigma \\beta X_t^{\\beta } \\ dW_t\\\\ &amp;=\\left(\\alpha \\beta+\\frac{1}{2}\\sigma ^2\\beta (\\beta -1)\\right) Y_t\\ dt+\\sigma \\beta Y_t \\ dW_t\\\\ &amp;= \\alpha^Y Y_t\\ dt + \\sigma^Y Y_t\\ dW_t, \\end{align*}\\] where \\(\\alpha^Y=\\left(\\alpha \\beta+\\frac{1}{2}\\sigma ^2\\beta (\\beta -1)\\right)\\) and \\(\\sigma^Y =\\sigma \\beta\\). Futhermore \\(Y_0=y_0=x_0^\\beta\\). Then by definition of GBM we have that \\(Y_t\\) is a GBM as desired. \\(\\square\\) Exercise 3. (Bjork 5.6) Suppose that \\(X\\) satisfies the SDE \\[ dX_t = \\alpha X_t\\ dt + \\sigma X_t\\ dW_t, \\] and \\(Y\\) satisfies \\[ dY_t = \\gamma Y_t\\ dt+\\delta Y_t\\ dV_t, \\] where \\(V\\) is a Brownian motion which is independent of \\(W\\). Define \\(Z=X/Y\\) and derive an SDE for \\(Z\\) by computing \\(dZ\\). If \\(X\\) is nominal income and \\(Y\\) describe inflation then \\(Z\\) describes real income. Solution. We have that for the function \\(f(t,x,y)=x/y\\) and wish to determine the derivative of the stochastic process \\(Z_t=f(t,X_t,Y_t)\\). We do this by applying Ito’s formula in the multidimensional case. That is \\[\\begin{align*} df(t,X_t,Y_t)&amp;=\\frac{\\partial f}{\\partial t}(t,X_t,Y_t)\\ dt + \\frac{\\partial f}{\\partial x}(t,X_t,Y_t)\\ dX_t + \\frac{\\partial f}{\\partial y}(t,X_t,Y_t)\\ dY_t\\\\ &amp;+\\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}(t,X_t,Y_t)(dX_t)^2 + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial y^2}(t,X_t,Y_t)(dY_t)^2\\\\ &amp;+\\frac{1}{2}\\frac{\\partial^2 f}{\\partial x\\partial y}(t,X_t,Y_t)(dX_t)(dY_t)\\\\ &amp;=\\frac{1}{Y_t}(\\alpha X_t\\ dt + \\sigma X_t\\ dW_t)-\\frac{X_t}{Y_t^2}(\\gamma Y_t\\ dt+\\delta Y_t\\ dV_t)+\\frac{1}{2}2\\frac{X_t}{Y_t^3}(\\gamma Y_t\\ dt+\\delta Y_t\\ dV_t)^2\\\\ &amp;-\\frac{1}{2}\\frac{1}{Y_t^2}(\\gamma Y_t\\ dt+\\delta Y_t\\ dV_t)(\\alpha X_t\\ dt + \\sigma X_t\\ dW_t) \\end{align*}\\] Calculating further gives \\[\\begin{align*} (dY_t)^2&amp;=\\gamma^2 Y_t^2 (dt)^2+\\delta^2Y_t^2 (dV_t)^2+2\\gamma Y_t\\delta Y_t\\ dt\\cdot dV_t\\\\ &amp;=0 +\\delta^2Y_t^2 dt + 0=\\delta^2Y_t^2 dt\\\\ (dX_t)(dY_t)&amp;=(\\gamma Y_t\\ dt+\\delta Y_t\\ dV_t)(\\alpha X_t\\ dt + \\sigma X_t\\ dW_t)\\\\ &amp;=\\gamma Y_t \\alpha X_t\\ (dt)^2 +\\gamma Y_t\\sigma X_t\\ dt\\cdot dW_t\\\\ &amp;+\\delta Y_t \\alpha X_t\\ dt\\cdot dV_t+\\gamma Y_t\\sigma X_t\\ (dW_t)(dV_t)\\\\ &amp;=0+0+0+0=0 \\end{align*}\\] Hence we conclude that \\[\\begin{align*} df(t,X_t,Y_t)&amp;=\\alpha\\frac{X_t}{Y_t}\\ dt + \\sigma \\frac{X_t}{Y_t}\\ dW_t-\\gamma \\frac{X_tY_t}{Y_t^2}\\ dt+\\delta \\frac{X_tY_t}{Y_t^2}\\ dV_t\\\\ &amp;+\\frac{1}{2}2\\frac{X_t}{Y_t^3}\\delta^2Y_t^2 dt\\\\ &amp;=\\left(\\alpha Z_t-\\gamma Z_t+Z_t\\delta^2\\right)\\ dt+ \\sigma Z_t\\ dW_t+\\delta Z_t\\ dV_t\\\\ &amp;=\\left(\\alpha -\\gamma +\\delta^2\\right)Z_t\\ dt+ \\sigma Z_t\\ dW_t+\\delta Z_t\\ dV_t. \\end{align*}\\] As desired the above is the SDE for the process \\(Z_t\\). \\(\\square\\) Exercise 4. (Bjork 5.9) Use a stochastic representation result in order to solve the following boundary value problem in the domain \\([0,T]\\times\\mathbb{R}\\). \\[ \\frac{\\partial F}{\\partial t}+\\mu x\\frac{\\partial F}{\\partial x}+\\frac{1}{2}\\sigma^2x^2\\frac{\\partial^2F}{\\partial x^2}=0, \\] with \\(F(T,x)=\\log(x^2)\\). Here \\(\\mu\\) and \\(\\sigma\\) are assumed to be know constants. Solution. We use proposition 5.5 Feymann-Kac with \\(\\mu(t,x)=\\mu x\\) and \\(\\sigma(t,x)=\\sigma x\\). We know that, given that the process \\[ \\sigma X_t \\frac{\\partial F}{\\partial x}(x,X_t)\\in \\mathcal{L}^2, \\] Then \\(F\\) has stochastic representation \\[ F(t,x)=E[\\log(X_T^2)\\ \\vert\\ X_t=x], \\] with stochastic process \\(X_t\\) satisfying the SDE \\[ dX_t=\\mu X_t\\ dt+\\sigma X_t\\ dW_t,\\hspace{20pt} X_t=x. \\] Now, since \\(X_t\\) satisfies the above SDE, we see that \\(X_t\\) is a GBM. Then by proposition 5.2 we have \\[ X_T=x\\exp\\left\\{\\left(\\mu - \\frac{1}{2}\\sigma^2\\right)(T-t)+\\sigma(W_T-W_t)\\right\\}. \\] Inserting this we find that \\[\\begin{align*} F(t,x)&amp;=E\\left.\\left[\\log(x^2\\exp\\left\\{\\left(2\\mu - \\sigma^2\\right)(T-t)+2\\sigma(W_T-W_t)\\right\\})\\ \\right\\vert\\ X_t=x\\right]\\\\ &amp;=E\\left.\\left[\\log(x^2)+\\left(2\\mu - \\sigma^2\\right)(T-t)+2\\sigma(W_T-W_t)\\ \\right\\vert\\ X_t=x\\right]\\\\ &amp;=2\\log(x)+\\left(2\\mu - \\sigma^2\\right)(T-t)+2\\sigma E\\left.\\left[W_T-W_t\\ \\right\\vert\\ X_t=x\\right]\\\\ &amp;=2\\log(x)+\\left(2\\mu - \\sigma^2\\right)(T-t). \\end{align*}\\] Using that the Brownian motion has increments with mean 0. \\(\\square\\) Exercise 5. (Bjork 5.13) Solve the boundary value problem \\[ \\frac{\\partial F}{\\partial t}(t,x,y)+\\frac{1}{2}\\sigma^2 \\frac{\\partial^2F}{\\partial x^2}(t,x,y)+\\frac{1}{2}\\delta^2\\frac{\\partial^2F}{\\partial y^2}(t,x,y)=0, \\] with \\(F(T,x,y)=xy\\). Solution. We see if this problem fit into the context of Feymann-Kac’s multi-dimensional proposition 5.8. Comparing the above PDE with the propositions we see that \\[ \\mu_i(t,X_t,Y_t)=0 \\] for \\(i=1,2\\) representing the assets \\(X_t\\) and \\(Y_t\\). We furthermore have the matrix \\(C\\) \\[\\begin{align*} C&amp;=\\sigma(t,X_t,Y_t)\\sigma(t,X_t,Y_t)^\\top\\\\ &amp;= \\begin{bmatrix} \\sigma_{1,1} &amp;\\sigma_{1,2}\\\\ \\sigma_{2,1}&amp; \\sigma_{2,2} \\end{bmatrix}\\begin{bmatrix} \\sigma_{1,1} &amp;\\sigma_{2,1}\\\\ \\sigma_{1,2}&amp; \\sigma_{2,2} \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} \\sigma_{1,1}^2+\\sigma_{1,2}^2 &amp; \\sigma_{1,1}\\sigma_{2,1}+\\sigma_{2,2}\\sigma_{1,2}\\\\ \\sigma_{1,1}\\sigma_{2,1}+\\sigma_{2,2}\\sigma_{1,2} &amp; \\sigma_{2,2}^2+\\sigma_{2,1}^2 \\end{bmatrix} \\end{align*}\\] Where we have that the only none-zero entrances is the diagonal with \\[\\begin{align*} C_{1,1}&amp;=\\sigma_{1,1}^2+\\sigma_{1,2}^2 = \\sigma^2,\\\\ C_{2,2}&amp;=\\sigma_{2,2}^2+\\sigma_{2,1}^2=\\delta^2. \\end{align*}\\] and obviously \\(r=0\\) and \\(\\Phi(x,y)=xy\\). From proposition 5.8 we then have that \\(F\\) has stochastic representation: \\[ F(t,x,y)=e^{-r(T-t)}E^Q[\\Phi(X_T,Y_T)\\ \\vert\\ X_t=x,Y_t=y]\\\\ =E^Q[X_TY_T\\ \\vert\\ X_t=x,Y_t=y] \\] Having given the \\(C\\) matrix we have the following \\[ C= \\begin{bmatrix} \\sigma^2 &amp; 0\\\\ 0 &amp; \\delta^2 \\end{bmatrix}=\\sigma\\sigma ^\\top\\iff\\sigma = \\begin{bmatrix} \\sigma &amp; 0\\\\ 0 &amp; \\delta \\end{bmatrix}. \\] Hence in the stochastic representation \\(X\\) and \\(Y\\) has dynamics \\[ \\left\\{ \\begin{matrix} dX_t=\\sigma\\ dW_t\\\\ dY_t=\\delta\\ dV_t. \\end{matrix} \\right. \\] where \\(W\\) and \\(B\\) are independent Brownian motions. In particular we have that \\(X\\) and \\(Y\\) are maringales and independent i.e. \\[ F(t,x,y)=E^Q_{X_t=x}[X_T]\\cdot E^Q_{Y_t=y}[Y_T]=xy. \\] And so we arrive at the desired result. \\(\\square\\) Exercise 6. (Exam 2017/18, problem 1, question (a)-(b)) Let \\(W_t\\) denote a Brownian motion and let \\[ \\mathcal{F}_t=\\mathcal{F}_t^W=\\sigma(\\{W_s\\ \\vert\\ 0\\le s\\le t\\}). \\] Let \\(T&gt;0\\) be a given and fixed time. Let \\(f(t)\\) be a bounded deterministic continuous function. Define the two processes \\[ \\begin{cases} X_t=\\int_0^tf(u)\\ dW_u,\\\\ M^{(\\lambda)}_t=\\exp\\left\\{\\lambda X_t-\\frac{\\lambda^2}{2}\\int_0^t f^2(u)\\ du\\right\\}, \\end{cases} \\] where \\(\\lambda\\in\\mathbb{R}\\) is a constant. Show that \\(M^{(\\lambda)}\\) is a martingale with \\(E[M_t^{(\\lambda)}]=1\\). Let \\(0&lt;s&lt;t\\) and \\(\\lambda_1,\\lambda_2\\in \\mathbb{R}\\) be given and fixed. Show that \\[\\begin{align*} M^{(\\lambda_1)}_s&amp;=E\\left[\\left.\\frac{M^{(\\lambda_1)}_sM^{(\\lambda_2)}_t}{M^{(\\lambda_2)}_s} \\ \\right\\vert\\ \\mathcal{F}_s\\right]\\\\ &amp;=E\\left[\\left.\\exp\\left\\{\\lambda_1X_s+\\lambda_2(X_t-X_s)-\\frac{1}{2}\\lambda_1^2\\int_0^sf^2(u)\\ du- \\frac{1}{2}\\lambda_2^2\\int_s^tf^2(u)\\ du\\right\\} \\ \\right\\vert\\ \\mathcal{F}_s\\right] \\end{align*}\\] Show that \\(X_s\\) and \\(X_t-X_s\\) are normally distributed and independent. Solution (a). First, we see that since \\(X_t\\) is on integral form we know that \\[ \\begin{cases} dX_t=f(t)\\ dW_t\\\\ X_0=0. \\end{cases} \\] Hence we may represent \\(M\\) as \\(M^{(\\lambda)}_t=g(t,X_t,Y_t)\\) given by \\[ g(t,x,y)=\\exp\\left\\{\\lambda x-\\frac{\\lambda^2}{2}y \\right\\}, \\] where \\(Y_t=\\int_0^t f^2(u)\\ du\\) with dynamics \\[ \\begin{cases} dY_t=f^2(t)\\ dt\\\\ Y_0=0. \\end{cases} \\] Hence by the multidimensional Ito’s formula we have the dynamics of \\(M\\) given by \\[\\begin{align*} dM^{(\\lambda)}_t&amp;=g_t\\ dt+g_x\\ dX_t+g_y\\ dY_t+\\frac{1}{2}g_{yy}\\ (dY_t)^2+\\frac{1}{2}g_{xx}\\ (dX_t)^2 +f_{xy}(dX_t)(dY_t)\\\\ &amp;=0+\\lambda g\\ dX_t-\\frac{\\lambda^2}{2}g\\ dY_t+0+\\frac{1}{2}\\lambda ^2g\\ (dX_t)^2+0\\\\ &amp;=\\lambda M_t^{(\\lambda)} f(t)\\ dW_t-\\frac{1}{2}\\lambda^2M_t^{(\\lambda)} f^2(t)\\ dt+\\frac{1}{2}\\lambda M_t^{(\\lambda)} f^2(t)\\ dt\\\\ &amp;=\\lambda f(t)M_t^{(\\lambda)}\\ dW_t, \\end{align*}\\] And so we see that \\(M\\) is a martingale as it only has dynamics wrt. the Brownian motion \\(W\\) (assuming \\(\\lambda f_tM_t^{(\\lambda)}\\in\\mathcal{L}^2\\)). Furthermore we have that \\[ M_0^{(\\lambda)}=g(0,X_0,Y_0)=\\exp\\left\\{\\lambda X_0-\\frac{1}{2}\\lambda ^2 Y_0\\right\\}=e^0=1 \\] and so we have \\(E[M_t^{(\\lambda)}]=M_0^{(\\lambda)}=1\\) as desired. \\(\\square\\) Solution (b). “(i)” We have from the previous exercise \\[\\begin{align*} &amp;\\frac{M^{(\\lambda_1)}_sM^{(\\lambda_2)}_t}{M^{(\\lambda_2)}_s}\\\\ &amp;=\\exp\\left\\{\\lambda_1 X_s-\\frac{1}{2}\\lambda_1^2\\int_0^s f^2(u)\\ du\\right\\}\\exp\\left\\{\\lambda_2 X_t-\\frac{1}{2}\\lambda_2^2\\int_0^t f^2(u)\\ du\\right\\}\\exp\\left\\{\\frac{1}{2}\\lambda_2^2\\int_0^s f^2(u)\\ du-\\lambda_2 X_s\\right\\}\\\\ &amp;=\\exp\\left\\{\\lambda_1 X_s-\\frac{1}{2}\\lambda_1^2\\int_0^s f^2(u)\\ du+\\lambda_2 X_t-\\frac{1}{2}\\lambda_2^2\\int_0^t f^2(u)\\ du+\\frac{1}{2}\\lambda_2^2\\int_0^s f^2(u)\\ du-\\lambda_2 X_s\\right\\}\\\\ &amp;=\\exp\\left\\{\\lambda_1 X_s+\\lambda_2 (X_t-X_s)-\\frac{1}{2}\\lambda_1^2\\int_0^s f^2(u)\\ du-\\frac{1}{2}\\lambda_2^2\\int_s^t f^2(u)\\ du\\right\\} \\end{align*}\\] and so the conclusion follows. \\(\\square\\) “(ii)” We have that from lemma 4.18 that \\[ X_s=\\int_0^sf(u)\\ dW_u\\sim \\mathcal{N}\\left(0,\\int_0^sf^2(u)\\ dW_u\\right) \\] furthermore we have that \\[ X_t-X_s=\\int_s^tf(u)\\ dW_u\\sim \\mathcal{N}\\left(0,\\int_s^tf^2(u)\\ dW_u\\right). \\] In regard to the independence claim we could check identity below \\[ E[e^{t_1X}e^{t_2 Y}]=E[e^{t_1X}]E[e^{t_2Y}] \\] where \\(X,Y\\) are independent random variables. The above identity holds if and only if \\(X\\) and \\(Y\\) are independent. From above we have that \\[ M_s^{(\\lambda_1)}=E[e^{\\lambda_1X_s}e^{\\lambda_2(X_t-X_s)}\\ \\vert\\ \\mathcal{F}_s]e^{-\\frac{1}{2}\\lambda_1^2\\int_0^s f^2(u)\\ du-\\frac{1}{2}\\lambda_2^2\\int_s^t f^2(u)\\ du} \\] and so taking expectation we have \\[ 1=E[e^{\\lambda_1X_s}e^{\\lambda_2(X_t-X_s)}]e^{-\\frac{1}{2}\\lambda_1^2\\int_0^s f^2(u)\\ du-\\frac{1}{2}\\lambda_2^2\\int_s^t f^2(u)\\ du} \\] Which the gives \\[ E[e^{\\lambda_1X_s}e^{\\lambda_2(X_t-X_s)}]=e^{\\frac{1}{2}\\lambda_1^2\\int_0^s f^2(u)\\ du+\\frac{1}{2}\\lambda_2^2\\int_s^t f^2(u)\\ du}=E[e^{\\lambda_1X_s}]E[e^{\\lambda_2(X_t-X_s)}] \\] and so the conclusion is that \\(X_s\\) and \\(X_t-X_s\\) are independent. \\(\\square\\) Exercise 7. (Exam 2019/20, problem 1, question (a)) Solution. Exercise 8. (Exam 2020/21, problem 1, question (a)-(b)) Solution. Extra-Exercise 1. (Bjork 5.7) Solution. Extra-Exercise 2. (Bjork 5.8) Solution. Extra-Exercise 3. (Bjork 5.10) Solution. Extra-Exercise 4. (Bjork 5.11) Solution. Extra-Exercise 5. (Bjork 5.12) Solution. "],["week-4.html", "2.4 Week 4", " 2.4 Week 4 Exercise 1. (Bjork 7.1) Consider the standard Black-Scholes model and a \\(T\\)-claim \\(\\mathcal{X}\\) of the form \\(\\mathcal{X}=\\Phi(S_t)\\). Denote the corresponding arbitrage free price processes by \\(\\Pi_t\\). Show that, under the martingale measure \\(Q\\), \\(\\Pi_t\\) has a local rate of return equal to the short rate \\(r\\). In other words shot that \\(\\Pi_t\\) has a differential of the form \\[ d\\Pi_t=r\\Pi_t\\ dt+g_t\\ dW_t^Q. \\] Hint: Use the \\(Q\\)-dynamics of \\(S\\) together with the fact that \\(F\\) satisfies the pricing PDE. Show that, under the martingale measure \\(Q\\), the process \\(Z_t=\\frac{\\Pi_t}{B_t}\\) is a martingale. More precisely show that the stochastic differential for \\(Z\\) has zero drift term, i.e. is of the form \\[ dZ_t=Z_t\\sigma_t^Z\\ dW_t^Q. \\] Determine also the diffusion process \\(\\Sigma_t^Z\\) (in terms of the pricing function \\(F\\) and its derivatives). Solution (a). First we have that the dynamics of \\(S\\) and \\(B\\) are given by \\[\\begin{align*} dS_t&amp;=\\mu S_t\\ dt + \\sigma S_t\\ dW_t,\\\\ dB_t&amp;=rB_t\\ dt. \\end{align*}\\] We know that \\(\\Pi_t[X]=F(t,S_t)\\) for some smooth function \\(F\\) hence by Ito’s formula we have \\[\\begin{align*} d\\Pi_t&amp;=\\frac{\\partial F}{\\partial t}(t,S_t)\\ dt +\\frac{\\partial F}{\\partial s}(t,S_t)\\ dS_t+\\frac{1}{2}\\frac{\\partial^2 F}{\\partial s^2}(t,S_t)\\ (dS_t)^2\\\\ &amp;=F_t\\ dt+F_s(r S_t\\ dt + \\sigma S_t\\ dW^Q_t)+\\frac{1}{2}F_{ss}(r S_t\\ dt + \\sigma S_t\\ dW^Q_t)^2\\\\ &amp;=(F_t+r S_t F_s)\\ dt+F_s\\sigma S_t\\ dW^Q_t+\\frac{1}{2}F_{ss}\\sigma ^2S_t^2\\ dt\\\\ &amp;=(F_t+r S_tF_s+\\frac{1}{2}F_{ss}\\sigma ^2S_t^2)\\ dt+F_s\\sigma S_t\\ dW^ Q_t. \\end{align*}\\] By setting \\(g_t=F_s\\sigma S_t\\) and restating the Black-Scholes equation we have \\[\\begin{align*} rF=F_t+rsF_s+\\frac{1}{2}F_{ss}\\sigma ^2 s^2 \\end{align*}\\] hence \\[ d\\Pi_t=r F\\ dt+g_t\\ dW^Q_t \\] as desired. \\(\\square\\) Solution (b). Consider the stochastic process \\(Z_t=F(t,\\Pi_t,B_t)\\) given by \\(F(t,\\pi,b)=\\frac{\\pi}{b}\\). Then by Ito’s formula we have the dynamics of \\(Z\\) as \\[\\begin{align*} dZ_t&amp;=F_t\\ dt+F_\\pi\\ d\\Pi_t+F_b\\ dB_t+\\frac{1}{2}F_{\\pi\\pi}\\ (d\\Pi_t)^2+\\frac{1}{2}F_{bb}\\ (dB_t)^2+F_{b\\pi}\\ (d\\Pi_t)(dB_t)\\\\ &amp;=0+\\frac{1}{B_t}\\ d\\Pi_t-\\frac{\\Pi_t}{B_t^2}\\ dB_t+\\frac{1}{2}0g_t\\ dt+\\frac{\\Pi_t}{B_t^3}0-\\frac{1}{B_t^2}0\\\\ &amp;=\\frac{1}{B_t}(r \\Pi_t\\ dt + g_t\\ dW^Q_t)-\\frac{\\Pi_t}{B_t^2}(rB_t\\ dt)\\\\ &amp;=\\frac{1}{B_t}(r\\Pi_t\\ dt-r\\Pi_t\\ dt+g_t\\ dW_t^Q)\\\\ &amp;=\\frac{1}{B_t}g_t\\ dW_t^Q. \\end{align*}\\] hence we have that \\(Z_t\\) is a \\(Q\\)-martingale since it only has dynamics in terms of the Brownian motion \\(W^Q\\). Additionally we may represent the process as \\[ Z_t=\\Pi_0+\\int_0^t\\frac{g_s}{B_s}\\ dW_s^Q. \\] From this it is clear that \\(Z_t\\) is a \\(Q\\)-martingale. \\(\\square\\) Exercise 2. (Bjork 7.2) Consider the standard Black-Scholes model. An innovative company, F&amp;H Inc., has produced the derivative “the Golden Logarith”, henceforth abbreviated as the GL. The holder of a GL with maturity time \\(T\\), denotet as \\(GL_t\\), will, at time \\(T\\), obtain the sum \\(\\Phi(S_T)=\\log\\ S_T\\). Note that if \\(S_T&lt;1\\) this means that the holder has to pay a positive amount to F&amp;H Inc. Determine the arbitrage free price process for the \\(GL_t\\). Solution. We know that in the BS model the simple derivative has to have the smooth pricing function \\(F(t,s)\\) that is the solution to the boundary value problem. \\[ \\left\\{ \\begin{matrix} F_t(t,s) + rsF_s(t,s)+\\frac{1}{2}\\sigma^2 s^2F_{ss}(t,s)-rF(t,s)=0\\\\ F(T,s)=\\Phi(s). \\end{matrix} \\right. \\] Which has the stochastic representation (proposition 7.11) given by the risk neutral valuation formula: \\[ F(t,s)=e^{-r(T-t)}E^Q_{t,s}[\\Phi(S_T)]=e^{-r(T-t)}E^Q_{t,s}[\\log(S_T)], \\] with \\(S_t\\) having dynamics \\[ dS_t=rS_t\\ dt+\\sigma S_t\\ dW_t^Q. \\] Then by Ito’s formula on the function \\(f(t,s)=\\log(s)\\) we have \\[ d(\\log(S_t))=d f(t,S_t)=f_t\\ dt+f_s\\ dS_t+\\frac{1}{2}f_{ss}(dS_t)^2. \\] Since \\(f_t=0\\) and \\(f_s=1/s\\) and \\(f_{ss}=-1/s^2\\) we have \\[\\begin{align*} d(\\log(S_t))&amp;=\\frac{1}{S_t}(rS_t\\ dt+\\sigma S_t\\ dW_t^Q)-\\frac{1}{2}\\sigma^2S_t^2\\frac{1}{S_t^2}\\ dt\\\\ &amp;=\\left(r-\\frac{1}{2}\\sigma^2\\right)\\ dt+\\sigma \\ dW_t^Q. \\end{align*}\\] Then given that \\(S_t=s\\) we have \\[\\begin{align*} \\log(S_u)&amp;=\\log s+\\int_t^ur-\\frac{1}{2}\\sigma^2\\ dv+\\sigma\\int_t^u dW_v^Q\\\\ &amp;=\\log s+\\left(r-\\frac{1}{2}\\sigma^2\\right)(u-t)+\\sigma(W_u^Q-W_t^Q) \\end{align*}\\] Taking expectation we then have \\[ E^Q[\\log(S_T)\\ \\vert\\ S_t=s]=\\log s+\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t), \\] given that \\[ \\Pi_t(\\Phi(S_t))=e^{-r(T-t)}\\log s+e^{-r(T-t)}\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t). \\] The arbitrage free price is the the above. \\(\\square\\) Exercise 3. (Bjork 7.4) Consider the standard Black-Scholes model. Derive the arbitrage free price process for the \\(T\\)-claim \\(X\\) where \\(X\\) is given by \\(X=S_T^\\beta\\). Here \\(\\beta\\) is a known constant. Solution. We may solve the pricing problem by evaluating the risk neautral valuation formula 7.11 given by \\[ F(t,s)=e^{-r(T-t)}E^Q_{t,s}[\\Phi(S_T)]=e^{-r(T-t)}E^Q_{t,s}[S_T^\\beta]. \\] We know that \\(S_t\\) is a Geometric Brownian motion with drift \\((r-\\sigma^2/2)\\) and diffusion \\(\\sigma\\) i.e. \\[ S_T=s\\cdot\\exp\\left\\{\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t)+\\sigma (W_T^Q-W_t^Q)\\right\\} \\] wrt. the martingale-measure \\(Q\\). Here we assume that \\(S_t=s\\). Then we know that \\(S_T^\\beta\\) is likewise a GBM given as \\[ S_T^\\beta=s^\\beta\\cdot\\exp\\left\\{\\beta\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t)+\\beta\\sigma (W_T^Q-W_t^Q)\\right\\}. \\] Evaluating the price process then becomes \\[\\begin{align*} \\Pi_t&amp;=e^{-r(T-t)}E^Q\\left[s^\\beta\\cdot\\exp\\left\\{\\beta\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t)+\\beta\\sigma (W_T^Q-W_t^Q)\\right\\}\\right]\\\\ &amp;=e^{-r(T-t)}s^\\beta e^{\\beta\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t)}E^Q\\left[e^{\\beta\\sigma(W_T^Q-W_t^Q)}\\right]. \\end{align*}\\] Evaluating the expectation may be done by calculating the MGF of a \\(\\mathcal{N}(0,T-t)\\) variable i.e. \\[ E^Q\\left[e^{\\beta\\sigma(W_T^Q-W_t^Q)}\\right]=e^{0\\cdot 1+(T-t)(\\beta\\sigma)^2/2}=e^{\\beta^2\\sigma^2\\frac{T-t}{2}}. \\] Combining these two equations yields \\[ \\log \\Pi_t=-r(T-t)+\\beta\\log s+\\beta\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t)+\\beta^2\\sigma^2\\frac{T-t}{2}\\\\ =-r(T-t)+\\beta\\log s+\\left(\\beta r+\\frac{1}{2}\\sigma^2\\beta(\\beta-1)\\right)(T-t). \\] We arrive at the arrived result. \\(\\square\\) Exercise 4. (Bjork 7.5) A so-called binary option is a claim which pays a certain amount if the stock prices at a certain date falls within some pre-specified interval. Otherwise nothing will be paid out. Consider a binary option which pays \\(K\\) dollars to the holder at date \\(T\\) if the stock price at time \\(T\\) is in the interval \\([\\mu,\\beta]\\). Determine the arbitrage free price. The pricing formula will involve the standard Gaussian cumulative distrbution function \\(N\\). Solution. First we see that the claim may be written on the form \\[ \\Phi(S_T)=1_{S_T\\in [a,b]}K, \\] using the values \\(a&lt;b\\) for the interval endpoints (instead of \\(\\mu,\\beta\\)). We then by the risk neutral valuation formula must have \\[ \\Pi_t=e^{-r(T-t)}E^Q_{t,s}[1_{S_T\\in [a,b]}K]. \\] Under closer inspection we see that we must evaluating the expectation of the indicator under the measure \\(Q\\). Then we have \\[ E^Q_{t,s}[1_{S_T\\in [a,b]}]=Q(S_T\\in[a,b]\\ \\vert\\ S_t=s)=(*). \\] When assuming \\(S_t=s\\) we must have that under \\(Q\\) that \\[ S_T=s\\cdot \\exp\\left\\{\\left(r-\\frac{1}{2}\\sigma ^2\\right)(T-t)+\\sigma\\left(W_T^Q-W_t^Q\\right)\\right\\}. \\] Then \\[\\begin{align*} (*)&amp;=Q(S_T\\le b\\ \\vert\\ S_t=s)-Q(S_T&lt;a\\ \\vert\\ S_t=s)\\\\ &amp;=Q(S_T\\le b\\ \\vert\\ S_t=s)-Q(S_T\\le a\\ \\vert\\ S_t=s)\\\\ &amp;=Q\\left(s\\cdot \\exp\\left\\{\\left(r-\\frac{1}{2}\\sigma ^2\\right)(T-t)+\\sigma\\left(W_T^Q-W_t^Q\\right)\\right\\}\\le b\\right)\\\\ &amp;-Q\\left(s\\cdot \\exp\\left\\{\\left(r-\\frac{1}{2}\\sigma ^2\\right)(T-t)+\\sigma\\left(W_T^Q-W_t^Q\\right)\\right\\}\\le a\\right)\\\\ &amp;=Q\\left(\\frac{1}{\\sqrt{T-t}}\\left(W_T^Q-W_t^Q\\right)\\le\\frac{1}{\\sigma\\sqrt{T-t}}\\left\\{\\log b-\\log s+\\left(\\frac{1}{2}\\sigma ^2-r\\right)(T-t)\\right\\}\\right)\\\\ &amp;-Q\\left(\\frac{1}{\\sqrt{T-t}}\\left(W_T^Q-W_t^Q\\right)\\le\\frac{1}{\\sigma\\sqrt{T-t}}\\left\\{\\log a-\\log s+\\left(\\frac{1}{2}\\sigma ^2-r\\right)(T-t)\\right\\}\\right)\\\\ &amp;=N\\left(\\frac{1}{\\sigma\\sqrt{T-t}}\\left\\{\\log b-\\log s+\\left(\\frac{1}{2}\\sigma ^2-r\\right)(T-t)\\right\\}\\right)\\\\ &amp;-N\\left(\\frac{1}{\\sigma\\sqrt{T-t}}\\left\\{\\log a-\\log s+\\left(\\frac{1}{2}\\sigma ^2-r\\right)(T-t)\\right\\}\\right)\\\\ &amp;=N(d_b)-N(d_a), \\end{align*}\\] as \\(W_T^Q-W_t^Q\\) is \\(\\mathcal{N}(0,T-t)\\) distributed under the \\(Q\\)-measure. The function \\(d_c\\) is defined as \\[ d_c=\\frac{1}{\\sigma\\sqrt{T-t}}\\left\\{\\log c-\\log s+\\left(\\frac{1}{2}\\sigma ^2-r\\right)(T-t)\\right\\}, \\] as expected. We then arrive at the price \\[ \\Pi_t=e^{-r(T-t)}K(N(d_b)-N(d_a)), \\] as desired. \\(\\square\\) Exercise 5. (Bjork 7.6) Consider the standard Black-Scholes model. Derive the arbitrage free price process for the \\(T\\)-claim \\(X\\) where \\(X\\) is given by \\(X=\\frac{S_{T_1}}{S_{T_0}}\\). The times \\(T_0\\) and \\(T_1\\) are given and the claim is paid out at time \\(T_1\\). Solution. Firstly, we may use that \\(S\\) is a GBM. Setting \\(S_t=s\\) we have that \\[\\begin{align*} X&amp;=\\frac{S_{T_1}}{S_{T_0}}=\\frac{s\\cdot \\exp\\left\\{\\left(r-\\frac{1}{2}\\sigma^2\\right)(T_1-t) +\\sigma\\left(W_{T_1}^Q-W_t^Q\\right)\\right\\}}{s\\cdot \\exp\\left\\{\\left(r-\\frac{1}{2}\\sigma^2\\right)(T_0-t) +\\sigma\\left(W_{T_0}^Q-W_t^Q\\right)\\right\\}}\\\\ &amp;=\\exp\\left\\{\\left(r-\\frac{1}{2}\\sigma^2\\right)(T_1-t-T_0+t) +\\sigma\\left(W_{T_1}^Q-W_t^Q-W_{T_0}^Q+W_t^Q\\right)\\right\\}\\\\ &amp;=\\exp\\left\\{\\left(r-\\frac{1}{2}\\sigma^2\\right)(T_1-T_0) +\\sigma\\left(W_{T_1}^Q-W_{T_0}^Q\\right)\\right\\}. \\end{align*}\\] Then it follows from proposition 7.11 that the price process is given by \\[\\begin{align*} \\Pi_t&amp;=e^{-r(T_1-t)}E^Q_{t,s}[X]\\\\ &amp;=e^{-r(T_1-t)}E^Q\\left[\\exp\\left\\{\\left(r-\\frac{1}{2}\\sigma^2\\right)(T_1-T_0) +\\sigma\\left(W_{T_1}^Q-W_{T_0}^Q\\right)\\right\\}\\right]\\\\ &amp;=\\exp\\left\\{-r(T_1-t)+\\left(r-\\frac{1}{2}\\sigma^2\\right)(T_1-T_0)\\right\\}E^Q\\left[\\exp\\left\\{\\sigma\\left(W_{T_1}^Q-W_{T_0}^Q\\right)\\right\\}\\right]\\\\ &amp;=\\exp\\left\\{-r(T_1-t)+\\left(r-\\frac{1}{2}\\sigma^2\\right)(T_1-T_0)+\\frac{1}{2}\\sigma^2(T_1-T_0)\\right\\}\\\\ &amp;=\\exp\\left\\{-r(T_1-t)+r(T_1-T_0)\\right\\}\\\\ &amp;=\\exp\\left\\{-r(T_1+T_0-T_1-t)\\right\\}=\\exp\\left\\{-r(T_0-t)\\right\\} \\end{align*}\\] as desired. \\(\\square\\) Exercise 6. (Exam 2017/18, problem 2, question (c)-(d)) Consider a standard Black-Scholes model, that is, a model consisting of a bank account \\(B_t\\) with \\(P\\)-dynamics given by \\[ dB_t=rB_t\\ dt,\\ B_0=1 \\] and a stock \\(S_t\\) with \\(P\\)-dynamics given by \\[ dS_t=\\alpha S_t\\ dt+\\sigma S_t\\ d\\overline{W}_t,\\ S_0=s&gt;0 \\] where \\(r,\\alpha\\in\\mathbb{R}\\) and \\(\\sigma &gt;0\\) are constants and \\(\\overline{W}_t\\) is a \\(P\\)-Brownian motion. Let \\(T&gt;0\\) be a given and fixed date. Consider the derivative that at time \\(T\\) pays \\[ X=\\max\\left\\{\\min\\left\\{S_T,K_2\\right\\},K_1\\right\\}, \\] where \\(0&lt;K_1&lt;K_2\\) are constants. Determine the arbitrage free price of derivative \\(X\\) at time \\(t&lt;T\\). Consider a new derivative that at time \\(T\\) pays \\[ Y=(S^2_T-K^2)^+-(K^2-S^2_T)^+. \\] Determine the arbitrage free price of derivative \\(Y\\) at time \\(t&lt;T\\). Find a hedging portfolio for derivative \\(Y\\). Let \\(h(t)=(h_0(t),h_1(t))\\) be a portfolio where \\[ h_0(t)=-e^{r(T-2t)+\\sigma^2(T-t)}S^2(t) \\] is the number of units in the bank account at time \\(t\\) and \\[ h_1(t)=2e^{(r+\\sigma^2)(T-t)}S(t) \\] is the number of shares in the stock at time \\(t\\). Let \\(V^h(t)\\) denote the associated value process. Determine whether the portfolio \\(h\\) is self-financing or not. Compute \\(V^h(T)\\). Solution (b). (i): We start by seeing that the derivative pays out \\[ Y= \\begin{cases} S_T^2-K^2 &amp; \\text{if }S_T^2\\ge K^2,\\\\ -(K^2-S_T^2) &amp;\\text{if }S_T^2&lt; K^2. \\end{cases} \\] hence the payout is \\(Y=S_T^2-K^2=\\Phi(S_T)\\) where \\(\\Phi(s)=s^2-K^2\\). That is \\(Y\\) is in fact a simple claim. We have from the risk neutral valueation formula 7.11 that \\[\\begin{align*} \\Pi_t[Y]&amp;=e^{-r(T-t)}E^Q_{t,s}[S_T^2-K^2]\\\\ &amp;=e^{-r(T-t)}E^Q_{t,s}[S_T^2]-e^{-r(T-t)}K^2. \\end{align*}\\] Recall that under the martingale measure \\(Q\\) we have that \\(S_t\\) is a GBM hence \\[ S_t=s\\cdot \\exp\\left\\{\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t)+\\sigma\\left(W_T^Q-W_t^Q\\right)\\right\\} \\] then \\[ S_T^2=s^2\\cdot \\exp\\left\\{2\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t)+2\\sigma\\left(W_T^Q-W_t^Q\\right)\\right\\}. \\] Inserting this into the risk neutral valuation formula we get \\[\\begin{align*} \\Pi_t[Y]&amp;=e^{-r(T-t)}E^Q_{t,s}[S_T^2]-e^{-r(T-t)}K^2\\\\ &amp;=e^{-r(T-t)}s^2e^{2\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t)} E^Q\\left[\\exp\\left\\{2\\sigma\\left(W_T^Q-W_t^Q\\right)\\right\\}\\right]-e^{-r(T-t)}K^2\\\\ &amp;=e^{-r(T-t)}s^2e^{2\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t)}e^{\\frac{1}{2}4\\sigma^2(T-t)}-e^{-r(T-t)}K^2\\\\ &amp;=e^{-r(T-t)}\\left(s^2e^{(2r-\\sigma^2)(T-t)+\\frac{1}{2}4\\sigma^2(T-t)}-K^2\\right)\\\\ &amp;=e^{-r(T-t)}\\left(s^2e^{(2r+\\sigma^2)(T-t)}-K^2\\right). \\end{align*}\\] The arbitrage free price of the derivative is then given above. \\(\\square\\) (ii): From theorem 8.5 we can determine a hedging portfolio with weightings \\[\\begin{align*} w_t^B&amp;=\\frac{\\Pi_t-S_t\\frac{\\partial\\Pi}{\\partial s}}{\\Pi_t}\\\\ &amp;=1-\\frac{S_t2S_te^{-r(T-t)}e^{(2r+\\sigma^2)(T-t)}}{e^{-r(T-t)}\\left(S_t^2e^{(2r+\\sigma^2)(T-t)}-K^2\\right)}\\\\ &amp;=1-\\frac{2S_t^2e^{(2r+\\sigma^2)(T-t)}}{S_t^2e^{(2r+\\sigma^2)(T-t)}-K^2}\\\\ &amp;=1-\\frac{2}{1-K^2S_t^{-2}e^{(2r+\\sigma^2)(t-T)}}\\\\ w_t^S&amp;=\\frac{2}{1-K^2S_t^{-2}e^{(2r+\\sigma^2)(t-T)}}. \\end{align*}\\] In absolute terms we will hold the portfolio \\[\\begin{align*} h_t^S&amp;=2S_te^{-r(T-t)}e^{(2r+\\sigma^2)(T-t)}\\\\ h_t^B&amp;=\\frac{e^{-r(T-t)}\\left(s^2e^{(2r+\\sigma^2)(T-t)}-K^2\\right)-S_th_t^S}{B_t}\\\\ &amp;=\\frac{e^{-r(T-t)}\\left(s^2e^{(2r+\\sigma^2)(T-t)}-K^2\\right)-S_th_t^S}{e^{rt}}\\\\ &amp;=e^{-rT}s^2e^{(2r+\\sigma^2)(T-t)}-e^{-rT}K^2-e^{-rt}S_th_t^S. \\end{align*}\\] The portfolio above will hedge \\(Y\\) with probability one. \\(\\square\\) Solution (c). We assume no dividends and no consumption that is \\(c_t=0\\) and \\(dD_t^i=0\\) for \\(i=0,1\\). Then the portfolio is self-financing if and only if the value process has dynamics. \\[ h_0(t)\\ dB_t+h_1(t)\\ dS_t=0 \\] This is given in lemma 6.12. THE BELOW IS IN WORKS AND NOT CORRECT! Now we have that the value process is given by \\[ V_t^h=h_0(t)B_t+h_1(t)S_t. \\] Using the representation \\(V_t^h=f(h_0(t),B_t)+f(h_1(t),S_t)\\) given by \\(f(x,y)=xy\\) we have \\[ dV_t^h=df(h_0(t),B_t)+df(h_1(t),S_t). \\] Using Ito’s formula on each term we have \\[\\begin{align*} df(h_0(t),B_t)&amp;=B_t\\ dh_0(t)+h_0(t)\\ dB_t+(dB_t)(dh_0(t)),\\\\ df(h_1(t),S_t)&amp;=S_t\\ dh_1(t)+h_1(t)\\ dS_t+(dS_t)(dh_1(t)),\\\\ \\end{align*}\\] since of cause \\(f_{xx}=f_{yy}=0\\). We can the determine the dynamics of the portfolio by \\[\\begin{align*} dh_0(t)&amp;=-(-2t-\\sigma^2)S_t^2e^{r(T-2t)+\\sigma^2(T-t)}\\ dt\\\\ &amp;-2S_te^{r(T-2t)+\\sigma^2(T-t)}\\ dS_t\\\\ &amp;-\\frac{1}{2}2e^{r(T-2t)+\\sigma^2(T-t)}\\ (dS_t)^2\\\\ &amp;=(-2t-\\sigma^2)h_0(t)\\ dt+\\frac{2}{S_t}h_0(t)\\ (\\mu S_t\\ dt+\\sigma S_t\\ dW_t)+\\frac{1}{S_t^2}h_0(t) \\sigma^2S_t^2\\ dt\\\\ &amp;=(\\mu-1)2h_0(t)\\ dt+2\\sigma h_0(t)\\ dW_t \\end{align*}\\] and \\[\\begin{align*} dh_1(t)&amp;=(-r-\\sigma^2)2e^{(r+\\sigma^2)(T-t)}S_t\\ dt\\\\ &amp;+2e^{(r+\\sigma^2)(T-t)}\\ dS_t+0\\\\ &amp;=(-r-\\sigma^2)h_1(t)\\ dt+\\frac{1}{S_t}h_1(t)(\\mu S_t\\ dt+\\sigma S_t\\ dW_t)\\\\ &amp;=(-r-\\sigma^2+\\mu)h_1(t)\\ dt+h_1(t)\\sigma \\ dW_t\\\\ \\end{align*}\\] And so in total \\[\\begin{align*} dV_t^h(t)&amp;=df(h_0(t),B_t)+df(h_1(t),S_t)\\\\ &amp;=B_t\\ dh_0(t)+h_0(t)\\ dB_t+(dB_t)(dh_0(t))\\\\ &amp;+S_t\\ dh_1(t)+h_1(t)\\ dS_t+(dS_t)(dh_1(t))\\\\ &amp;=B_t\\ ((\\mu-1)2h_0(t)\\ dt+2\\sigma h_0(t)\\ dW_t)+h_0(t)\\ rB_t\\ dt+0\\\\ &amp;+S_t\\ ((-r-\\sigma^2+\\mu)h_1(t)\\ dt+h_1(t)\\sigma \\ dW_t)+h_1(t)\\ (\\mu S_t\\ dt+\\sigma S_t\\ dW_t)+\\sigma^2S_th_1(t)\\ dt\\\\ &amp;=\\left[B_t(\\mu-1)2h_0(t)+h_0(t)rB_t+S_t(-r-\\sigma^2+\\mu)h_1(t)+h_1\\mu S_t+\\sigma^2S_th_1(t)\\right]\\ dt\\\\ &amp;+\\left[B_t2\\sigma h_0(t)+S_th_1\\sigma+h_1\\sigma S_t\\right]\\ dW_t\\\\ &amp;=\\left[(2\\mu-2+r)B_th_0(t)+(-r+2\\mu)S_th_1(t)\\right]\\ dt\\\\ &amp;+\\left[B_t h_0(t)+h_1 S_t\\right]2\\sigma\\ dW_t\\\\ &amp;=V_t^h2\\mu\\ dt+V_t^h\\ dW_t \\end{align*}\\] Solution (d). We compute \\(V_T^h\\) easily by inserting \\(h_0\\) and \\(h_1\\) below \\[\\begin{align*} V_T^h&amp;=B_Th_0(T)+S_Th_1(T)\\\\ &amp;=B_T\\left(-e^{r(T-2T)+\\sigma^2(T-T)}S_T^2\\right)+S_T\\left(2e^{(r+\\sigma^2)(T-T)}S_T\\right)\\\\ &amp;=-S_T^2+2S_T^2=S_T^2. \\end{align*}\\] and so \\(h\\) hedge the payout \\(\\Phi(S_T)=S_T^2\\). \\(\\square\\) Solution. Exercise 7. (Exam 2018/19, problem 1) Solution. Exercise 8. (Exam 2018/19, problem 2, question (a)) Solution. Extra-Exercise 1. (Bjork 7.7) Solution. "],["week-5.html", "2.5 Week 5", " 2.5 Week 5 Exercise 1. (Bjork 10.1) Consider the standard Black-Scholes model. Fix the time of maturity \\(T\\) and consider the following \\(T\\)-claim \\(X\\): \\[ X= \\begin{cases} K &amp; \\text{if }S_T\\le A,\\\\ K+A-S_T &amp; \\text{if }A&lt;S_T&lt;K+ A,\\\\ 0 &amp; \\text{if }S_T &gt; K+ A. \\end{cases} \\] This contract can be replicated using a portfoliom consisting solely of bonds, stock, and European call options, which is constant over time. Determine this portfolio as well as the arbitrage free price of the contract. Solution. We see that the put option with strike \\(K+A\\) gives the payout \\[ P_{K+A}(S_T)= \\begin{cases} K+A-S_T &amp; \\text{if }S_T\\le A,\\\\ K+A-S_T &amp; \\text{if }A&lt;S_T&lt;K+ A,\\\\ 0 &amp; \\text{if }S_T &gt; K+ A. \\end{cases} \\] Hence this asset will “almost” gives the wanted payout except for the event \\((S_T\\le A)\\). If we find an asset giving the payout \\(A-S_T\\) if and only if the event \\((S_T\\le A)\\) occurs, we may short this asset. It happens that the put with strike \\(A\\) has the wanted payout that is \\[ P_{A}(S_T)= \\begin{cases} A-S_T &amp; \\text{if }S_T\\le A,\\\\ 0 &amp; \\text{if }A&lt;S_T&lt;K+ A,\\\\ 0 &amp; \\text{if }S_T &gt; K+ A. \\end{cases} \\] Then making the portfolio of one long position in the put \\(P_{K+A}\\) and a short position in the put \\(P_A\\) will replicate \\(X\\). We know from the put-call parity that this exact portfolio may be replicated by \\(K+A-A=K\\) long zero-coupon bonds, long call with strike \\(K+A\\) and short call with strike \\(A\\). Notice no position is taking on the underlying stock since we both go long and short on a put. Let us check if this portfolio give the wantet payout: \\[ V^h_T= \\begin{cases} K +0-0 = K &amp; \\text{if }S_T\\le A,\\\\ K + 0 - (S_T-A)=K+A-S_T &amp; \\text{if }A&lt;S_T&lt;K+ A,\\\\ K + (S_T-K-A)-(S_T-A)=0 &amp; \\text{if }S_T &gt; K+ A. \\end{cases} \\] Then the portfolio give the desired payout. Given the price process for the call option and the zero-coupon bond we have that the value process is given by \\[\\begin{align*} V_t^h&amp;=Ke^{-r(T-t)}+c(K+A;t,T)-c(A;t,T)\\\\ &amp;=e^{-r(T-t)}\\left\\{K+(K+A)N(d_2(K+A;t,S_t)-AN(d_2(A;t,S_t))\\right\\}\\\\ &amp;+S_t\\left\\{N(d_1(K+A;t,S_t))-N(d_1(A;t,S_t))\\right\\} \\end{align*}\\] with \\(d_1\\) and \\(d_2\\) as given in the Black-Scholes formula. The price of the portfolio is given by the above value process. \\(\\square\\) Exercise 2. (Bjork 10.2) The setup is the same as the previous exercise. Here the contract is a so-called straddle, defined by \\[ X= \\begin{cases} K-S_T &amp; \\text{if }0&lt;S_T\\le K,\\\\ S_T-K &amp; \\text{if }S_T&gt;K. \\end{cases} \\] Determine the constant replicating portfolio as well as the arbitrage free price of the contract. Solution. We search for portfolio paying the payout above. Recall that a call option with strike \\(K\\) has payout \\(S_T-K\\) if \\(S_T\\ge K\\) and a put option with strike \\(K\\) has payout \\(K-S_T\\) if \\(S_T\\le K\\). Hence by longing one call with strike \\(K\\) and long one put with strike \\(K\\) will give the desired payout. We know that we kan replicate this portfolio by buying \\(K\\) zero-coupon bonds, long two call options with strike \\(K\\) and shorting the underlying stock. Lets see what this yields \\[ V_T^h = \\begin{cases} K+2\\cdot 0-S_T=K-S_T &amp; \\text{if }0&lt;S_T\\le K,\\\\ K+2\\cdot(S_T-K)-S_T=S_T-K &amp; \\text{if }S_T&gt;K. \\end{cases} \\] As desired. The price is the then value process with \\[ \\Pi_t=V_t^h=Ke^{-r(T-t)}+2c(t,T)-S_t. \\] Giving the desired result. \\(\\square\\) Exercise 3. (Bjork 10.3) The setup is the same as the previous exercise. We will now study a so-called bull spread (see Fig. 10.7). With this contract we can, to a limited extent, take advantage of an increase in the market price while being protected from a decrease. The contract is defined by \\[ X= \\begin{cases} B &amp; \\text{if }S_T&gt;B,\\\\ S_T &amp; \\text{if }A\\le S_T\\le B,\\\\ A &amp;\\text{if }S_T&lt; A. \\end{cases} \\] We have of course the relation \\(A&lt;B\\). Determine the constant replicating portfolio as well as the arbitrage free price of the contract. Solution. Again we search for a replicating portfolio. If we long one stock we get the payout \\(S_T\\) only and so we want to recieve an additional payout on the events \\(S_T\\) falls outside the interval \\([A,B]\\). We want to have an asset paying \\(-(S_T-A)\\) on the event \\(S_T&lt;A\\) and an asset paying \\(-(S_T-B)\\) on the event \\(S_T&gt;B\\). The two assets are: one put with strike \\(A\\) giving the payout \\(A-S_T\\) and a short position call option with strke \\(B\\). Hence we may replicate the put with \\(A\\) bonds, one call and a short position. In total we hold \\(A\\) bonds, one call option with strike \\(A\\) and a short on a call with strike \\(B\\). We will then recieve the payout: \\[ V_T^h= \\begin{cases} A+S_T-A-(S_T-B)=B &amp; \\text{if }S_T&gt;B,\\\\ A + S_T - A-0=S_T&amp; \\text{if }A\\le S_T\\le B,\\\\ A+0+0=A &amp;\\text{if }S_T&lt; A. \\end{cases} \\] As desired. The value process then give the portfolio price: \\[ \\Pi_t=V_t^h=Ae^{-r(T-t)}+c(A;t,T)-c(B;t,T) \\] as desired. \\(\\square\\) Exercise 4. (Exam 2017/18, problem 2, question (a)-(b)) Consider a standard Black-Scholes model, that is, a model consisting of a bank account \\(B_t\\) with \\(P\\)-dynamics given by \\[ dB_t=rB_t\\ dt,\\ B_0=1 \\] and a stock \\(S_t\\) with \\(P\\)-dynamics given by \\[ dS_t=\\alpha S_t\\ dt+\\sigma S_t\\ d\\overline{W}_t,\\ S_0=s&gt;0 \\] where \\(r,\\alpha\\in\\mathbb{R}\\) and \\(\\sigma &gt;0\\) are constants and \\(\\overline{W}_t\\) is a \\(P\\)-Brownian motion. Let \\(T&gt;0\\) be a given and fixed date. Consider the derivative that at time \\(T\\) pays \\[ X=\\max\\left\\{\\min\\left\\{S_T,K_2\\right\\},K_1\\right\\}, \\] where \\(0&lt;K_1&lt;K_2\\) are constants. Determine the arbitrage free price of derivative \\(X\\) at time \\(t&lt;T\\). Consider a new derivative that at time \\(T\\) pays \\[ Y=(S^2_T-K^2)^+-(K^2-S^2_T)^+. \\] Determine the arbitrage free price of derivative \\(Y\\) at time \\(t&lt;T\\). Find a hedging portfolio for derivative \\(Y\\). Let \\(h(t)=(h_0(t),h_1(t))\\) be a portfolio where \\[ h_0(t)=-e^{r(T-2t)+\\sigma^2(T-t)}S^2(t) \\] is the number of units in the bank account at time \\(t\\) and \\[ h_1(t)=2e^{(r+\\sigma^2)(T-t)}S(t) \\] is the number of shares in the stock at time \\(t\\). Let \\(V^h(t)\\) denote the associated value process. Determine whether the portfolio \\(h\\) is self-financing or not. Compute \\(V^h(T)\\). Solution (a). We see that the derivative is the bull spread given by the payout function \\[ X= \\begin{cases} K_2 &amp; \\text{if }S_T&gt;K_2,\\\\ S_T &amp; \\text{if }K_1\\le S_T\\le K_2,\\\\ K_1 &amp;\\text{if }S_T&lt; K_1. \\end{cases} \\] We know from exercise 10.3 that this can be replicated by holding \\(K_1\\) bonds, one call option with strike \\(K_1\\) and a short on a call with strike \\(K_2\\). The arbitrage free price of the derivative is then the value process of the mentioned portfolio i.e. \\[ \\Pi_t[X]=K_1 e^{-r(T-t)}+c(K_1;t,T)-c(K_2;t,T), \\] where \\(c\\) denotes the pricing function for a European call option (non-instructive parameters supressed). \\(\\square\\) Solution (b). (i): We start by seeing that the derivative pays out \\[ Y= \\begin{cases} S_T^2-K^2 &amp; \\text{if }S_T^2\\ge K^2,\\\\ -(K^2-S_T^2) &amp;\\text{if }S_T^2&lt; K^2. \\end{cases} \\] hence the payout is \\(Y=S_T^2-K^2=\\Phi(S_T)\\) where \\(\\Phi(s)=s^2-K^2\\). That is \\(Y\\) is in fact a simple claim. We have from the risk neutral valueation formula 7.11 that \\[\\begin{align*} \\Pi_t[Y]&amp;=e^{-r(T-t)}E^Q_{t,s}[S_T^2-K^2]\\\\ &amp;=e^{-r(T-t)}E^Q_{t,s}[S_T^2]-e^{-r(T-t)}K^2. \\end{align*}\\] Recall that under the martingale measure \\(Q\\) we have that \\(S_t\\) is a GBM hence \\[ S_t=s\\cdot \\exp\\left\\{\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t)+\\sigma\\left(W_T^Q-W_t^Q\\right)\\right\\} \\] then \\[ S_T^2=s^2\\cdot \\exp\\left\\{2\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t)+2\\sigma\\left(W_T^Q-W_t^Q\\right)\\right\\}. \\] Inserting this into the risk neutral valuation formula we get \\[\\begin{align*} \\Pi_t[Y]&amp;=e^{-r(T-t)}E^Q_{t,s}[S_T^2]-e^{-r(T-t)}K^2\\\\ &amp;=e^{-r(T-t)}s^2e^{2\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t)} E^Q\\left[\\exp\\left\\{2\\sigma\\left(W_T^Q-W_t^Q\\right)\\right\\}\\right]-e^{-r(T-t)}K^2\\\\ &amp;=e^{-r(T-t)}s^2e^{2\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t)}e^{\\frac{1}{2}4\\sigma^2(T-t)}-e^{-r(T-t)}K^2\\\\ &amp;=e^{-r(T-t)}\\left(s^2e^{(2r-\\sigma^2)(T-t)+\\frac{1}{2}4\\sigma^2(T-t)}-K^2\\right)\\\\ &amp;=e^{-r(T-t)}\\left(s^2e^{(2r+\\sigma^2)(T-t)}-K^2\\right). \\end{align*}\\] The arbitrage free price of the derivative is then given above. \\(\\square\\) (ii): From theorem 8.5 we can determine a hedging portfolio with weightings \\[\\begin{align*} w_t^B&amp;=\\frac{\\Pi_t-S_t\\frac{\\partial\\Pi}{\\partial s}}{\\Pi_t}\\\\ &amp;=1-\\frac{S_t2S_te^{-r(T-t)}e^{(2r+\\sigma^2)(T-t)}}{e^{-r(T-t)}\\left(S_t^2e^{(2r+\\sigma^2)(T-t)}-K^2\\right)}\\\\ &amp;=1-\\frac{2S_t^2e^{(2r+\\sigma^2)(T-t)}}{S_t^2e^{(2r+\\sigma^2)(T-t)}-K^2}\\\\ &amp;=1-\\frac{2}{1-K^2S_t^{-2}e^{(2r+\\sigma^2)(t-T)}}\\\\ w_t^S&amp;=\\frac{2}{1-K^2S_t^{-2}e^{(2r+\\sigma^2)(t-T)}}. \\end{align*}\\] In absolute terms we will hold the portfolio \\[\\begin{align*} h_t^S&amp;=2S_te^{-r(T-t)}e^{(2r+\\sigma^2)(T-t)}\\\\ h_t^B&amp;=\\frac{e^{-r(T-t)}\\left(s^2e^{(2r+\\sigma^2)(T-t)}-K^2\\right)-S_th_t^S}{B_t}\\\\ &amp;=\\frac{e^{-r(T-t)}\\left(s^2e^{(2r+\\sigma^2)(T-t)}-K^2\\right)-S_th_t^S}{e^{rt}}\\\\ &amp;=e^{-rT}s^2e^{(2r+\\sigma^2)(T-t)}-e^{-rT}K^2-e^{-rt}S_th_t^S. \\end{align*}\\] The portfolio above will hedge \\(Y\\) with probability one. \\(\\square\\) Exercise 5. (Exam 2019/20, problem 2) Solution. Exercise 6. (Exam 2020/21, problem 2, question (a)-(b)) Solution. Exercise 7. (Exam 2020/21, problem 3, question (b)) Solution. Extra-Exercise 1. (Bjork 10.4) Solution. "],["week-6.html", "2.6 Week 6", " 2.6 Week 6 Exercise 1. (Exam 2017/18, problem 1, question (c)) Let \\(W_t\\) denote a Brownian motion and let \\[ \\mathcal{F}_t=\\mathcal{F}_t^W=\\sigma(\\{W_s\\ \\vert\\ 0\\le s\\le t\\}). \\] Let \\(T&gt;0\\) be a given and fixed time. Let \\(f(t)\\) be a bounded deterministic continuous function. Define the two processes \\[ \\begin{cases} X_t=\\int_0^tf(u)\\ dW_u,\\\\ M^{(\\lambda)}_t=\\exp\\left\\{\\lambda X_t-\\frac{\\lambda^2}{2}\\int_0^t f^2(u)\\ du\\right\\}, \\end{cases} \\] where \\(\\lambda\\in\\mathbb{R}\\) is a constant. Compute the mean value of \\(M^{(\\lambda)}_T\\log(M^{(\\lambda)}_T)\\). Solution (c). We recall the definition of \\(M_t^{(\\lambda)}\\) and observe that \\[ \\log M_t^{(\\lambda)}=\\lambda X_t-\\frac{1}{2}\\lambda ^2\\int_0^t f^2(u)\\ du. \\] Furthermore we have the dynamics of \\(M^{(\\lambda)}\\) given by the differential form \\[ dM_t^{(\\lambda)}=\\lambda f(t)M_t^{(\\lambda)}\\ dW_t. \\] with \\(M_0^{(\\lambda)}=1\\). Since we know that \\(M_t^{(\\lambda)}\\) is a martingale we have \\[ E^P[M_T^{(\\lambda)}]=E^P[M_0^{(\\lambda)}]=1, \\] and so we may define a new probability measure as \\[ d\\tilde{P}=M_T^{(\\lambda)}\\ dP \\] on \\(\\mathcal{F}_T\\). We then have a new Brownian motion \\(\\tilde{W}\\) such that \\[ dW_t=\\lambda f(t)\\ dt + d\\tilde{W}_t. \\] We can then see \\[\\begin{align*} E^P[M_T^{(\\lambda)}\\log M_T^{(\\lambda)}]&amp;=\\int M_T^{(\\lambda)}\\log M_T^{(\\lambda)}\\ dP=\\int M_T^{(\\lambda)}\\log M_T^{(\\lambda)} \\frac{1}{M_T^{(\\lambda)}}\\ d\\tilde{P}\\\\ &amp;=\\int \\log M_T^{(\\lambda)}\\ d\\tilde{P}=E^{\\tilde{P}}[\\log M_T^{(\\lambda)}]. \\end{align*}\\] Then we can evaluate the mean value by seeing the \\(X\\) has representation wrt. \\(\\tilde{P}\\) by \\[ X_t=\\int_0^tf(u)\\ (\\lambda f(u)\\ du + d\\tilde{W}_u)=\\lambda\\int_0^tf^2(u)\\ du+\\int_0^tf(u)\\ d\\tilde{W}_u. \\] Giving that \\[\\begin{align*} E^P[M_T^{(\\lambda)}\\log M_T^{(\\lambda)}]&amp;=E^{\\tilde{P}}[\\log M_T^{(\\lambda)}]\\\\ &amp;=E^{\\tilde{P}}\\left[ \\lambda X_T-\\frac{1}{2}\\lambda ^2\\int_0^T f^2(u)\\ du \\right]\\\\ &amp;=E^{\\tilde{P}}\\left[ \\lambda^2\\int_0^Tf^2(u)\\ du+\\lambda\\int_0^Tf(u)\\ d\\tilde{W}_u-\\frac{1}{2}\\lambda ^2\\int_0^T f^2(u)\\ du \\right]\\\\ &amp;=\\lambda E^{\\tilde{P}}\\left[\\frac{1}{2} \\lambda\\int_0^Tf^2(u)\\ du+\\int_0^Tf(u)\\ d\\tilde{W}_u \\right]\\\\ &amp;=\\frac{1}{2} \\lambda^2\\int_0^Tf^2(u)\\ du+\\lambda E^{\\tilde{P}}\\left[\\int_0^Tf(u)\\ d\\tilde{W}_u \\right]\\\\ &amp;=\\frac{1}{2} \\lambda^2\\int_0^Tf^2(u)\\ du \\end{align*}\\] Since \\[ \\tilde{X}_T=\\int_0^Tf(u)\\ d\\tilde{W}_u, \\] is a \\(\\tilde{P}\\)-martingale. \\(\\square\\) Exercise 2. (Exam 2018/19, problem 2, question (b)i and (c)-(d)) Solution. Exercise 3. (Exam 2019/20, problem 1, question (b)-(c)) Solution. Exercise 4. (Exam 2019/20, problem 3, question (b)) Solution. Exercise 5. (Exam 2020/21, problem 1, question (c)) Solution. Exercise 6. (Exam 2020/21, problem 2, question (c)-(d)) Solution. "],["week-7.html", "2.7 Week 7", " 2.7 Week 7 Exercise 1. (Exam 2018/19, problem 2, question (b).ii) Solution. Exercise 2. (Exam 2017/18, problem 3) Solution. Exercise 3. (Exam 2018/19, problem 3) Solution. Exercise 4. (Exam 2019/20, problem 3, question (a) and (c)-(e)) Solution. Exercise 5. (Exam 2020/21, problem 3, question (a) and (c)) Solution. "],["probabilistic-machine-learning.html", "Chapter 3 Probabilistic Machine Learning ", " Chapter 3 Probabilistic Machine Learning "],["week-1-1.html", "3.1 Week 1", " 3.1 Week 1 Exercise 1. Let \\(\\tilde m = \\arg\\min_{m\\in\\mathcal G} r(m)\\). Proof that \\[ r(\\hat m_n)-r(\\tilde m)\\le 2\\sup_{m\\in\\mathcal G}\\left\\vert\\hat R_n(m)-r(m) \\right\\vert. \\] Solution. We recall that per definition \\[ R(\\hat m_n)=\\mathbb E[L(Y,\\hat m_n(X))\\ \\vert\\ \\mathcal D_n]\\quad \\text{and}\\quad r(\\hat m_n)=\\mathbb E[R(\\hat m_n)] \\] for some estimator \\(\\hat m_n\\). Assume now that \\(m(\\mathcal D_n)=\\hat m_n\\in \\mathcal G\\) is the estimator given the data \\(\\mathcal D_n\\) from the precedure \\(m\\). We led \\(\\tilde m\\) be the Bayes estimator on \\(\\mathcal G\\). We therefore have \\[\\begin{align*} r(\\hat m_n)-r(\\tilde m)&amp;=r(\\hat m_n)-r(\\tilde m)+\\hat R_n(\\hat m_n)-\\hat R_n(\\hat m_n)+\\hat R_n(\\tilde m)-\\hat R_n(\\tilde m)\\\\ &amp;=r(\\hat m_n)-\\hat R_n(\\hat m_n)+\\hat R_n(\\tilde m)-r(\\tilde m)+\\underbrace{\\hat R_n(\\hat m_n)-\\hat R_n(\\tilde m)}_{\\le 0}\\\\ &amp;\\le \\Big(r(\\hat m_n)-\\hat R_n(\\hat m_n)\\Big)+\\Big(\\hat R_n(\\tilde m)-r(\\tilde m)\\Big)\\\\ &amp;\\le \\Big\\vert \\hat R_n(\\hat m_n)-r(\\hat m_n)\\Big\\vert+\\Big\\vert \\hat R_n(\\tilde m)-r(\\tilde m)\\Big\\vert\\\\ &amp;\\le2\\sup_{m\\in \\mathcal G} \\Big\\vert \\hat R_n(m)-r(m)\\Big\\vert \\end{align*}\\] as desired. \\(\\square\\) We consider the Poisson deviance loss \\[ L_{\\text{pois:dev}}(y_1,y_2)=2\\left(y_1\\log \\frac{y_1}{y_2}-y_1+y_2\\right). \\] Show that \\(\\arg\\min r(m)=m^*(x)=E[Y\\ \\vert\\ X=x]\\). In the squared loss case \\(L_2(y_1,y_2)=(y_1-y_2)^2\\), we used its Hilbert space property to derive that for any estimator \\(\\hat m_n(x)\\) \\[ r_2(\\hat m_n(x))-r_2(m^*(x))=\\mathbb E[L_2(\\hat m_n(x),m^*(x))] \\] Show that an anaolgue result for the he Poisson deviance loss is not true, i.e., \\[\\begin{align*} r_{\\text{pois:dev}}(\\hat m_n(x))-r_{\\text{pois:dev}}(m^*(x))&amp;=\\mathbb E\\left[2\\left(Y\\log \\frac{m^*(x)}{\\hat m_n(x)}-m^*(x)+\\hat m_n(x)\\right)\\right]\\\\ &amp;\\ne \\mathbb E[L_{\\text{pois:dev}}(\\hat m_n(x),m^*(x))] \\end{align*}\\] Solution (1). We have that \\[\\begin{align*} r(m)&amp;=\\mathbb E\\left[2\\left(Y\\log \\frac{Y}{m(X)}-Y+m(X)\\right)\\right]\\\\ &amp;=\\mathbb E\\left[ \\mathbb E\\left.\\left[2\\left(Y\\log \\frac{Y}{m(x)}-Y+m(x)\\right)\\ \\right\\vert\\ X=x\\right]\\right]\\\\ \\end{align*}\\] We see that the integrand is differentiable in \\(m=m(x)\\) and so we have \\[\\begin{align*} \\frac{\\partial}{\\partial m}r(m)&amp;=\\frac{\\partial}{\\partial m}\\mathbb E\\left[ \\mathbb E\\left.\\left[2\\left(Y\\log \\frac{Y}{m}-Y+m\\right)\\ \\right\\vert\\ X=x\\right]\\right]\\\\ &amp;=\\mathbb E\\left[ \\mathbb E\\left.\\left[\\frac{\\partial}{\\partial m}2\\left(Y\\log \\frac{Y}{m}-Y+m\\right)\\ \\right\\vert\\ X=x\\right]\\right]\\\\ &amp;=\\mathbb E\\left[ \\mathbb E\\left.\\left[2\\left(-Y \\frac{m}{Y}\\frac{Y}{m^2}+1\\right)\\ \\right\\vert\\ X=x\\right]\\right]\\\\ &amp;=\\mathbb E\\left[ \\mathbb E\\left.\\left[2\\left(-\\frac{Y}{m}+1\\right)\\ \\right\\vert\\ X=x\\right]\\right]\\\\ &amp;=-2\\mathbb E\\left[ \\mathbb E\\left.\\left[\\frac{Y}{m}\\ \\right\\vert\\ X=x\\right]\\right]+2 \\end{align*}\\] Setting this equal to zero gives \\[ 1=\\mathbb E\\left[ \\mathbb E\\left.\\left[\\frac{Y}{m}\\ \\right\\vert\\ X=x\\right]\\right]=\\frac{1}{m}\\mathbb E\\left[ \\mathbb E\\left.\\left[Y\\ \\right\\vert\\ X=x\\right]\\right] \\] Giving that the expectation is minimized for \\[ m=E\\left.\\left[Y\\ \\right\\vert\\ X=x\\right] \\] as desired. \\(\\square\\) Solution (2). Take any estimator \\(\\hat m_n\\) and consider the risk associated with the estimator: \\[\\begin{align*} r_{\\text{pois:dev}}(\\hat m_n(X))-r_{\\text{pois:dev}}(m^*(x))&amp;=\\mathbb E\\left[2\\left(Y\\log \\frac{Y}{\\hat m_n(X)}-Y+\\hat m_n(X)\\right)\\right]-\\mathbb E\\left[2\\left(Y\\log \\frac{Y}{ m^*(X)}-Y+m^*(X)\\right)\\right]\\\\ &amp;=\\mathbb E\\left[2\\left(Y\\log \\frac{Y}{\\hat m_n(X)}-Y+\\hat m_n(X)-Y\\log \\frac{Y}{ m^*(X)}+Y-m^*(X)\\right)\\right]\\\\ &amp;=\\mathbb E\\left[2\\left(Y\\log \\frac{Ym^*(X)}{\\hat m_n(X)Y}+\\hat m_n(X)-m^*(X)\\right)\\right]\\\\ &amp;=\\mathbb E\\left[2\\left(Y\\log \\frac{m^*(X)}{\\hat m_n(X)}+\\hat m_n(X)-m^*(X)\\right)\\right] \\end{align*}\\] Yielding the desired result. \\(\\square\\) Exercise 3. We consider a simple regression case with no explanatory variables. We denote by \\(\\hat m_{n,1}\\) the sample mean and by \\(\\hat m_{n,2}\\) the sample mean. Furthermore, \\(\\hat R_{n,1}\\) and \\(\\hat R_{n,2}\\) denote the empirical risk with respect to the \\(L_1\\) loss and the squared loss respectively. Generate 10,000 iid observations \\(y_1,...,y_{10000}\\) from a standard normal distribution. Compare \\[ \\hat R_{n,1}(\\hat m_{n,1}),\\hat R_{n,1}(\\hat m_{n,2}),\\hat R_{n,2}(\\hat m_{n,1}),\\hat R_{n,2}(\\hat m_{n,2}). \\] Generate 10,000 iid observations \\(y_1,...,y_{10000}\\) from a \\(t\\)-distribution with one degree of freedom. Compare \\[ \\hat R_{n,1}(\\hat m_{n,1}),\\hat R_{n,1}(\\hat m_{n,2}),\\hat R_{n,2}(\\hat m_{n,1}),\\hat R_{n,2}(\\hat m_{n,2}). \\] What conclusion can you draw from the two exercises? Solution (1). We set the seed to 1 set.seed(1) and generate the \\(n=10000\\) samples. set.seed(1) y &lt;- rnorm(n =10000, mean = 0, sd = 1) We may now compute the sample mean and median. hat_m_1 &lt;- median(y) hat_m_2 &lt;- mean(y) One may recall that \\(\\text{median}(X)=\\mathbb E[X]=0\\) for \\(X\\sim \\mathcal N(0,1)\\) and so we would expect \\(\\hat m_{n,1}\\approx \\hat m_{n,2}\\). We can compute the empirical risk wrt. the \\(L_1\\) and \\(L_2\\) loss. hat_R_11 &lt;- sum(abs(y-hat_m_1))/length(y) hat_R_11 ## [1] 0.8060947 hat_R_12 &lt;- sum(abs(y-hat_m_2))/length(y) hat_R_12 ## [1] 0.8061373 hat_R_21 &lt;- sum((y-hat_m_1)**2)/length(y) hat_R_21 ## [1] 1.024851 hat_R_22 &lt;- sum((y-hat_m_2)**2)/length(y) hat_R_22 ## [1] 1.024763 Although these are empirical risk we have not computed an estimate of the risk as we should generate more samples of the risk. As such we draw using the above method \\(S=1000\\) samples of the risks and compute the means. S &lt;- 1000 n &lt;- 10000 hat_R &lt;- rowMeans( #Generate S samples (in 4 x S-matrix) sapply(1:S, function(s) { #Generate y&#39;s y &lt;- rnorm(n,mean=0,sd=1) #Compute estimates hat_m_1 &lt;- median(y) hat_m_2 &lt;- mean(y) #Compute risks c( sum(abs(y-hat_m_1))/length(y), sum(abs(y-hat_m_2))/length(y), sum((y-hat_m_1)**2)/length(y), sum((y-hat_m_2)**2)/length(y) ) }) #The output i then taken rowMeans on ) We may compare the results in the table below. Measure Value \\(\\hat R_{n,1}(\\hat m_{n,1})\\) 0.7979859 \\(\\hat R_{n,1}(\\hat m_{n,2})\\) 0.7980077 \\(\\hat R_{n,2}(\\hat m_{n,1})\\) 1.0004189 \\(\\hat R_{n,2}(\\hat m_{n,2})\\) 1.0003643 We can see that the following holds: For the \\(L_1\\) loss the empirical median does better than the empirical mean, For the squared loss the empirical median does worse than the empirical mean. Solution (2). We set the seed to 1 set.seed(1) and generate the \\(n=10000\\) samples. set.seed(1) y &lt;- stats::rt(n =10000, df = 1) We may now compute the sample mean and median. hat_m_1 &lt;- median(y) hat_m_2 &lt;- mean(y) One may recall that \\(\\text{median}(X)=\\mathbb E[X]=0\\) for \\(X\\sim \\mathcal t(1)\\) and so we would expect \\(\\hat m_{n,1}\\approx \\hat m_{n,2}\\). We can compute the empiracal risk wrt. the \\(L_1\\) and \\(L_2\\) loss. S &lt;- 1000 n &lt;- 10000 hat_R &lt;- rowMeans( #Generate S samples (in 4 x S-matrix) sapply(1:S, function(s) { #Generate y&#39;s y &lt;- rt(n,df=1) #Compute estimates hat_m_1 &lt;- median(y) hat_m_2 &lt;- mean(y) #Compute risks c( sum(abs(y-hat_m_1))/length(y), sum(abs(y-hat_m_2))/length(y), sum((y-hat_m_1)**2)/length(y), sum((y-hat_m_2)**2)/length(y) ) }) #The output i then taken rowMeans on ) We may compare the results in the table below. Measure Value \\(\\hat R_{n,1}(\\hat m_{n,1})\\) 11.7586085 \\(\\hat R_{n,1}(\\hat m_{n,2})\\) 16.7838177 \\(\\hat R_{n,2}(\\hat m_{n,1})\\) 1.8456626^{7} \\(\\hat R_{n,2}(\\hat m_{n,2})\\) 1.84548^{7} We see that for the \\(t\\)-distribution with one degree of freedom we see the same relation as in the normal case. The median does better in the \\(L_1\\) loss but worse under the squared loss. Solution (3). We can see that the risk is far greater than the normal case. Comparing the density functions of a standard normal distribution and the \\(t\\)-distribution with one degree of freedom we see that the mean is the same but the variance is far greater than the normal case. In fact, with degrees of freedom below 2 we have that the variance is infinite. This gives that for a given sample size the risk will tend to infinity as \\(n\\) grows to infinity. This in turn explains the large values we see in the risk. We see that for the \\(t\\)-distribution with one degree of freedom we see the same relation as in the normal case. The median does better in the \\(L_1\\) loss but worse under the squared loss. Exercise 4. We want to practise model tuning with the mlr3 package. Go through the following steps: Install and load the relevant ml3 packages: mlr3, mlr3learners, mlr3tuning, mlr3mbo. Create a task Load the mtcars data (write: data(mtcars)) Use the as_task_regr to create a task with mpg as target Set regr.xgboost as learner with corresponding search space; e.g., \\[\\begin{align*} eta &amp;= to\\_tune(0, 1)\\\\ nrounds &amp;= to\\_tune(10, 5000)\\\\ max\\_depth &amp;= to\\_tune(1, 20)\\\\ colsample\\_bytree &amp;= to\\_tune(0.1, 1)\\\\ subsample &amp;= to\\_tune(0.1, 1) \\end{align*}\\] Tune your learner on you task using the tune function with Resampling method: 5-fold cross validation Measure: squared loss Method: mbo or random search Terminator: 10 evaluations Fit your learner on the task using the optimal hyper parameters calculated Solution (1). We install the required packages. packages &lt;- c(&quot;mlr3&quot;, &quot;mlr3learners&quot;, &quot;mlr3tuning&quot;, &quot;mlr3mbo&quot;) install.packages(packages, dependencies = TRUE) Solution (2). We start by loading the data. data(&quot;mtcars&quot;) head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 We now transform the data into a task. #Load the relevant libraries library(mlr3) library(mlr3learners ) library(mlr3tuning) library(mlr3mbo) #Start task task_mtcars = as_task_regr( mtcars, target = &quot;mpg&quot;, id = &quot;cars&quot; ) We may now split out data set into random partions of a training dataset and a testing dataset. #Split data splits = partition(task_mtcars) splits ## $train ## [1] 3 4 8 9 10 21 25 30 32 6 12 16 17 22 23 24 29 31 18 19 20 ## ## $test ## [1] 1 2 5 27 7 11 13 14 15 26 28 We see that the testing dataset has \\(N_1=11\\) and the training dataset has \\(N_2=21\\). We will be training the model on the training dataset. Therefore we start a task on the subset. task_mtcars_train = as_task_regr( mtcars[splits$train,], target = &quot;mpg&quot;, id = &quot;cars_train&quot; ) Solution (3). We now initiate a learner. # load regression xgboost learner_xgboost = lrn(&quot;regr.xgboost&quot;) We can now look at the current configuration of the leaner by looking at the parameter space of the hyperparameter. as.data.table(learner_xgboost$param_set)[,c(&quot;id&quot;,&quot;class&quot;,&quot;lower&quot;, &quot;upper&quot;,&quot;nlevels&quot;)] ## id class lower upper nlevels ## 1: alpha ParamDbl 0 Inf Inf ## 2: approxcontrib ParamLgl NA NA 2 ## 3: base_score ParamDbl -Inf Inf Inf ## 4: booster ParamFct NA NA 3 ## 5: callbacks ParamUty NA NA Inf ## 6: colsample_bylevel ParamDbl 0 1 Inf ## 7: colsample_bynode ParamDbl 0 1 Inf ## 8: colsample_bytree ParamDbl 0 1 Inf ## 9: disable_default_eval_metric ParamLgl NA NA 2 ## 10: early_stopping_rounds ParamInt 1 Inf Inf ## 11: early_stopping_set ParamFct NA NA 3 ## 12: eta ParamDbl 0 1 Inf ## 13: eval_metric ParamUty NA NA Inf ## 14: feature_selector ParamFct NA NA 5 ## 15: feval ParamUty NA NA Inf ## 16: gamma ParamDbl 0 Inf Inf ## 17: grow_policy ParamFct NA NA 2 ## 18: interaction_constraints ParamUty NA NA Inf ## 19: iterationrange ParamUty NA NA Inf ## 20: lambda ParamDbl 0 Inf Inf ## 21: lambda_bias ParamDbl 0 Inf Inf ## 22: max_bin ParamInt 2 Inf Inf ## 23: max_delta_step ParamDbl 0 Inf Inf ## 24: max_depth ParamInt 0 Inf Inf ## 25: max_leaves ParamInt 0 Inf Inf ## 26: maximize ParamLgl NA NA 2 ## 27: min_child_weight ParamDbl 0 Inf Inf ## 28: missing ParamDbl -Inf Inf Inf ## 29: monotone_constraints ParamUty NA NA Inf ## 30: normalize_type ParamFct NA NA 2 ## 31: nrounds ParamInt 1 Inf Inf ## 32: nthread ParamInt 1 Inf Inf ## 33: ntreelimit ParamInt 1 Inf Inf ## 34: num_parallel_tree ParamInt 1 Inf Inf ## 35: objective ParamUty NA NA Inf ## 36: one_drop ParamLgl NA NA 2 ## 37: outputmargin ParamLgl NA NA 2 ## 38: predcontrib ParamLgl NA NA 2 ## 39: predictor ParamFct NA NA 2 ## 40: predinteraction ParamLgl NA NA 2 ## 41: predleaf ParamLgl NA NA 2 ## 42: print_every_n ParamInt 1 Inf Inf ## 43: process_type ParamFct NA NA 2 ## 44: rate_drop ParamDbl 0 1 Inf ## 45: refresh_leaf ParamLgl NA NA 2 ## 46: reshape ParamLgl NA NA 2 ## 47: sampling_method ParamFct NA NA 2 ## 48: sample_type ParamFct NA NA 2 ## 49: save_name ParamUty NA NA Inf ## 50: save_period ParamInt 0 Inf Inf ## 51: scale_pos_weight ParamDbl -Inf Inf Inf ## 52: seed_per_iteration ParamLgl NA NA 2 ## 53: skip_drop ParamDbl 0 1 Inf ## 54: strict_shape ParamLgl NA NA 2 ## 55: subsample ParamDbl 0 1 Inf ## 56: top_k ParamInt 0 Inf Inf ## 57: training ParamLgl NA NA 2 ## 58: tree_method ParamFct NA NA 5 ## 59: tweedie_variance_power ParamDbl 1 2 Inf ## 60: updater ParamUty NA NA Inf ## 61: verbose ParamInt 0 2 3 ## 62: watchlist ParamUty NA NA Inf ## 63: xgb_model ParamUty NA NA Inf ## id class lower upper nlevels As the text says we set som of the parameters to as specific section of the parameter space. my_xg_learner = lrn(&quot;regr.xgboost&quot;, eta = to_tune(0, 1), nrounds = to_tune(10, 5000), max_depth = to_tune(1, 20), colsample_bytree = to_tune(0.1, 1), subsample = to_tune(0.1, 1)) Solution (4). We tune a regression model using mbo search. instance = tune( method = tnr(&quot;mbo&quot;), ### tuning method task = task_mtcars_train, learner = my_xg_learner, resampling = rsmp(&quot;cv&quot;, folds = 5), #### resampling method: 5-fold cross validation measures = msr(&quot;regr.rmse&quot;), #### root mean squared error terminator = trm(&quot;evals&quot;, n_evals =10) #### terminator ) We can consider the the estimates from each subset and the fitted parameters. #All 20 runs as.data.table(instance$archive)[, c(&quot;eta&quot;, &quot;nrounds&quot;, &quot;max_depth&quot;, &quot;regr.rmse&quot;)] ## eta nrounds max_depth regr.rmse ## 1: 0.917875208 3533 11 3.802051 ## 2: 0.765106068 412 8 2.964021 ## 3: 0.674223188 4335 7 3.464838 ## 4: 0.101590241 2508 18 2.433159 ## 5: 0.818736983 1039 19 2.859263 ## 6: 0.014765501 2093 3 2.229626 ## 7: 0.872891106 3868 4 2.897375 ## 8: 0.610313191 2177 11 3.501744 ## 9: 0.028048314 4693 2 2.165364 ## 10: 0.373920733 548 18 2.626296 ## 11: 0.160639059 4116 2 2.187469 ## 12: 0.796255760 4447 6 3.551414 ## 13: 0.285658641 3817 5 2.380419 ## 14: 0.002011162 4297 20 2.381016 ## 15: 0.675239580 3975 1 3.539176 ## 16: 0.616630552 4012 3 2.610140 ## 17: 0.151641272 1924 5 2.284830 ## 18: 0.368378715 382 17 3.098928 ## 19: 0.475511641 2419 19 2.568179 ## 20: 0.004646809 1678 10 3.029619 #Optimal fitting instance$result ## nrounds eta max_depth colsample_bytree subsample learner_param_vals ## 1: 4693 0.02804831 2 0.4502029 0.7789301 &lt;list[8]&gt; ## x_domain regr.rmse ## 1: &lt;list[5]&gt; 2.165364 #Optimal parameters instance$result_learner_param_vals ## $nthread ## [1] 1 ## ## $verbose ## [1] 0 ## ## $early_stopping_set ## [1] &quot;none&quot; ## ## $nrounds ## [1] 4693 ## ## $eta ## [1] 0.02804831 ## ## $max_depth ## [1] 2 ## ## $colsample_bytree ## [1] 0.4502029 ## ## $subsample ## [1] 0.7789301 Solution (5). We can now use optimal paramteres in instance$result_learner_param_vals to create a learner that we may fit to the training data. #Define new tuner xgb_tuned = lrn(&quot;regr.xgboost&quot;, id = &quot;xgb tuned&quot;) #Set parameters to the optimals from before xgb_tuned$param_set$values = instance$result_learner_param_vals We now fit the data. xgb_tuned$train(task_mtcars_train) We can now predict onto the testing data. predictions = xgb_tuned$predict_newdata(mtcars[splits$test,]) predictions ## &lt;PredictionRegr&gt; for 11 observations: ## row_ids truth response ## 1 21.0 19.91326 ## 2 21.0 19.95671 ## 3 18.7 17.03209 ## --- ## 9 10.4 11.40222 ## 10 27.3 31.67276 ## 11 30.4 24.25820 #empirical risk risk &lt;- mean((predictions$truth-predictions$response)**2) Using a diagram we can see. That the model does pretty well. In fact the empirical risk is 6.55. "],["week-2-1.html", "3.2 Week 2", " 3.2 Week 2 From now on, given sample size \\(n\\), a dimension \\(p\\), and a correlation parameter \\(0\\le \\texttt{rho}&lt;1\\) we will generate data \\(X\\) via the following code. z_1 &lt;- rnorm(p*n,0,(1-rho)^(1/2)) z_0 &lt;- rnorm(n,0,rho^(1/2)) X &lt;- 2.5*atan(z_1+z_0)/pi dim(X) &lt;- c(n,p) The resulting features will have approximately support on \\([−1, 1]\\) and will have approximately pairwise correlation of size \\(\\texttt{rho}\\). Given the distribution of \\(\\varepsilon\\) and a regression function \\(m\\), we then generate data \\((X_i,Y_i)_{i=1,...,n}\\) via \\[ Y_i=m(X_i)+\\varepsilon_i, \\] where \\(\\varepsilon_i\\) are iid copies of \\(\\varepsilon\\) and \\(X_i\\) is the \\(i\\)th row of \\(X\\). Exercise 1. We want to try out ordinary least squares regression, lasso, ridge regression and elastic net on some different data generating settings (Model 1–4). n p s rho m \\(\\varepsilon\\) Model 1 1000 100 5 0.3 \\(\\sum_{j=1}^s x_j\\) \\(\\mathcal N(0,1)\\) Model 2 1000 100 100 0.3 \\(\\sum_{j=1}^s x_j\\) \\(\\mathcal N(0,1)\\) Model 3 1000 100 5 0.3 \\(\\sum_{j=1}^s 0.1x_j\\) \\(\\mathcal N(0,1)\\) Model 4 1000 100 100 0.3 \\(\\sum_{j=1}^s 0.1x_j\\) \\(\\mathcal N(0,1)\\) Given one set of training data for each model tune (i.e. estimate optimal hyperprameter) via 5-fold cross-validation using a search method and number of evals of your choice. You should now have an estimated optimal hyperparameter for every combination of model and method. Generate 100 test sets each of size n for every model and calculate empirical mean and standard deviation of the test error for each method and model using the hyperparameters calculated in (a). Create a table of your results (each missing entry should show hyperparameter: mean(sd)): Least Squares Ridge Lasso Elastic Net Model 1 \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) Model 2 \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) Model 3 \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) Model 4 \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) Discuss why test error alone might often not be the (only) quantity of interest. Solution (a). We start by simulating the data with the seed 1 for the training data and 2 for testing data. #Generate data generateData &lt;- function(n,p,s,rho,m,epsilon,seed) { set.seed(seed) z_1 &lt;- rnorm(p*n,0,(1-rho)^(1/2)) z_0 &lt;- rnorm(n,0,rho^(1/2)) X &lt;- 2.5*atan(z_1+z_0)/pi dim(X) &lt;- c(n,p) Y &lt;- m(X) + rnorm(n,0,epsilon) dat &lt;- data.frame(Y = Y, X=X) return(list(dat=dat,Y=Y,X=X)) } Model 1 We may now estimate under model 1 by setting the parameters and generating the data. #Model 1 n &lt;- 1000 p &lt;- 100 s &lt;- 5 rho &lt;- 0.3 m &lt;- function(x) { rowSums(x[,1:s])} epsilon &lt;- 1 data_train &lt;- generateData(n,p,s,rho,m,epsilon,1)$dat data_test &lt;- generateData(n,p,s,rho,m,epsilon,2)$dat head(data_train[,1:10]) ## Y X.1 X.2 X.3 X.4 X.5 ## 1 1.8309143 -0.07193125 0.75183303 -0.2377002 0.6451155 -0.3789598 ## 2 1.4939672 0.28091848 0.67874915 -0.7546614 0.3929413 0.5626808 ## 3 0.3964524 -0.60794426 -0.61996908 0.6615121 0.5492820 0.1712623 ## 4 0.1283924 0.65701309 -0.05919154 0.1445119 -0.5932112 -0.7514690 ## 5 -2.2628749 0.14449216 -0.02706905 -0.1097656 -0.7651413 -0.8446573 ## 6 1.7979838 -0.27896899 -0.65200577 0.5846706 0.6634413 0.5440794 ## X.6 X.7 X.8 X.9 ## 1 -0.5537303 -0.06689199 -0.4727280 0.4609656 ## 2 0.5075691 -0.49311711 0.6294640 -0.3555881 ## 3 -0.8191675 -0.89258755 0.3435102 -0.7775838 ## 4 0.5050303 -0.21505308 0.4511738 0.6831925 ## 5 0.5573768 -0.23918147 -0.2927378 -0.7521317 ## 6 -0.4938379 0.52113194 0.1657710 0.1270910 head(data_test[,1:10]) ## Y X.1 X.2 X.3 X.4 X.5 ## 1 -2.985333 -0.70532654 -0.06742898 -0.8900989 -0.9190374 0.04862538 ## 2 -2.057488 -0.13035879 0.35533591 0.5180006 -0.6532359 -0.81848843 ## 3 -2.218149 0.75916886 -0.49168949 -0.3268390 0.1625926 -0.41165505 ## 4 -0.176265 -0.77652654 -0.36866613 0.2601691 0.3100563 -0.86620293 ## 5 1.780030 0.39826072 -0.06440487 0.8050035 0.8184016 0.50856826 ## 6 -2.539077 -0.01983207 0.25397105 -0.7543274 -0.6668646 -0.66975619 ## X.6 X.7 X.8 X.9 ## 1 -0.8684129 -0.73370531 0.28994890 -0.36455945 ## 2 -0.5053897 0.23098162 -0.24853540 0.37132666 ## 3 -0.1681142 0.09490974 -0.61895581 -0.36122745 ## 4 -0.1963916 -0.59210876 0.43663155 -0.51196618 ## 5 0.1529196 -0.36622001 0.66508279 0.09886413 ## 6 -0.6373294 -0.71339909 -0.09165955 -0.05701028 We can estimate the optimal parameters under least squares estimation with the lm function. ls_lm &lt;- lm(Y ~ ., data = data_train) We can use predict to see the model prediction vs. the true values. fit &lt;- data.frame(response = predict(ls_lm,data_test), truth = data_test$Y) We can also use mlr3 to tune optimal hyperparameters for a lasso, ridge and elastic net estimator. We start as usual by slitting the data into training and testing datasets and starting a task. #Load the relevant libraries library(mlr3) library(mlr3learners ) library(mlr3tuning) library(mlr3mbo) #Start task task_model1 = as_task_regr( data_train, target = &quot;Y&quot;, id = &quot;model1&quot; ) We can now initiate a leaner. We start by fitting the hyperparameter for the lasso learner. learner_lasso = lrn(&quot;regr.glmnet&quot;) We use the glmnet as it is an algorithm that solves the minimizing problem \\[ \\hat{\\beta}_\\lambda^{\\text {glmnet }} =\\underset{\\beta \\in \\mathbb{R}^p}{\\arg \\min }\\left\\{\\hat{R}_n(\\beta)+J_\\lambda(\\beta)\\right\\},\\hspace{15pt}J_\\lambda(\\beta)=\\lambda \\left[\\alpha\\sum_{j=1}^p\\left|\\beta_j\\right|+\\frac{1-\\alpha}{2}\\sum_{j=1}^p\\beta_j^2\\right], \\] One may recall that the lasso estimator is given by \\[ \\hat{\\beta}_\\lambda^{\\text {lasso }} =\\underset{\\beta \\in \\mathbb{R}^p}{\\arg \\min }\\left\\{\\hat{R}_n(\\beta)+J_\\lambda(\\beta)\\right\\},\\hspace{15pt}J_\\lambda(\\beta)=\\lambda \\sum_{j=1}^p\\left|\\beta_j\\right|, \\] and so the lasso estimator is a specialcase of the glmnet with \\(\\alpha=1\\). From the above we see that the minimizing algorithm punishes large beta’s. The hyperparameter to tune is then \\(\\lambda\\ge 0\\) (s is the syntax for \\(\\lambda\\)). Looking at the parameters in the learner_lasso one can see the entire hyperparameter space. as.data.table(learner_lasso $param_set)[, list(id, class, lower, upper, nlevels)] ## id class lower upper nlevels ## 1: alignment ParamFct NA NA 2 ## 2: alpha ParamDbl 0 1 Inf ## 3: big ParamDbl -Inf Inf Inf ## 4: devmax ParamDbl 0 1 Inf ## 5: dfmax ParamInt 0 Inf Inf ## 6: eps ParamDbl 0 1 Inf ## 7: epsnr ParamDbl 0 1 Inf ## 8: exact ParamLgl NA NA 2 ## 9: exclude ParamInt 1 Inf Inf ## 10: exmx ParamDbl -Inf Inf Inf ## 11: family ParamFct NA NA 2 ## 12: fdev ParamDbl 0 1 Inf ## 13: gamma ParamDbl -Inf Inf Inf ## 14: grouped ParamLgl NA NA 2 ## 15: intercept ParamLgl NA NA 2 ## 16: keep ParamLgl NA NA 2 ## 17: lambda ParamUty NA NA Inf ## 18: lambda.min.ratio ParamDbl 0 1 Inf ## 19: lower.limits ParamUty NA NA Inf ## 20: maxit ParamInt 1 Inf Inf ## 21: mnlam ParamInt 1 Inf Inf ## 22: mxit ParamInt 1 Inf Inf ## 23: mxitnr ParamInt 1 Inf Inf ## 24: newoffset ParamUty NA NA Inf ## 25: nlambda ParamInt 1 Inf Inf ## 26: offset ParamUty NA NA Inf ## 27: parallel ParamLgl NA NA 2 ## 28: penalty.factor ParamUty NA NA Inf ## 29: pmax ParamInt 0 Inf Inf ## 30: pmin ParamDbl 0 1 Inf ## 31: prec ParamDbl -Inf Inf Inf ## 32: relax ParamLgl NA NA 2 ## 33: s ParamDbl 0 Inf Inf ## 34: standardize ParamLgl NA NA 2 ## 35: standardize.response ParamLgl NA NA 2 ## 36: thresh ParamDbl 0 Inf Inf ## 37: trace.it ParamInt 0 1 2 ## 38: type.gaussian ParamFct NA NA 2 ## 39: type.logistic ParamFct NA NA 2 ## 40: type.multinomial ParamFct NA NA 2 ## 41: upper.limits ParamUty NA NA Inf ## id class lower upper nlevels Let us tune a lasso learner by setting alpha = 1 and tuning s = totune(0,1) i.e. searching for \\(\\lambda \\in [0,1]\\). #Define the learner my_lasso_learner = lrn(&quot;regr.glmnet&quot;, s= to_tune(0, 1), alpha=1) #Tune the learner instance = tune( method = tnr(&quot;mbo&quot;), ### tuning method task = task_model1, learner = my_lasso_learner, resampling = rsmp(&quot;cv&quot;, folds = 5), #### resampling method: 5-fold cross validation measures = msr(&quot;regr.rmse&quot;), #### root mean squared error terminator = trm(&quot;evals&quot;, n_evals = 100) #### terminator ) #One might go through the archive #as.data.table(instance$archive) instance$result ## s learner_param_vals x_domain regr.rmse ## 1: 0.06178124 &lt;list[3]&gt; &lt;list[1]&gt; 1.037023 Now that we have tuned the algorithm we can fit the model with the above \\(s\\) (0.0618). lasso_tuned = lrn(&quot;regr.glmnet&quot;) lasso_tuned$param_set$values = instance$result_learner_param_vals lasso_tuned$train(task_model1) Lets quickly look at the predictions. predictions = lasso_tuned$predict_newdata(data_test) One can get the coefficients of \\(\\beta\\) by predicting the data frame I = diag(1,nrow = 100) being the identity. #Make identity matrix I &lt;- diag(x=1,nrow = p) %&gt;% as.data.frame() colnames(I) &lt;- colnames(data_train)[2:(p+1)] predictions = lasso_tuned$predict_newdata(I) beta_lasso &lt;- predictions$response #Look at the beta beta_lasso ## [1] 0.99915093 0.79086275 0.99541349 0.94102686 0.93627752 0.03315185 ## [7] 0.03315185 0.03315185 0.03315185 0.03315185 0.10981076 0.03315185 ## [13] 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 ## [19] 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 ## [25] 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 ## [31] 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 ## [37] 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 ## [43] 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 ## [49] 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 ## [55] 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 ## [61] 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 ## [67] 0.14390239 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 ## [73] 0.03315185 0.07565291 0.03315185 0.03315185 0.03315185 0.03315185 ## [79] 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 ## [85] 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 0.03315185 ## [91] 0.03315185 0.03315185 0.03315185 0.03315185 0.06264766 0.04897566 ## [97] 0.03315185 0.03315185 0.03315185 0.03315185 One can see that the values is close to the true value \\(\\beta=(1,1,1,1,1,0,...,0)\\). We can now estimate the parameters in ridge regression by setting \\(\\alpha = 0\\) and then getting a scaled version of \\(\\lambda\\). #Define the learner my_ridge_learner = lrn(&quot;regr.glmnet&quot;, s= to_tune(0, 1), alpha=0) #Tune the learner instance = tune( method = tnr(&quot;mbo&quot;), ### tuning method task = task_model1, learner = my_ridge_learner, resampling = rsmp(&quot;cv&quot;, folds = 5), #### resampling method: 5-fold cross validation measures = msr(&quot;regr.rmse&quot;), #### root mean squared error terminator = trm(&quot;evals&quot;, n_evals = 100) #### terminator ) #One might go through the archive #as.data.table(instance$archive) instance$result ## s learner_param_vals x_domain regr.rmse ## 1: 0.2199674 &lt;list[3]&gt; &lt;list[1]&gt; 1.100857 We fit the model with the above \\(s\\) (0.22). ridge_tuned = lrn(&quot;regr.glmnet&quot;) ridge_tuned$param_set$values = instance$result_learner_param_vals ridge_tuned$train(task_model1) Lets quickly look at the predictions. predictions = ridge_tuned$predict_newdata(data_test) For good measures we calculate the beta’s as in lasso. #Make identity matrix I &lt;- diag(x=1,nrow = p) %&gt;% as.data.frame() colnames(I) &lt;- colnames(data_train)[2:(p+1)] predictions &lt;- ridge_tuned$predict_newdata(I) beta_ridge &lt;- predictions$response #Look at the beta beta_ridge ## [1] 9.572446e-01 7.686100e-01 9.449922e-01 9.287205e-01 8.981642e-01 ## [6] 9.963081e-02 1.190889e-01 6.680920e-02 -2.360800e-02 2.529436e-02 ## [11] 2.085035e-01 -1.952593e-02 2.693690e-03 -6.889561e-02 2.116001e-02 ## [16] 2.781748e-02 1.121865e-01 9.137978e-02 2.454655e-02 4.820396e-02 ## [21] 4.539692e-02 -1.432497e-03 1.660669e-02 7.668634e-02 7.671318e-02 ## [26] 4.975334e-02 7.260690e-02 -8.629362e-03 7.456080e-02 5.825794e-02 ## [31] -4.527776e-02 6.807175e-02 -1.271848e-02 -2.748982e-02 1.203625e-01 ## [36] 7.651944e-02 6.392636e-02 1.101667e-02 1.734406e-02 6.945025e-02 ## [41] 2.999642e-02 -3.187841e-02 -1.155532e-01 -5.211477e-02 5.219365e-02 ## [46] 8.412732e-05 2.443844e-02 1.710180e-02 -4.820703e-02 5.932595e-02 ## [51] -4.195687e-03 3.171758e-02 8.042820e-03 -4.803883e-02 3.413274e-02 ## [56] 5.680607e-02 2.805170e-03 1.155576e-02 9.527637e-02 -4.649393e-02 ## [61] -1.231626e-02 8.193416e-02 1.487057e-02 -7.600475e-03 7.944417e-02 ## [66] -2.025782e-02 1.928551e-01 2.477441e-02 6.769122e-03 3.546825e-02 ## [71] 1.145399e-01 6.519184e-03 3.783939e-02 1.520270e-01 5.808354e-02 ## [76] -5.988705e-03 -9.409675e-03 -5.117966e-02 -5.673331e-02 1.858218e-02 ## [81] 7.114004e-02 -6.238344e-02 -5.586778e-02 4.614891e-02 1.147374e-02 ## [86] 7.358890e-02 -7.752170e-02 4.166919e-02 -3.679839e-02 1.558774e-02 ## [91] 5.154878e-02 9.195644e-02 7.103979e-02 8.703200e-02 1.601304e-01 ## [96] 1.444098e-01 5.452382e-02 -8.407149e-02 -6.720288e-02 8.402896e-02 Finally, let us tune an elastic net. #Define the learner my_elastic_learner = lrn(&quot;regr.glmnet&quot;, s= to_tune(0, 1), alpha= to_tune(0,1)) #Tune the learner instance = tune( method = tnr(&quot;mbo&quot;), ### tuning method task = task_model1, learner = my_elastic_learner, resampling = rsmp(&quot;cv&quot;, folds = 5), #### resampling method: 5-fold cross validation measures = msr(&quot;regr.rmse&quot;), #### root mean squared error terminator = trm(&quot;evals&quot;, n_evals = 100) #### terminator ) #One might go through the archive #as.data.table(instance$archive) instance$result ## s alpha learner_param_vals x_domain regr.rmse ## 1: 0.0623403 0.9998859 &lt;list[3]&gt; &lt;list[2]&gt; 1.037026 We fit the model with the above \\(s\\) (0.0623). elastic_tuned = lrn(&quot;regr.glmnet&quot;) elastic_tuned$param_set$values = instance$result_learner_param_vals elastic_tuned$train(task_model1) Lets quickly look at the predictions. predictions = elastic_tuned$predict_newdata(data_test) For good measures we calculate the beta’s as in lasso. #Make identity matrix I &lt;- diag(x=1,nrow = p) %&gt;% as.data.frame() colnames(I) &lt;- colnames(data_train)[2:(p+1)] predictions &lt;- elastic_tuned$predict_newdata(I) beta_elastic &lt;- predictions$response #Look at the beta beta_elastic ## [1] 0.99882093 0.79064649 0.99510364 0.94068530 0.93595611 0.03316896 ## [7] 0.03316896 0.03316896 0.03316896 0.03316896 0.10956374 0.03316896 ## [13] 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 ## [19] 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 ## [25] 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 ## [31] 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 ## [37] 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 ## [43] 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 ## [49] 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 ## [55] 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 ## [61] 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 ## [67] 0.14366364 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 ## [73] 0.03316896 0.07532564 0.03316896 0.03316896 0.03316896 0.03316896 ## [79] 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 ## [85] 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 0.03316896 ## [91] 0.03316896 0.03316896 0.03316896 0.03316896 0.06245603 0.04861821 ## [97] 0.03316896 0.03316896 0.03316896 0.03316896 Let’s lastly look at how the different estimators do with respect to the true beta. We may now estimate the remaining three models. Model 2-4. We generate the data and estimate under the model. We define a function that compute the estimators. getEstimates &lt;- function(n,p,s,rho,m,epsilon){ #Get data data_train &lt;- generateData(n,p,s,rho,m,epsilon,1)$dat data_test &lt;- generateData(n,p,s,rho,m,epsilon,2)$dat #Identity matrix for parameters I &lt;- diag(x=1,nrow = p) %&gt;% as.data.frame() colnames(I) &lt;- colnames(data_train)[2:(p+1)] #Make ls model ls_lm &lt;- lm(Y ~ ., data = data_train) beta_ls &lt;- predict(ls_lm,I) predictions_ls &lt;- predict(ls_lm,data_test) #Make task task_model = as_task_regr( data_train, target = &quot;Y&quot; ) #Tune lasso my_lasso_learner &lt;- lrn(&quot;regr.glmnet&quot;, s= to_tune(0, 1), alpha= 1) lasso_instance &lt;- tune( method = tnr(&quot;mbo&quot;), ### tuning method task = task_model, learner = my_lasso_learner, resampling = rsmp(&quot;cv&quot;, folds = 5), #### resampling method: 5-fold cross validation measures = msr(&quot;regr.rmse&quot;), #### root mean squared error terminator = trm(&quot;evals&quot;, n_evals = 100) #### terminator ) lasso_tuned = lrn(&quot;regr.glmnet&quot;) lasso_tuned$param_set$values = lasso_instance$result_learner_param_vals lasso_tuned$train(task_model) predictions_lasso = lasso_tuned$predict_newdata(data_test) beta_lasso &lt;- lasso_tuned$predict_newdata(I)$response #Tune ridge my_ridge_learner &lt;- lrn(&quot;regr.glmnet&quot;, s= to_tune(0, 1), alpha= 0) ridge_instance &lt;- tune( method = tnr(&quot;mbo&quot;), ### tuning method task = task_model, learner = my_ridge_learner, resampling = rsmp(&quot;cv&quot;, folds = 5), #### resampling method: 5-fold cross validation measures = msr(&quot;regr.rmse&quot;), #### root mean squared error terminator = trm(&quot;evals&quot;, n_evals = 100) #### terminator ) ridge_tuned = lrn(&quot;regr.glmnet&quot;) ridge_tuned$param_set$values = ridge_instance$result_learner_param_vals ridge_tuned$train(task_model) predictions_ridge = ridge_tuned$predict_newdata(data_test) beta_ridge &lt;- ridge_tuned$predict_newdata(I)$response #Tune elastic net my_elastic_learner &lt;- lrn(&quot;regr.glmnet&quot;, s= to_tune(0, 1), alpha= to_tune(0, 1)) elastic_instance &lt;- tune( method = tnr(&quot;mbo&quot;), ### tuning method task = task_model, learner = my_elastic_learner, resampling = rsmp(&quot;cv&quot;, folds = 5), #### resampling method: 5-fold cross validation measures = msr(&quot;regr.rmse&quot;), #### root mean squared error terminator = trm(&quot;evals&quot;, n_evals = 100) #### terminator ) elastic_tuned = lrn(&quot;regr.glmnet&quot;) elastic_tuned$param_set$values = elastic_instance$result_learner_param_vals elastic_tuned$train(task_model) predictions_elastic = elastic_tuned$predict_newdata(data_test) beta_elastic &lt;- elastic_tuned$predict_newdata(I)$response #Combine beta&#39;s beta_true &lt;- m(diag(1,nrow = p)) beta &lt;- data.frame( j = 1:p, true = beta_true, ls = beta_ls, lasso = beta_lasso, ridge = beta_ridge, elastic_net = beta_elastic ) #Combining predictions predictions &lt;- data.frame( i = 1:n, Truth = data_test$Y, ls = predictions_ls, lasso = predictions_lasso$response, ridge = predictions_ridge$response, elastic_net = predictions_elastic$response ) return(list( data_train = data_train, data_test = data_test, beta = beta, predictions = predictions, instances = list( lasso = lasso_instance, ridge = ridge_instance, elastic_net = elastic_instance ) )) } Now we simply gather the results. results1 &lt;- getEstimates( n = 1000, p = 100, s = 5, rho = 0.3, epsilon = 1, m = function(x){rowSums(x[,1:5])} ) results2 &lt;- getEstimates( n = 1000, p = 100, s = 100, rho = 0.3, epsilon = 1, m = function(x){rowSums(x[,1:100])} ) results3 &lt;- getEstimates( n = 1000, p = 100, s = 5, rho = 0.3, epsilon = 1, m = function(x){rowSums(0.1*x[,1:5])} ) results4 &lt;- getEstimates( n = 1000, p = 100, s = 100, rho = 0.3, epsilon = 1, m = function(x){rowSums(0.1*x[,1:100])} ) Solution (b). We simply generate \\(N=100\\) test sets and compute the mean squared error for each run, then take the empirical mean and standard deviation of the 100 samples of the empirical mean squared error. N &lt;- 100 computeMSE &lt;- function(n,p,s,rho,m,epsilon,results,N){ MSE &lt;- sapply(1:N+1, function(j) { #Get data data_test &lt;- generateData(n,p,s,rho,m,epsilon,j)$dat mse_ls &lt;- mean((data_test$Y - as.matrix(data_test[,2:(p+1)]) %*% results$beta$ls)^2) mse_lasso &lt;- mean((data_test$Y - as.matrix(data_test[,2:(p+1)]) %*% results$beta$lasso)^2) mse_ridge &lt;- mean((data_test$Y - as.matrix(data_test[,2:(p+1)]) %*% results$beta$ridge)^2) mse_elastic &lt;- mean((data_test$Y - as.matrix(data_test[,2:(p+1)]) %*% results$beta$elastic_net)^2) return(c(mse_ls,mse_lasso,mse_ridge,mse_elastic)) }) df &lt;- data.frame( method = c(&quot;LS&quot;,&quot;Lasso&quot;,&quot;Ridge&quot;,&quot;Elastic net&quot;), mean_MSE = rowMeans(MSE), sd_MSE = sapply(1:4, function(i) sd(MSE[i,])) ) return(df) } MSE_model1 &lt;- computeMSE( n = 1000, p = 100, s = 5, rho = 0.3, epsilon = 1, m = function(x){rowSums(x[,1:5])}, results = results1, N ) MSE_model2 &lt;- computeMSE( n = 1000, p = 100, s = 100, rho = 0.3, epsilon = 1, m = function(x){rowSums(x[,1:5])}, results = results2, N ) MSE_model3 &lt;- computeMSE( n = 1000, p = 100, s = 5, rho = 0.3, epsilon = 1, m = function(x){rowSums(0.1*x[,1:5])}, results = results3, N ) MSE_model4 &lt;- computeMSE( n = 1000, p = 100, s = 100, rho = 0.3, epsilon = 1, m = function(x){rowSums(0.1*x[,1:5])}, results = results4, N ) MSE_means &lt;- rbind( MSE_model1$mean_MSE,MSE_model2$mean_MSE, MSE_model3$mean_MSE,MSE_model4$mean_MSE ) colnames(MSE_means) &lt;- c(&quot;Least Squares&quot;,&quot;Lasso&quot;,&quot;Ridge&quot;,&quot;Elastic net&quot;) row.names(MSE_means) &lt;- c(&quot;Model 1&quot;,&quot;Model 2&quot;,&quot;Model 3&quot;, &quot;Model 4&quot;) library(kableExtra) kbl(MSE_means) Least Squares Lasso Ridge Elastic net Model 1 1.512701 1.839267 1.579840 1.843050 Model 2 797.525805 788.769559 793.827888 793.827772 Model 3 1.512701 1.885876 1.705212 1.890599 Model 4 12.609626 12.198187 13.090003 12.987817 Solution (c). In general, it is better to have as small of a test error as possible. It it however also important not to overcomplicate the model if not all variables is relevant. Ridge regression does well in the models with fewer explanatory variable \\(s\\) as it punishes under \\(L^2\\) distance rather than \\(L^1\\). This means that ridge does a better job in setting \\(\\beta_j\\approx 0\\) for \\(j&gt; s\\). "],["week-3-1.html", "3.3 Week 3", " 3.3 Week 3 Exercise 1. Random forests are known for their computational speed and can be used in settings with thousands of features. Here we discuss how it is usually decided where to split a node. Assume we have n observations at a node, and want to find the optimal split. The mtry provides us with a list of possible features to split. Fix one feature and assume that observations in that node are sorted according to the value of that feature. In the current node the squared loss is \\[ Q=\\sum_{i=1}^n(x_i-\\overline x)^2=\\sum_{i=1}^nx_i^2-n\\overline{x}^2. \\] Define \\[ Q_L(k)=\\sum_{i=1}^k(x_i-\\overline{x}_k^+)^2,\\qquad Q_R(k)=\\sum_{i=k+1}^n(x_i-\\overline{x}_k^-)^2 \\] where \\[ \\overline{x}_k^+=\\frac{1}{k}\\sum_{i=1}^kx_i\\quad \\text{and}\\quad \\overline{x}_k^-=\\frac{1}{n-k}\\sum_{i=k+1}^nx_i. \\] The node is split at that \\(k\\) that minimizes \\(Q_L(k)+Q_R(k)\\). Discuss how the optimal \\(k\\) can be found more efficiently than calculating for every \\(k\\) \\(Q_L(k)+Q_R(k)\\) from scratch. Solution. Exercise 2. We extend the framework from previous week and now also consider classification tasks. When considering a classification task, the only change in the data generation compared to the regression setting is that we generate the response \\(Y\\) via \\[ Y=\\frac{\\text{sign}(m(X_i)+\\varepsilon_i)+1}{2}. \\] Try out least squares regression and classification with least squares and binary loss. As learners, we will work with linear regression/logistic regression (with e.g. elastic net penalization), generalized additive models from the mgcv package and random forest via the ranger package. Part of the task is to figure out how to optimize the (penalty) parameters to be successful in the sparse settings as given below. We will consider the following models. n p s rho m \\(\\varepsilon\\) Model 1 1000 100 5 0.3 \\(\\sum_{j=1}^s x_j\\) \\(\\mathcal N(0,1)\\) Model 3 1000 100 5 0.3 \\(\\sum_{j=1}^s 0.1x_j\\) \\(\\mathcal N(0,1)\\) Model 5 1000 100 5 0.3 \\(\\sum_{j=1}^s m_j(x_j)\\) \\(\\mathcal N(0,1)\\) Model 6 1000 100 5 0.3 \\(\\sum_{j=1}^s m_j(x_j)+\\sum_{j=1}^{s-1} m_j(x_jx_{j+1})\\) \\(\\mathcal N(0,1)\\) Here \\(m_j(x_j)=(-1)^j2\\sin(\\pi x_j)\\) For each of the three settings (regression: squared loss, classification: squared loss, classification: binary loss), given one set of training data for each model tune (i.e. estimate optimal hyperprameter) via 5-fold cross-validation using a search method and number of evals of your choice. You should now have an estimated optimal hyperparameter for every combination of setting, model and learner. Generate 100 test sets each of size n for every setting, model and learner and calculate empirical mean and standard deviation of the test error for each setting, model and learner using the hyperparameters calculated in (a). Create three tables ofor your results (each missing entry should show hyperparameter: mean(sd)): Linear model glm Random forest Model 1 \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) Model 3 \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) Model 5 \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) Model 6 \\(\\cdot\\) \\(\\cdot\\) \\(\\cdot\\) Solution (a). Solution (b). "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
