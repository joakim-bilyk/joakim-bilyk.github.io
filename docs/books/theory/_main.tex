% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
% INSERT PACKAGES
\usepackage{imakeidx}
\makeindex
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{wrapfig}
\usepackage{graphics}
\usepackage{titling}
\usepackage{fancyhdr}
\pagenumbering{gobble}
\addtocontents{toc}{\protect\thispagestyle{empty}}
\addtocontents{title}{\protect\thispagestyle{empty}}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{subfig}
\usepackage{float}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Complete Theory},
  pdfauthor={Joakim Bilyk},
  pdfkeywords={probability theory, insurance mathematics, life insurance,
non-life insurance, stochastic differential equations.},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Complete Theory}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Msc in Actuarial Mathematics}
\author{Joakim Bilyk}
\date{February 07, 2023}

\begin{document}
%\maketitle


{
%title
\begin{titlepage}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE University of Copenhagen}\\[4cm] % Name of your university/college
\textsc{\Large Msc in Actuarial Mathematics}\\[0.5cm] % Major heading such as course name
%\textsc{\large Assignment 1}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries \thetitle}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
\textsc{\theauthor} \\
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Date:} \\
\textsc{\thedate} \\
\end{flushright}
\end{minipage}\\[2cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

%{\large \thedate}\\[2cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

\includegraphics[width = 0.45\textwidth]{logo_ku.png}% Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace
\end{titlepage}

\thispagestyle{empty}
\begin{center}
\textbf{\large Abstract}
\end{center}

This document contain a comprehensive outline of theory on probability theory and mathematical statistics applied in finance, life insurance and non-life insurance.

Keywords: \emph{probability theory, insurance mathematics, life insurance,
non-life insurance, stochastic differential equations.}
\vfill
\pagebreak

\setcounter{tocdepth}{3}
\tableofcontents
}
\thispagestyle{empty}
\newpage
\setcounter{page}{1}
\pagenumbering{arabic}
\pagestyle{fancy}
\hypertarget{references}{%
\chapter{References}\label{references}}

Below is given the abbreviations used when referencing to books:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3235}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1471}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5294}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Chapter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Abbreviation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} \\
\midrule()
\endhead
Finance & Bjork & \emph{Arbitrage Theory in Continuous Time (Fourth edition)} by Thomas Bjork, Oxford University Press (2019). \\
Measure Theory & Bjork & \emph{Arbitrage Theory in Continuous Time (Fourth edition)} by Thomas Bjork, Oxford University Press (2019). \\
Linear Algebra & Wiki & Wikipedia \\
Random Variables & Bjork & \emph{Arbitrage Theory in Continuous Time (Fourth edition)} by Thomas Bjork, Oxford University Press (2019). \\
& Hansen & \emph{Stochastic Processes} (2. edition) by Ernst Hansen (2021). \\
Continuous Time Stochastic Processes & Bjork & \emph{Arbitrage Theory in Continuous Time (Fourth edition)} by Thomas Bjork, Oxford University Press (2019). \\
Discrete Time Stochastic Processes & Hansen & \emph{Stochastic Processes} (2. edition) by Ernst Hansen (2021). \\
\bottomrule()
\end{longtable}

\hypertarget{life-insurance-mathematics}{%
\chapter{Life Insurance Mathematics}\label{life-insurance-mathematics}}

Noget indhold

\hypertarget{finance}{%
\chapter{Finance}\label{finance}}

This topic revolves around the theory of the Brownian motion and martingale processes. Other main topics are the binomial model and an introduction to financial derivatives. Financial derivatives is contingent on the outcome of a stochastic process at some future time \(t=T\) and often is a function \(\Phi\) of some assets price \(S_t\). As such the derivative will give a stochastic payout, at time \(t=T\) of the size \(X_T=\Phi(S_T)\). Naturally we want to say something about the \emph{fair} price of the derivative in the form of

\[\Pi_t(X_T)=\mathbb{E}\left[\Phi(S_T)\ \vert\ \mathcal{F}_t\right],\]

where \(\mathcal{F}_t\subset\mathcal{F}\) is the available information at time \(t\). We will by defualt intepret the times \(t=0\) as \emph{today} and \(t=T\) as \emph{tomorrow}. This indeed require some fundamental understanding of the behaviour of the asset price \(S_t\). This lead us over to discussing the process in center of the \emph{Black-Scholes} model: the Brownian motion.

\hypertarget{discrete-time-models}{%
\section{Discrete time models}\label{discrete-time-models}}

\hypertarget{one-period-time-models}{%
\subsection{One-period time models}\label{one-period-time-models}}

The study of this course is the \textbf{European call} option\index{European call option} (and \emph{put} option). This financial derivative\index{financial derivative} is an agreement between two parties where the holder of the option has the right to \emph{``exercise''} the derivative, at a future time \(t=T\). Exercising means buying an asset at a certain agreed opon price-strike \(K\). In the case of the put-option: the holder has the right (but not obligation) to sell the asset at the strike price \(K\). As such the derivative has the payoff

\[\text{Call}\ \text{option:}\hspace{10pt}\Phi(S_T)=(S_T-K)^+,\hspace{20pt}\text{Put}\ \text{option:}\hspace{10pt}\Phi(S_T)=(K-S_T)^+.\]

Our objective is to understand when an arbitrage exist and to find the fair price of these derivative. The strategy in pricing is finding a replicating portfolio with the same payoff as the option (with probability one) and then price the derivative accordingly.

\hypertarget{model-description}{%
\subsubsection{Model description}\label{model-description}}

In the one-period model we consider the simplest possible market. We have two distinct times \(t=0\) (today) and \(t=1\) (tomorrow) and we may buy any portfolio as a mixture of bonds and one stock. We denote the bonds price by \(B_t\) and the stocks price by \(S_t\) and we assume the following:

\[
B_0=1,\ B_1=1+R,\hspace{20pt}S_0=s,\ S_1=\left\{\begin{matrix}s\cdot u, & with\ probability\ p_u.\\s\cdot d, & with\ probability\ p_d.\end{matrix}\right.
\]

We may introduce \(Z\) as the random variable

\[
Z=u\cdot (I)+d\cdot (1-I),
\]

for an bernoulli variable \(I\) with succes probability \(p_u\). Naturally, we assume \(d\le (1+R)\le u\) (this is imperative to ensure no arbitrage as we will see).

\hypertarget{portfolios-and-arbirtage}{%
\subsubsection{Portfolios and arbirtage}\label{portfolios-and-arbirtage}}

We study any portfolio on the \((B,S)\) market as a vector \(h=(x,y)\) where \(x\) is the amount of bonds and \(y\) is the amount of stock held in the portfolio. Notice that we allow for shorting, that is \(x<0\) or \(y<0\). As such, we have that \(h\in \mathbb{R}^2\). In this we have made some unrealistic, but attractable assumptions included in the assumptions:

\begin{itemize}
\tightlist
\item
  We allow short positions and fractional holding, i.e.~\(h\in \mathbb{R}^2\),
\item
  We assume no spread between ask and bids,
\item
  No transaction costs and
\item
  A completely liquid market i.e.~we may borrow and buy as much stock and bonds as wanted.
\end{itemize}

Given that we have chosen a portfolio \(h\) we may introduce the value process.

\textbf{Definition 2.1. (Bjork)} \emph{The \textbf{value process}\index{value process} of the porfolio \(h\in\mathbb{R}^2\) is the stochastic process}

\[V^h_t=xB_t+yS_t,\ t=0,1.\]

Given this notation we may define what an arbitrage is.

\textbf{Definition 2.2. (Bjork)} \emph{An \textbf{arbitrage}\index{arbitrage} is a portfolio \(h\) with the properties: 1) \(V^h_0=0\), 2) \(P(V^h_1\ge 0)=1\) and 3) \(P(V^h_1>0)>0\).}

That is \(h\) is an deterministic money-machine where we at least never loose any money. Granted the bonds give a determinictic non-negative return, but an arbitrage does not require any money out of pocket. With the notion of an arbitrage we will show the first proposition regarding the choice of \(R,u,d\) as defined above.

\textbf{Proposition 2.3. (Bjork)} \emph{The one-period binomial model\index{one-period binomial model} is arbitrage free if and only if the following inequality hold:}

\[d\le (1+R)\le u.\tag{2.1}\]

\noindent\makebox[\linewidth]{\rule{\textwidth}{0.4pt}}

\textbf{Proof.}

The statement is proofed by contradiction. Assume that \(d>1+R\) holds. Then by definition \(u>d>1+R\). Notice that any portfolio satisfying \(V_0^h=0\) must satisfy

\[0=xB_0+yS_0=x+ys\iff x=-ys\]

That is for some choice \(y\) the only arbitrage candidate is the portfolio \(h=(-ys,y)\). Calculating the value at time \(t=1\) we have

\[V_1^h=-ys\cdot(1+R)+y\cdot s\cdot Z=ys(Z-1-R)\]

However since \(Z\ge d\) we have \(Z-(1+R)\ge 0\) and therefore an arbitrage (for \(y>0\)). The other inequality \(1+R>u\) follows analog steps. Simply choose some \(y<0\) and the result follows. \(\blacksquare\)

\noindent\makebox[\linewidth]{\rule{\textwidth}{0.4pt}}
From inequality (2.1) we see that since \(1+R\) is between \(u\) and \(d\) we may find a pair \(q_d,q_u\ge 0\) with \(q_d+q_u=1\) such that

\[1+R=q_u\cdot u+q_d\cdot d.\]

This yields the important risk neutral valuation formula as summed op in the following definition

\textbf{Definition 2.4. (Bjork)} \emph{A probability measure \(Q\) is called a \textbf{martingale meausre}\index{martingale meausre} if the following condition holds:}

\[S_0=\frac{1}{1+R}E^Q[S_1].\]

The above measure \(Q\) is the measure \(Q(Z=d)=q_d\) and \(Q(Z=u)=q_u\) for the binomial model. This does in fact yield the risk neautral valuation formula:
\begin{align*}
S_0&=\frac{1}{1+R}E^Q[S_1]=\frac{1}{1+R}(Q(Z=d)\cdot d\cdot s+Q(Z=u)\cdot u\cdot s)\\
&=s\frac{1}{1+R}(q_d\cdot d+q_u\cdot u)=s,
\end{align*}
where we simply use \(1+R=q_d\cdot d+q_u\cdot u\). We call this the risk neautral valuation formula because it in some sense gives an expected discounted value of the future stock price. We end this endavour with reformulating the arbitrage proposition and determining the values of the \(Q\)-measure.

\textbf{Proposition 2.5. (Bjork)} \emph{The one-period binomial model is arbitrage free if and only if there exists a martingale measure \(Q\).}

\textbf{Proposition 2.6. (Bjork)} \emph{The one-period binomial model has martingale probabilities given by:}

\[\left\{\begin{matrix}q_u=\frac{(1+R)-d}{u-d},\\ q_u=\frac{u-(1+R)}{u-d}.\end{matrix}\right.\]

\hypertarget{contingent-claims}{%
\subsubsection{Contingent Claims}\label{contingent-claims}}

This chapter revolves around the financial derivative and we start by stating the definition of the financial derivative.

\textbf{Definition 2.7. (Bjork)} \emph{A \textbf{contingent claim}\index{contingent claim} (financial derivative)\index{financial derivative} is \emph{any} stochastic variable \(X\) of the form \(\Phi(Z)\), where \(Z\) is the stochastic varible driving the stock price process.}

We may also call the function \(\Phi\) the \textbf{contract function}\index{contract function} as it states how the contract is resolved once the stochastic variable \(Z\) has been realised. Our objective is now to study, what a buyer of said contract would have to pay at any given time \(t\). We call the fair price of \(X\) at time \(t\): \(\Pi_t[X]\). As such it is easy to see that the fair price at the time of maturity \(T\) is simply the payout \(X\) i.e.~\(\Pi_T[X]=X\). Our strategy is to find a replicating portfolio \(h\) and determine the price of said portfolio.

\textbf{Definition 2.8. (Bjork)} \emph{A contingent claim \(X\) can be \textbf{replicated}\index{replicated}, or said to be \textbf{reachable}\index{reachable} if there exist a portfolio \(h\) such that}

\[
V_1^h=X,
\]

\emph{with probability one. In that case, we say that the portfolio \(h\) is a \textbf{hedging} portfolio\index{hedging portfolio} or a \textbf{replicating} portfolio.\index{replicating portfolio} If all claims can be replicated we say that the market is \textbf{complete}\index{complete market}.}

Our pricing strategy is then to determine the value process of the replicating portfolio and then by the first pricing principle below we say that the price is imply the value of the replicating portfolio.

\textbf{Pricing principle 1.} If a clain \(X\) is reachable with replicating portfolio \(h\), then the only reasonable price process for \(X\) is given by

\[
\Pi_t[X]=V_t^h.
\]

Notice, that this assumes that a replicating portfolio exist and even so we have a uniqueness statement to solve. We end this section by writing two important results.

\textbf{Proposition 2.9. (Bjork)} \emph{Suppose that a claim \(X\) is reachable with replicating portfolio \(h\). Then any price at time \(t\ge 0\) of the claim \(X\) other than the value process of \(h\) will lead to an arbitrage on the extended market \((B,S,X)\).}

\textbf{Proposition 2.10. (Bjork)} \emph{If the one-period binomial model is free of arbitrage, then it is also complete.}

The hedging portfolio in the one-period binomial model is given by the portfolio \((x,y)\) below
\begin{align*}
x&=\frac{1}{1+R}\cdot\frac{u\Phi(d)-d\Phi(u)}{u-d},\tag{2.2}\\
y&=\frac{1}{s}\cdot\frac{\Phi(u)-\Phi(d)}{u-d}.\tag{2.3}
\end{align*}

\hypertarget{risk-neutral-valuation}{%
\subsubsection{Risk Neutral Valuation}\label{risk-neutral-valuation}}

We see that since the one-period model is complete we can price any contingent claim and we see that
\begin{align*}
\Pi_0[X]&=\frac{1}{1+R}\cdot\frac{u\Phi(d)-d\Phi(u)}{u-d}+s\frac{1}{s}\cdot\frac{\Phi(u)-\Phi(d)}{u-d}\\
&=\frac{1}{1+R}\left\{\frac{u\Phi(d)-d\Phi(u)}{u-d}+(1+R)\frac{\Phi(u)-\Phi(d)}{u-d}\right\}\\
&=\frac{1}{1+R}\left\{\frac{(1+R)-d}{u-d}\Phi(u)+\frac{u-(1+R)}{u-d}\Phi(d)\right\}\\
&=\frac{1}{1+R}E^Q[X].
\end{align*}
i.e.~the price at time \(t=0\) should simply be the expected discounted payout according to the martingale measure. This leads to the important pricing proposition:\index{risk-neutral valueation formula}

\textbf{Proposition 2.11. (Bjork)} \emph{If the one-period binomial model is free of arbitrage, then the arbitrage free price of a contingent claim \(X\) is given by}

\[
\Pi_0[X]=\frac{1}{1+R}E^Q[X].\tag{2.4}
\]

\emph{Here the martingale measure \(Q\) is uniquely determined by the relation}

\[
S_0=\frac{1}{1+R}E^Q[S_1],\tag{2.5}
\]

and the explicit expressions for \(q_u\) and \(q_d\) are given in proposition 2.6. Furthermore the claim \(X\) can be replicated using the portfolio
\begin{align*}
x&=\frac{1}{1+R}\cdot\frac{u\Phi(d)-d\Phi(u)}{u-d},\tag{2.6}\\
y&=\frac{1}{s}\cdot\frac{\Phi(u)-\Phi(d)}{u-d}.\tag{2.7}
\end{align*}

\newpage

\hypertarget{multi-period-model}{%
\subsection{Multi-period model}\label{multi-period-model}}

The one-period binomial model can easily be extended to a multi-period model,\index{multi-period model} by assuming that the bond and stock pricess evolve by the processes:

\[
t\ge1:\ B_t=(1+R)B_{t-1}\hspace{20pt}\text{and}\hspace{20pt}B_0=1,
\]

\[
t\ge1:\ S_t=Z_{t-1}S_{t-1}\hspace{20pt}\text{and}\hspace{20pt}S_0=s,
\]

where we obviously have that \(B_t=(1+R)^t\) for \(t\ge 0\). In the above \(Z_t\) is \(u\) with probability \(p_u\) and \(d\) with probability \(p_d\). In this context, we need to define a portfolio in terms of a strategy.

\textbf{Definition 2.13. (Bjork)} \emph{A \textbf{portfolio strategy}\index{portfolio strategy} is a stochastic process on \(\{1,...,T\}\)}

\[
h=\left\{h_t=(x_t,y_t);\ t=1,...,T\right\}
\]

\emph{such that \(h_t\) is a function of \(S_0,S_1,...,S_{t-1}\). For a given portfolio strategy \(h\) we set \(h_0=h_1\) by convention. The associated \textbf{value process}\index{value process} corresponding to the portfolio \(h\) is defined by}

\[
V_t^h=x_t(1+R)+y_tS_t.
\]

Given this notation we may define what an arbitrage is, but first we introduce the notion of a self-financing portfolio. A self-financing portfolio in an intuative sense is a portfolio that is not withdrawn from or deposited into.

\textbf{Definition 2.14. (Bjork)} \emph{A portfolio strategy \(h\) is said to be \textbf{self-financing}\index{self-financing portfolio} if the following condition holds for all \(t=0,...,T-1\):}

\[
x_t(1+R)+y_tS_t=x_{t+1}+y_{t+1}S_t.
\]

The above equation says that the portfolio purchased at time \(t\) and helt until \(t+1\) \((x_{t+1},y_{t+1})\) can only be financed by the market value of the portfolio held from \([t-1,t)\) i.e.~\((x_{t},y_{t})\). We now define an arbitrage.

\textbf{Definition 2.15. (Bjork)} \emph{An \textbf{arbitrage} is a self-financing portfolio \(h\) with the properties: 1) \(V^h_0=0\), 2) \(P(V^h_T\ge 0)=1\) and 3) \(P(V^h_T>0)>0\).}

The multiperiod binomial model has an just like the oneperiod model a result regarding when an arbitrage exists.

\textbf{Lemma 2.16. (Bjork)} \emph{If \(d\le (1+R)\le u\) (eq. 2.8) then the multiperiod model is arbitrage-free.}

As one can see, the multiperiod model is rather similar to the one period model. We wil in the following summarise equivalent statements for the multiperiod model as the ones in the oneperiod model.

\textbf{Definition 2.17. (Bjork)} \emph{The martingale probabilities \(q_u\) and \(q_d\) are defined as the probabilities for which the relation below holds.}

\[
s=\frac{1}{1+R}E^Q[S_{t+1}\ \vert\ S_t].
\]

\textbf{Proposition 2.18. (Bjork)} \emph{The martingale probabilities \(q_u\) and \(q_d\) are given by}

\[
\left\{\begin{matrix}q_u=\frac{(1+R)-d}{u-d},\\ q_u=\frac{u-(1+R)}{u-d}.\end{matrix}\right.
\]

\textbf{Definition 2.19. (Bjork)} \emph{A \textbf{contingent claim}\index{contingent claim} is a stochastic variable \(X\) of the form}

\[
X=\Phi(S_T),
\]

\emph{where the \textbf{contract function}\index{contract function} \(\mathbf{\Phi}\) is some given real valued function.}

\textbf{Definition 2.20. (Bjork)} \emph{A given contingent claim \(X\) is said to be \textbf{reachable}\index{reachable} if there exists a self-financing portfolio \(h\) such that}

\[
V_T^h=X,
\]

\emph{with probability one. In that case we say that the portfolio \(h\) is a \textbf{hedging} portfolio\index{hedging portfolio} or a \textbf{replicating} portfolio\index{replicating portfolio}. If all claims can be replicated we say that the market is \emph{(dynamically)} \textbf{complete}\index{complete market}.}

\textbf{Pricing principle 2. (Bjork)} \emph{If a claim \(X\) is reachable with replicating portfolio \(h\), then the only reasonable price process for \(X\) os given by}

\[
\Pi_t[X]=V_t^h,\ t=0,1,...,T.
\]

\textbf{Proposition 2.21. (Bjork)} \emph{Assume \(X\) is reachable by \(h\), then any price other than \(V_t^h\) for some \(t\ge 0\) leads to an arbitrage opportunity.}

\textbf{Proposition 2.22. (Bjork)} \emph{The multiperiod model is complete, i.e.~every claim can be replicated by a self-financing portfolio.}

\textbf{Proposition 2.24. (Bjork)} \emph{\textbf{(Binomial algorithm)}\index{Binomial algorithm} Consider a \(T\)-claim \(X=\Phi(S_T)\). Then this claim can be replicated using af self-financing portfolio. If \(V_t(k)\) denotes the value of the portfolio at the node \((t,k)\) (\(k\) referring to \(k\) amount of up-moves for the stock), then \(V_t(k)\) can be computed recursively by the scheme}

\[
\left\{\begin{matrix}V_t(k)=\frac{1}{1+R}\left\{q_uV_{t+1}(k+1)+q_dV_{t+1}(k)\right\},\\ V_T(k)=\Phi(su^kd^{T-k}).\end{matrix}\right.
\]

\emph{where the martingale probabilities \(q_u\) and \(q_d\) are given by}

\[
\left\{\begin{matrix}q_u=\frac{(1+R)-d}{u-d},\\ q_u=\frac{u-(1+R)}{u-d}.\end{matrix}\right.
\]

\emph{With the notation as above, the hedging portfolio is given by}

\[
\left\{\begin{matrix}x_t(k)=\frac{1}{1+R}\cdot\frac{uV_t(k)-dV_t(k+1)}{u-d},\\ y_t(k)=\frac{1}{S_{t-1}}\cdot\frac{V_t(k+1)-V_t(k)}{u-d}.\end{matrix}\right.
\]

\emph{In particular, the arbitrage free price of the claim at \(t=0\) is given by \(V_0(0)\).}

\textbf{Example.}

\begin{center}\includegraphics[width=0.75\linewidth]{_main_files/figure-latex/unnamed-chunk-1-1} \end{center}

Consider \(R=0.04\), \(s=100\), \(u=1.1\), \(d=0.9\), \(p_u=0.6\) and \(p_d=0.4\). We consider a model of length \(T=2\) and we want to evaluate the price of the european call option with srike \(K=90\) that is the contingent claim

\[
X=(S_T-K)^+,\hspace{20pt}\Phi(s)=(s-K)^+.
\]

For each time \(t\) we know the replicating portfolio, if we know the payoff the following period. Therefore we start from the leaves of the tree and work towards the root. Since the strike price is \(K=90\) the end result will be the following payoffs:
\begin{align*}
u^2:\hspace{20pt}&(121-90)^+=31\\
ud:\hspace{20pt}&(99-90)^+=9\\
du:\hspace{20pt}&(99-90)^+=9\\
d^2:\hspace{20pt}&(81-90)^+=0
\end{align*}
Therefore by the risk neautral valuation formula with \(q_u=\frac{(1+R)-d}{u-d}=0.7\) and \(q_d=\frac{u-(1+R)}{u-d}=0.3\) we have that the cost of the replicating portfolio at time \(t=1\) is respectively
\begin{align*}
u:\hspace{20pt}&\frac{1}{1+R}\left\{31\cdot q_u + 9 \cdot q_d\right\}\approx 23.46\\
d:\hspace{20pt}&\frac{1}{1+R}\left\{9\cdot q_u + 0 \cdot q_d\right\}\approx 6.06
\end{align*}
To replicate this payoff at time \(t=1\) we can use the risk neutral valuation formula once more to find the base cost of the replicating portfolio i.e.~the price of \(X\) at time \(t=0\)

\[
\frac{1}{1+R}\left\{23.46\cdot q_u + 6.06 \cdot q_d\right\}\approx 17.54.
\]

Working from the root to the leaves we can now calculate the hedging portfolio at time \(t=0,1\) for each path. For time \(t=0\) we calculate
\begin{align*}
x=&\frac{1}{1+R}\cdot \frac{u\cdot 6.06-d\cdot 23.46}{u-d}\approx -69.46,\\
y=&\frac{1}{s}\cdot\frac{23.46-6.06}{u-d}\approx0.87
\end{align*}
We see by calculations that this does indeed replicate the payoff at time \(t=1\):
\begin{align*}
u:\hspace{20pt}&V_1^h=(1+R)\cdot x + 110\cdot y\approx 23.46,\\
d:\hspace{20pt}&V_1^h=(1+R)\cdot x + 90\cdot y\approx 6.06.
\end{align*}
We also see by calculation that the initial portfolio does cost the expected 17.54 as

\[
x\cdot 1+y\cdot100=87-69.46=17.54.
\]

Following these steps at time \(t=1\) the portfolios \((-86.54,1)\) (for the up-scenario) and \((-38.94,0.5)\) (for the down-scenario) would arise. Notice when calculating \(y\) one has to use the current price \(S_1=S_0\cdot Z\) not \(S_0\). One should also check by similar calculations as above, that these portfolios does indeed replicate the payoff of the contingent claim \(X\). \(\square\)

\textbf{Proposition 2.25. (Bjork)} \emph{The arbitrage free price at \(t=0\) of a \(T\)-claim \(X\) is given by}

\[
\Pi_0[X]=\frac{1}{(1+R)^T}E^Q[X]
\]

\emph{where \(Q\) denotes the martingale measure, or more explicitly}

\[
\Pi_0[X]=\frac{1}{(1+R)^T}\sum_{k=0}^T\binom{T}{k}q_u^kq_d^{T-k}\Phi(su^kd^{T-k}).
\]

\textbf{Example.}

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.75\textwidth]{figures/BS_call_price.png}
  \end{center}
  \caption{The pricing function of the European call option.}
\end{figure}

We follow an analog example as the one after proposition 2.24. Let \(K=90\) and we see that
\begin{align*}
&\Pi_0[X]\\
&=\frac{1}{(1+0.04)^2}\sum_{k=0}^2\binom{2}{k}\cdot0.7^k\cdot0.3^{2-k}\cdot\Phi(100\cdot 1.1^k\cdot0.9^{2-k})\\
&=0.9245562\cdot\left(\underbrace{1\cdot 1\cdot0.09\cdot0}_{k=0}+\underbrace{2\cdot 0.7\cdot0.
3\cdot 9}_{k=1}+\underbrace{1\cdot 0.49\cdot1\cdot31}_{k=2}\right)\\
&=0.9245562\cdot\left(0+3.78+15.19\right)\\
&=17.53883
\end{align*}
Since we know that \(K\) must meaningfully range in \([0,121]\) we could try to calculate the price of the contingent claim at time \(t=0\) for all integers in this interval. We see that the price range between \(S_0\) and 0 as expected. One can also see that the price changes slope at the prices 99 and 121 as the function is linear in \(\Phi\) and som realisations loose any effect on the price when the strike is higher than the outcome. \(\square\)

\textbf{Proposition 2.26. (Bjork)} \emph{The condition \(d<(1+R)<u\) is necessary and sufficient condition for absence of arbitrage.}

\newpage

\hypertarget{generelised-one-period-model}{%
\subsection{Generelised one-period model}\label{generelised-one-period-model}}

In the previous we had the simpel model where we only had one stochastic asset \(S\) and only one stochastic variable \(Z\) determining the future stock price. Now we will generelise this model by introducing \(N\) assets and introducing som stochastic behaviour to the system.

\hypertarget{model-specification}{%
\subsubsection{Model specification}\label{model-specification}}

We consider the market consisting of a collection of stochastic prices assets \(i=1,...,N\) with \(N\)-dimensional price process.

\[
S_t=\begin{bmatrix} S_t^1\\
\vdots\\
S_t^N\end{bmatrix}
\]

We now assume that \(S_t\) is defined on a background space with finite sample space \(\Omega = \{\omega_1,...,\omega_M\}\) with associated probabilities \(p_j=P(\omega_j)\), \(j=1,...,M\). We can then for eact time \(t=1,...,T\) define the \(N\times M\) matrix \(D_t\) as such

\[
D_t=\begin{bmatrix} S_t^1(\omega_1)&\cdots &S_t^1(\omega_M)\\
\vdots &\ddots & \vdots\\
S_t^N(\omega_1) &\cdots&S_t^M(\omega_M)\end{bmatrix}.
\]

We will assume that \(S_0^1>0\) and \(S_1^1(\omega_j)>0\), \(j=1,...,M\).

\hypertarget{absence-of-arbitrage}{%
\subsubsection{Absence of Arbitrage}\label{absence-of-arbitrage}}

We now define a \textbf{portfolio}\index{portfolio} as an \(N\)-dimensional row vector

\[
h=\begin{bmatrix} h^1, \dots,h^N\end{bmatrix}
\]

representing the amount of assets held at time \(t=0\) and held until \(t=1\). The \textbf{value process} is then

\[
V^h_t=h\cdot S_t=\sum_{i=1}^N h^iS_t^i,\ t=0,1.\tag{3.1}
\]

For a given \(\omega_j\in\Omega\) we have the realisation

\[
V_t^h=hS_t(\omega_j)=hd_j=(hD)_j.
\]

\textbf{Definition 3.1. (Bjork)} \emph{The portfolio \(h\) is an \textbf{arbitrage portfolio}\index{arbitrage portfolio} fil it satisfies the conditions: \(V_0^h=0\), \(P(V_1^h\ge 0)=1\) and \(P(V_1^h>0)>0\).}

\textbf{Lemma 3.2. (Bjork)} \emph{\textbf{(Farkas' Lemma)}\index{Farkas' Lemma} Suppose that \(d_0,d_1,...,d_M\) are column vectors in \(\mathbb{R}^N\). Then exactly one of the following problems possesses a solution.}

\begin{itemize}
\tightlist
\item
  \textbf{Problem 1}: \emph{There exist \(\lambda_1,...,\lambda_M\ge0\) such that \(d_0=\sum_{j=1}^M\lambda_jd_j\).}
\item
  \textbf{Problem 2}: \emph{There exist \(h\in\mathbb{R}^N\) such that \(h^\top d_0<0\) and \(h^\top d_j\ge 0\) for \(j=1,...,M\).}
\end{itemize}

We now investegate this system for any possible arbitrage portfolios. However first we acknowledge that there exist a nominal price system \(S_t\) and a normalised price system \(Z_t\). The latter we define as the nominel pricess under the numeraire \(S_t^1\) that is

\[
Z_t=\begin{bmatrix} S_t^1/S_t^1\\
S_t^2/S_t^1\\
\vdots\\
S_t^N/S_t^1\end{bmatrix}=\begin{bmatrix} 1\\
S_t^2/S_t^1\\
\vdots\\
S_t^N/S_t^1\end{bmatrix}.
\]

The reason for introducing the normalized price system is that we can without much effort translate results in this system to the nominal system and the normalised system is easier to analize. For this, however, we need af few results.

\textbf{Lemma 3.3. (Bjork)} \emph{With notation as above, the following hold.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{The \(Z_t\) value process i related to the \(S_t\) value process by}
  \[
    V_t^{h,Z}=hZ_t=\frac{1}{S_t^1}V_t^h.
    \]
\item
  \emph{A portfolio is an arbitrage in the \(S_t\) system if and only if there is an arbitrage in the \(Z_t\) system.}
\item
  \emph{In the \(Z_t\) price system, the numeraie asset \(Z^1\)\index{numeraie asset} has unit constant prices i.e.~\(Z_t^1=1\) for all \(t\ge 0\).}
\end{enumerate}

One of the reason that the normalised system is attractable is that the numeraire asset is constant i.e.~risk free in the normalised system. Let us formulate our first main result.

\textbf{Proposition 3.4. (Bjork)} \emph{The market is arbitrage free if and only if there exists strictly positive real numbers \(q_1,...,q_M\ge 0\) with \(q_1+\cdots + q_M=1\) (eq. 3.2) (probability vector) such that the following vector equality holds}

\[
\begin{bmatrix} Z_0^1\\
\vdots\\
Z_N^1\end{bmatrix}=\begin{bmatrix} Z_1^1(\omega_1)\\
\vdots\\
Z_1^N(\omega_1)\end{bmatrix}q_1+\cdots +\begin{bmatrix} Z_1^1(\omega_M)\\
\vdots\\
Z_1^N(\omega_M)\end{bmatrix}q_M.\tag{3.3}
\]

\hypertarget{martingale-measures}{%
\subsubsection{Martingale Measures}\label{martingale-measures}}

\textbf{Definition 3.5. (Bjork)} \emph{Given the objective probability measure \(P\) on \((\Omega,\mathcal{F},P)\), we say that another probability measure \(Q\) defined on \(\Omega\) is \textbf{equivalent}\index{equivalent measure} to \(P\) if}

\[
\forall A\in\mathcal{F}:P(A)=0\iff Q(A)=0,
\]

or equivalently

\[
\forall A\in\mathcal{F}:P(A)=1\iff Q(A)=1.
\]

\textbf{Definition 3.7. (Bjork)} \emph{Consider the market model above and set \(S^1\) as the numeraire asset. We say that a probability measure \(Q\) defined on \(\Omega\) is a \textbf{martingale measure}\index{martingale measure} if it satisfies the following conditions:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{\(Q\) is equivalent to \(P\), i.e.~\(Q\sim P\).}
\item
  \emph{For every \(i=1,...,N\), the normalized asset price process}
  \[
    Z_t^i=\frac{S_t^i}{S_t^1},
    \]
  \emph{is martingale under the measure \(Q\).}
\end{enumerate}

\textbf{Theorem 3.8. (Bjork)} \textbf{(First Fundamental Theorem)}\index{First Fundamental Theorem} \emph{Given a fixed numeraire, ther market is free of arbitrage possibilities if and only if there exists a martingale measure \(Q\).}

By assuming that the numeraire asset is risk free (i.e.~does not depend on \(\omega\)) then by scaling we can derive the short interest rate as

\[
1+R=\frac{S_1^1}{S_0^1}.
\]

With this in mind we can formulate theorem 3.8 in its more widely used form.

\textbf{Theorem 3.9. (Bjork)} \textbf{(First Fundamental Theorem)} \emph{Assume that there exist a risk free asset, and denote the corresponding risk free interest rate by \(R\). Then the market is arbitrage free if and only if there exist a measure \(Q\sim P\) such that}

\[
S_0^i=\frac{1}{1+R}E^Q[S_1^i],\hspace{20pt}\text{for all}\ i=1,...,N.\tag{3.9}
\]

\hypertarget{martingale-pricing}{%
\subsubsection{Martingale Pricing}\label{martingale-pricing}}

Moving forward we will assume that there exist a risk free asset and we will denote it by \(B_t\) (\(B_t=S^1_t/S^1_0\)).

\textbf{Definition 3.10. (Bjork)} \emph{A \textbf{contingent claim}\index{contingent claim} is any random variable \(X\), defined on the sample space \(\Omega\).}

To ensure no arbitrage in the extended market containing the \(N\) assets and the contingent claim we can apply the first fundamental pricing theorem on the extended market.

\textbf{Proposition 3.11. (Bjork)} \emph{Consider a given claim \(X\). In order to avoid arbitrage, \(X\) must then be priced according to the formula}

\[
\Pi_0[X]=\frac{1}{1+R}E^Q[X],\tag{3.10}
\]

\emph{where \(Q\) is a martingale measure for the underlying market \((\Pi,S^1,...,S^N)\).}

\hypertarget{completeness}{%
\subsubsection{Completeness}\label{completeness}}

Given that a market is arbitrage-free we may run into a uniqueness issue when determining the price of a contingent claim. If a martingale measure exist we will very much like it to be unique as this will ensure that the price from the risk neutral valuation formula is unique. To this we need the market to be complete.

\textbf{Definition 3.12. (Bjork)} \emph{Consider a contingent claim \(X\). If there exists a portfolio \(h\), based on the underlying assets, such that}

\[
V_1^h=X,\ \text{with probability 1}\tag{3.11}
\]

\emph{i.e.}

\[
V_1^h(\omega_j)=X(\omega_j),\ j=1,...,M,\tag{3.12}
\]

\emph{then we say that \(X\) is \textbf{replicated}\index{replicated}, or \textbf{hedged}\index{hedged} by \(h\). Such a portfolio \(h\) is called a replicating, or hedging portfolio\index{replicating portfolio}\index{hedging portfolio}. If every contingent claim can be replicated, we say that the market is \textbf{complete}.}

We can now formulate a proposition on when the market is complete in terms of the matrix \(D\).

\textbf{Proposition 3.13. (Bjork)} \emph{The market is complete if and only if the rows of the matrix \(D\) span \(\mathbb{R}^M\), i.e.~if and only if \(D\) has rank \(M\).}

Now we formulate the second fundamental pricing theorem in terms of the martingale measure \(Q\).

\textbf{Proposition 3.14. (Bjork)} \textbf{(Second Fundamental Theorem)}\index{Second Fundamental Theorem} \emph{Assume that the model is arbitrage free i.e.~\(Q\) exist. Then the market is unique if and only if the martingale measure is unique.}

\hypertarget{stochastic-discount-factors}{%
\subsubsection{Stochastic Discount Factors}\label{stochastic-discount-factors}}

\textbf{Definition 3.16. (Bjork)} \emph{The random variable \(L\) on \(\Omega\) is defined by}

\[
L(\omega_i)=\frac{q_i}{p_i},\hspace{20pt} i=1,...,M.
\]

\textbf{Definition 3.17. (Bjork)} \emph{Assume the absence of arbitrage, and fix a martingale measure \(Q\). With notation as above, the \textbf{stochastic discount factor}\index{stochastic discount factor} (or ``state price deflator''\index{state price deflator}) is the random variable \(\Lambda\) on \(\Omega\) by}

\[
\mathbf{M}(\omega)=\frac{1}{1+R}\cdot L(\omega).\tag{3.19}
\]

\textbf{Proposition 3.18. (Bjork)} \emph{The arbitrage free price of any claim \(X\) is given by the formula}

\[
\Pi_0[X]=E^P[\mathbf{M}\cdot X]\tag{3.20}
\]

\emph{where \(\mathbf{M}\) is a stochastic discount factor.}

\pagebreak

\hypertarget{self-financing-portfolios}{%
\section{Self-financing portfolios}\label{self-financing-portfolios}}

We move forward in this chapter by first defining a self-financing portfolio in discrete time and then by letting the step length tend to zero obtain the continuous time analogue.

\hypertarget{discrete-time-sf-portfolio}{%
\subsection{Discrete time SF portfolio}\label{discrete-time-sf-portfolio}}

We consider \(N\) different adapted price processes \(S^1,...,S^N\). We use the following definition.

\textbf{Definition 6.1. (Bjork)} \emph{We use the following definitions.}

\begin{itemize}
\tightlist
\item
  \emph{\(S_n^i\) is th price of asset \(i\) at time \(n\),}
\item
  \emph{\(h_n^i\) is the number of units of asset \(i\) held during \([n,n+1)\), that is bought at time \(n\),}
\item
  \emph{\(d_n^i\) is the dividends from asset \(i\) in the time-interval \([n-1,n)\), that is recieved at time \(n\),}
\item
  \emph{\(h_n\) is the portfolio \((h_n^1,...,h_n^N)\) held during \([n,n+1)\),}
\item
  \emph{\(c_n\) is the consumption i.e.~withdrawel at time \(n\) (negative being deposits/saving),}
\item
  \emph{\(V_n\) is the value of the portfolio just before time \(n\) i.e.~of the portfolio \(h_{n-1}\) at time \(n\).}
\end{itemize}

We are now ready to define the self-financing portfolio

\textbf{Definition 6.2. (Bjork)} \emph{A \textbf{self-financing portfolio supporting the consumption stream}\index{self-financing portfolio}\index{consumption stream} \(\mathbf{c}\) is a portfolio adhering to the \textbf{budget constraint}\index{budget constraint} given as}

\[
h_{n+1}S_{n+1}+c_{n+1}=h_nS_{n+1}+h_nd_{n+1.}
\]

\emph{The interpretation being, that we may only use funds obtained from selling the old portfolio \(h_n\) and recieved in dividends to buy the new portfolio \(h_{n+1}\) and consume the amount \(c_{n+1}\).}

Before studying the self-financing portfolio we define the operator \(\Delta\) (in definition 6.3) as the increment \(\Delta x_n=x_{n+1}-x_n\) of a countable sequence \((x_n)_{n\in\mathbb{N}_0}\). Notice that we define the increment forward so the increment \(n\) is the increment over the time period \([n,n+1)\) with the first increment being \([0,1)\). Using this notation we can derive the lemma below.

\textbf{Lemma 6.4. (Bjork)} \emph{For any pair of sequences of real numbers \((x_n)_{n\in\mathbb{N}_0}\) and \((y_n)_{n\in\mathbb{N}_0}\) we have the relations}
\begin{align*}
\Delta(xy)_n&=x_n\Delta y_n+y_{n+1}\Delta x_n,\tag{6.5}\\
\Delta(xy)_n&=y_n\Delta x_n+x_{n+1}\Delta y_n,\tag{6.6}\\
\Delta(xy)_n&=x_n\Delta y_n+y_n\Delta x_n+\Delta x_n\Delta y_n.\tag{6.7}
\end{align*}
\emph{This is also valid if the sequances are \(N\)-dimensional, where we interpret the products above as scalar products (\(xy^\top\)).}

Using these definitions and the lemma above we see that the dynamics of the self-financing portfolio is given below.

\textbf{Proposition 6.6. (Bjork)} \emph{The dynamics of any self-financing portfolio supporting the consumption stream \(c\) are given by}

\[
\Delta V_n=h_n \Delta S_n+h_nd_{n+1}-c_{n+1},\tag{6.11}
\]

\emph{or, in more detail}

\[
\Delta V_n=\sum_{i=1}^Nh_n^i(\Delta S_n^i+d^i_{n+1})-c_{n+1}.\tag{6.12}
\]

We may rewrite the dividends as accumulating dividends \(D^i_n=\sum_{k=1}^nd^i_k\) and see that \(d_{n+1}^i=\Delta D^i_n\) and so the above condition is equivalent with.

\textbf{Proposition 6.8. (Bjork)} \emph{The dynamics of any self-financing portfolio supporting the consumption stream \(c\) are given by}

\[
\Delta V_n=h_n \Delta S_n+h_n\Delta D_n-c_{n+1},\tag{6.15}
\]

\emph{or, in more detail}

\[
\Delta V_n=\sum_{i=1}^Nh_n^i(\Delta S_n^i+\Delta D^i_n)-c_{n+1}.\tag{6.16}
\]

\hypertarget{continuous-time-sf-portfolio}{%
\subsection{Continuous time SF portfolio}\label{continuous-time-sf-portfolio}}

Formulating the dynamics of the self-financing portfolio in continuous time is easy work given the discrete setup above. However since we now are in continuous time we will change the \(n\) with a \(t\) and cosider the behavour \(V_{t+dt}-V_t\) as we let \(dt\to 0\). First we formulate some basic notation.

\textbf{Definition 6.9. (Bjork)} \emph{We use the following definitions.}

\begin{itemize}
\tightlist
\item
  \emph{\(S_t^i\) is th price of asset \(i\) at time \(t\),}
\item
  \emph{\(h_t^i\) is the number of units of asset \(i\) held at time \(t\),}
\item
  \emph{\(D_t^i\) is the cumulative dividend processs for asset \(i\),}
\item
  \emph{\(h_t\) is the portfolio \((h_t^1,...,h_t^N)\) held at time \(t\),}
\item
  \emph{\(c_t\) is the consumption rate at time \(n\) (negative being deposits/saving),}
\item
  \emph{\(V_t\) is the value of the portfolio at time \(t\) i.e.~of the portfolio \(h_t\) at time \(t\).}
\end{itemize}

Given these definitions we may define a portfolio strategy that is self-financing.

\textbf{Definition 6.10. (Bjork)} \emph{Let \(S\) be and adapted \(N\)-dimensional price process. We define the following}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{A \textbf{portfolio strategy}\index{portfolio strategy} is any adapted \(N\)-dimensional process \(h\).}
\item
  \emph{The \textbf{value process}\index{value process} \(V^h\) corresponding to the portfolio \(h\) is given by}
  \[
    V_t^h=\sum_{i=1}^N h_t^iS_t^i.\tag{6.17}
    \]
\item
  \emph{A \textbf{consumption process}\index{consumption process} is any adapted one-dimensional process \(c\).}
\item
  \emph{A portfolio-consumption pair \((h,c)\) is called \textbf{self-financing} if the value process \(V^h\) satisfies the condition}
  \[
    dV_t^h=\sum_{i=1}^N h_t^i(dS_t^i+d D^i_t)-c_t\ dt,\tag{6.18}
    \]
  \emph{i.e.~if}
  \[
    dV_t^h=h_t\ dS_t + h_t\ dD_t -c_t\ dt.
    \]
\item
  \emph{The \textbf{gain process}\index{gain process} \(G\) is defined by}
  \[
    G_t=S_t+D_t\tag{6.19}
    \]
  \emph{so we can write the self-financing condition\index{self-financing condition} as}
  \[
    dV_t=h_t\ dG_t-c_t\ dt.\tag{6.20}
    \]
\item
  \emph{The portfolio \(h\) is said to be \textbf{Markovian}\index{Markovian} if it is of the form}
  \[
    h_t=h(t,S_t),
    \]
  \emph{for some function \(h : \mathbb{R}_+\times \mathbb{R}^N\to\mathbb{R}^N\).}
\end{enumerate}

\hypertarget{portfolio-weights}{%
\subsection{Portfolio weights}\label{portfolio-weights}}

\textbf{Definition 6.11. (Bjork)} \emph{For a given portfolio \(h\) the corresponding \textbf{relative portfolio}\index{relative portfolio} or \textbf{portfolio weights}\index{portfolio weights} \(w\) are defined by}

\[
w_t^i=\frac{h_t^iS_t^i}{V_t^h},\ i=1,...,N,\tag{6.21}
\]

\emph{so, in particular, we have \(\sum_{i=1}^N w_i=1\).}

\textbf{Lemma 6.12. (Bjork)} \emph{A portfolio-consumption pair\index{portfolio-consumption pair} \((h,c)\) is self-financing if and only if}

\[
dV_t^h=V_t^h\sum_{i=1}^N w_t^i\frac{dS_t^i+dD_t^i}{S_t^i}-c_t\ dt\tag{6.22}
\]

\emph{or equivalently with the absolute weights}

\[
dV_t^h=\sum_{i=1}^N h_t^i(dS_t^i+dD_t^i)-c_t\ dt.
\]

\textbf{Lemma 6.13. (Bjork)} \emph{Consider the case with no dividends. Let \(c\) be a consumption process, and assume that there exist a scalar process \(Z\) and a vector process \(q=(q^1,...,q^N)\) such that}

\[
dZ_t=Z_t\sum_{i=1}^N q_t^i\frac{dS_t^i}{S_t^i}-c_t\ dt,\tag{6.23}
\]

\emph{and \(\sum_{i=1}^Nqq^i=1\) (eq. 6.24). Now define a portfolio \(h\) by}

\[
h_t^i=\frac{q_t^iZ_t}{S_t^i}.\tag{6.25}
\]

\emph{Then the value process \(V^h\) is given by \(V^h=Z\), the pair \((h,c)\) is self-financing, and the corresponding relative portfolio \(w\) is given by \(w=q\).}

\newpage

\hypertarget{black-scholes-pde}{%
\section{Black-Scholes PDE}\label{black-scholes-pde}}

The Black-Scholes model revolves arround SDE's as seen above. In this model we have two assets a risk free asset \(B\) and a stochastic priced asset \(S\). We therefore start by defining what we mean by a quote-on-qoute \emph{risk free} asset.

\textbf{Definition 7.1. (Bjork)} \emph{The price process \(B\) is the price of a \textbf{risk free asset}\index{risk free asset} if it has the dynamics}

\[
dB_t=r_t B_t\ dt,\tag{7.1}
\]

\emph{where \(r\) is any \(\mathcal{F}_t\) adapted process.}

We see from this definition that the meaning of ``risk free'' is the property, that \(B\) is priced locally deterministic in the sence that \(r\) is adapted and therefore known at time \(t\) and we therefore know the yield on a short term basis. This is also why we may call \(r\) the \textbf{short interest rate}\index{short interest rate}. Given the dynamics above, we know that \(B\) in fact is represented by the process

\[
B_t=B_0e^{\int_0^tr_s\ ds},
\]

for some \(B_0\) initial value. We will moving forward assume that \(B_0=1\). The stochastic asset \(S\) has dynamics.

\[
dS_t=\mu(t,S_t)\ dt + \sigma(t,S_t)\ dW_t,\tag{7.2}
\]

where as usual \(\mu\) and \(\sigma\) are deterministic functions and \(W_t\) is a standard Brownian motion. Note that the risk free asset has a similarly process with \(\sigma = 0\). We may now include this in the definition of the Black-Scholes model.

\textbf{Definition 7.2. (Bjork)} \emph{The \textbf{Black-Scholes model}\index{Black-Scholes model} consists of two assets with dynamics given by}
\begin{align*}
dB_t&=rB_t\ dt,\tag{7.3}\\
dS_t&=\mu S_t\ dt+\sigma S_t\ dW_t,\tag{7.4}
\end{align*}
\emph{where \(r,\mu,\sigma\) are deterministic constants.}

\textbf{Definition 7.3. (Bjork)} \emph{A \textbf{zero coupon bond}\index{zero coupon bond} with maturity \(T\) (henceforth ``\(T\)-bond''\index{$T$-bond}) is an asset which pays the holder the face value 1 dollar at time \(T\). The price at time \(n\) of a \(T\)-bond is denoted by \(p(n,T)\).}

\textbf{Definition 7.4.} \emph{The (possible stochastic) discrete \textbf{short rate}\index{short rate} \(r_n\), for the period \([n,n+1]\), is defined as}

\[
p(n,n+1)=\frac{1}{1+ r_n}.\tag{7.6}
\]

From this short rate we may derive the dynamics of the bank account recieving zero-coupon rates for each distinct time interval.

\textbf{Definition 7.5. (Bjork)} \emph{The dynamics of the bank account\index{bank account} are given by}

\[
\Delta B_n=r_n B_n.\tag{7.7}
\]

\hypertarget{contingent-claims-and-arbitrage}{%
\subsection{Contingent Claims and Arbitrage}\label{contingent-claims-and-arbitrage}}

\textbf{Definition 7.6. (Bjork)} \emph{A \textbf{European call option}\index{European call option} with \textbf{exercise price}\index{exercise price} (or strike price\index{strike price}) \(K\) and \textbf{time of maturity}\index{time of maturity} (exercise date\index{exercise date}) \(T\) on the \textbf{underlying asset} \(S\) is a contract defined by the following clauses:}

\begin{itemize}
\tightlist
\item
  \emph{The holder of the option has, at time \(T\), the right to buy one share of the underlying stock at the price \(K\) dollars from the underwriter of the option.}
\item
  \emph{The holder of the option is in no way obliged to buy the underlying stock.}
\item
  \emph{The right to buy the underlying stock at the price \(K\) can only be exercised at the precise time \(T\).}
\end{itemize}

Obviously, we also have the \textbf{european put} option which gives the owner the right to sell an asset at price \(K\) at time \(T\). Let os formally define a contingent claim.

\textbf{Definition 7.7.} \emph{Consider a financial market with vector price process \(S\). A \textbf{contingent claim} with \textbf{date of maturity} \(T\), also called a \(T\)-claim, is any random variable \(\mathcal{X}\in\mathcal{F}_T^S\). A contingent claim \(\mathcal{X}\) is called a \textbf{simple} claim if it is of the form \(\mathcal{X} = \Phi(S_t)\). The function \(\Phi\) is called the \textbf{contract function}.}

\textbf{Definition 7.8. (Bjork)} \emph{An \textbf{arbitrage} possibility\index{arbitrage portfolio} on a financial market is a self-financed portfolio \(h\) such that}
\begin{align*}
V^h(0)&=0,\tag{7.13}\\
P(V_T^h\ge0)&=1,\tag{7-14}\\
P(V_T^h>0)&>0.\tag{7.15}
\end{align*}
\emph{We say that the market is \textbf{arbitrage free} if there are no arbitrage possibilities.}

\textbf{Definition 7.9. (Bjork)} \emph{Suppose that there exists a self-financing portfolio \(h\), such that the value process \(V^h\) has the dynamics}

\[
d V_t^h=k_tV_t^h\ dt,\tag{7.16}
\]

\emph{where \(k\) is an adapted process. Then it must hold that \(k_t=r_t\) for all \(t\), ore there exists an arbitrage possibility.}

\textbf{Theorem 7.10. (Bjork)} \textbf{(Black-Scholes equation)}\index{Black-Scholes equation} \emph{Assume that the market is specified by the equations}
\begin{align*}
dB_t&=rB_t\ dt,\tag{7.18}\\
dS_t&=\mu(t,S_t) S_t\ dt+\sigma(t,S_t)S_t\ dW_t,\tag{7.19}
\end{align*}
\emph{and that we want to price a contingent claim of the form \(\mathcal{X}=\Phi(S_t)\) (eq. 7.20). Then the only pricing function of the form \(\Pi_t[\Phi(S_t)]=F(t,S_t)\) (eq. 7.21) which is consistent with the absence of arbitrage in the market \([B_t,S_t,\Pi_t]\) is when \(F\) is the solution of the following boundary value problem in the domain \([0,T]\times\mathbb{R}_+\):}
\begin{align*}
F_t(t,s)+rsF_s(t,s)+\frac{1}{2}s^2\sigma^2(t,s)F_{ss}(t,s)-rF(t,s)&=0,\\
F(T,s)&=\Phi(s).
\end{align*}

\hypertarget{risk-neutral-valuation-1}{%
\subsection{Risk Neutral Valuation}\label{risk-neutral-valuation-1}}

\textbf{Theorem 7.11. (Bjork)} \textbf{(Risk Neutral Valuation)}\index{Risk Neutral Valuation formula} \emph{The arbitrage free price of the claim \(\Phi(S_t)\) is given by \(\Pi_t[\Phi]=F(t,S_t)\), where \(F\) is given by the formula}

\[
F(t,s)=e^{-r(T-t)}E^Q_{t,s}[\Phi(S_T)],\tag{7.43}
\]

\emph{where the \(Q\)-dynamics of \(S\) are those of}

\[
dS_t=rS_t\ dt+S_t\sigma(t,S_t)\ dW_t^Q.\tag{7.42}
\]

\textbf{Property 7.12. (Bjork)} \textbf{(The Martingale Property)}\index{Martingale Property} \emph{In the Black-Scholes model, the price process \(\Pi_t\) for every traded asset, be it the underlying or derivate asset, has the property the the normalized price process}

\[
Z_t=\frac{\Pi_t}{B_t},
\]

\emph{(including \(S_t/B_t\)) is a martingale under the measure \(Q\).}

\hypertarget{black-scholes-formula}{%
\subsection{Black-Scholes formula}\label{black-scholes-formula}}

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.75\textwidth]{figures/BS_sim.png}
  \end{center}
\end{figure}

This chapter will center on deriving the famous Black-Scholes formula. We start by laying out the assumptions of the model. We have a market consiting of two assets: a stochastic prices asset \(S\) and a risk free asset \(B\). The prices processes have dynamics:
\begin{align*}
dS_t&=\mu S_t\ dt+\sigma S_t\ dW_t,\tag{7.45}\\
dB_t&=r B_t\ dt,\tag{7.44}
\end{align*}
where \(S_0=s\) and \(B_0=1\) (by assumption). Now from Feymann-Kac and the definition of arbitrage we know that a simple claim \(\Phi(S_t)\) has the arbitrage free price given by the risk neutral valueation formula.

\[
F(t,s)=e^{-r(T-t)}E^Q_{t,s}[\Phi(S_T)],\tag{7.46}
\]

where \(Q\) is a probability measure, namely a Martingale measure, such that the dynamics of \(S\) under this measure is

\[
dS_t=r S_t\ dt+\sigma S_t\ dW^Q_t,\tag{7.47}
\]

with \(W_t^Q\) being a Brownian motion wrt. to the probability measure \(Q\) (not \(P\)). The above still has the initial condition \(S_0=s\). Given these assumptions we may formulate the Black-Scholes formula.

\textbf{Theorem 7.13. (Bjork)} \textbf{(Black-Scholes formula)}\index{Black-Scholes formula} \emph{The price of the european call option with strikeprice \(K\) and maturity \(T\) (contract function \(\Phi(S_t)=\left( S_t - K\right)^+\)) takes the form \(\Pi_t=F(t,s)\), where}

\[
F(t,s)=s N(d_1(t,s))-e^{-r(T-t)}KN(d_2(t,s)),\tag{7.52}
\]

\emph{where \(N\) is the distribution-function for an \(\mathcal{N}(0,1)\)-distributed random variable and}
\begin{align*}
d_1(t,s)&=\frac{1}{\sigma \sqrt{T-t}}\left(\log\left(\frac{s}{K}\right)+\left(r+\frac{1}{2}\sigma^2\right)(T-t)\right),\tag{7.53}\\
d_2(t,s)&=d_1(t,s)-\sigma\sqrt{T-t}.\tag{7.54}
\end{align*}

\noindent\makebox[\linewidth]{\rule{\textwidth}{0.4pt}}

\textbf{Proof.}

We let the market be given in terms of the price processes \(S\) and \(B\) with dynamics.
\begin{align*}
dS_t&=\mu S_t\ dt+\sigma S_t\ dW_t,\\
dB_t&=r B_t\ dt,
\end{align*}
with \(B_t=1\) and \(S_t=s\). We assume that \(\mu,\sigma, r\) are deterministic real numbers. Consider the contingent claim

\[
\Phi(S_t)=\left( S_t - K\right)^+,
\]

that is the European call option. Let \(Q\) be a martingale measure such that the dynamics of \(S\) may be written as

\[
dS_t=r S_t\ dt+\sigma S_t\ dW^Q_t,
\]

then \(S_t\) is clearly a GBM wrt. the measure \(Q\). Therefore we know the solution given in terms of the increment of the Brownian motion \(W^Q\) as follows

\[
S_u=s\cdot \exp\left\{\left(r-\frac{1}{2}\sigma^2\right)(u-t)+\sigma\left(W_u^Q-W_t^Q\right)\right\},
\]

for some initial condition \(S_t=s\). From theorem 7.10 we know that the only pricing function which takes the form

\[
\Pi_t[\Phi(S_T)]=F(t,S_t),
\]

can only be consistent with the absence of arbitrage if \(F\) is the solution the the boundary value problem
\begin{align*}
F_t(t,s)+rsF_s(t,s)+\frac{1}{2}s^2\sigma^2F_{ss}(t,s)-rF(t,s)&=0,\\
F(T,s)&=\Phi(s).
\end{align*}
From Feymann-Kac we then know that the stochastic representation of such a solution take the form

\[
F(t,s)=e^{-r(T-t)}E_{t,s}^Q[\Phi(S_T)].
\]

Here the superscript refers to taking mean value with respect to the measure \(Q\). This gives the solution to the pricing function

\[
F(t,s)=e^{-r(T-t)}\int \Phi(S_T)\ dQ.
\]

Under the measure \(Q\) we have that for \(u\ge t\):

\[
Z_u=\log (S_u/s)\sim \mathcal{N}\left(\left(r-\frac{1}{2}\sigma^2\right)(u-t),\sigma\sqrt{u-t}\right)
\]

Hence we may set \(u=T\) and observe that
\begin{align*}
F(t,s)&=e^{-r(T-t)}\int_{-\infty}^\infty \Phi(se^z) f(z)\ dz\\
&=e^{-r(T-t)}\int_{-\infty}^\infty (se^z-K)^+ f(z)\ dz\\
&=e^{-r(T-t)}\int_{\log\left(\frac{K}{s}\right)}^{\infty} (se^z-K) f(z)\ dz\\
&=e^{-r(T-t)}\left(s\int_{\log\left(\frac{K}{s}\right)}^{\infty} e^z f(z)\ dz-K\int_{\log\left(\frac{K}{s}\right)}^{\infty} f(z)\ dz\right),
\end{align*}
where we used that \(f\) is the distribution function of a normal distributed random variable with mean \((r-\sigma^2/2)(T-t)\) and variance \(\sigma\sqrt{T-t}\) and that

\[
(se^z-K)^+ \ge 0\iff se^z\ge K\iff z\ge \log\left(\frac{K}{s}\right)
\]

Using that the MGF of a \(X\sim\mathcal{N}(\alpha, \beta^2)\) variable is

\[
E[e^{tX}]=e^{\alpha t+\frac{1}{2}\beta ^2t^2},
\]

and the shorthand \(N(t)\) for the distribution function of the standard normal distribution, we have
\begin{align*}
F(t,s)&=e^{-r(T-t)}\left(sE\left[e^{Z_T}1_{Z_T\ge \log\left(\frac{K}{s}\right)}\right]-K P\left(Z_T\ge \log\left(\frac{K}{s}\right)\right)\right)\\
&=e^{-r(T-t)}s\exp\left\{\left(r-\frac{1}{2}\sigma^2\right)(T-t)+\frac{1}{2}\sigma^2(T-t)\right\}E\left[1_{Z_T\ge \log\left(\frac{K}{s}\right)}\right]\\
&-e^{-r(T-t)}K P\left(X\ge\frac{1}{\sigma\sqrt{T-t}}\left( \log\left(\frac{K}{s}\right)-(r-\sigma^2/2)(T-t)\right)\right)\\
&=sE\left[1_{Z_T\ge \log\left(\frac{K}{s}\right)}\right]-e^{-r(T-t)}K P\left(X\le\frac{1}{\sigma\sqrt{T-t}}\left(\log\left(\frac{s}{K}\right)+(r-\sigma^2/2)(T-t)\right)\right)\\
&=sN(d_1(s,t))-e^{-r(T-t)}K N\left(d_2(s,t)\right),
\end{align*}
as desired. \(\blacksquare\)

\newpage

\hypertarget{completeness-and-hedging}{%
\section{Completeness and Hedging}\label{completeness-and-hedging}}

We derived the pricing function of the european call option above and introduced the theory around boundary value problems and Feymann-Kac solution to the partial differential stochastic equation. Now we want to see if a portfolio exists such that it gives the payout \(\Phi(S_T)\) with probability one.

In order to do this, we return to the concept of hedge and replication.

\textbf{Definition 8.1. (Bjork)} \emph{We say that a \(T\)-claim \(\mathcal{X}\) can be \textbf{replicated}\index{replicated}, alternatively the it is \textbf{reachable}\index{reachable} or \textbf{hedgeable}\index{hedgeable}, if there exists a self-financing portfolio \(h\) such that}

\[
V_T^h=\mathcal{X},\ P-\text{a.s.}\tag{8.1}
\]

\emph{In this case we say that \(h\) is a \textbf{hedge} against \(\mathcal{X}\). Alternatively, \(h\) is called a \textbf{replicating} or \textbf{hedging} portfolio\index{replicating portfolio}\index{hedging portfolio}. If every contingent claim is reachable we say that the market is \textbf{complete}\index{complete market}.}

If we can find a portfolio \(h\) that reaches \(\mathcal{X}\) in value over the time period \([t,T]\) it must mean, that holding the portfolio is equivalent with holding the contract itself. We therefore have the natural assumption that the price process must satisfie \(\Pi_t[\mathcal{X}]=V_t^h\) for all \(t\ge 0\). How this relates to the absence of arbitrage is given below.

\textbf{Proposition 8.2. (Bjork)} \emph{Suppose \(\mathcal{X}\) is hedged using the portfolio \(h\). Then the only price process \(\Pi_t[\mathcal{X}]\) which is consistent with no arbitrage is given by \(\Pi_t[\mathcal{X}]=V_t^h\). Furthermore, if \(\mathcal{X}\) can be hedged by both \(h\) and \(g\) then \(V_t^g=V_t^h\) for all \(t\) with probability one.}

\hypertarget{completeness-in-black-scholes}{%
\subsection{Completeness in Black-Scholes}\label{completeness-in-black-scholes}}

The Black-Scholes model will be investegated in the following. We start by stating the important theorem.

\textbf{Theorem 8.3. (Bjork)} \emph{Consider the Black-Scholes model given by}
\begin{align*}
dS_t&=\mu(t,S_t) S_t\ dt+\sigma(t,S_t) S_t\ dW_t,\tag{8.2}\\
dB_t&=r B_t\ dt,\tag{8.3}
\end{align*}
\emph{The model above is complete.}

The following lemma gives us replicability of a \textbf{simple} claim\index{simple claim} (which we will restrict ud to).

\textbf{Lemma 8.4. (Bjork)} \emph{Suppose that there exist an adapted process \(V\) and an adapted process \(w=[w^B,w^S]\) with \(w^B_t+w^S_t=1\) (eq. 8.4) for all \(t\ge 0\), such that}
\begin{align*}
dV_t&=V_t(w_t^Br+w_t^S\mu(t,S_t))\ dt+V_tw_t^S\sigma(t,S_t)\ dW_t,\tag{8.5}\\
V_t&=\Phi(S_t).\tag{8.5}
\end{align*}
\emph{Then the claim \(\mathcal{X}=\Phi(S_t)\) can be replicated using \(w\) as the relative portfolio. The corresponding value process is given by the process \(V\) and the absolute portfolio \(h\) is given by}
\begin{align*}
h_t^B&=\frac{w_t^B V_t}{B_t},\tag{8.6}\\
h_t^S&=\frac{w_t^S V_t}{S_t}.\tag{8.7}
\end{align*}

Doing some heuristics we come up with some clever weights, which turns on to adhere to the boundary value problem formulated in the Black-Scholes equation. Given that the weights gives rise to the desired value process, we have succesfully found the portfolio weight (see lemma above).

\textbf{Theorem 8.5. (Bjork)} \emph{Consider the Black-Scholes model given in (8.3)-(8.4), and a simple contingent claim \(\mathcal{X}=\Phi(S_t)\). Define \(F\) as the solution to the boundary value problem}
\begin{align*}
F_t(t,s)+rsF_s(t,s)+\frac{1}{2}s^2\sigma^2F_{ss}(t,s)-rF(t,s)&=0,\tag{8.17}\\
F(T,s)&=\Phi(s).\tag{8.17}
\end{align*}
\emph{Then \(\mathcal{X}\) can be replicated by the relative portfolio}
\begin{align*}
w_t^B&=\frac{F(t,S_t)-S_tF_s(t,S_t)}{F(t,S_t)},\tag{8.18}\\
w_t^S&=\frac{S_tF_s(t,S_t)}{F(t,S_t)}.\tag{8.19}
\end{align*}
\emph{The corresponding absolute portfolio is given by}
\begin{align*}
h_t^B&=\frac{F(t,S_t)-S_tF_s(t,S_t)}{B_t},\tag{8.20}\\
h_t^S&=F_s(t,S_t),\tag{8.21}
\end{align*}
\emph{and the value process \(V^h\) is given by}

\[
V^h_t=F(t,S_t).\tag{8.22}
\]

\textbf{Proposition 8.6. (Bjork)} \emph{Consider the Black-Scholes model given in (8.3)-(8.4), and a contingent claim on the form \(\mathcal{X}=\Phi(S_T,Z_T)\) (eq. 8.29). We define the process \(Z_t\) as}

\[
Z_t=\int_0^tg(u,S_u)\ du,\tag{8.30}
\]

\emph{for some choice of the deterministic function \(g\). Then \(\mathcal{X}\) can be replicated using a relative portfolio given by}
\begin{align*}
w_t^B&=\frac{F(t,S_t,Z_t)-S_tF_s(t,S_t,Z_t)}{F(t,S_t,Z_t)},\tag{8.31}\\
w_t^S&=\frac{S_tF_s(t,S_t,Z_t)}{F(t,S_t,Z_t)}.\tag{8.32}
\end{align*}
\emph{where \(F\) is the solution to the boundary value problem}
\begin{align*}
F_t(t,s,z)+rsF_s(t,s,z)+\frac{1}{2}s^2\sigma^2F_{ss}(t,s,z)-rF(t,s,z)&=0,\tag{8.33}\\
F(T,s,z)&=\Phi(s,z).\tag{8.33}
\end{align*}
\emph{The corresponding value process is given by \(V_t=F(t,S_t,Z_t)\), and \(F\) has the stochastic representation}

\[
F(t,s,z)=e^{-r(T-t)}E^Q_{t,s,z}[\Phi(S_T,Z_T)],\tag{8.34}
\]

\emph{where the \(Q\)-dynamics are given by}
\begin{align*}
dS_u&=rS_u\ du + S_u\sigma(u,S_u)\ dW^Q_u,\tag{8.35}\\
S_t&=s,\tag{8.36}\\
dZ_u&=g(u,S_u)\ du,\tag{8.37}\\
Z_t&=z.\tag{8.38}
\end{align*}

\hypertarget{absence-of-arbitrage-1}{%
\subsection{Absence of Arbitrage}\label{absence-of-arbitrage-1}}

In general we have conflicting forces when evaluating when a certain market is arbitrage free and/or complete. We have in simple terms the non-rigorous theorem below.

\textbf{Meta-theorem 8.3.1. (Bjork)} \emph{Let \(N\) denote the number of underlying \textbf{traded} assets in the model \textbf{excluding} the risk free asset, and let \(R\) denote the number of random sources driving the price system. Genericallly we then have the following statements.}

\begin{itemize}
\tightlist
\item
  \emph{The model is arbitrage free if and only if \(N\le R\).}
\item
  \emph{The model is complete if and only if \(N\ge R\).}
\item
  \emph{The model is arbitrage free and complete if and only if \(N=R\).}
\end{itemize}

\hypertarget{incomplete-markets}{%
\subsection{Incomplete Markets}\label{incomplete-markets}}

We \index{incomplete market}assume a market with a risk free asset and one risky assets with dynamics

\[
dX_t=\mu(t,X_t)\ dt+\sigma(t,X_t)\ dW_t.\tag{9.1}
\]

We want to find a unique price of a derivative on a functional form of the risky asset. We assume that we cannot invest in the asset representing the process \(X_t\) and so we can solely write contracts based on the observation \(X_T\). The problem here is that we can only short or long the risk free asset and so no derivative is replicable.

The way we solve this problem is by having the market set the price of risk and universally price derivatives based on this given price process. We then have the assumptions

\textbf{Assumption 9.2.1} \emph{We have the market given with the only investable asset \(B\) with dynamics}

\[
dB_t=rB_t\ dt.\tag{9.2}
\]

\emph{We furthermore, have an empirically observable stochastic process \(X\) which is \textbf{not} the price process of any traded asset. The \(P\)-dynamics of \(X\) is given by}

\[
dX_t=\mu(t,X_t)\ dt+\sigma(t,X_t)\ dW_t.
\]

\textbf{Assumption 9.2.2} \emph{There is a liquid market for every contingent claim.}

\textbf{Assumption 9.2.3} \emph{We assume that}

\begin{itemize}
\tightlist
\item
  \emph{There is a liquid, frictionless market for each of the contingent claims \(\mathcal{Y}\) and \(\mathcal{Z}\).}
\item
  \emph{The market prices of the claims are of the form}
  \[ \Pi_t[\mathcal{Y}] = F(t,X_t),\]
  \[ \Pi_t[\mathcal{Z}] = G(t,X_t),\]
  \emph{where \(F\) and \(G\) are smooth real valued function.}
\end{itemize}

From Ito's formula we have the dynamics
\begin{align*}
dF=\mu_F F\ dt+\sigma_F F\ dW,\tag{9.4}\\
dG=\mu_G G\ dt+\sigma_G G\ dW.\tag{9.5}
\end{align*}
Where the processes \(\mu_F\) and \(\sigma_F\) are given by
\begin{align*}
\mu_F&=\frac{F_t+\mu F_x+\frac{1}{2}\sigma^2 F_{xx}}{F},\\
\sigma_F&=\frac{\sigma F_x}{F}.
\end{align*}
By forming a portfolio of the two contracts we lead to the relation.

\[
\frac{\mu_F-r}{\sigma_F}=\frac{\mu_G-r}{\sigma_G}.
\]

This gives the important insight.

\textbf{Proposition 9.1. (Bjork)} \emph{Assume that the market for derivatives is free of arbitrage. Then there exists a universal process \(\lambda(t,X_t)\) such that, with probability one, and for all \(t\), we have}

\[
\frac{\mu_F(t,X_t)-r}{\sigma_F(t,X_t)}=\mu(t,X_t),\tag{9.7}
\]

\emph{regardless of the specific choice of the derivative \(F\).}

\textbf{Proposition 9.2. (Bjork)} \emph{Assume absence of arbitrage, the pricing function \(F(t,x)\) of the \(T\)-claim \(\Phi(X_T)\) solves the following boundary value problem.}
\begin{align*}
F_t(t,x)+\mathcal{A}F(t,x)-rF(t,x)&=0,\hspace{15pt}&(t,x)\in (0,T)\times \mathbb{R},\tag{9.8}\\
F(T,x)&=\Phi(x), &x\in\mathbb{R},\tag{9.9}
\end{align*}
\emph{where}

\[
\mathcal{A}F(t,x)=\left\{\mu(t,x)-\lambda(t,x)\sigma(t,x)\right\}F_x(t,x)+\frac{1}{2}\sigma^2(t,x)F_{xx}(t,x).
\]

\textbf{Proposition 9.3. (Bjork)} \textbf{(Risk neutral valuation)}\index{risk-neutral valueation formula} \emph{Assuming absence of arbitrage, the pricing function \(F(t,x)\) of the \(T\)-claim \(\Phi(X_T)\) is given by the formula}

\[
F(t,x)=e^{-r(T-t)}E^Q_{t,x}[\Phi(X_T)].\tag{9.11}
\]

\emph{The dynamics of \(X\) under the martingale measure \(Q\) are given by}

\[
dX_t=\left\{\mu(t,x)-\lambda(t,x)\sigma(t,x)\right\}F_x(t,x)+\sigma(t,x)\ dW^Q_t,
\]

\emph{where \(W^Q\) is a \(Q\)-Brownien motion.}

\newpage

\hypertarget{parity-relations}{%
\section{Parity relations}\label{parity-relations}}

\hypertarget{put-call-parity}{%
\subsection{Put-call Parity}\label{put-call-parity}}

The notion of continuous rebalancing the replicating portfolio require leads to problems in the real world. Trading does cost some money (typical in fractions) and so contiuous balancing would make the portfolio go to 0 rather quickly. Why? The Brownian motion has unbounded variation and so we would have to sell and buy the portfolio uncountable many time in any interval and the shift in weight is not neglible. Because of this we would like to see which claims we can replicate by buying and holding a combination of assets and derivatives.

\textbf{Proposition 10.1. (Bjork)} \emph{Let \(\Phi\) and \(\Psi\) be contract functions for the \(T\)-claims \(\mathcal{X}=\Phi(S_T)\) and \(\mathcal{Y}=\Psi(S_T)\). Then for any real numbers \(\alpha\) and \(\beta\) we have the following price relation:}

\[
\Pi_t[\alpha \Phi + \beta\Psi]=\alpha \Pi_t[\Phi]+\beta\Pi_t[\Psi].\tag{10.1}
\]

If we consider the basic contract functions
\begin{align*}
\Phi_S(x)&=x,\tag{10.2}\\
\Phi_B(x)&=1,\tag{10.3}\\
\Phi_{C,K}(x)&=(x-K)^+,\tag{10.4}\\
\Phi_{P,K}(x)&=(K-x)^+.
\end{align*}
That is a contract paying (respectively): the price of one stock, 1 dollar, one european call and one european put both with strike \(K\). It is clear that the following prices are
\begin{align*}
\Pi_t[\Phi_S]&=S_t,\tag{10.5}\\
\Pi_t[\Phi_B]&=e^{-r(T-t)},\tag{10.6}\\
\Pi_t[\Phi_{C,K}]&=c(t,S_t;K,T),\tag{10.7}\\
\Pi_t[\Phi_{P,K}]&=p(t,S_t;K,T).
\end{align*}
Where \(c(t,s,K,T,r,\sigma)\) and \(p(t,s,K,T,r,\sigma)\) are the pricing function of the european call and put option. We see that we can replicate these payouts by: buying the stock today and selling at time \(T\), buying a zero coupon \(T\)-bond with face value 1, buying the call and put option.

Then we can by choosing \(\alpha,\beta,\gamma_1,...,\gamma_n\) form a portfolio consisting of \(\alpha\) stocks, \(\beta\) \(T\)-bonds and \(\gamma_i\) call options with maturity \(T\) and strike \(K_i\). The price is then a linear combination given the choice (se proposition 10.1).

The put option is not includet in the above portfolio as we have the put-call parity below

\textbf{Proposition 10.2. (Bjork)} \textbf{(Put-call parity)}\index{Put-call parity} \emph{Consider a European call and a European put, both with strike \(K\) and time of maturity \(T\). Then we have the relation.}

\[
p(t,s) = Ke^{-r(T-t)}+c(t,s)-s.\tag{10.11}
\]

\emph{In particular the put option can be replicated by a constant portfolio consisting of \(K\) zero coupon \(T\)-bonds, a European call option and a single short position in the underlying stock.}

We now have the pleasing proposition given the class of claims we can reach with the buy-and-hold portfolio with \(T\)-bonds, stock and call options

\textbf{Proposition 10.3. (Bjork)} \emph{Fix an arbitrary continuous contract function \(\Phi\) with compact support. Then the corresponding contract can be replicated with arbitrary precision (in sup-norm) using a constant portfolio consisting only of bonds, call options and the underlying stock.}

\hypertarget{the-greeks}{%
\subsection{The Greeks}\label{the-greeks}}

When holding a portfolio we may denote the pricing function by \(P(t,s)\). Here we only have one \textbf{underlying} asset with price process \(S_t\). We now have two types of risk:

\begin{itemize}
\tightlist
\item
  Price changes in the underlying asset.
\item
  Misspecifications in the model parameters.
\end{itemize}

These two risk give rise to ``the greeks'' as defined below.

\textbf{Definition 10.4. (Bjork)} \emph{The greeks of a portfolio is given by}

\[
\Delta=\frac{\partial P}{\partial s},\ \Gamma=\frac{\partial^2 P}{\partial s^2},\ \rho=\frac{\partial P}{\partial r},\ \Theta=\frac{\partial P}{\partial t},\ \mathcal{V}=\frac{\partial P}{\partial s}.
\]

For the call option in particular we have the following derivatives.

\textbf{Proposition 10.5. (Bjork)} \emph{The greeks\index{the greeks} of a portfolio consisting of a single European call option with maturity \(T\) and strike price \(K\) have the following greeks (\(\varphi\) denoting the density function of a \(\mathcal{N}(0,1)\)-variable):}
\begin{align*}
\Delta&=N(d_1),\tag{10.17}\\
\Gamma&=\frac{\varphi(d_1)}{s\sigma\sqrt{T-t}},\tag{10.18}\\
\rho&=K(T-t)e^{-r(T-t)}N(d_2),\tag{10.19}\\
\Theta&=-\frac{s\varphi(d_1)\sigma}{2\sqrt{T-t}}-rKe^{-r(T-t)}N(d_2),\tag{10.20}\\
\mathcal{V}&=s\varphi(d_1)\sqrt{T-t}\tag{10.21}.
\end{align*}

\newpage

\hypertarget{fundamental-pricing-theorem-i-and-ii}{%
\section{Fundamental pricing theorem I and II}\label{fundamental-pricing-theorem-i-and-ii}}

We start by stating the following theorem.

\textbf{Theorem 11.1. (Bjork)} \emph{If at least on of the assets \(S^1,...,S^N\) has diffusion term which is non-zero at all times, and if naive portfolio strategies are admitted, then the model admits arbitrage.}

We will go as follows. Derive the fundamental pricing theorem 1 and 2 in a setting with zero interest rate. Then we will extend the result in general by choosing a simple numeraire. We start by defining some basic notation.

\textbf{Definition 11.2. (Bjork)} \emph{Define the process \(h\) as}

\[
h=[h^0,h^S]:=[h^0,h^1,...,h^N]
\]

\emph{We define the following.}

\begin{itemize}
\tightlist
\item
  \emph{For a process \(h\), its \textbf{value process} \(V_t^h\) is defined by}
  \[
    V_t^h=h^0_t\cdot 1+\sum_{i=1}^Nh_t^iS_t^i,\tag{11.3}
    \]
  \emph{or in compact form}
  \[
    V_t^h=h_t^0\cdot 1 + h_t^S S_t\tag{11.4}
    \]
\item
  \emph{An adapted process \(h^S\) is called \textbf{admissible}\index{admissible portfolio} if there exists a non-negative real number \(\alpha\) (which may depend on the choice of \(h^S\)) such that}
  \[
    \int_0^th_u^SdS_u\ge -\alpha,\tag{11.5}
    \]
  \emph{for all \(t\in[0,T]\). A process \(h\), is called an \textbf{admissible portfolio} process if \(h^S\) is admissible.}
\item
  \emph{An admissible portfolio is said to be \textbf{self-financing}, if}
  \[
    V_t^h=V_0^h+\int_0^th_u^S\ dS_u,\tag{11.6}
    \]
  \emph{i.e.~if}
  \[
    dV_t^h=h_t^S\ dS_t.\tag{11.7}
    \]
\end{itemize}

\textbf{Lemma 11.3. (Bjork)} \emph{For any adapted process \(h^S\) satisfying the admissibility condition above, and for any real number \(x\), there exists a unique adapted process \(h^0\), such that:}

\begin{itemize}
\tightlist
\item
  \emph{The process \(h\) defined by \(h=[h^0,h^S]\) is self-financing.}
\item
  \emph{The value process is given by}
  \[
    V_t^h=x+\int_0^th_u^S\ dS_u.\tag{11.8}
    \]
\end{itemize}

\emph{In particular, the space \(\mathcal{K}_0\) of portfolio values, reachable at time \(T\) by means of a self-financing portfolio with zero initial cost is given by}

\[
\mathcal{K}_0=\left\{\int_0^Th_t^S\ dS_u :\ h^S\ \text{is}\ \text{admissible}\right\}.\tag{11.9}
\]

\textbf{Definition 11.4. (Bjork)} \emph{A probability measure \(Q\) on \(\mathcal{F}_T\) is called \textbf{equivalent martingale measure}\index{equivalent martingale measure} for the market model, the numeraire \(S^0\), and the time interval \([0,T]\), if it has the following properties:}

\begin{itemize}
\tightlist
\item
  \emph{\(Q\sim P\) on \(\mathcal{F}_T\), so \(P\) and \(Q\) are equivalent.}
\item
  \emph{All price processes \(S^0,S^1,...,S^N\) are martingales under \(Q\) on the time interval \([0,T]\).}
\end{itemize}

\emph{An equivalent martingale measure will often be referred to as just ``a martingale measure'' or as ``an EMM''. If \(Q\sim P\) has the property that \(S^0,S^1,...,S^N\) are local martingales, then \(Q\) is called a \textbf{local martingale measure}\index{local martingale measure}.}

\textbf{Theorem 11.5. (Bjork)} \textbf{(The First Fundamental Theorem)}\index{the First Fundamental Theorem} \emph{The model is arbitrage free ``essentially'' if and only if there existis a (local) martingale measure \(Q\).}

\textbf{Definition 11.6. (Bjork)} \emph{With the notation above, we say that the model admits}

\begin{itemize}
\tightlist
\item
  \emph{\textbf{No Arbitrage} (NA)\index{no arbitrage, NA} if}
  \[
    \mathcal{C}\cap L_+^\infty=\{0\},\tag{11.21}
    \]
\item
  \emph{\textbf{No Free Lunch with Vanishing Risk} (NFLVR)\index{No Free Lunch with Vanishing Risk, NFLVR} if}
  \[
    \tilde{\mathcal{C}}\cap L_+^\infty=\{0\},\tag{11.22}
    \]
  \emph{where \(\tilde{\mathcal{C}}\) denotes the closure of \(\mathcal{C}\) in \(L^\infty\).}
\end{itemize}

\textbf{Theorem 11.7. (Bjork)} \textbf{(Kreps-Yan Separation Theorem)}\index{Kreps-Yan Separation Theorem} \emph{If \(\mathcal{C}\) is weak* closed, and if}

\[
\mathcal{C}\cap L_+^\infty=\{0\},
\]

\emph{then there exists a random variable \(L\in L^1\) such that \(L\) is \(P\) almost surely strictly positive, and}

\[
E^P[L\cdot X]\le 0,
\]

\emph{for all \(X\in\mathcal{C}\).}

\textbf{Proposition 11.8. (Bjork)} \emph{If the asset price processes are uniformly bounded, then the condition NFLVR implies that \(\mathcal{C}\) is weak* closed.}

\textbf{Theorem 11.9. (Bjork)} \textbf{(First Fundamental Theorem)}\index{First Fundamental Theorem} \emph{Assume that the asset price process \(S\) is bounded. Then there exists an equivalent martingale measure if and only if the model satisfies NFLVR.}

\textbf{Theorem 11.10. (Bjork)} \textbf{(First Fundamental Theorem)} \emph{Assume that the asset price process \(S\) is locally bounded. Then there exists an equivalent martingale measure if and only if the model satisfies NFLVR.}

\textbf{Assumption 11.4.1. (Bjork)} \emph{We assume that }\(S_t^0>0\) \(P\)\emph{-a.s. for all }\(t\ge 0\).

\textbf{Definition 11.11. (Bjork)} \emph{The \textbf{normalized economy}\index{normalized economy} (also referred to as the ``Z-economy'') is defined by the price vector process \(Z\), where}

\[
Z_t=\frac{S_t}{S_t^0}.
\]

\textbf{Definition 11.12. (Bjork)}

\begin{itemize}
\tightlist
\item
  \emph{A \textbf{portfolio stragegy} is any adapted \((N+1)\)-dimensional process}
  \[
    h_t=[h_t^0,h_t^1,...,h_t^N].
    \]
\item
  \emph{The \textbf{S-value process} \(V_t^S\) corresponding to the portfolio \(h\) is \(h_tS_t\).}
\item
  \emph{The \textbf{Z-value process} \(V_t^Z\) corresponding to the portfolio \(h\) is \(h_tZ_t\).}
\item
  \emph{A portfolio is said to be \textbf{admissible} if it is admissible as an \(Z\) portfolio.}
\item
  \emph{An admissible portfolio is \textbf{S-self-balancing} if}
  \[
    dV_t^S=\sum_{i=0}^Nh_t^i\ dS_t^i\tag{11.26}
    \]
\item
  \emph{An admissible portfolio is \textbf{Z-self-balancing} if}
  \[
    dV_t^Z=\sum_{i=0}^Nh_t^i\ dZ_t^i.\tag{11.28}
    \]
\end{itemize}

\textbf{Lemma 11.13. (Bjork)} \textbf{(Invariance Lemma)}\index{Invariance Lemma} \emph{With assumptions as above, the following hold.}

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  \emph{A portfolio \(h\) is S-self-financing if and only if it is Z-self-financing.}
\item
  \emph{The value processes \(V^S\) and \(V^Z\) are connected by}
  \[
    V_t^Z=\frac{1}{S_t^0}\cdot V_t^S.
    \]
\item
  \emph{A claim \(\mathcal{Y}\) is S-replical if and only if the claim}
  \[
    \frac{\mathcal{Y}}{S_T^0}
    \]
  \emph{is Z-replicable.}
\item
  \emph{The model is S arbitrage free if and only if it is Z arbitrage free.}
\end{enumerate}

\textbf{Theorem 11.14. (Bjork)} \textbf{(The First Fundamental Theorem)} \emph{Consider the market model \(S^0,S^1,...,S^N\) where we assume that \(S^0_t>0\), \(P\)-a.s. for all \(t\ge 0\). Assume furthermore that \(S^0,S^1,...,S^N\) are locally bounded. Then the followin conditions are equivalent:}

\begin{itemize}
\tightlist
\item
  \emph{The model satisfies NFLVR.}
\item
  \emph{There exists a measure \(Q\sim P\) such that the processes}
  \[
    Z^0,Z^1,...,Z^N,
    \]
  \emph{are local martingales under \(Q\).}
\end{itemize}

\hypertarget{completeness-1}{%
\subsection{Completeness}\label{completeness-1}}

\textbf{Lemma 11.15. (Bjork)} \emph{Consider a given \(T\)-claim \(X\). Fix a martingale measure \(Q\) and assume that the normalized claim \(X/S^0_T\) is integrable. If the \(Q\)-martingale \(M\), defined by}

\[
M_t=E^Q\left[\left. \frac{X}{S^0_T}\right\vert \mathcal{F}_t\right],\tag{11.34}
\]

\emph{admits an integral representation of the form}

\[
M_t=x+\sum_{i=1}^N\int_0^th_s^i\ dZ_s^i,\tag{11.35}
\]

\emph{then \(X\) can be hedged in the S-economy. Furthermore, the replicating portfolio \((h^0,h^1,...,h^N)\) is given by the above for \(h^i\), \(i=1,...,N\) and \(h_t^0=M_t-\sum_{i=1}^Nh_t^iZ_t^i\).}

\textbf{Theorem 11.16. (Bjork)} \textbf{(Jacod)}\index{Jacod} \emph{Let \(\mathcal{M}\) denote the convex set of equivalent martingale measures. Then, for any fixed \(Q\in\mathcal{M}\), the following statements are equivalent:}

\begin{itemize}
\tightlist
\item
  \emph{Every \(Q\) local martingale \(M\) has dynamics of the form}
  \[
    dM_t=\sum_{i=1}^Nh_s^i\ dZ_s^i.
    \]
\item
  \emph{\(Q\) is an extremal point of \(\mathcal{M}\).}
\end{itemize}

\textbf{Theorem 11.17. (Bjork)} \textbf{(The Second Fundamental Theorem)}\index{The Second Fundamental Theorem} \emph{Assume that the market is arbitrage free and consider a fixed numeraire asset \(S^0\). Then the market is complete if and onlt if the martingale measure \(Q\), corresponding to the numeraire \(S^0\), is unique.}

\hypertarget{risk-neutral-valuation-formula}{%
\subsection{Risk Neutral Valuation Formula}\label{risk-neutral-valuation-formula}}

We have the setting of a market consisting of the assets \(S^0,...,S^N\) of \(N+1\) assets. We consider the numeraire \(S^0\) being a risk free asset. We introduce a price of contingent claim \(X\), such that the extended market consisting of the price process of \(X\) and the \(N+1\) assets is arbitrage free. Alternatively, we can, equivalently, find a replicating portfolio \(h\) such that \(V^h_T=X\) with probability one.

\textbf{Theorem 11.18. (Bjork)} \textbf{(General Pricing Equation)}\index{General Pricing Equation} \emph{The arbitrage free price process for the \(T\)-claim \(X\) is given by}

\[
\Pi_t[X]=S_t^0E^Q\left[\left.\frac{X}{S^0_T}\right\vert \mathcal{F}_t\right],\tag{11.41}
\]

\emph{where \(Q\) is the (not necessarily unique) martingale measure for the a priori given market \(S^0,S^1,...,S^N\), with \(S^0\) as the numeraire.}

If if we assume that the bank account takes the form

\[
S_t^0=S_0^0e^{-\int_0^tr(s)\ ds},
\]

where \(r\) is the short rate, then we have the familier \emph{risk neutral valuation formula}.

\textbf{Theorem 11.19. (Bjork)} \textbf{(Risk Neutral Valuation Formula)}\index{Risk Neutral Valuation Formula} \emph{Assuming the existence of a short rate, the pricing formula takes the form}

\[
\Pi_t[X]=E^Q\left[\left.e^{-\int_0^Tr(s)\ ds}X\right\vert \mathcal{F}_t\right],\tag{11.42}
\]

\emph{where \(Q\) is the (not necessarily unique) martingale measure with the bank account as the numeraire.}

\textbf{Definition 11.20. (Bjork)} \emph{A \textbf{zero coupon bond}\index{zero coupon bond} with \textbf{maturity date} \(T\), also called a \(T\)-bond, is a contract which guarantees the holder one dollar to be paid on the date \(T\). The price at time \(t\) of a bond with maturity date \(T\) is denoted by \(p(t,T)\).}

\textbf{Proposition 11.21. (Bjork)} \emph{The price of a zero coupon \(T\)-bond\index{$T$-bond} is given by}

\[
p(t,T)=E^Q\left[\left.e^{-\int_t^Tr(s)\ ds}\right\vert \mathcal{F}_t\right],\tag{11.43}
\]

\emph{and in particular we have \(p(T,T)=1\) for all \(T\ge 0\) (eq. 11.44).}

\hypertarget{stochastic-discount-factors-1}{%
\subsection{Stochastic Discount Factors}\label{stochastic-discount-factors-1}}

\textbf{Definition 11.22. (Bjork)} \emph{Assume the existence of a short rate \(r\). For any fixed martingale measure \(Q\), let the likelihood process\index{likelihood process} \(L\) be defined by}

\[
L_t=\frac{dQ}{dP},\ on\ \mathcal{F}_t.\tag{11.48}
\]

\emph{The \textbf{stochastic discount factor}\index{stochastic discount factor} (SDF) process \(\mathbf{M}\), corresponding to \(Q\), is defined as}

\[
\mathbf{M}_t=e^{-\int_0^tr(s)\ ds}L_t\ \ \left(=\frac{1}{B_t}\cdot L_t\right).\tag{11.49/50}
\]

\textbf{Proposition 11.23. (Bjork)} \emph{Assume absence of arbitrage. With notation as above, the following hold:}

\begin{itemize}
\tightlist
\item
  \emph{For any sufficiently integrable \(T\)-claim \(X\), the arbitrage free price is given by}
  \[
    \Pi_t[X]=E^P\left[\left. \frac{\mathbf{M}_T}{\mathbf{M}_t} X \ \right\vert\ \mathcal{F}_t\right].\tag{11.51}
    \]
\item
  \emph{For any arbitrage free asset price process \(S\) (derivative or underlying) the process \(\mathbf{M}_tS_t\) is a (local) \(P\)-martingale.}
\item
  \emph{The \(P\)-dynamics of \(\mathbf{M}\) are given by}
  \[
    d\mathbf{M}_t=-r_t\mathbf{M}_t\ dt+\frac{1}{B_t}\ dL_t.\tag{11.53}
    \]
\end{itemize}

\hypertarget{summary}{%
\subsection{Summary}\label{summary}}

\textbf{Theorem 11.24. (Bjork)} \textbf{(First Fundamental Theorem)} \emph{The market model is free of arbitrage if and only if there exists a \textbf{martingale measure}, i.e.~a measure \(Q\sim P\) such that the processes}
\[
\frac{S_t^0}{S_t^0},\frac{S_t^1}{S_t^0},...,\frac{S_t^N}{S_t^0}
\]
\emph{are (local) martingales under \(Q\).}

\textbf{Proposition 11.25. (Bjork)} \emph{If the numeraire \(S^0\) is the money account, i.e.}
\[
S^0_t=e^{\int_0^t r(s)\ ds},
\]
\emph{where \(r\) is the (possibly stochastic) short rate, and if we assume that all processes are Brownian driven, then a measure \(Q\sim P\) is a martingale measure if and only if all assets \(S^0,S^1,...,S^N\) have the short rate as their local rates of return, i.e.~if the \(Q\)-dynamics are of the form}
\[
dS_t^i=S_t^ir_t\ dt+S_t^i \sigma_t^i\ dW_t^Q,\tag{11.54}
\]
\emph{where \(W^Q\) is a (multidimensional) \(Q\)-Brownian motion.}

\textbf{Theorem 11.26. (Bjork)} \textbf{(Second Fundamental Theorem)} \emph{Assuming absence of arbitrage, the market model is complete if and only if the martingale measure \(Q\) is unique.}

\textbf{Proposition 11.27. (Bjork)}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{In order to avoid arbitrage, \(X\) must be priced according to the formula}
  \[
    \Pi_t[X]=S^0_tE^Q\left[\left. \frac{X}{S^0_T}\ \right\vert\ \mathcal{F}_t\right],\tag{11.55}
    \]
  \emph{where \(Q\) is a martingale measure for \([S^0,S^1,...,S^N]\), with \(S^0\) as the numeraire.}
\item
  \emph{In particular, we can choose the bank account \(B_t\), as the numeraire. Then \(B\) has the dynamics}
  \[
    dB_t=r_tB_t\ dt,\tag{11.56}
    \]
  \emph{where \(r\) is the (possibly stochastic) short rate process. In this case the pricing formula above reduces to}
  \[
    \Pi_t[X]=E^Q\left[\left. e^{-\int_t^T r(s)\ ds}X\ \right\vert\ \mathcal{F}_t\right].\tag{11.57}
    \]
\item
  \emph{As a special case, the price of a zero coupon \(T\)-bond is given by}
  \[
    p(t,T)=E^Q\left[\left. e^{-\int_t^T r(s)\ ds}\ \right\vert\ \mathcal{F}_t\right].\tag{11.58}
    \]
\item
  \emph{Defining the stochastic discount factor \(\mathbf{M}\) by \(\mathbf{M}_t=B_t^{-1}L_t\) we also have the pricing formula.}
  \[
    \Pi_t[X]=E^Q\left[\left. \frac{\mathbf{M}_T}{\mathbf{M}_t}X\ \right\vert\ \mathcal{F}_t\right].\tag{11.59}
    \]
\item
  \emph{Different choices of \(Q\) will generically give rise to different price processes for a fixed claim \(X\). However, if \(X\) is attainable then all choices of \(Q\) will produce the same price process, which then is given by}
  \[
    \Pi_t[X]=V_t^h,\tag{11.60}
    \]
  \emph{where \(h\) is the hedging portfolio. Different choices of hedging portfolios (if such exist) will produce the same price process.}
\item
  \emph{In particular, for every replicable claim \(X\) it holds that}
  \[
    V_t^Q=E^Q\left[\left. e^{-\int_t^T r(s)\ ds}X\ \right\vert\ \mathcal{F}_t\right].\tag{11.61}
    \]
\end{enumerate}

\newpage

\hypertarget{mathematics-of-the-martingale-approach}{%
\section{Mathematics of the martingale approach}\label{mathematics-of-the-martingale-approach}}

\hypertarget{martingale-representation-theorem}{%
\subsection{Martingale representation theorem}\label{martingale-representation-theorem}}

\textbf{Theorem 12.1. (Bjork)} \textbf{(Representation of Brownian Functionals)}\index{Representation of Brownian Functionals} \emph{Let \(W\) be a \(d\) dimensional Brownian motions, and let \(X\) be a random variable such that}

\begin{itemize}
\tightlist
\item
  \(X\in\mathcal{F}^W_T\),
\item
  \(E[\vert X\vert]<\infty\).
\end{itemize}

\emph{Then there exist uniquely determined \(\mathcal{F}^W_t\)-adapted processes \(h^1,...,h^d\), such that \(X\) has the representation}

\[
X=E[X]+\sum_{i=1}^d\int_0^Th^i_s\ dW_s^i.\tag{12.2}
\]

\emph{Under the additional assumption}

\[
E[X^2]<\infty,
\]

\emph{then \(h^1,...,h^d\) are in \(\mathcal{L}^2\).}

\textbf{Theorem 12.2. (Bjork)} \textbf{(The Martingale Representation Theorem)}\index{Martingale Representation Theorem} \emph{Let \(W\) be a \(d\) dimensional Brownian motions, and assume that the filtration \(\mathbf{F}\) is defined as}

\[
\mathcal{F}_t=\mathcal{F}^W_t,\hspace{20pt}t\in[0,T].
\]

\emph{Let \(M\) be any \(\mathcal{F}_t\)-adapted martingale. Then there exist uniquely determined \(\mathcal{F}_t\)-adapted processes \(h^1,...,h^d\) such that \(M\) has the representation}

\[
M_t=M_0+\sum_{i=1}^d\int_0^t h_s^i\ dW_s^i,\hspace{20pt}t\in[0,T].\tag{12.9}
\]

\emph{If the martingale \(M\) is square integrable, then \(h^1,...,h^d\) are in \(\mathcal{L}^2\).}

\hypertarget{girsanov-theorem}{%
\subsection{Girsanov theorem}\label{girsanov-theorem}}

\textbf{Theorem 12.3. (Bjork)} \textbf{(The Girsanov Theorem)}\index{Girsanov Theorem} \emph{Let \(W\) be a \(d\) dimensional \(P\)-Brownian motion on \((\Omega,\mathcal{F},P,\mathbf{F})\) and let \(\varphi\) be any \(d\)-dimensional adapted column vector process. Choose a fixed \(T\) and define the process \(L\) on \([0,T]\) by}
\begin{align*}
dL_t&=\varphi^\top_t L_t\ dW_t,\tag{12.16}\\
L_0&=1,\tag{12.17}
\end{align*}
\emph{i.e.}

\[
L_t = \exp\left\{\int_0^t \varphi^\top_s\ dW_s - \frac{1}{2}\int_0^t \Vert\varphi_s\Vert ^2\ ds\right\}.
\]

\emph{Assume that}

\[
E^P[L_T]=1,\tag{12.18}
\]

\emph{and define the new probability measure \(Q\) on \(\mathcal{F}_T\) by}

\[
L_T=\frac{dQ}{dP},\hspace{15pt}on\ \mathcal{F}_T.\tag{12.19}
\]

\emph{Then}

\[
dW_t=\varphi\ dt+dW_t^Q,\tag{12.20}
\]

\emph{where \(W^Q\) is a \(d\) dimensional \(Q\)-Brownian motion or equivalently}

\[
W_t^Q=W_t-\int_0^t\varphi_s\ ds\tag{12.21}
\]

\emph{is a standard \(Q\)-Brownian motion.}

We will often refere to \(\varphi\) as the \textbf{Girsanov kernel}\index{Girsanov kernel} of the measure transformation. Furthermore, we have written on component form above and the \(L\) dynamics will have the form

\[
dL_t=L_t\sum_{i=1}^d\varphi^i_t\ dW_t^i,
\]

and \(L\) will have the explicit form

\[
L_t=\exp\left\{\sum_{i=1}^d\int_0^t\varphi^i_s\ dW_s^i - \frac{1}{2}\int_0^d\sum_{i=1}^d(\varphi^i_s)^2\ ds\right\}.
\]

The conclusion of the Girsanov Theorem is thwn that we can write

\[
dW_t^i=\varphi_t^i\ dt+dW_t^{Q,i},
\]

for \(i=1,...,d\) where \(W_t^{Q,1},...,W_t^{Q,d}\) are independent standard Brownian motions under \(Q\).

\textbf{Definition 12.4. (Bjork)} \emph{For any Brownian motion \(W\) and any kernel process \(\varphi\), the \textbf{Doleans exponential} process\index{Doleans exponential process} \(\mathcal{E}\) is defined by}

\[
\mathcal{E}(\varphi\bullet W)_t=\exp\left\{\int_0^t\varphi^\top_s\ dW_s -\frac{1}{2}\int_0^t\Vert \varphi\Vert^2_s\ ds\right\}.\tag{12.24}
\]

\textbf{Lemma 12.5. (Bjork)} \textbf{(The Novikov Condition)}\index{Novikov Condition} \emph{Assume that the Girsanov kernel \(\varphi\) is such that}

\[
E^P\left[e^{\frac{1}{2}\int_0^T\Vert \varphi_t\Vert^2\ dt}\right]<\infty.\tag{12.27}
\]

\emph{Then \(L\) is a martingale and in particular \(E^P[L_T]=1\).}

\textbf{Theorem 12.6. (Bjork)} \textbf{(The Converse of the Girsanov Theorem)}\index{Girsanov Theorem, converse} \emph{Let \(W\) be a \(d\)-dimensional standard \(P\)-Brownian motion on \((\Omega,\mathcal{F},P,\mathbf{F})\) and assume that}

\[
\mathcal{F}_t=\mathcal{F}^W_t,\ \forall t.
\]

\emph{Assume that there exists a probability measure \(Q\) such that \(Q<<P\) on \(\mathcal{F}_T\). Then there exists an adapted process \(\varphi\) such that the likelihood process \(L\) has the dynamics}
\begin{align*}
dL_t&=L_t\varphi^\top_t\ dW_t,\\
L_0&=1.
\end{align*}

This gives us a recipe to transform dynamics of Ito processes under the measure \(Q\) as we may rewrite the dynamics of the Brownian motion. We therefore have for an Ito process \(X\) with dynamics

\[
dX_t=\mu(t,X_t)X_t\ dt+\sigma(t,X_t) X_t\ dW_t,
\]

may be transformed under \(Q\) as
\begin{align*}
dX_t&=\mu(t,X_t)X_t\ dt+\sigma(t,X_t) X_t\ dW_t\\
&=\mu(t,X_t)X_t\ dt+\sigma(t,X_t) X_t\ (\varphi_t\ dt+dW_t^Q)\\
&=\left(\mu(t,X_t) + \varphi_t\right) X_t\ dt + \sigma(t,X_t)X_t\ dW_t^Q.
\end{align*}
This may lead us into deducing that

\[
\mu(t,X_t)+\sigma(t,X_t)\varphi_t=r_t\iff\varphi_t=\frac{r_t-\mu(t,X_t)}{\sigma(t,X_t)}.
\]

We furthermore have the Levy characterisation of a Brownian motion.

\textbf{Theorem. (Remark FinKont)} \textbf{(Levy Characterisation of Brownian motion)}\index{Levy Characterisation of Brownian motion} \emph{Let \(X_t\) be an Ito process with \(X_0=0\). Then \(X_t\) is a Brownian motion if and only if the two processes \(X_t\) and \(X_t^2-t\) are continuous martingales.}

\newpage

\hypertarget{black-scholes-model---martingale-approach}{%
\section{Black-Scholes model - martingale approach}\label{black-scholes-model---martingale-approach}}

We consider the standard Black-Scholes model with a single risk free asset and risky asset with dynamics
\begin{align*}
dS_t &= \mu S_t\ dt+\sigma S_t\ dW_t,\tag{13.1}\\
dB_t &= r B_t\ dt.\tag{13.2}
\end{align*}
We want check whether the model is arbitrage free on any time interval \([0,T]\), and find a (perhaps unique) martingale measure such that we may apply the fundamental pricing theorem 1 and 2. From Girsanov this endavour is equivalent with searching for a (perhaps unique) Girsanov kernel \(\varphi\). We therefore define as usual the likelihood process

\[
dL_t=\varphi_ tL_t\ dW_t,
\]

and setting \(dQ=L_T\ dP\) on \(\mathcal{F}_T\), we know from Girsanov theorem that

\[
dW_t=\varphi_t\ dt+dW_t^Q.
\]

Inserting into the Black-Scholes model we have

\[
dS_t=S_t(\mu + \sigma \varphi_t)\ dt+\sigma S_t\ dW_t^Q.
\]

We know that for \(Q\) to be a martingale measure we know that the local rate of return under \(Q\) of \(S\) must be the short rate \(r\) i.e.~we have

\[
\mu + \sigma \varphi_t=r\iff \varphi_t=\frac{r-\mu}{\sigma}=-\frac{\mu -r}{\sigma},\tag{13.3}
\]

and so we see the Girsanov kernel is \textbf{constant and deterministic}. The process has the economic interpretation that the Girsanov kernel is the risk premium per unit volatility.

\textbf{Lemma 13.1. (Bjork)} \emph{The Girsanov kernel \(\varphi\) is given by}

\[
\varphi = -\lambda
\]

\emph{where the market price of risk \(\lambda\) is defined by}

\[
\lambda =\frac{r-\mu}{\sigma}.
\]

We therefore have determined \emph{a martingale} and so we have the result.

\textbf{Theorem 13.2. (Bjork)} \emph{The Black-Scholes model above is arbitrage free.}

We could in general have that \(\mu,\sigma,r\) are adapted processes. If this is the case we would have to show the Nivokov condition.

Pricing then of any \(T\)-claim \(X\) then is given by the risk neatral pricing formula

\[
\Pi_t[X]=e^{-r(T-t)}E^Q[X\ \vert\ \mathcal{F}_t],\tag{13.7}
\]

where the \(Q\) dynamics of \(S\) has local drift \(r\) and volatility from the \(Q\)-brownian motion \(W^Q\).

\textbf{Theorem 13.3. (Bjork)} \emph{The Black-Scholes model above is complete. This also holds for the more general model where \(r,\mu,\sigma\) are adapted processes.}

Hedging is the possible by considiering a \(T\) claim with

\[
E^Q\left[\frac{X}{B_T}\right]<\infty.
\]

Notice the numeraire \(B_t\) as in the normalized \(Z\)-economy. Consider now the \(Q\)-martingale

\[
M_t=E^Q\left[\left. \frac{X}{B_T}\ \right\vert\ \mathcal{F}_t\right],\tag{13.9}
\]

and it now follows from lemma 11.15 the the model is complete if we can find a process \(h_t^1\) such that

\[
dM_t=h_t^1\ dZ_t^i.\tag{13.10}
\]

In order to prove existance of such a process \(h^1\) we use the Martingale Representation Theorem 12.2, which says that there \emph{exists} a process \(g_t\) such that

\[
dM_t=g_t\ dW_t^Q.\tag{13.11}
\]

We can now combine these two equation by the following \(Q\) dynamics

\[
dZ_t^1=Z_t^1\sigma\ dW_t^Q.\tag{13.12}
\]

Hence we have

\[
dM_t=h_t^1Z_t^1\sigma\ dW_t^Q=g_t\ dW_t^Q\ \Rightarrow\ h_t^1=\frac{g_t}{Z_t^1\sigma}.
\]

\textbf{Theorem 13.4. (Bjork)} \emph{In the Black-Scholes model every \(T\)-claim \(X\) satisfying}

\[
E^Q\left[\frac{X}{B_T}\right]<\infty
\]

\emph{can be replicated. The replicating portfolio is given by}
\begin{align*}
h_t^1&=\frac{g_t}{Z_t^1\sigma},\tag{13.13}\\
h_t^0&=M_t-h_t^1Z_t^1,\tag{13.14}
\end{align*}
\emph{where \(M\) is defined by the above and \(g\) is defined by above.}

If the \(T\)-claim is simple that is \(X=\Phi(S_T)\) we may solve a boundary value problem with Feymann-Kac to arrive at the familiar result.

\textbf{Proposition 13.5. (Bjork)} \emph{In the Black-Scholes model every \(T\)-claim on the form \(X=\Phi(S_T)\). Then \(X\) can be replicated by the portfolio}
\begin{align*}
h_t^0&=\frac{F(t,S_t)-S_t\frac{\partial F}{\partial s}(t,S_t)}{B_t},\tag{13.15}\\
h_t^1&=\frac{\partial F}{\partial s}(t,S_t),\tag{13.15}
\end{align*}
\emph{where \(F\) solves the \textbf{Black-Scholes equation}\index{Black-Scholes equation}}
\begin{align*}
\frac{\partial F}{\partial t}(t,s)+rs\frac{\partial F}{\partial s}(t,s)+\frac{1}{2}\sigma^2s^2 \frac{\partial^2 F}{\partial s^2}(t,s)-rF(t,s)&=0,\tag{13.16}\\
F(T,s)&=\Phi(s).\tag{13.16}
\end{align*}
\emph{Furthermore the value process for the replicating portfolio is given by}

\[
V_t=F(t,S_t).
\]

\newpage

\hypertarget{multidimensional-models}{%
\section{Multidimensional models}\label{multidimensional-models}}

We specify the general model by the assumptions below.

\textbf{Assumption 14.0.1} \emph{We assume the following:}

\begin{itemize}
\tightlist
\item
  \emph{There are \(n\) risky assets \(S^1,...,S^n\).}
\item
  \emph{Under the objective probability measure \(P\)\index{objective probability measure}, the \(S\)-dynamics are given by}
  \[
    dS_t^i=\mu_t^iS_t^i\ dt +S_t^i\sum_{j=1}^N\sigma_t^{ij}\ dW_t^j,\tag{14.1}
    \]
  \emph{for \(i=1,...,n\).}
\item
  \emph{The coefficients processes \(\mu^i\) and \(\sigma^{ij}\) above are assumed to be adapted.}
\item
  \emph{We have a standard risk free asset with price process \(B\) with dynamics}
  \[
    dB_t=r_tB_t\ dt,\tag{14.2}
    \]
  \emph{where the short rate process \(r\) is assumed to be an adapted stochastic process.}
\end{itemize}

We can use the representation of \(\mu^i\), \(\sigma^{ij}\) and \(S^i\) as vectors and matrices on the form.

\[
\mu =
\begin{bmatrix}
\mu ^1\\
\vdots\\
\mu ^n
\end{bmatrix},\ \sigma=
\begin{bmatrix}
\sigma^{1,1}& \cdots & \sigma^{i,N}\\
\vdots & \ddots & \vdots\\
\sigma^{n,1}&\cdots&\sigma^{n,N}
\end{bmatrix},\ D(S)=
\begin{bmatrix}
S^{1}& \cdots & 0\\
\vdots & \ddots & \vdots\\
0&\cdots&S^n
\end{bmatrix}.
\]

And so we have the model on compact form.
\begin{align*}
dS_t&= D(S_t)\mu_t\ dt+D(S_t)\sigma_t\ dW_t,\tag{14.3}\\
dB_t&=r_tB_t\ dt.\tag{14.4}
\end{align*}
Now using Girsanov Theorem we can define the prospective likelihood process\index{likelihood process} \(L\) by
\begin{align*}
dL_t&=L_t\varphi_t^\top\ dW_t,\tag{14.5}\\
L_0&=1,\tag{14.6}
\end{align*}
where \(\varphi\) is an adapted \(N\)-dimensional process. Then our candidate martingale measure \(Q\) is given by \(dQ=L_t\ dP\) on \(\mathcal{F}_t\) and the Girsanov theorem give the dynamics

\[
dW_t=\varphi_t\ dt+dW_t^Q,\tag{14.7}
\]

where \(W^Q\) is a standard \(Q\)-Brownian motion. Inserting into the \(P\)-dynamics we obtain.

\[
dS_t=D(S_t)[\mu_t+\sigma_t\varphi_t]\ dt+D(S_t)\sigma_t\ dW_t^Q.\tag{14.8}
\]

The from (11.54) we know that \(Q\) is a martingale measure if and only if the local rate of return (the \(dt\)-term) is the short interest rate i.e.~if and only if

\[
\mu_t+\sigma_t\varphi=\mathbf{r}_t,\tag{14.9}
\]

where \(\mathbf{r}_t=[r,...,r]^\top\in\mathbb{R}^n\). We then have that

\[
\sigma_t\varphi_t = \mathbf{r}_t-\mu_t,\tag{14.11}
\]

where we want to solve for \(\varphi\). Thus we may write a condition for the absence of arbitrage in linear algebra terms.

\textbf{Proposition 14.1. (Bjork)} \emph{A necessary condition for absence of arbitrage is that}

\[
\mathbf{r}_t-\mu_t\in Im[\sigma_t]
\]

\emph{with probability one for each \(t\). A sufficient condition for absence of arbitrage is that there exists a process \(\varphi\) which solves (14.11) and such that \(L\) is a martingale.}

Note that it is not enought for \(\varphi\) to solce (14.11). We also need that \(L\) is a martingale.

\textbf{Definition 14.2. (Bjork)} \emph{A Girsanov kernel \(\varphi\) is said to be \textbf{admissible} if it generates a martingale measure, i.e.~it solves (14.11) and \(L\) is a true martingale.}

\textbf{Definition 14.3. (Bjork)} \emph{The model above is said to be \textbf{generically arbitrage free}\index{generically arbitrage free} if it is arbitrage free for every choice of \(\mu\).}

\textbf{Proposition 14.4. (Bjork)} \emph{Disregarding integrability problems the model is generically arbitrage free if and only if, for each \(t\le T\) and \(P\)-a.s., the mapping}

\[
\sigma_t : \mathbb{R}^B\to \mathbb{R}^n
\]

\emph{is surjective, i.e.~if and only if the volatility matrix \(\sigma_t\) has rank \(n\).}

\textbf{Proposition 14.5. (Bjork)} \emph{Assume that the model i generically arbitrage free and that the filtration \(\mathbf{F}\) is defined by}

\[
\mathcal{F}_t=\mathcal{F}^W_t.\tag{14.14}
\]

\emph{Then, disregarding integrability problems, the model is complete if and only if \(n=N\) and the volatility matrix \(\sigma_t\) is invertible \(P\)-a.s. for each \(t\le T\).}

\textbf{Assumption 14.3.1. (Bjork)} \emph{We assume that the model is generically free of arbitrage, i.e.~that}

\[
Im[\sigma_t]=\mathbb{R}^n,\tag{14.16}
\]

\emph{for all \(t\) and with probability one. We also assume that the model is purely Brownian driven, i.e.~that \(\mathcal{F}_t=\mathcal{F}_t^W\).}

\textbf{Proposition 14.6. (Bjork)} \emph{Under assumption 14.3.1 the model is complete if and only if}

\[
Im[\sigma_t^\top]=\mathbb{R}^N.\tag{14.23}
\]

\emph{If the model is complete then, using the notation of chapter 11, the replicating portfolio \([h^0,h^S]\) is given by}
\begin{align*}
h_t^S&=g_t\sigma_t^{-1}D^{-1}(Z_t),\tag{14.24}\\
h_t^2&=M_t-h_tZ_t.\tag{14.25}
\end{align*}
\emph{With \(M_t\) defined by}

\[
M_t=E^Q\left[\left. \frac{\mathcal{X}}{B_T}\ \right\vert\ \mathcal{F}_t \right].\tag{14.17}
\]

\textbf{Theorem 14.7. (Bjork)} \textbf{(The Second Fundamental Theorem)}\index{The Second Fundamental Theorem} \emph{Under assumptions 14.3.1 the model is complete if and only if the martingale measure is unique. This is equivalent with the statements: \(Ker[\sigma_t]=\{0\}\), \(Im[\sigma_t^\top]=\mathbb{R}^N\) and \(\sigma_t^{-1}\) exists (i.e.~\(\sigma_t\) is invertible).}

Pricing of any \(T\)-claim \(\mathcal{X}\) is now given by the risk neutral valuation formula

\[
\Pi_t[\mathcal{X}] = E^Q\left[\left. e^{-\int_t^Tr_u\ du}\mathcal{X} \ \right\vert\ \mathcal{F}_t\right],\tag{14.27}
\]

where \(Q\) is some choice of martingale measure. Alternatively we can write the price as

\[
\Pi_t[\mathcal{X}] = E^Q\left[\left. \frac{\mathbf{M}_T}{\mathbf{M}_t}\mathcal{X} \ \right\vert\ \mathcal{F}_t\right],\tag{14.29}
\]

where \(\mathbf{M}\) is the stochastic discount factor, defined by

\[
\mathbf{M}_t=\frac{1}{B_t}L_t.
\]

If we have a simple claim i.e.~of the form \(\mathcal{X}=\Phi(S_t)\) and if \(S\) is Markovian we have

\[
e^{-r(T-t)}E^Q[\Phi(S_t)\ \vert\ \mathcal{F}_t]=e^{-r(T-t)}E^Q[\Phi(S_t)\ \vert\ S_t],
\]

and then the pricing process must be of the form \(\Pi_t[\Phi]=F(t,S_t)\). We then have \(F\) to be the solutions to the PDE
\begin{align*}
F_t(t,s)+\sum_{i=1}^nrs_iF_i(t,s)+\frac{1}{2}\text{tr}\left\{\sigma^\top D(S)F_{ss}D(S)\sigma\right\}-rF(t,s)&=0,\tag{14.31}\\
F(T,s)&=\Phi(s),\tag{14.31}
\end{align*}
where \(F_i=\frac{\partial F}{\partial s_i}\) and \(F_{ss}\) denotes the Hessian matrix. Furthermore, \(\text{tr}(A)\) denotes the trace of \(A\) i.e.~the sum of the diagonal. We have that the hedging portfolio has value process \(V_t^h=F(t,S_t)\) with dynamics

\[
dV_t^h=\sum_{i=1}^n F_i(t,S_t)\ dS_t^i.
\]

Then we must gave the solution
\begin{align*}
h_t^i&=\frac{\partial F}{\partial s_i}(t,S_t),\hspace{10pt}i=1,...,n,\tag{14.32}\\
h_t^0&=\frac{1}{B_t}\left\{F(t,S_t)-\sum_{i=1}^n \frac{\partial F}{\partial s_i}(t,S_t)\ S_t^i\right\}.\tag{14.33}
\end{align*}

\textbf{Proposition 14.8. (Bjork)} \emph{With \(L\)-dynamics as in \(dL_t=L_t\varphi^\top_t\ dW_t\), the \(\mathbf{M}\)-dynamics are}

\[
d\mathbf{M}_t=-r_t\mathbf{M}_t\ dt+\mathbf{M}_t\varphi_t^\top\ dW_t,\tag{14.39}
\]

\emph{or alternatively in terms of the market price of risk \(\lambda_t=-\varphi_t\)}

\[
d\mathbf{M}_t=-r_t\mathbf{M}_t\ dt-\mathbf{M}_t\lambda_t^\top\ dW_t.\tag{14.40}
\]

\textbf{Proposition 14.9. (Bjork)} \textbf{(The Hansen-Jagannathan Bounds)}\index{The Hansen-Jagannathan Bounds} \emph{Assume generic absence of arbitrage. Then the following holds for all assets, underlying or derivative, and for all admissible Girsanov kernels \(\varphi\), and market prices of risk \(\lambda\).}

\[
\frac{\vert \mu_t^p - r_t\vert}{\Vert \sigma_t ^p\Vert}\le \Vert \varphi_t\Vert,\hspace{15pt} \frac{\vert \mu_t^p - r_t\vert}{\Vert \sigma_t ^p\Vert}\le \Vert \lambda_t\Vert.\tag{14.42}
\]

\hypertarget{non-life-insurance-mathematics}{%
\chapter{Non-Life Insurance Mathematics}\label{non-life-insurance-mathematics}}

Noget indhold

\hypertarget{probabilistic-machine-learning}{%
\chapter{Probabilistic Machine Learning}\label{probabilistic-machine-learning}}

\hypertarget{quantative-risk-management}{%
\chapter{Quantative Risk Management}\label{quantative-risk-management}}

\hypertarget{the-loss-variable}{%
\section{The Loss Variable}\label{the-loss-variable}}

We describe the financial risk with random variables defined on a filtered probability space \(\left(\Omega,\mathcal{F},P,\left\{\mathcal{F}_t\right\}_{t\in \mathbb{R}_+}\right)\). We assume that \emph{the value process} \(V_t\), denoting the market value of the portfolio at time \(t\), is adapted to the filtration i.e.~may be determined at time \(t\) or from information available at time \(t\). We will be considering discrete time jumps of size \(\Delta t\) for simplicity and assume that

\begin{itemize}
\tightlist
\item
  the portfolio remains fixed over the time horizon \([t,t+\Delta t)\),
\item
  there are no income or payments made during the time period.
\end{itemize}

This means in particular that the value

\[
\Delta V_{t+\Delta t}=V_{t+\Delta t}-V_t,
\]

only depends on the valuation of the components in the portfolio. We will henceforth be using the notation \(t\) and \(t+1\) and so forth denoting the intervals \([t+n\Delta t,t+(n+1)\Delta t)\) i.e.~\(t\) may be any time and the integer representing how many time jumps of size \(\Delta t\) has been made since \(t\).

Under the assumptions above it is reasonable to assume that \(V_t\) is Markovian in the sense that there exist \(d\ge 1\) random sources \(\mathbf{Z}_t=(Z_{t,1},...,Z_{t,d})^\top\) such that

\[
V_t=f(t,\mathbf{Z}_t)\tag{2.2}
\]

for some measurable function \(f : \mathbb{R}_+\times \mathbb{R}^d\to\mathbb{R}\). We will call \(\mathbf{Z}_t\) the \emph{risk factors} associated with the portfolio. The could be for instance the stockprice of a stock held in the portfolio. We may now define the \emph{loss variable} as \(L_{t+1}:=-(V_{t+1}-V_t)\) and \emph{risk-factor changes} as \(\mathbf{X}_{t+1}:=\mathbf{Z}_{t+1}-\mathbf{Z}_t\) and see that

\[
L_{t+1}=-\left(f(t+1,\mathbf{Z}_t+\mathbf{X}_{t+1})-f(t,\mathbf{Z}_t)\right),\tag{2.3}
\]

if one assume differentiability of \(f\) we may approximate \(L_{t+1}\) as

\[
L_{t+1}^\Delta:=-\left(\frac{\partial f}{\partial t}(t,\mathbf{Z}_t)+\sum_{i=1}^d \frac{\partial f}{\partial z_i}(t,\mathbf{Z}_t)\mathbf{X}_{t+1,i}\right).\tag{2.4}
\]

This is obviously a nice linear but the approximation error may be large if \(\Delta t\) is large. We are interested in the the distribution of \(L_{t+1}\) such that we may determine sufficiently capital holding in realtion to the risk associated with the assets and liabilities held on the firm's balance sheet.

\hypertarget{risk-measures}{%
\subsection{Risk measures}\label{risk-measures}}

In the general sense, a \emph{risk measure} is simply a measurable function \(\rho : \mathbb{L} \to\mathbb{R}\) taking a loss variable \(L\in \mathbb{L}\) as input and associating a number \(\rho(L)\) as output. In this setting we let \(\mathbb{L}\) be the set of all real-valued random variables. We may interpret this as the riskyness of the portfolio with associated loss variable \(L\). That is say we have two loss variable from the value processes \(V\) and \(V'\) i.e.~\(L\) and \(L'\) we say that \(V\) is riskier than \(V'\) if and only if \(\rho(L)\ge \rho(L')\).

We may now consider the fundamental definition of a coherent risk measures as in \href{https://www.researchgate.net/publication/227614132_Coherent_Measures_of_Risk}{\emph{Coherent Measures of Risk}} by Artzner, Delbean, Eber and Heath (1999), by first stating the definition of a risk measure.

\textbf{Definition.} \textbf{(Risk Measure)} \emph{Let \(\mathbb{L}\) be the set of all real valued random variable i.e.}

\[
\mathbb{L}=\left\{X\ \vert \ X : (\Omega,P,\mathcal{F})\to (\mathbb{R},\mathbb{B})\right\}
\]

\emph{Any mapping \(\rho : \mathbb{L} \to\mathbb{R}\) is called a measure of risk.}

We now want some properties to be satisfied by the mapping \(\rho\) such that it is a sensable measure of risk. To this we define four axioms as.

\textbf{Definition.} \textbf{(Coherent Risk Measure)} \emph{Let \(\rho\) be a measure of risk. We say that \(\rho\) is a coherent risk measure if and only if \(\rho\) satisfies the axioms below.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Translation invariance:} \emph{Let \(\alpha\in\mathbb{R}\) and \(r\) be a predictable process we have \(\rho(X+\alpha\cdot r)=\rho(X)+\alpha\).}
\item
  \textbf{Subadditivity:} \emph{Let \(X_1,X_2\in\mathbb{L}\), then \(\rho(X_1+X_2)\le \rho(X_1)+\rho(X_2)\).}
\item
  \textbf{Positive homogeneity:} \emph{Let \(c>0\), then \(\rho(cX)=c\rho(X)\).}
\item
  \textbf{Monotonicity:} \emph{Let \(X,Y\in\mathbb{L}\) such that \(X\le Y\) \(P\)-a.s., then \(\rho(X)\le \rho(Y)\).}
\end{enumerate}

\emph{Remark:} We see that axiom (1) gives the equation \(\rho(X+\rho(X)\cdot r)=0\) i.e.~we may hedge the risk by holding \(\rho(X)\) in a asset with rate of return \(r\). The axiom (2) ensures that we take into account the correlation of multiple assets i.e.~we will in general not experience the maximal loss of two sources at the same time (although this is possible). If \(X_1\) and \(X_2\) satisfies \(\text{Corr}(X_1,X_2)=1\) then \(\rho(X_1+X_2)= \rho(X_1)+\rho(X_2)\).

There exist alot of different tangible measures of risk, some being coherent others non-coherent. The most well studied include Value at Risk VaR and Expected Shortfall ES. These two measures are in the realm of the loss distribution approach, however before studying these we introduce a few other worthy mentions: Factor sensitivity measures and scenario based risk measures:

\textbf{Factor sensitivity measures} are measures on the form \(\rho=g(\nabla L)\) where \(g\) is some \(d\)-dimensional measurable function. In this \(\nabla L\) is the gradient i.e.

\[
\nabla L=\nabla \Big(f(t,\mathbf{Z}_t)-f(t+1,\mathbf{Z}_{t+1})\Big)=\left(\frac{\partial L}{\partial z_1},...,\frac{\partial L}{\partial z_d}\right).
\]

\textbf{Scenario based risk measures} are measures where one assume that a collection \(\mathbf{x}=(x_1,...,x_N)\) of events \(x_i\in \Omega\) such that \(\sum_{i=1}^N P(x_i)=1-\varepsilon\) for some small \(\varepsilon>0\). We may then associate the measures \(\psi\) as

\[
\psi(L)=\max\left\{w_1L(x_1),...,w_NL(x_N)\right\},
\]

with \(\mathbf{w}\ge 0\) and \(\sum_{i=1}^Nw_i=1\) being weights. That is \(\psi\) gives the largest weighted loss. A natural way of choosing \(w_i\) is such that \(w_i\approx P(x_i)\), but one may also weight certain events with a larger weight for a more prudent measure. Notice also that the criteria \(\sum_{i=1}^N P(x_i)=1-\varepsilon\) does not necessarily have to be satisfied when considering the worst possible outcomes.

\hypertarget{value-at-risk}{%
\subsubsection{Value at Risk}\label{value-at-risk}}

The Value at Risk, henceforth VaR, is a quantile measure of the loss distribution \(F_L\) associated with \(L\). We define VaR as such:

\textbf{Definition 2.8. (McNeil)} \textbf{(Value-at-Risk)} \emph{Let \(\alpha\in (0,1)\) (\(\alpha\) being large) we define the VaR of a portfolio with loss variable \(L\) at the confidence level \(\alpha\) is a given by}
\begin{align*}
\text{VaR}_\alpha(L)&=\inf\left\{ l\in\mathbb{R}\ :\ P(L>l)\le 1-\alpha \right\}\\
&=\inf\left\{ l\in\mathbb{R}\ :\ F_L(l)\ge \alpha \right\}\\
&=F^{\leftarrow}_L(\alpha),
\end{align*}
\emph{where \(F^{\leftarrow}_L\) is the generalized inverse of \(F_L\).}

\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.48\textwidth]{figures/VaR_ES.png}
  \end{center}
\end{wrapfigure}

There exist alot of different tangible measures of risk, some being coherent others non-coherent. The most well studied include Value at Risk VaR and Expected Shortfall ES. These two measures are in the realm of the loss distribution approach, however before studying these we introduce a few other worthy mentions: Factor sensitivity measures and scenario based risk measures:

\hypertarget{measure-theory}{%
\chapter{Measure theory}\label{measure-theory}}

\hypertarget{equivalent-probability-measures}{%
\section{Equivalent Probability Measures}\label{equivalent-probability-measures}}

\hypertarget{the-radon-nikodym-theorem}{%
\subsection{The Radon-Nikodym Theorem}\label{the-radon-nikodym-theorem}}

\textbf{Definition A.50. (Bjork)} \emph{Consider a measurable space \((X,\mathcal{F})\) on which there are defined two seperate measures \(\mu\) and \(\nu\):}

\begin{itemize}
\item
  \emph{If, for all \(A\in \mathcal{F}\), it holds that}
  \[
    \mu(A)=0\ \Rightarrow\ \nu(A)=0,\tag{A.7}
    \]
  t\_hen \(\nu\) is said to be \textbf{absolutely continuous}\index{absolutely continuous} with respect to \(\mu\) on \(\mathcal{F}\) and we write this as \(\nu < < \mu\).\_
\item
  \emph{If we have both \(\mu << \nu\) and \(\nu << \mu\), then \(\mu\) and \(\nu\) said to be \textbf{equivalent}\index{equivalent measures} and we write \(\mu\sim \nu\).}
\item
  \emph{If there exists two events, \(A\) and \(B\) such that:}

  \begin{itemize}
  \tightlist
  \item
    \(A\cup B=X\),
  \item
    \(A\cap B=\emptyset\),
  \item
    \(\mu(B)=0\), and \(\nu(A)=0\),
  \end{itemize}

  \emph{then \(\nu\) and \(\mu\) are said to be mutually \textbf{singular}\index{singular}, and we write \(\mu\ \bot\ \nu\).}
\end{itemize}

\textbf{Theorem A.52. (Bjork)} \textbf{(The Radon-Nikodym Theorem)}\index{Radon-Nikodym Theorem} \emph{Consider the measure space \((X,\mathcal{F},\mu)\), where we assume that \(\mu\) is finite, i.e.~that \(\mu(X)<\infty\). Let \(\nu\) be a measure on \((X,\mathcal{F})\) such that \(\nu <<\mu\) on \(\mathcal{F}\). Then there exists a non-negative function \(f : X\to \mathbb{R}\) such that:}
\begin{align*}
&f\ \text{is}\ \mathcal{F}\text{-measurable}\tag{A.9}\\
&\int_X f(x)\ d\mu(x)<\infty,\tag{A.10}\\
&\nu(A)=\int_Af(x)\ d\mu(x),\ \text{for all}\ A\in \mathcal{F}.\tag{A.11}
\end{align*}
\emph{The function \(f\) is called the \textbf{Radon-Nikodym derivative}\index{Radon-Nikodym derivative} of \(\nu\) w.r.t. \(\mu\). It is uniquely determined \(\mu\)-a.e. and we write}

\[
f(x)=\frac{d\nu(x)}{d\mu(x)},\tag{A.12}
\]

\emph{or alternatively}

\[
d\nu(x)=f(x)\ d\mu(x).\tag{A.13}
\]

\hypertarget{equivalent-probability-measures-1}{%
\subsection{Equivalent Probability Measures}\label{equivalent-probability-measures-1}}

\textbf{Lemma B.38. (Bjork)} \emph{For two probability measures \(P\) and \(Q\), the relation \(P\sim Q\) on \(\mathcal{F}\) holds if and only if \(P(A)=1\) if and only if \(Q(A)=1\) for all \(A\in\mathcal{F}\).}

\textbf{Proposition B.39. (Bjork)} \emph{Assume that \(Q << P\) on \(\mathcal{F}\) and that \(\mathcal{G}\subseteq \mathcal{F}\). Then the Radon-Nikodym derivatives \(L^{\mathcal{F}}\) and \(L^{\mathcal{G}}\) are related by}

\[
L^{\mathcal{G}}=E^P[L^{\mathcal{F}}\ \vert\ \mathcal{G}].\tag{B.17}
\]

\textbf{Proposition B.41. (Bjork)} \textbf{(Bayes' Theorem)}\index{Bayes' Theorem} \emph{Assume that \(X\) is a random variable on \((\Omega, \mathcal{F},P)\), and let \(Q\) be another probability measure on \((\Omega,\mathcal{F})\) the Radon-Nikodym derivative}

\[
L=\frac{d Q}{dP}
\]

\emph{on \(\mathcal{F}\). Assume that \(X\in L^1(\Omega,\mathcal{F},Q)\) and \(\mathcal{G}\) is a sigma-algebra with \(\mathcal{G}\subseteq \mathcal{F}\).\index{equivalent probability measures} Then}

\[
E^Q[X\ \vert\ \mathcal{G}]=\frac{E^P[L\cdot X\ \vert\ \mathcal{G}]}{E^P[L\ \vert\ \mathcal{G}]},\ Q-a.s.\tag{B.18}
\]

\hypertarget{likelihood-processes}{%
\subsection{Likelihood processes}\label{likelihood-processes}}

\textbf{Proposition C.12. (Bjork)} \emph{Consider a filtered probability space \((\Omega, \mathcal{F},P,\mathcal{F}_t)\) on a compact interval \([0,T]\). Suppose \(L_T\) is some non-negative integrable random variable in \(\mathcal{F}_T\). We can then define a new measure \(Q\) on \(\mathcal{F}_T\) by setting}

\[
dQ=L_T\ dP
\]

\emph{on \(\mathcal{F}_T\) and if \(E^P[L_T]=1\) the measure \(Q\) will also be a probability measure. The likelihood process\index{likelihood process} \(L\), defined by}

\[
L_t=\frac{dQ}{dP},\ on\ \mathcal{F}_t,\tag{C.8}
\]

\emph{is a \((P,\mathcal{F}_t)\)-martingale.}

\textbf{Proposition C.13. (Bjork)} \emph{A process \(M\) is a \(Q\)-martingale if and only if the process \(L\cdot M\) is a \(P\)-martingale.}

\hypertarget{random-variables}{%
\chapter{Random Variables}\label{random-variables}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\textbf{Definition 1.1. (Hansen)} \emph{A \textbf{real-valued random variable}\index{random variable} \(X\) on a probability space \((\Omega, \mathbb{F},P)\) is a measurable map \(X : (\Omega,\mathbb{F})\to (\mathbb{R},\mathbb{B})\).}

We never specify the background space \((\Omega, \mathbb{F},P)\) however we always assume \(X\) is \(\mathbb{F}-\mathbb{B}\) measurable. This assumption implies \((X\in A)\in \mathbb{F}\) for every \(A\in \mathbb{B}\). We may want to show measurability for constructed variables and so it surfises to show measurability for generaters for \(\mathbb{B}\) such as checking \((X\le a)\in\mathbb{F}\) for every \(a\in\mathbb{R}\).

\textbf{Definition 1.2. (Hansen)} \emph{The \textbf{distribution}\index{distribution} of a real-valued random variable \(X\), defined on a probability space \((\Omega,\mathbb{F},P)\), is the collection of probability values}
\begin{align*}
    P(X\in A)\hspace{15pt}\text{for}\ A\in \mathbb{B}.\tag{1.3}
\end{align*}
\emph{In other words: the distribution of \(X\) is the image measure \(X(P)\) on \((\mathbb{R},\mathbb{B})\).}

\textbf{Lemma 1.3. (Hansen)} \emph{Let \(X\) and \(X'\) be two real-valued random variables on a probability space \((\Omega,\mathbb{F},P)\). If}
\begin{align*}
    P(X=X')=1
\end{align*}
\emph{then \(X\) and \(X'\) has the same distribution.}

An often used way of summarizing the distribution is through the \textbf{distribution function}\index{distribution function} \(F(x)=P(X\le x)\) for some \(x\in\mathbb{R}\).

\textbf{Definition 1.4. (Hansen)} \emph{A real-valued random variable \(X\) has a \textbf{discrete} distribution\index{discrete distribution} if there is a countable set \(S\subset\mathbb{R}\) such that \(P(X\in S)=1\).}

Usually \(S\) is one of \(\mathbb{N},\mathbb{Z},\mathbb{Q}\) or a subset of these. We may in the discrete case define the distribution by the point probabilities \(P(X=x)=p(x)\) for \(x\in S\).

\textbf{Definition 1.5. (Hansen)} \emph{A real-valued random variable \(X\) has a distribution with \textbf{density}\index{density} \(f : \mathbb{R}\to [0,\infty)\) if}
\begin{align*}
    P(X\in A)=\int_Af(x)dx\hspace{15pt}\text{for}\ A\in \mathbb{B}.\tag{1.5}
\end{align*}
\emph{If this is the case we will write \(X(P)=f\cdot m\) or \(X\sim f\cdot m\).}

\textbf{Definition 1.6. (Hansen)} \emph{A real-valued random variable \(X\) defined on a probability space \((\Omega, \mathbb{F},P)\) is said to have \(p\)'th moment for som \(p>0\) if}
\begin{align*}
    E\vert X\vert^p<\infty\tag{1.12}
\end{align*}
\emph{The collection of all variables that satisfies (1.12) is denoted by \(\mathcal{L}^p(\Omega,\mathbb{F},P)\).}

Recall the definition of the \textbf{expectation}\index{expectation} of \(X\) by
\begin{align*}
    E\, X=\int XdP \in R\cup \{-\infty,+\infty\}.\tag{1.11}
\end{align*}
We recall that for any measurable function \(f : \mathbb{R}\to \mathbb{R}\) that are continuous on a set \(A\in\mathbb{B}\) such that \(P(X\in A)=1\) we may change variable simply by computing
\begin{align*}
    E\, f(X)=\int f\circ XdP=\int f(x)dX(P)(x).
\end{align*}

\textbf{Lemma 1.7. (Hansen)} \emph{(Markov's inequality)\index{Markov's inequality} Let \(X\) be a non-negative random variable. For any \(c>0\) it holds that}
\begin{align*}
    P(X\ge c)\le \frac{E\, X}{c}\left(\le \frac{E\ X^n}{c^n}\text{ or }\le \frac{E\left(\varphi(X)\right)}{\varphi(c)}\right).\tag{1.14}
\end{align*}
\emph{for some \(\varphi\) non-negative monotome increasing function.}

Some other versions of Markov's inequality can be found in the form of \textbf{Chebyshev's inequality}, \textbf{Chebyshev-Cantelli's inequality} or \textbf{Jensen's inequality}\index{Chebyshev's inequality}\index{Chebyshev-Cantelli's inequality}\index{Jensen's inequality} repectively: Let \(X\) be a real-valued random variable in \(\mathcal{L}^2(\Omega,\mathbb{F},P)\) it holds for any \(\varepsilon>0\).
\begin{align*}
    P\left(\vert X-E\, X\vert \ge \varepsilon\right)&\le \frac{V\, X}{\varepsilon^2}\tag{1.15}\\
    P\left( X-E\, X \ge \varepsilon\right)&\le \frac{V\, X}{V\, X+\varepsilon^2}\tag{prob: 1.13(c)}\\
    \varphi\left(E\ X\right)&\le E\left( \varphi(X)\right)
\end{align*}
for some convex function \(\varphi\).

\textbf{Lemma 1.8. (Hansen)} \emph{Let \(X\) be a non-negative random variable. It holds that}
\begin{align*}
    E\, X=\int_0^\infty P(X>t)dt.\tag{1.16}
\end{align*}
\emph{where the integral on the right hand side is with respect to Lebesgue measure.}

\textbf{Definition 1.9. (Hansen)} \emph{The \textbf{joint distribution}\index{joint distribution} of real-valued random variables \(X_1,...,X_k\), defined on a probability space \((\Omega, \mathbb{F},P)\), is the collection of probability values}
\begin{align*}
    P(\mathbf{X}\in A)\hspace{15pt}\text{for}\ A\in\mathbb{B}_k.\tag{1.21}
\end{align*}
\emph{In other words: the joint distribution of \(X_1,...,X_k\) (or simply: the distribution of \(\mathbf{X}\)) is the image measure \(\mathbf{X}(P)\) on \(\left(\mathbb{R}^k,\mathbb{B}_k\right)\).}

\textbf{Definition 1.11. (Hansen)} \emph{Real-valued random variables \(X_1,...,X_k\), defined on a probability space \((\Omega, \mathbb{F},P)\), are \textbf{jointly independent}\index{independent, jointly} if}
\begin{align*}
    P\left(X_1\in A_1,...,X_k\in A_k\right)=\prod_{i=1}^kP(X_i\in A_i)\hspace{15pt}\text{for}\ A_1,...,A_k\in\mathbb{B}.\tag{1.23}
\end{align*}
\emph{In other words: the variables are independent if the joint distribution \(\mathbf{X}(P)\) equals the product measure \(X_1(P)\otimes ... \otimes X_k(P)\).}

\textbf{Theorem 1.12. (Hansen)} \emph{Let \(X_1,...,X_k\) be real-valued random variables defined on a probability space \((\Omega, \mathbb{F},P)\). If the variables are independent and if \(E\vert X_i\vert<\infty\) for \(i=1,...,k\), then the product \(X_1\cdot ...\cdot X_k\) has first moment and}
\begin{align*}
    E\left(X_1\cdot ... \cdot X_k\right)=\prod_{i=1}^kE\, X_i\tag{1.24}
\end{align*}

The equality only holds for two independent variables. However the \textbf{Cauchy-Schwarz inequality}\index{Cauchy-Schwarz inequality} which closely resembles (1.24) holds wether or not \(X\) or \(Y\) are independent:
\begin{align*}
    \left(E\vert XY\vert\right)^2\le E\, X^2\, E\, Y^2\text{ or }E\vert XY\vert \le \sqrt{E\ X^2}\sqrt{E\ Y^2}.\tag{1.25}
\end{align*}
Furthermore the theorem give rise to a measure for dependence i.e.~the \textbf{covariance}\index{covariance} between two variables \(X\) and \(Y\)
\begin{align*}
    \text{Cov}(X,Y)=E\left((X-E\,X)(Y-E\,Y)\right)=E(XY)-(E\, X)(E\, Y)\tag{1.26}
\end{align*}
with Cov\((X,Y)\ne 0\) if and only if \(X\) and \(Y\) are dependent. With Cov\((X,Y)=0\) the test is inconlusive. However independence implies Cov\((X,Y)=0\).

\newpage

\hypertarget{conditional-expectation}{%
\section{Conditional expectation}\label{conditional-expectation}}

The theory of conditional expectation is well-known from courses on the bachelor. Because of this we will only summarise the most important results.

We consider a background space \((\Omega,\mathcal{F},P)\) and a sub-sigma algebra \(\mathcal{G}\subseteq \mathcal{F}\). We assume that some stochastic variable is \(\mathcal{F}\)-measurable, that is the mapping \(X : (\Omega,\mathcal{F},P) \to (\mathbb{R},\mathbb{B},m)\) is \(\mathcal{F}-\mathbb{B}\)-measurable i.e.~\(\forall B\in\mathbb{B} : \{X\in B\}\in\mathcal{F}\). For some random variable \(Z\) defined on the subspace \((\Omega,\mathcal{G},P)\), we say that \(Z\) is the conditional expectation of \(X\) given \(\mathcal{G}\) if

\[
\forall G\in\mathcal{G} : \int_G Z(\omega)\ dP(\omega)=\int_G X(\omega)\ dP(\omega).
\]

This fact is summed up in the definition below.

\textbf{Definition B.27. (Bjork)} \textbf{(Conditional expectation)} \emph{Let \((\Omega,\mathcal{F},P)\) be a probability space and \(X\) a random variable in \(L^1(\Omega,\mathcal{F},P)\) (\(\vert X\vert\) is integrable). Let furthermore \(\mathcal{G}\) be a sigma-algebra such that \(\mathcal{G}\subseteq \mathcal{F}\). If \(Z\) is a random variable with the properties that:}

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  \emph{\(Z\) is \(\mathcal{G}\)-measurable.}
\item
  \emph{For every \(G\in\mathcal{G}\) it holds that}
  \[\int_G Z(\omega)\ dP(\omega)=\int_G X(\omega)\ dP(\omega).\tag{B.5}\]
\end{enumerate}

\emph{Then we say that \(Z\) is the \textbf{conditional expectation of \(X\) given the sigma-algebra \(\mathcal{G}\)}. In that case we denote \(Z\) by the symbol \(E[X\ \vert\ \mathcal{G}]\).}

We see that from the above it always holds that \(X\) satisfies (ii). It does not, however, always hold that \(X\) is \(\mathcal{G}\)-measurable. Given this fact it is not trivial that a random variable \(E[X\ \vert\ \mathcal{G}]\) even exists. This nontriviality is fortunatly resolved by the Radon-Nikodym theorem.

\textbf{Theorem B.28. (Bjork)} \textbf{(Existance and uniqueness of Conditional expectation)} \emph{Let \((\Omega,\mathcal{F},P)\), \(X\) and \(\mathcal{G}\) be given as in the definition above. Then the following holds:}

\begin{itemize}
\tightlist
\item
  \emph{There will always exist a random variable \(Z\) satisfying conditions (i)-(ii) above.}
\item
  \emph{The variable \(Z\) is unique, i.e.~if both \(Y\) and \(Z\) satisfy (i)-(ii) then \(Y=Z\) \(P\)-a.s.}
\end{itemize}

This result ensures that we may condition on any sigma-algebra for instance \(\mathcal{G}=\sigma(Y)\) in that case we (pure notation) write

\[
E[X\ \vert\ \sigma(Y)]=E[X\ \vert\ Y],\hspace{20pt}\sigma(Y)=\sigma\left(\left\{ Y\in A,\ A\in\mathbb{B}\right\}\right).
\]

In the above \(\sigma(Y)\) is simply the smallest sigma-algebra containing all the pre-images of \(Y\), that is the smallest sigma-algebra making \(Y\) measurable! Giving this foundation there are a few properties conditional expectation have which is rather useful (for instance the tower property).

Below we assume: Let \((\Omega,\mathcal{F},P)\) be a probability space and \(X,Y\) be random variables in \(L^1(\Omega,\mathcal{F},P)\).

\textbf{Proposition B.29.} \textbf{(Monotinicity/Linearity of Conditional expectation)} \emph{The following holds:}

\[X\le Y\ \Rightarrow\ E[X\ \vert\ \mathcal{G}]\le E[Y\ \vert\ \mathcal{G}],\hspace{20pt}P-\text{a.s.}\tag{B.6}\]
\[E[\alpha X + \beta Y\ \vert\ \mathcal{G}]=\alpha E[X\ \vert\ \mathcal{G}]+ \beta E[Y\ \vert\ \mathcal{G}],\hspace{20pt}\forall \alpha,\beta\in\mathbb{R}.\tag{B.7}\]

\textbf{Proposition B.30. (Bjork)} \textbf{(Tower property)} \emph{Assume that it holds that \(\mathcal{H}\subseteq\mathcal{G}\subseteq\mathcal{F}\). Then the following hold:}

\[E[E[X\vert \mathcal{G}]\vert\mathcal{H}]=E[X\vert \mathcal{H}],\tag{B.8}\]
\[E[X]=E[E[X\vert \mathcal{G}]].\tag{B.9}\]

\textbf{Proposition B.31. (Bjork)} \emph{Assume \(X\) is \(\mathcal{G}\) and that both \(X,Y\) and \(XY\) are in \(L^1\) (only assuming \(Y\) is \(\mathcal{F}\)-measurable), then}

\[E[X\vert\mathcal{G}]=X,\hspace{20pt}P-\text{a.s.}\tag{B.11}\]
\[E[XY\vert\mathcal{G}]=XE[Y\vert\mathcal{G}],\hspace{20pt}P-\text{a.s.}\tag{B.12}\]

\textbf{Proposition B.32. (Bjork)} \textbf{(Jensen inequality)} \emph{Let \(f:\mathbb{R}\to\mathbb{R}\) be a convex (measurable) function and assume \(f(X)\) is in \(L^1\). Then}

\[f(E[X\vert\mathcal{G}])\le E[f(X)\vert\mathcal{G}],\hspace{20pt}P-\text{a.s.}\]

\textbf{Proposition B.37. (Bjork)} \emph{Let \((\Omega,\mathcal{F},P)\) be a given probability space, let \(\mathcal{G}\) be a sub-sigma-algebra of \(\mathcal{F}\), and let \(X\) be a square integrable random variable.
Consider the problem of minimizing}

\[E\left[(X-Z)^2\right]\]

\emph{where \(Z\) is allowed to vary over the class of all square integrable \(\mathcal{G}\) measurable random variables. The optimal solution \(\hat{Z}\) is then given by.}

\[\hat{Z}=E[X\vert\mathcal{G}].\]

\noindent\makebox[\linewidth]{\rule{\textwidth}{0.4pt}}

\textbf{Proof.}

Let \(X\in L^2(\Omega,\mathcal{F},P)\) be a random variable. Now consider an arbitrary \(Z\in L^2(\Omega,\mathcal{G},P)\). Recall that \(\mathcal{G}\subset \mathcal{F}\) and so \(X\) is also in \(Z\in L^2(\Omega,\mathcal{G},P)\), as it is bothe square integrable and \(\mathcal{G}\)-measurable. Then

\[E\left[Z\cdot(X-E[X\vert\mathcal{G}])\right]=E\left[Z\cdot X\right]-E\left[Z\cdot E[X\vert\mathcal{G}]\right].\]

Then by using the law of total expectation and secondly that \(Z\) is \(\mathcal{G}\)-measurable we have that

\[E\left[Z\cdot X\right]=E\left[E[Z\cdot X\vert\mathcal{G}]\right]=E\left[Z\cdot E[ X\vert\mathcal{G}]\right].\]

Combining the two equations gives the desired result. Obviously, we have that

\[X-Z=X-Z+E[X\vert\mathcal{G}]-E[X\vert\mathcal{G}]=(X-E[X\vert\mathcal{G}])+(E[X\vert\mathcal{G}]-Z).\]

Then squaring the terms gives

\[(X-Z)^2=(X-E[X\vert\mathcal{G}])^2+(E[X\vert\mathcal{G}]-Z)^2+2(X-E[X\vert\mathcal{G}])(E[X\vert\mathcal{G}]-Z)\]

Taking expectation on each side and using linearity of the expectation we have that

\[E[(X-Z)^2]=E\left[(X-E[X\vert\mathcal{G}])^2\right]+E\left[(E[X\vert\mathcal{G}]-Z)^2\right]+2E\left[(X-E[X\vert\mathcal{G}])(E[X\vert\mathcal{G}]-Z)\right].\]

We can now use that \(E[X\vert\mathcal{G}]-Z\) is \(\mathcal{G}\)-measurable with the above result on the last term.

\[E[(X-Z)^2]=E\left[(X-E[X\vert\mathcal{G}])^2\right]+E\left[(E[X\vert\mathcal{G}]-Z)^2\right].\]

Now since \(X\) is given the term \(E\left[(X-E[X\vert\mathcal{G}])^2\right]\) is simply a constant not depending on the choice og \(Z\). The optimal choice of \(Z\) is then \(E[X\vert\mathcal{G}]\) since this minimizes the second term. The statement is then proved.

\newpage

\hypertarget{independence}{%
\section{Independence}\label{independence}}

\textbf{Definition 3.1. (Hansen)} \emph{Let \((\Omega,\mathbb{F},P)\) be a probability space. Two events \(A,B\in\mathbb{F}\) are \textbf{independent}\index{independent} if}
\begin{align*}
    P(A\cap B)=P(A)P(B)\tag{3.1}
\end{align*}

\textbf{Definition 3.4. (Hansen)} \emph{Let \((\Omega,\mathbb{F},P)\) be a probability space and let \(\mathbb{G},\mathbb{H}\subset \mathbb{F}\) be two classes of measurable sets. We sat that \(\mathbb{G}\) and \(\mathbb{H}\) are independent, written \(\mathbb{G}\perp \!\!\! \perp\mathbb{H}\), if}
\begin{align*}
    P(A\cap B)=P(A)P(B)\hspace{15pt}\text{for all}\ A\in\mathbb{G},B\in\mathbb{H}.\tag{3.2}
\end{align*}

\textbf{Lemma 3.5. (Hansen)} \emph{Let \((\Omega,\mathbb{F},P)\) be a probability space and let \(\mathbb{G},\mathbb{H}\subset \mathbb{F}\) be two classes of measurable sets. Let \(\mathbb{G}_1\subset \mathbb{G}\) and \(\mathbb{H}_1\subset\mathbb{H}\) be two subclasses. If \(\mathbb{G}\perp \!\!\! \perp \mathbb{H}\) then it holds that \(\mathbb{G}_1\perp \!\!\! \perp \mathbb{H}_1\).}

\textbf{Definition 3.6. (Hansen)} \emph{A class \(\mathbb{H}\) of subsets of \(\Omega\) is a \textbf{Dynkin class}\index{Dynkin class} if}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\Omega \in\mathbb{H}\),
\item
  \(A,B\in\mathbb{H},A\subset B\hspace{15pt}\Rightarrow\hspace{15pt}B\setminus A\in\mathbb{H}\),
\item
  \(A_1,A_2,...\in\mathbb{H},A_1\subset A_2\subset ...\hspace{15pt}\Rightarrow\hspace{15pt}\bigcup_{n=1}^\infty A_n\in\mathbb{H}\).
\end{enumerate}

\textbf{Lemma 3.7. (Hansen)} \emph{(Dynkin) Let \(\mathbb{D}\subset \mathbb{H}_0\subset \mathbb{H}\) be three nested classes of subsets of \(\Omega\). if}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\sigma(\mathbb{D})=\mathbb{H}\),
\item
  \(A,B\in\mathbb{D}\hspace{10pt}\Rightarrow\hspace{10pt}A\cap B\in\mathbb{D}\)
\item
  \(\mathbb{H}_0\) \emph{is a Dynkin class.}
\end{enumerate}

\emph{then it holds that \(\mathbb{H}_0=\mathbb{H}\).}

\textbf{Lemma 3.8. (Hansen)} \emph{Let \((\Omega,\mathbb{F},P)\) be a probability space, and let \(A\in\mathbb{F}\) be a fixed event. The class}
\begin{align*}
    \mathbb{H}=\{B\in \mathbb{F}\ \vert\ A\perp \!\!\! \perp B\}
\end{align*}
\emph{is a Dynkin class.}

\textbf{Theorem 3.9. (Hansen)} \emph{Let \((\Omega,\mathbb{F},P)\) be a probability space, and let \(\mathbb{G}_1,\mathbb{G}_2\subset \mathbb{F}\) be two sigma-algebras. Let \(\mathbb{D}_1\) and \(\mathbb{D}_2\) be two classes such that \(\sigma(\mathbb{D}_i)=\mathbb{G}_i\) for \(i=1,2\).}
\emph{If both \(\mathbb{D}_1\) and \(\mathbb{D}_2\) are \(\cap\)-stable then it holds that}
\begin{align*}
    \mathbb{D}_1\perp \!\!\! \perp\mathbb{D}_2\hspace{10pt}\Rightarrow \hspace{10pt} \mathbb{G}_1\perp \!\!\! \perp\mathbb{G}_2.
\end{align*}

\textbf{Definition 3.10. (Hansen)} \emph{Two real-valued random variable \(X\) and \(Y\) on a background space \((\Omega,\mathbb{F},P)\) are \textbf{independent}, written \(X\perp \!\!\! \perp Y\), if the corresponding sigma-algebras \(\sigma(X)\) and \(\sigma(Y)\) are independent.}

\textbf{Definition 3.15. (Hansen)} \emph{Let \((\Omega,\mathbb{F},P)\) be a probability space, and let \(\mathbb{G}_1,...,\mathbb{G}_n\subset \mathbb{F}\) be finitely many classes of measurable sets. We say that \(\mathbb{G}_1,...,\mathbb{G}_n\) are \textbf{jointly independent}\index{jointly independent}, written \(\mathbb{G}_1\perp \!\!\! \perp...\perp \!\!\! \perp\mathbb{G}_n\), if}
\begin{align*}
    P(A_1\cap ...\cap A_n=\prod_{i=1}^nP(A_i)\hspace{15pt}\text{for }A_1\in\mathbb{G}_1,...,A_n\in\mathbb{G}_n.\tag{3.8}
\end{align*}

\textbf{Lemma 3.16. (Hansen)} \emph{Let \((\Omega,\mathbb{F},P)\) be a probability space, and let \(\mathbb{G}_1,...,\mathbb{G}_n\subset \mathbb{F}\) be finitely many classes of measurable sets. It holds that}
\begin{align*}
    \mathbb{G}_1\perp \!\!\! \perp...\perp \!\!\! \perp\mathbb{G}_n\hspace{10pt}\Rightarrow \hspace{10pt} \mathbb{G}_1\perp \!\!\! \perp...\perp \!\!\! \perp\mathbb{G}_{n-1}
\end{align*}
\emph{provided that \(\Omega\in\mathbb{G}_n\).}

\textbf{Theorem 3.17. (Hansen)} \emph{Let \((\Omega,\mathbb{F},P)\) be a probability space, and let \(\mathbb{G}_1,...,\mathbb{G}_n\subset \mathbb{F}\) be sigma-algebras. Let \(\mathbb{D}_1,...,\mathbb{D}_n\) be classes such that \(\sigma(\mathbb{D}_i)=\mathbb{G}_1\) for \(i=1,...,n\). Suppose that for all lengths \(k=2,...,n\) an all choices of indices \(\le j_1<...<j_k\le n\) it holds that}
\begin{align*}
    \mathbb{D}_{j_1}\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{D}_{j_k}\tag{3.9}
\end{align*}
\emph{If all the generators \(\mathbb{D}_i\) are \(\cap\)-stable, then it holds that \(\mathbb{G}_1\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{G}_n\).}

\textbf{Lemma 3.18. (Hansen)} \emph{(Grouping) Let \((\Omega,\mathbb{F},P)\) be a probability space, and let \(\mathbb{G}_1,...,\mathbb{G}_n\subset \mathbb{F}\) be sigma-algebras. It holds that}
\begin{align*}
    \mathbb{G}_1\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{G}_n \hspace{10pt}\Rightarrow \hspace{10pt} \mathbb{G}_1\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{G}_{n-2}\perp \!\!\! \perp\sigma(\mathbb{G}_{n-1},\mathbb{G}_n).
\end{align*}

\textbf{Definition 3.19. (Hansen)} \emph{The real-valued random variables \(X_1,...,X_n\) on a background space \((\Omega,\mathbb{F},P)\) are \textbf{jointly independent}, written \(X_1\perp \!\!\! \perp ... \perp \!\!\! \perp X_n\), if the corresponding sigma-algebras \(\sigma(X_1),...,\sigma(X_n)\) are jointly independent.}

\textbf{Definition 3.20. (Hansen)} \emph{Let \((\Omega,\mathbb{F},P)\) be a probability space, and let \((\mathbb{G}_i)_{i\in I}\) be a family of classes of measurable sets. We say that the family \((\mathbb{G}_i)_{i\in I}\) is \textbf{jointly independent} if any finite subfamily is jointly independent.}

\textbf{Theorem 3.21. (Hansen)} \emph{Let \((\Omega,\mathbb{F},P)\) be a probability space, and let \(\mathbb{G}_1,\mathbb{G}_2,...\subset \mathbb{F}\) be sigma-algebras. Let \(\mathbb{D}_1,\mathbb{D}_2,...\) be classes such that \(\sigma(\mathbb{D}_n)=\mathbb{G}_n\) for all \(n\in\mathbb{N}\). Suppose that for all lengths \(k\in\mathbb{N}\) and all choices of indices \(1\le j_1< ... < j_k\) it holds that}
\begin{align*}
    \mathbb{D}_{j_1}\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{D}_{j_k}.\tag{3.14}
\end{align*}
\emph{If all the generators \(\mathbb{D}_n\) are \(\cap\)-stable, then it holds that \(\mathbb{G}_1\perp \!\!\! \perp \mathbb{G}_2 \perp \!\!\! \perp ...\).}

\textbf{Lemma 3.22. (Hansen)} \emph{Let \((\Omega,\mathbb{F},P)\) be a probability space, and let \(\mathbb{G}_1,\mathbb{G}_2,...\subset \mathbb{F}\) be sigma-algebras. It holds that}
\begin{align*}
    \mathbb{G}_1\perp \!\!\! \perp \mathbb{G}_2 \perp \!\!\! \perp ... \hspace{10pt}\Rightarrow \hspace{10pt} \mathbb{G}_1\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{G}_n \perp \!\!\! \perp \sigma(\mathbb{G}_{n+1},\mathbb{G}_{n+2}, ... ).
\end{align*}

\textbf{Definition 3.23. (Hansen)} \emph{The real-valued random variables \((X_i)_{i\in I}\) on a background space \((\Omega,\mathbb{F},P)\) are \textbf{jointly independent} if the corresponding sigma-algebras \((\sigma(X_i))_{i\in I}\) are jointly independent.}

\textbf{Definition 3.28. (Hansen)} \emph{Let \((\Omega,\mathbb{F},P)\) be a probability space. A sigma-algebra \(\mathbb{G}\subset \mathbb{F}\) satisfies a \textbf{zero-one law}\index{zero-one law} if}
\begin{align*}
    P(A)\in\{0,1\}\hspace{15pt}\text{for all}\ A\in\mathbb{G}.
\end{align*}

\textbf{Theorem 3.29. (Hansen)} \emph{Let \((\Omega,\mathbb{F},P)\) be a probability space and let \(\mathbb{G}\subset \mathbb{F}\) be a sigma-algebra. The following three conditions are equivalent:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{For any sigma-algebra \(\mathbb{H}\subset \mathbb{F}\) it holds that \(\mathbb{G} \perp \!\!\! \perp \mathbb{H}\),}
\item
  \emph{It holds that \(\mathbb{G}\perp \!\!\! \perp\mathbb{G}\),}
\item
  \emph{\(\mathbb{G}\) satisfies a 0-1 law.}
\end{enumerate}

\textbf{Definition 3.30. (Hansen)} \emph{Let \(X_1,X_2,...\) be real-valued random variables on a background space \((\Omega,\mathbb{F},P)\). The \textbf{tail sigma-algebra}\index{tail sigma-algebra} of the process is defined as}
\begin{align*}
    \mathbb{J}(X_1,X_2,...)=\bigcap_{n=1}^\infty \sigma(X_n,X_{n+1}, ... ).
\end{align*}

\textbf{Theorem 3.32. (Hansen)} \emph{(Kolmogorov's zero-one law)\index{Kolmogorov's zero-one law} Let \(X_1,X_2,...\) be real-valued random variables on a background space \((\Omega,\mathbb{F},P)\). If \(X_1 \perp \!\!\! \perp X_2 \perp \!\!\! \perp ...\) then the tail-algebra \(\mathbb{J}(X_1,X_2,...)\) satisfies a 0-1 law.}

\textbf{Lemma 3.35. (Hansen)} \emph{(2nd half of Borel-Cantelli)\index{Borel-Cantelli, 2nd half} Let \((\Omega,\mathbb{F},P)\) be a probability space, and let \(A_1,A_2,...\) be a sequence of \(\mathbb{F}\)-measurable sets. If \(A_1 \perp \!\!\! \perp A_2 \perp \!\!\! \perp ...\) then it holds that}
\begin{align*}
    \sum_{n=1}^\infty P(A_n)<\infty \iff P(A_n\text{ i.o.})=0.
\end{align*}

\newpage

\hypertarget{moment-generating-function}{%
\section{Moment generating function}\label{moment-generating-function}}

Let \(X\) be a random variable with distribution function \(F(x)=P(X\le x)\) and \(Y\) be a random variable with distribution function \(G(y)=P(Y\le y)\).

\textbf{Definition. (Ex. FinKont)} \emph{The moment generating function or Laplace transform of \(X\) is}

\[\psi_X(\lambda)=E\left[e^{\lambda X}\right]=\int_{-\infty}^\infty e^{\lambda x}dF(x)\]

\emph{provided the expectation is finite for \(\vert\lambda\vert<h\) for some \(h>0\).}

The MGF uniquely determine the distribution of a random variable, due to the following result.

\textbf{Theorem. (Ex. FinKont)} \textbf{(Uniqueness)} \emph{If \(\psi_X(\lambda)=\psi_Y(\lambda)\) when \(\vert\lambda\vert<h\) for some \(h>0\), then \(X\) and \(Y\) has the same distribution, that is, \(F=G\).}

There is also the following result of independence for Moment generating functions.

\textbf{Theorem. (Ex. FinKont)} \textbf{(Independence)} \emph{If}

\[E\left[e^{\lambda_1X+\lambda_2Y}\right]=\psi_X(\lambda_1)\psi_Y(\lambda_2)\]

\emph{for \(\vert\lambda_i\vert<h\) for \(i=1,2\) for some \(h>0\), then \(X\) and \(Y\) are independent random variables.}

\newpage

\hypertarget{standard-distributions}{%
\section{Standard distributions}\label{standard-distributions}}

\hypertarget{normal-disribution}{%
\subsection{Normal disribution}\label{normal-disribution}}

The following gives a comprehensive table of the standard some properties.\index{Normal disribution}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3056}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1667}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5278}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Symbol
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Normal distribution
\end{minipage} \\
\midrule()
\endhead
Definition & \(\sim\) & \(X\sim\mathcal{N}(\mu,\sigma^2)\) \\
Parameters & \(\theta\in \Theta\) & \(\theta=(\mu,\sigma^2)\in \mathbb{R}\times \mathbb{R}_+\) \\
Support & \(\text{Im}(X)\) & \(x\in \mathbb{R}\) \\
Density & \(f\) & \(\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\left(\frac{x-\mu}{\sqrt{2\sigma^2}}\right)^2}\) \\
Distribution & \(F\) & \(\frac{1}{2}\left(1+N\left(\frac{x-\mu}{\sqrt{2\sigma^2}}\right)\right)\) \\
Mean value & \(E[X]\) & \(\mu\) \\
Variance & \(\text{Var}(X)\) & \(\sigma^2\) \\
MGF* & \(\psi_X=E[e^{\lambda X}]\) & \(e^{\mu \lambda+\frac{1}{2}\lambda^2\sigma^2}\) \\
Characteristic function & \(\varphi_X(t)=E[e^{itX}]\) & \(e^{it\mu-\frac{1}{2}\sigma^2 t^2}\) \\
\bottomrule()
\end{longtable}

In the table above we used the abbreviations: *MGF = Moment Generating function.

We also used the shorthand: \(N\) being the distribution of a standard normal distributed variable \(\mathcal{N}(0,1)\).

\hypertarget{discrete-time-stochastic-processes}{%
\chapter{Discrete Time Stochastic Processes}\label{discrete-time-stochastic-processes}}

\hypertarget{convergence-concepts}{%
\section{Convergence concepts}\label{convergence-concepts}}

We start this chapter by refering to a sequence \(X_1,X_2,...\) of real-valued random variables as a \textbf{proces}. Consider the event \((X_n\to X)=\left\{\omega\in\Omega\ \vert\ X_m(\omega)\to X(\omega)\ \text{for}\ n\to \infty\right\}\). We want to study such convergence in detail. However first we check measurability. Consider a family \((A_i)_{i\in I}\subset \Omega\) and observe that
\begin{align*}
    \Big\{\omega\in \Omega\ \vert\ \forall i\in I : \omega \in A_i\Big\}=\bigcap_{i\in I} A_i,\tag{2.1}\\
    \Big\{\omega\in \Omega\ \vert\ \exists i\in I : \omega \in A_i\Big\}=\bigcup_{i\in I} A_i.\tag{2.2}
\end{align*}
From the standard \(N,\varepsilon\) definition of a convergent sequence \((x_n)_{n\in \mathbb{N}}\) we may formulate this convergens in the stochastic setting:
\begin{align*}
    (X_n\to X)&=\Big\{\omega\in \Omega\ \vert\ \forall\varepsilon>0 \exists N\in \mathbb{N} \forall n\ge N : \vert X_n(\omega)-X(\omega)\vert <\varepsilon\Big\}\\
    &=\Big(\forall\varepsilon>0 \exists N\in \mathbb{N} \forall n\ge N : \vert X_n-X\vert <\varepsilon\Big)\\
    &=\bigcap_{\epsilon\in \mathbb{R}^+}\Big(\exists N\in \mathbb{N} \forall n\ge N : \vert X_n-X\vert<\varepsilon\Big)\\
    &=\bigcap_{\epsilon\in \mathbb{R}^+}\bigcup_{N=1}^\infty\Big( \forall n\ge N : \vert X_n-X\vert<\varepsilon\Big)\\
    &=\bigcap_{\epsilon\in \mathbb{R}^+}\bigcup_{N=1}^\infty\bigcap_{n=N}^\infty\Big(  \vert X_n-X\vert<\varepsilon\Big)\in \mathbb{F}\hspace{15pt}\text{for all}\ \varepsilon>0.
\end{align*}
Hence \((X_n\to X)\) lies in \(\mathbb{F}\) since \((\vert X_n-X\vert <\varepsilon)\) lies in \(\mathbb{F}\) since \(X_n-X\) is measurable.

\textbf{Lemma 2.1. (Hansen)} \emph{Let \(X,X_1,X_2,...\) be real-valued random variables on \((\Omega, \mathbb{F},P)\). It holds that}
\begin{align*}
    (X_n\to X)\in \mathbb{F}.
\end{align*}

\textbf{Definition 2.2. (Hansen)} \emph{Let \(X,X_1,X_2,...\) be real-valued random variables on \((\Omega,\mathbb{F},P)\). We say that \(X_n\) \textbf{converges} to \(X\) \textbf{almost surely}\index{almost surely convergence}\index{convergence almost surely}, written \(X_n\stackrel{\text{a.s.}}{\to}X\), if}
\begin{align*}
    P(X_n\to X)=1.\tag{2.6}
\end{align*}

\textbf{Lemma 2.3. (Hansen)} \emph{Let \(X,X',X_1,X_2,...\) be real-valued random variables on \((\Omega,\mathbb{F},P)\). If \(X_n\stackrel{\text{a.s.}}{\to}X\) and \(X_n\stackrel{\text{a.s.}}{\to}X'\) then \(X=X'\) almost surely.}

\textbf{Lemma 2.7. (Hansen)} \emph{Let \(X_1,X_2,...\) be real-valued random variables on \((\Omega,\mathbb{F},P)\). Then}
\begin{align*}
    \Big((X_n)\text{ is Cauchy}\Big)\in \mathbb{F}.
\end{align*}

\textbf{Lemma 2.8. (Hansen)} \emph{Let \(X_1,X_2,...\) be real-valued random variables on \((\Omega,\mathbb{F},P)\). If \(P\Big((X_n)\text{ is Cauchy}\Big)=1\) then there exists and \(\mathbb{F}\)-measurable real-valued random variable \(X\) such that \(X_n\stackrel{\text{a.s.}}{\to}X\).}

\textbf{Theorem 2.10. (Hansen)} \emph{Let \(X_1,X_2,...\) and \(Y_1,Y_2,...\) be real-valued random variables on \((\Omega,\mathbb{F},P)\). Assume that the \(X\)-process and the \(Y\)-process have the same distribution in the sense that \((X_1,...,X_n)\) has the same distribution ad \((Y_1,...,Y_n)\) for all \(n\in\mathbb{N}\).}
\emph{If \(X_n\stackrel{\text{a.s.}}{\to}X\) for som limit variable \(X\), there is a limit variable \(Y\) such that \(Y_n\stackrel{\text{a.s.}}{\to}Y\).}

\textbf{Definition 2.11. (Hansen)} \emph{Let \(\mathbf{X},\mathbf{X}_1,\mathbf{X}_2,...\) be \(\mathbb{R}^k\) valued random variables on \((\Omega, \mathbb{F},P)\). We say that \(\mathbf{X}_n\) converges to \(\mathbf{X}\) \textbf{almost surely}, written \(\mathbf{X}_n\stackrel{\text{a.s.}}{\to}\mathbf{X}\), if}
\begin{align*}
    \vert\mathbf{X}_n-\mathbf{X}\vert \stackrel{\text{a.s.}}{\to} 0.\tag{2.15}
\end{align*}

\textbf{Lemma 2.12. (Hansen)} \emph{Let \(\mathbf{X},\mathbf{X}_1,\mathbf{X}_2,...\) be \(\mathbb{R}^k\) valued random variables on \((\Omega, \mathbb{F},P)\) such that \(\mathbf{X}_n\stackrel{\text{a.s.}}{\to}\mathbf{X}\). Let \(f : \mathbb{R}^k\to\mathbb{R}^m\) be a measurable map.}
\emph{Assume that there is a set \(A\in \mathbb{B}_k\) such that \(f\) is continuous on \(A\) and such that \(P(\mathbf{X}\in A)=1\). Then it holds that \(f(\mathbf{X}_n)\stackrel{\text{a.s.}}{\to} f(\mathbf{X})\).}

\textbf{Definition 2.13. (Hansen)} \emph{Let \(X,X_1,X_2,...\) be real-valued random variables on \((\Omega,\mathbb{F},P)\). We say that \(X_n\) \textbf{converges} to \(X\) \textbf{in probability}\index{convergence in probability}, written \(X_n\stackrel{\text{P}}{\to} X\), if}
\begin{align*}
    \forall \varepsilon>0 :\hspace{10pt} P\big(\vert X_n-X\vert\ge \varepsilon\big)\to 0\hspace{10pt}\text{for}\ n\to \infty.\tag{2.17}
\end{align*}

\textbf{Lemma 2.14. (Hansen)} \emph{Let \(X,X',X_1,X_2,...\) be real-valued random variables on \((\Omega,\mathbb{F},P)\). If \(X_n\stackrel{\text{P}}{\to} X\) and \(X_n\stackrel{\text{P}}{\to} X'\) then \(X=X'\) almost surely.}

\textbf{Lemma 2.14. (Hansen)} \emph{Let \(X,X',X_1,X_2,...\) be real-valued random variables on \((\Omega,\mathbb{F},P)\). If \(X_n\stackrel{\text{a.s.}}{\to} X\), then \(X_n\stackrel{\text{P}}{\to} X\).}

\textbf{Definition 2.17. (Hansen)} \emph{Let \(\mathbf{X},\mathbf{X}_1,\mathbf{X}_2,...\) be \(\mathbb{R}^k\) valued random variables on \((\Omega, \mathbb{F},P)\). We say that \(\mathbf{X}_n\) converges to \(\mathbf{X}\) \textbf{in probability}, written \(\mathbf{X}_n\stackrel{\text{P}}{\to} \mathbb{X}\), if}
\begin{align*}
    \vert \mathbf{X}_n-\mathbf{X}\vert \stackrel{\text{P}}{\to} 0.\tag{2.23}
\end{align*}

\textbf{Lemma 2.18. (Hansen)} \emph{Let \(X,Y,X_1,Y_1,X_2,Y_2,...\) be real-valued random variables on \((\Omega,\mathbb{F},P)\). It holds that}
\begin{align*}
    \begin{pmatrix}
    X_n\\Y_n
    \end{pmatrix}\stackrel{\text{P}}{\to} \begin{pmatrix}
    X\\Y
    \end{pmatrix}\hspace{15pt}\iff \hspace{15pt} X_n\stackrel{\text{P}}{\to} X\text{ and }Y_n\stackrel{\text{P}}{\to} Y.\tag{2.24}
\end{align*}

\textbf{Definition 2.19. (Hansen)} \emph{Let \(X,X_1,X_2,...\) be real-valued random variables in \(\mathcal{L}^p(\Omega,\mathbb{F},P)\) for some \(p\ge 1\). We say that \(X_n\) \textbf{converges} to \(X\) \textbf{in} \(\mathcal{L}^p\)\index{convergence in L-p}, written \(X_n\stackrel{\mathcal{L}^p}{\to} X\), if}
\begin{align*}
    \Vert X_n - X\Vert_p\to 0.\tag{2.27}
\end{align*}
\emph{Where the \(p\)'th norm is defined as the mapping \(\Vert \cdot \Vert_p : \Omega\to [0,\infty)\) given by \(X\mapsto \left(\int_\Omega \vert X\vert ^p\ dP\right)^{1/p}\).}

One might also define convergence in \(\mathcal{L}^p\) by simply saying if \(X_n\stackrel{\mathcal{L}^p}{\to} X\) then \(E\,\Vert X_n-X\Vert_p\to 0\).

\textbf{Lemma 2.20. (Hansen)} \emph{(Extended Cauchy-Schwarz inequality) Let \(X,Y\in\mathcal{L}^p(\Omega,\mathbb{F},P)\) for some \(p\ge 1\). For any \(a\in[0,p]\) it holds that}
\begin{align*}
    E\, \vert X\vert^a\vert Y\vert^{p-a}\le \Big(E\, \vert X\vert^p\Big)^{\frac{a}{p}}\Big(E\, \vert Y\vert^p\Big)^{\frac{p-a}{p}}.\tag{2.29}
\end{align*}

\textbf{Theorem 2.21. (Hansen)} \emph{Let \(X,X_1,X_2,...\) be real-valued random variables in \(\mathcal{L}^p(\Omega,\mathbb{F},P)\) for some \(p\in\mathbb{N}\). If \(X_n\stackrel{\mathcal{L}^p}{\to} X\), then it holds that \(E\, X_n^p\to E\, X^p\).}

\textbf{Lemma 2.22. (Hansen)} \emph{Let \(X,X_1,X_2,...\) be real-valued random variables in \(\mathcal{L}^p(\Omega,\mathbb{F},P)\) for some \(p\ge 1\). If \(X_n\stackrel{\mathcal{L}^p}{\to} X\), then \(X_n\stackrel{\text{P}}{\to} X\).}

\textbf{Lemma 2.25. (Hansen)} \emph{(Borel-Cantelli) Let \((\Omega,\mathbb{F},P)\) be a probability space, and let \(A_1,A_2,...\) be a sequence of \(\mathbb{F}\)-measurable sets. It holds that}
\begin{align*}
    \sum_{n=1}^\infty P(A_n)<\infty\hspace{10pt}\Rightarrow\hspace{10pt}P(A_n\ \text{i.o.})=0.
\end{align*}

Let \(A_1,A_2,...\) be a sequence of subsets of \(\Omega\). We define
\begin{align*}
    (A_n\ \text{i.o.})=\bigcap_{n=1}^\infty\bigcup_{m=n}^\infty A_m,\hspace{10pt}(A_n\ \text{evt.})=\bigcup_{n=1}^\infty\bigcap_{m=n}^\infty A_m.
\end{align*}
One might also define \(Y=\sum_{n=1}^\infty 1_{A_n}\) and realise that \((A_n\ \text{i.o.})=(Y=\infty)\) and \((A_n\ \text{evt.})=(Y<\infty)\). Also by de Morgan's law it follows that \((A_n\ \text{evt.})^c=(A_n^c\ \text{i.o.})\).

\textbf{Theorem 2.26. (Hansen)} \emph{Let \(X,X_1,X_2,...\) be real-valued random variables on \((\Omega,\mathbb{F},P)\). If}
\begin{align*}
    \forall \varepsilon>0:\hspace{10pt}\sum_{n=1}^\infty P(\vert X_n-X\vert \ge \varepsilon)<\infty,\tag{2.32}
\end{align*}
\emph{then it holds that \(X_n\stackrel{\text{a.s.}}{\to} X\).}

\textbf{Theorem 2.27. (Hansen)} \emph{Let \(X,X_1,X_2,...\) be real-valued random variables on \((\Omega,\mathbb{F},P)\). If \(X_n\stackrel{\text{P}}{\to} X\), then there is a subsequence \(X_{n_1},X_{n_2},...\) such that \(X_{n_k}\stackrel{\text{a.s.}}{\to} X\) for \(k\to \infty\).}

\textbf{Lemma 2.28. (Hansen)} \emph{Let \(\mathbf{X},\mathbf{X}_1,\mathbf{X}_2,...\) be \(\mathbb{R}^k\)-valued random variables on \((\Omega,\mathbb{F},P)\) such that \(\mathbf{X}_n\stackrel{\text{P}}{\to} \mathbf{X}\). Let \(f : \mathbb{R}^k\to \mathbb{R}^m\) be a measurable map.}
\emph{Assume that there is a set \(A\in\mathbb{B}_k\) such that \(f\) is continuous on \(A\) and such that \(P(\mathbf{X}\in A)=1\). Then it holds that \(f(\mathbf{X}_n)\stackrel{\text{P}}{\to} f(\mathbf{X})\).}

\textbf{Lemma.} \emph{(Fatou's Lemma) \index{Fatou's Lemma}Let \((\Omega,\mathbb{F},P)\) be a measure space (here probability space). Let \(f_n : \mathcal{X} \to [0,\infty]\), with \(\mathcal{X}\in\mathbb{F}\), be a sequence of non-negative measurable functions. Assume \(f_n\) converge pointwise to \(f : \mathcal{X}\to [0,\infty)\). Then}
\begin{align*}
    \int_{\mathcal{X}} \liminf_{n\to\infty} f_n\ dP\le \liminf_{n\to\infty} \int_{\mathcal{X}} f_n\ dP.
\end{align*}

\textbf{Lemma.} \emph{(Holder's Inequality) \index{Holder's Inequality} Let \((\Omega,\mathbb{F},P)\) be a measure space (here probability space). Let \(f\) and \(g\) be real-valued (or complex-valued) functions defined on \(\Omega\). Assume \(f\) and \(g\) are measurable. For any \(p,q\ge 1\) such that \(\frac{1}{p}+\frac{1}{q}=1\) it holds that }
\begin{align*}
    \left(\int_\Omega \vert fg\vert^1\ dP\right)^1\le \left(\int_\Omega \vert f\vert^p\ dP\right)^{1/p}\left(\int_\Omega \vert g\vert^q\ dP\right)^{1/q}
\end{align*}

\pagebreak

\hypertarget{sums-and-average-processes}{%
\subsection{Sums and average processes}\label{sums-and-average-processes}}

\textbf{Lemma 4.1. (Hansen)} \emph{Let \(X_1,...,X_n\) be independent real-valued random variables with \(E\, X_i^4<\infty\) for all \(i\). If \(E\, X_i=0\) for all \(i\) then it holds that}
\begin{align*}
    E\left(\sum_{i=1}^n X_i\right)^4=\sum_{i=1}^n E\, X_i^4+6\sum_{i=1}^{n-1}\sum_{j=i+1}^n E\, X_i^2\,E\,X_j^2.
\end{align*}

\textbf{Theorem 4.2. (Hansen)} \emph{(SLLN, weak form)\index{SLLN, weak form} Let \(X_1,X_2,...\) be a sequence of independent and identically distributed real-valued random variables. If \(E\, X_1^4<\infty\) it holds that}
\begin{align*}
    \frac{1}{n}\sum_{i=1}^n X_i \hspace{10pt}\stackrel{\text{a.s.}}{\to} \hspace{10pt}E\, X_1.\tag{4.3}
\end{align*}

\textbf{Theorem 4.10. (Hansen)} \emph{(Etemadi's maximal inequality)\index{Etemadi's maximal inequality} Let \(X_1,...,X_n\) be independent real-valued random variables. Consider the cumulative sums}
\begin{align*}
    S_k=\sum_{i=1}^kX_i\hspace{10pt}\text{for}\ k=1,..., n.
\end{align*}
\emph{For any \(\alpha >0\) it holds that}
\begin{align*}
    P\left(\max_{j=1,...,n}\ \vert S_j\vert\ge 3\alpha\right)\le 3\ \max_{j=1,...,n}\ P(\vert S_j\vert \ge \alpha).\tag{4.11}
\end{align*}

\textbf{Theorem 4.11. (Hansen)} \emph{(Levy's maximal inequality)\index{Levy's maximal inequality} Let \(X_1,...,X_n\) be independent real-valued random variables, each with a symmetric distribution. Consider the cumulative sums}
\begin{align*}
    S_k=\sum_{i=1}^kX_i\hspace{10pt}\text{for}\ k=1,..., n.
\end{align*}
\emph{For any \(\alpha>0\) it holds that}
\begin{align*}
    P\left(\max_{j=1,...,n}\ S_j\ge \alpha\right)\le 2 P(S_j\ge \alpha).\tag{4.13}
\end{align*}

\textbf{Corollary 4.12. (Hansen)} \emph{Let \(X_1,...,X_n\) be independent real-valued random variables, each with a symmetric distribution. Consider the cumulative sums}
\begin{align*}
    S_k=\sum_{i=1}^kX_i\hspace{10pt}\text{for}\ k=1,..., n.
\end{align*}
\emph{For any \(\alpha>0\) it holds that}
\begin{align*}
    P\left(\max_{j=1,...,n}\ \vert S_j\vert\ge \alpha\right)\le 2 P(\vert S_j\vert\ge \alpha).\tag{4.14}
\end{align*}

\textbf{Theorem 4.13. (Hansen)} \emph{(Skorokhod)\index{Skorokhod} Let \(X_1,X_2,...\) be a sequence of independent real-valued random variables, and consider the cumulative sums \(S_k=\sum_{i=1}^kX_i\). Let \(S\) be a potential limit variable. It holds that}
\begin{align*}
    S_n \stackrel{\text{P}}{\to} S\hspace{10pt}\Rightarrow\hspace{10pt} S_n\stackrel{\text{a.s.}}{\to} S.
\end{align*}

\textbf{Corollary 4.14. (Hansen)} \emph{(Khintchine-Kolmogorov)\index{Khintchine-Kolmogorov} Let \(X_1,X_2,...\) be a sequence of independent real-valued random variables. Assume \(E\, X_n^2<\infty\) and that \(E\, X_n=0\) for every \(n\in\mathbb{N}\). Consider the cumulative sums \(S_k=\sum_{i=1}^kX_i\). If}
\begin{align*}
    \sum_{n=1}^\infty E\, X_n^2<\infty\tag{4.18}
\end{align*}
\emph{then there exist a limit variable \(S\) such that \(S_n\to S\) almost surely and in \(\mathcal{L}^2\). The limit variable satisfies that}
\begin{align*}
    E\, S=0\hspace{10pt}\text{and}\hspace{10pt}V\, S=\sum_{n=1}^\infty V\, X_n.
\end{align*}

\textbf{Theorem 4.17. (Hansen)} \emph{Let \(X_1,X_2,...\) be a sequence of independent real-valued random variables, and consider the cumulative sums \(S_k=\sum_{i=1}^kX_i\). Let \(S\) be a potential limit variable. Assume that there is a constant \(c>0\) such that \(P(\vert X_n\vert \le c)=1\) for all \(n\), and assume that \(E\, X_n=0\) for all \(n\). The the three statements}
1. \(S_n\stackrel{\text{P}}{\to} S\),
2. \(S_n\stackrel{\text{a.s.}}{\to} S\)
3. \(S_n\stackrel{\mathcal{L}^2}{\to} S\)
\emph{are equivalent.}

\textbf{Lemma 4.18. (Hansen)} \emph{Let \(X_1,X_2,...\) be a sequence of independent real-valued random variables. Assume that there is a constant \(c>0\) such that \(P(\vert X_n\vert \le c)=1\) for all \(n\). If the associated random walk \(S_n=\sum_{i=1}^n X_i\) satisfies that \(S_n\to S\) almost surely for some limit variable then it holds that}
\begin{align}
    \text{1)}\hspace{10pt}& \sum_{n=1}^NE\, X_n\hspace{5pt}\text{converges in }\mathbb{R}\hspace{5pt}\text{for}\ N\to \infty,\\
    \text{2)}\hspace{10pt}&\sum_{n=1}^\infty V(X_n)<\infty.
\end{align}

\textbf{Theorem 4.19. (Hansen)} \emph{(Kolmogorov's 3-series theorem)\index{Kolmogorov's 3-series theorem} Let \(X_1,X_2,...\) be a sequence of independent real-valued random variables. Consider the assoiciated random walk \(S_n=\sum_{i=1}^n X_i\). If there is a cut-off value \(c>0\) such that the capped variables \(\tilde{X}_n=1_{\vert X_n\vert \le c}X_n\) satisfies that}
\begin{align*}
    \text{1)}\hspace{10pt}& \sum_{n=1}^\infty P(X_n\ne \tilde{X}_n)<\infty,\\
    \text{2)}\hspace{10pt}& \sum_{n=1}^N E\, \tilde{X}_n\ \text{converges in }\mathbb{R}\ \text{for}\ N\to \infty,\\
    \text{3)}\hspace{10pt}& \sum_{n=1}^\infty V(\tilde{X}_n)<\infty,
\end{align*}
\emph{then there is a real-valued limit variable \(S\) such that \(S_n\to S\) almost surely.}
\emph{Conversely, if \((S_n)_{n\in\mathbb{N}}\) is almost surely convergent, then the three series above converge for \textbf{any} cut-off value \(c>0\).}

\textbf{Lemma 4.20. (Hansen)} \emph{Let \((x_n)_{n\in\mathbb{N}}\) be a real-valued sequence, and let \(c\) be a real number. It holds that}
\begin{align*}
    x_n\to c\ \text{for}\ n\to \infty \hspace{10pt}\Rightarrow\hspace{10pt} \frac{1}{n}\sum_{i=1}^nx_i\to x\ \text{for}\ n\to\infty.
\end{align*}

\textbf{Lemma 4.21. (Hansen)} \emph{(Kronecker)\index{Kronecker} Let \((x_n)_{n\in\mathbb{N}}\) be real-valued sequence, and let \(c\) be a real number. It holds that}
\begin{align*}
    \sum_{i=1}^n\frac{x_i}{i}\to c \hspace{10pt}\Rightarrow\hspace{10pt} \frac{1}{n}\sum_{i=1}^nx_i\to 0
\end{align*}
\emph{for \(n\to \infty\).}

\textbf{Lemma 4.23. (Hansen)} \emph{Let \(X_1,X_2,...\) be a sequence of identically distributed real-valued random variables, and let \(\tilde{X}_n=1_{(\vert X_n\vert \le n}X_n\). If \(E\vert X_1\vert<\infty\) it holds that}
\begin{align*}
    \sum_{n=1}^\infty\frac{E\ \tilde{X}_n^2}{n^2}<\infty.
\end{align*}

\textbf{Theorem 4.24. (Hansen)} \emph{(SLLN, strong version)\index{SLLN, strong version} Let \(X_1,X_2,...\) be sequence of independent and identically distributed real-valued random variables. If \(E\vert X_1\vert <\infty\) it holds that}
\begin{align*}
    \frac{1}{n}\sum_{n=1}^\infty X_i\stackrel{\text{a.s.}}{\to} E\ X_1.\tag{4.24}
\end{align*}

\textbf{Theorem 4.25. (Hansen)} \emph{(SLLN, \(\mathcal{L}^p\)-version)\index{SLLN, $\mathcal{L}^p$-version} Let \(X_1,X_2,...\) be sequence of independent and identically distributed real-valued random variables. If \(E\vert X_1\vert^p <\infty\) for some \(p\ge 1\), then it holds that}
\begin{align*}
\frac{1}{n}\sum_{n=1}^\infty \stackrel{\mathcal{L}^p}{\to} E\ X_1.\tag{4.26}    
\end{align*}

\textbf{Lemma 4.26. (Hansen)} \emph{Let \(X_1,X_2,...\) be a sequence of pairwise independent, identically distributed real-valued random variables with \(E\vert X_1\vert <\infty\). Let \(n_1<n_2<...\) be a sequence of natural numbers. If there are constants \(C_1,C_2>0\) and \(\alpha>1\) such that}
\begin{align*}
    C_1\alpha^k\le n_k\le C_2\alpha^k\hspace{15pt}\text{for}\ k\to\infty\tag{4.27}
\end{align*}
\emph{then it holds that}
\begin{align*}
    \frac{1}{n_k}\sum_{i=1}^{n_k}X_i\stackrel{\text{a.s.}}{\to} E\ X_1\hspace{15pt}\text{for}\ k\to \infty.
\end{align*}

\textbf{Theorem 4.27. (Hansen)} \emph{(Etemahdi's version)\index{Etemahdi's version} Let \(X_1,X_2,...\) be a sequence of pairwise independent, identically distributed real-valued random variables. If \(E\vert X_1\vert<\infty\) it holds that}
\begin{align*}
    \frac{1}{n}\sum_{i=1}^nX_i\stackrel{\text{a.s.}}{\to} E\ X_1.\tag{4.30}
\end{align*}

\hypertarget{ergodic-theory}{%
\subsection{Ergodic Theory}\label{ergodic-theory}}

\textbf{Definition 5.3. (Hansen)} \emph{Let \((\mathcal{X},\mathbb{E})\) be a measurable space and let \(T : \mathcal{X}\to \mathcal{X}\) be measurable. A probability measure \(\mu\) on \((\mathcal{X},\mathbb{E})\) is \textbf{invariant}\index{invariant measure} under \(T\) if}
\begin{align*}
    \mu\big(T^{-1}(A)\big)=\mu(A)\hspace{10pt}\text{for all}\ A\in\mathbb{E}\tag{5.5}
\end{align*}
\emph{In this case we call the quadruple \((\mathcal{X},\mathbb{E},\mu,T)\) a \textbf{measure-preserving dynamical system}\index{measure-preserving dynamical system}.}

We say that a set \(A\in\mathbb{E}\) is an \textbf{invariant set} if \(T^{-1}(A)=A\) i.e.~the orbit of all \(x\in A\) stays in \(A\).

\textbf{Definition 5.5. (Hansen)} \emph{A measure-preserving dynamical system \((\mathcal{X},\mathbb{E},\mu,T)\) is \textbf{ergodic}\index{ergodic} if}
\begin{align*}
    T^{-1}(A)=A,\ A\in\mathbb{E} \hspace{10pt}\Rightarrow\hspace{10pt} \mu(A)\in\{0,1\}.\tag{5.7}
\end{align*}

\textbf{Definition 5.6. (Hansen)} \emph{A measure-preserving dynamical system \((\mathcal{X},\mathbb{E},\mu,T)\) is \textbf{mixing}\index{mixing} if}
\begin{align*}
    \mu(A\cap T^{-n}(B))\to \mu(A)\mu(B)\hspace{10pt}\text{for all}\ A,B\in\mathbb{E}.\tag{5.8}
\end{align*}

\textbf{Lemma 5.7. (Hansen)} \emph{If a measure-preserving dynamical system \((\mathcal{X},\mathbb{E},\mu,T)\) is mixing then it is also ergodic.}

\textbf{Lemma 5.8. (Hansen)} \emph{Let \((\mathcal{X},\mathbb{E},\mu,T)\) be a measure-preserving dynamical system. Let \(\mathbb{D}\) be an \(\cap\)-stable generator for \(\mathbb{E}\). If}
\begin{align*}
        \mu(A\cap T^{-n}(B))\to \mu(A)\mu(B)\hspace{10pt}\text{for all}\ A,B\in\mathbb{D}.\tag{5.10}
\end{align*}
\emph{then the system is mixing. (and ergodic)}

\textbf{Lemma 5.9. (Hansen)} \emph{Let \((\mathcal{X},\mathbb{E},\mu)\) be a probability space, and let \(T : \mathcal{X}\to \mathcal{X}\) be a measure-preserving map. Let \((\mathcal{Y},\mathbb{G})\) be another measurable space, and let \(S : \mathcal{Y} \to \mathcal{Y}\) be a measurable map.}
\emph{Suppose there is a measurable map \(\gamma : \mathcal{X}\to \mathcal{Y}\) such that the following diagram commutes:}

\begin{center}\includegraphics[width=0.25\linewidth]{_main_files/figure-latex/unnamed-chunk-7-1} \end{center}

\emph{Then \((\mathcal{Y},\mathbb{G},\gamma(\mu),S)\) is a measure-preserving dynamical system.}

\textbf{Lemma 5.10. (Hansen)} \emph{Let \((\mathcal{X},\mathbb{E},\mu,T)\) and \((\mathcal{Y},\mathbb{G},\nu,S)\) be two measure-preserving dynamical systems. Suppose there is a measurable map \(\gamma : \mathcal{X}\to\mathcal{Y}\) such that \(\nu =\gamma(\mu)\) and such that the diagram in lemma 5.9 commutes.}
\emph{If \((\mathcal{X},\mathbb{E},\mu,T)\) is ergodic then \((\mathcal{Y},\mathbb{G},\nu,S)\) is also ergodic.}

\textbf{Lemma 5.11. (Hansen)} \emph{(Maximal Ergodic Lemma)\index{Maximal Ergodic Lemma} Let \((\mathcal{X},\mathbb{E},\mu,T)\) be a measure-preserving dynamical system, and let \(f : \mathcal{X}\to \mathbb{R}\) be Borel measurable. If \(f\in \mathcal{L}^1(\mu)\) then it holds that}
\begin{align*}
    \int_{(M_n>0)}f\ d\mu\ge 0\tag{5.14}
\end{align*}
\emph{where \(M_n=\max\{0,S_1,S_2,...,S_n\}\) from the sequence}
\begin{align*}
    \Big(f(x), f\circ T(x),f\circ T^2(x),f\circ T^3(x),...\Big)\hspace{10pt}\text{with}\hspace{10pt}S_n=\sum_{i=0}^{n-1}f\circ T^i.
\end{align*}

\textbf{Theorem 5.12. (Hansen)} \emph{(Birkhoff's ergodic theorem)\index{Birkhoff's ergodic theorem} Let \((\mathcal{X},\mathbb{E},\mu,T)\) be an ergodic system. For \(f\in \mathcal{L}^1(\mu)\) it holds that}
\begin{align*}
    \frac{1}{n}\sum_{i=0}^{n-1}f\circ T^i\stackrel{\text{a.s.}}{\to} \int f\ d\mu.\tag{5.16}
\end{align*}

\textbf{Theorem 5.13. (Hansen)} \emph{(Ergodic theorem, \(\mathcal{L}^p\)-version)\index{Ergodic theorem, $\mathcal{L}^p$-version} Let \((\mathcal{X},\mathbb{E},\mu,T)\) be an ergodic system. If \(f\in \mathcal{L}^p(\mu)\) for some \(p\ge 1\) then it holds that}
\begin{align*}
    \frac{1}{n}\sum_{i=0}^{n-1}f\circ T^i\stackrel{\mathcal{L}^p}{\to}\int f\ d\mu.\tag{5.21}
\end{align*}

\textbf{Lemma 5.14. (Hansen)} \emph{Let \((\mathcal{X},\mathbb{E})\) be a measurable space. The measurable finite dimensional product sets in \(\mathcal{X}^{\mathbb{N}}\) form an \(\cap\)-stable generator for} \({\mathbb{E}}^{\otimes\mathbb{N}}\).

An element of the space \(\mathcal{X}^{\mathbb{N}}\) is a countable set of coordinates \(x_n\) for \(n\in\mathcal{N}\) with \(x_n\in\mathcal{X}\). A \textbf{finite dimensional product set}\index{finite dimensional product set} in \(\mathcal{X}^{\mathbb{N}}\) is set on the form
\begin{align*}
    A_1\times ... \times A_k\times \mathcal{X}\times \mathcal{X}\times ...
\end{align*}
where \(A_1,...,A_k\subset \mathcal{X}\). We also define the \textbf{projection sigma-algebra}\index{projection sigma-algebra} \(\mathbb{E}^{\otimes \mathbb{N}}\) as \(\sigma\left(\big(\hat{X}_n(\mathcal{X}^n)\big)_{n\in\mathbb{N}}\right)\) where \(\hat{X}_n(x_1,...,x_n)=x_n\).

\textbf{Definition 5.15. (Hansen)} \emph{Let \(X_1,X_2,...\) be a sequence of \((\mathcal{X},\mathbb{E})\)-valued random variable, defined on a background space \((\Omega,\mathbb{F},P)\), and let \(\mathbb{X}=(X_1,X_2,...)\) be their bundling. The \textbf{distribution} of the process is the image measure \(\mathbb{X}(P)\) on} \((\mathcal{X}^{\mathbb{N}},{\mathbb{E}}^{\otimes \mathbb{N}})\).

\textbf{Lemma 5.16. (Hansen)} \emph{Let \(\mathbb{X}=(X_1,X_2,...)\) and \(\mathbb{Y}=(Y_1,Y_2,...)\) be two \((\mathcal{X},\mathbb{E})\)-valued stochastic process, defined on a common background space . The two processes \(\mathbb{X}\) and \(\mathbb{Y}\) have the same distribution if and only if the have the same fidis.}
\emph{This can be checked by showing that}
\begin{align*}
    P(X_1\in A_i,...,X_k\in A_k)=P(Y_1\in A_1,...,Y_k\in A_k)\tag{5.25}
\end{align*}
\emph{for any \(k\in\mathbb{N}\) and any choice of \(A_1,...,A_k\in\mathbb{E}\).}

\textbf{Definition 5.18. (Hansen)} \emph{Let \(X_1,X_2,...\) be a sequence of \((\mathcal{X},\mathbb{E})\)-valued random variable, defined on a background space \((\Omega,\mathbb{F},P)\), and let \(\mathbb{X}=(X_1,X_2,...)\) be their bundling. Then we define:}
1. \emph{The proces \(\mathbb{X}\) is \textbf{stationary}\index{stationary} if the distrbution \(\mathbb{X}(P)\) is an \(S\)-invariant probability on} \(\Big(\mathbb{R}^{\mathbb{N}},\mathbb{B}^{\otimes \mathbb{N}}\Big)\),
2. \emph{The proces \(\mathbb{X}\) is \textbf{ergodic} if it is stationary and if the dynamical system} \(\Big(\mathbb{R}^{\mathbb{N}},\mathbb{B}^{\otimes \mathbb{N}},\mathbb{X}(P),S\Big)\) \emph{is ergodic.}
3. \emph{The proces \(\mathbb{X}\) is \textbf{mixing} if it is stationary and if the dynamical system} \(\Big(\mathbb{R}^{\mathbb{N}},\mathbb{B}^{\otimes \mathbb{N}},\mathbb{X}(P),S\Big)\) \emph{is mixing.}
\emph{with \(S\) being the \textbf{shift} map defined as \(S(x_1,x_2,...)=(x_2,x_3,...)\).}

\textbf{Theorem 5.20. (Hansen)} \emph{(Khintchine's ergodic theorem)\index{Khintchine's ergodic theorem} Let \(X_1,X_2,...\) be a stationary and ergodic sequence of real-valued random variables. If \(E\vert X_1\vert <\infty\) it holds that}
\begin{align*}
    \frac{1}{n}\sum_{i=1}^nX_i\stackrel{\text{a.s.}}{\to} E\ X_1.\tag{5.27}
\end{align*}
\emph{If \(E\vert X_1\vert ^p<\infty\) for some \(p\ge 1\), the convergence is also in \(\mathcal{L}^p\).}

\textbf{Theorem 5.21. (Hansen)} \emph{(Ergodic transformation theorem)\index{Ergodic transformation theorem} Let \(X_1,X_2,...\) be a sequence of real-valued random variables. For a measurable function} \(\phi : \big(\mathbb{R}^{\mathbb{N}},\mathbb{B}^{\otimes\mathbb{N}}\big)\to (\mathbb{R},\mathbb{B})\) \emph{we define new real-valued random variables}
\begin{align*}
    Y_n=\phi(X_n,X_{n+1},...)=\phi\circ S^{n-1}(\mathbb{X})\hspace{15pt}\text{for}\ n\in\mathbb{N}.
\end{align*}
\emph{If \(X_1,X_2,...\) is stationary and ergodic then \(Y_1,Y_2,...\) is also stationary and ergodic.}

\textbf{Definition 5.23. (Hansen)} \emph{Let \((X_n)_{n\in\mathbb{Z}}\) be a two-sided sequence of real-valued random variables, defined on a background space \((\Omega,\mathbb{F},P)\), and let \(\mathbb{X}\) be their bundling.}
1. \emph{The proces \(\mathbb{X}\) is \textbf{stationary} if the distrbution \(\mathbb{X}(P)\) is an \(S\)-invariant probability on} \(\Big(\mathbb{R}^{\mathbb{Z}},\mathbb{B}^{\otimes \mathbb{Z}}\Big)\),
2. \emph{The proces \(\mathbb{X}\) is \textbf{ergodic} if it is stationary and if the dynamical system} \(\Big(\mathbb{R}^{\mathbb{Z}},\mathbb{B}^{\otimes \mathbb{Z}},\mathbb{X}(P),S\Big)\) \emph{is ergodic.}
3. \emph{The proces \(\mathbb{X}\) is \textbf{mixing} if it is stationary and if the dynamical system} \(\Big(\mathbb{R}^{\mathbb{Z}},\mathbb{B}^{\otimes \mathbb{Z}},\mathbb{X}(P),S\Big)\) \emph{is mixing.}

\textbf{Theorem 5.25. (Hansen)} \emph{(Khintchine's ergodic theorem, two-sided version)\index{Khintchine's ergodic theorem, two-sided version} Let \((X_n)_{n\in\mathbb{Z}}\) be a two-sided sequence of real-valued random variables. If the sequence is stationary and ergodic and if \(E\vert X_1\vert <\infty\) then it holds that}
\begin{align*}
    \frac{1}{n}\sum_{i=1}^nX_i\stackrel{\text{a.s.}}{\to} E\ X_1.\tag{5.30}
\end{align*}
\emph{If \(E\vert X_1\vert^p<\infty\) for some \(p\ge 1\), the convergence is also in \(\mathcal{L}^p\).}

\textbf{Theorem 5.26. (Hansen)} \emph{(Ergodic transformation theorem)\index{Ergodic transformation theorem} Let \((X_n)_{n\in\mathbb{Z}}\) be a two-sided sequence of real-valued random variables. For a measurable function} \(\phi : \big(\mathbb{R}^{\mathbb{Z}},\mathbb{B}^{\otimes\mathbb{Z}}\big)\to (\mathbb{R},\mathbb{B})\) \emph{we define new real-valued random variables}
\begin{align*}
    Y_n=\phi\circ S^{n}(\mathbb{X})\hspace{15pt}\text{for}\ n\in\mathbb{Z}.
\end{align*}
\emph{If \((X_n)_{n\in\mathbb{Z}}\) is stationary and ergodic then \((Y_n)_{n\in\mathbb{Z}}\) is also stationary and ergodic.}

\hypertarget{weak-convergence}{%
\subsection{Weak Convergence}\label{weak-convergence}}

\textbf{Definition 6.1. (Hansen)} \emph{A sequence of probability measures \(\nu_1,\nu_2,...\) on \((\mathbb{R},\mathbb{B})\) is said to \textbf{converge weakly}\index{weakly convergence} to a limit probability measure \(\nu\) if}
\begin{align*}
    \int f\ d\nu_n\to \int f\ d\nu\hspace{15pt}\text{for every}\ f\in C_b(\mathbb{R})\tag{6.2}
\end{align*}
\emph{We write \(\nu_n\stackrel{\text{wk}}{\to} \nu\) to denote weak convergence.}

\textbf{Theorem 6.4. (Hansen)} \emph{(Scheffe's)\index{Scheffe's} Let \(\nu_1,\nu_2,...\) and \(\nu\) be probability measures on \((\mathbb{R},\mathbb{B})\). Assume that for some choice of basic measure \(\mu\) it holds that \(\nu_n=f_n\cdot \mu\) for every \(n\) and \(\nu = f\cdot \mu\) for suitable density functions \(f_n\) and \(f\). If}
\begin{align*}
    f_n(x)\to f(x)\hspace{15pt}\mu\text{-almost surely}
\end{align*}
\emph{then it holds that \(\nu_n\stackrel{\text{wk}}{\to} \nu\).}

\textbf{Lemma 6.8. (Hansen)} \emph{Let \(\mu\) and \(\nu\) be two probability measures on \((\mathbb{R},\mathbb{B})\). If}
\begin{align*}
    \int f\ d\mu=\int f\ d\nu\hspace{15pt}\text{for all}\ f\in C_b(\mathbb{R})\tag{6.7}
\end{align*}
\emph{then it holds that \(\mu=\nu\).}

\textbf{Theorem 6.9. (Hansen)} \emph{Let \(\nu_1,\nu_2,...\) be a sequence of probability measures on \((\mathbb{R},\mathbb{B})\) and let \(\mu\) and \(\nu\) be two extra probability measures. If}
\begin{align*}
    \nu_n\stackrel{\text{wk}}{\to} \mu\hspace{15pt}\text{and}\hspace{15pt}\nu_n\stackrel{\text{wk}}{\to} \nu
\end{align*}
\emph{then \(\mu=\nu\).}

\textbf{Definition 6.10. (Hansen)} \emph{A sequence of real-valued variables \(X_1,X_2,...\), defined on a common background space \((\Omega,\mathbb{F},P)\) is said to \textbf{converge in distrbution}\index{convergence in distribution} to a limit variable \(X\) if}
\begin{align*}
    \int f(X_n)\ dP\to \int f(X)\ dP\hspace{15pt}\text{for every}\ f\in C_b(\mathbb{R}).\tag{6.9}
\end{align*}
We will write \(X_n\stackrel{\mathcal{D}}{\to} X\) to denote convergence in distribution.

\textbf{Lemma 6.11. (Hansen)} \emph{Let \(X_1,X_2,...\) and \(X\) be real-valued random variables. It holds that}
\begin{align*}
    X_n\stackrel{\text{P}}{\to} X\hspace{10pt}\Rightarrow\hspace{10pt} X_n\stackrel{\mathcal{D}}{\to} X.
\end{align*}

\textbf{Lemma 6.12. (Hansen)} \emph{Let \(X_1,X_2,...\) be real-valued random variable and let \(x_0\in\mathbb{R}\). It holds that}
\begin{align*}
    X_n\stackrel{\mathcal{D}}{\to} x_0\hspace{10pt}\Rightarrow\hspace{10pt} X_n\stackrel{\text{P}}{\to} x_0.
\end{align*}

\textbf{Theorem 6.13. (Hansen)} \emph{Let \(\nu,\nu_1,\nu_2,...\) be probability measures on \((\mathbb{R},\mathbb{B})\). Let \(\mathcal{H}\) be a class of bounded, non-negative and measurable functions with the following approximation property: For \(f\in C_b(\mathbb{R})\) with \(f\ge 0\) there is a sequence \(h_1,h_2,...\) of \(\mathcal{H}\)-functions such that \(h_n\nearrow f\). If}
\begin{align*}
    \int h\ d\nu_n\to \int h\ d\nu\hspace{15pt}\text{for all}\ h\in\mathcal{H}.\tag{6.11}
\end{align*}
\emph{then it holds that \(\nu_n\stackrel{\text{wk}}{\to} \nu\).}

\textbf{Theorem 6.14. (Hansen)} \emph{Let \(\nu,\nu_1,\nu_2,...\) be probability measures on \((\mathbb{R},\mathbb{B})\). If}
\begin{align*}
    \int f\ d\nu_n\to \int f\ d\nu\hspace{15pt}\text{for all}\ f\in C_c(\mathbb{R})\tag{6.13}
\end{align*}
\emph{then it holds that \(\nu_n\stackrel{\text{wk}}{\to} \nu\).}

The class \(C_c(\mathbb{R})\) is denoted as the set of all continuous real-valued functions with compact support i.e.~there exist a \(M>0\) such that \(f(x)=0\) for all \(\vert x\vert>M\).

\textbf{Lemma 6.15. (Hansen)} \emph{Let \(\nu,\nu_1,\nu_2,...\) be probability measures on \((\mathbb{R},\mathbb{B})\), and let \(F,F_1,F_2,...\) be the corresponding distribution functions. If \(\nu_n\stackrel{\text{wk}}{\to}\nu\) then it holds that}
\begin{align*}
    F_n(x_0)\to F(x_0),
\end{align*}
\emph{whenever \(x_0\) is a point of continuity for \(F\).}

\textbf{Theorem 6.17. (Hansen)} \emph{(Helly-Bray) Let \(\nu,\nu_1,\nu_2,...\) be probability measures on \((\mathbb{R},\mathbb{B})\), and let \(F,F_1,F_2,...\) be the corresponding distribution functions. It holds that \(\nu_n\stackrel{\text{wk}}{\to}\nu\) if and only if there is a dense subset \(A\subset\mathbb{R}\) such that}
\begin{align*}
    F_n(x)\to F(x)\hspace{15pt}\text{for every}\ x\in A.\tag{6.16}
\end{align*}

\textbf{Theorem 6.18. (Hansen)} \emph{Let \(\nu,\nu_1,\nu_2,...\) be probability measures on \((\mathbb{R},\mathbb{B})\). Let \(F,F_1,F_2,...\) be the corresponding distribution functions, and let \(q,q_1,q_2,...\) be the corresponding quantile functions. If \(\nu_n\stackrel{\text{wk}}{\to}\nu\) then it holds that}
\begin{align*}
    q_n(p)\to q(p)
\end{align*}
\emph{for any \(p\in(0,1)\) such that the equation \(F(x)=p\) hast at most one solution.}

\textbf{Theorem 6.19. (Hansen)} \emph{(Skorokhod's representation theorem)\index{Skorokhod's representation theorem} Let \(\nu,\nu_1,\nu_2,...\) be probability measures on \((\mathbb{R},\mathbb{B})\). If \(\nu_n\stackrel{\text{wk}}{\to} \nu\) then it is possible to find random variables \(X,X_1,X_2,...\) on a background space \((\Omega,\mathbb{F},P)\) such that}
\begin{align*}
    X(P)=\nu,\ X_1(P)=\nu_1,\ X_2(P)=\nu_2, ...
\end{align*}
\emph{and such that \(X_n\stackrel{\text{a.s.}}{\to} X\).}

\textbf{Corollary 6.20. (Hansen)} \emph{Let \(\nu,\nu_1,\nu_2,...\) be probability measures on \((\mathbb{R},\mathbb{B})\) such that \(\nu_n\stackrel{\text{wk}}{\to}\nu\). Let \(h : \mathbb{R}\to\mathbb{R}\) be a bounded and measurable function. If there is a Boral-measurable set \(C\subset \mathbb{R}\) such that \(h\) is continuous in every point of \(C\) and such that \(\nu(C)=1\), then it holds that}
\begin{align*}
    \int h\ d\nu_n\to \int h\ d\nu.\tag{6.20}
\end{align*}

\textbf{Corollary 6.21. (Hansen)} \emph{(Portmanteau's lemma)\index{Portmanteau's lemma} Let \(\nu,\nu_1,\nu_2,...\) be probability measures on \((\mathbb{R},\mathbb{B})\) such that \(\nu_n\stackrel{\text{wk}}{\to}\nu\). For any open set \(G\subset \mathbb{R}\) it holds that}
\begin{align*}
    \liminf{\nu_n(G)}\ge \nu(G)\tag{6.21}
\end{align*}

\textbf{Definition 6.22. (Hansen)} \emph{The \textbf{characteristic function}\index{characteristic function} for a probability measure \(\nu\) on \((\mathbb{R},\mathbb{B})\) is the function \(\phi : \mathbb{R}\to \mathbb{C}\) given by}
\begin{align*}
    \phi(\theta)=\int e^{ix\theta}\ d\nu(x).\hspace{15pt}\text{for}\ \theta\in\mathbb{R}.\tag{6.23}
\end{align*}

Some useful observations include \(\vert e^{ix\theta}\vert = 1\) hence \(\phi(\theta)\le 1\) for all \(\theta\in\mathbb{R}\). We may also split the \(\phi\) into an imaginary part and a real part with Euler's cartesian form
\begin{align*}
    \phi(\theta)=\int \cos (x\theta)\ d\nu(x)+i\int \sin (x\theta)\ d\nu(x)\tag{6.24}
\end{align*}
And lastly we have the implication
\begin{align*}
    \nu_n\stackrel{\text{wk}}{\to} \nu \hspace{10pt}\Rightarrow\hspace{10pt} \phi_n(\theta)\to \phi(\theta)\hspace{10pt}\text{for all}\ \theta\in\mathbb{R}.
\end{align*}
Furthermore, if \(Y=\xi+\sigma X\) and \(X\sim \mathcal{N}(0,1)\) we have
\begin{align*}
    \phi_Y(\theta)=e^{i\xi\theta}e^{-\sigma^2\theta^2/2}.
\end{align*}

\textbf{Theorem 6.28. (Hansen)} \emph{The characteristic function for any probability measure \(\nu\) on \((\mathbb{R},\mathbb{B})\) is uniformly continuous.}

\textbf{Theorem 6.29. (Hansen)} \emph{Let \(\nu\) be a probability measure on \((\mathbb{R},\mathbb{B})\). If}
\begin{align*}
    \int \vert x\vert^k\ d\nu(x)<\infty
\end{align*}
\emph{for some \(k\in\mathbb{N}\), then the characteristic function \(\phi\) is \(C^k\) and it holds that}
\begin{align*}
    \phi^{(k)}(\theta)=i^k\int x^ke^{i\theta x}\ d\nu(x)\hspace{15pt}\text{for}\ \theta\in\mathbb{R}.\tag{6.31}
\end{align*}

\textbf{Definition 6.30. (Hansen)} \emph{The \textbf{convolution}\index{convolution} of two probability measures \(\nu\) and \(\xi\) on \((\mathbb{R},\mathbb{B})\) is the image measure}
\begin{align*}
    \nu * \xi=\kappa (\nu\otimes\xi)\tag{6.33}
\end{align*}
\emph{where \(\kappa : \mathbb{R}^2\to\mathbb{R}\) is the addition map \(\kappa(x,y)=x+y\).}

\textbf{Theorem 6.31. (Hansen)} \emph{Let \(\nu\) and \(\xi\) be two probability measures on \(\mathbb{R}\). If \(\xi=f\cdot m\), then the convolution \(\nu*\xi\) will have a density with respect to \(m\). The density is given as}
\begin{align*}
    g(x)=\int f(x-y)\ d\nu(y)\hspace{15pt}\text{for}\ x\in\mathbb{R}.\tag{6.35}
\end{align*}

\textbf{Lemma 6.31. (Hansen)} \emph{Let \(\nu_1\) and \(\nu_2\) be two probability measures on \((\mathbb{R},\mathbb{B})\) with characteristic functions \(\phi_1\) and \(\phi_2\). The convolution \(\nu_1*\nu_2\) has characteristic function \(\gamma\) given by}
\begin{align*}
    \gamma(\theta)=\phi_1(\theta)\phi_2(\theta)\hspace{15pt}\text{for}\ \theta\in\mathbb{R}.\tag{6.37}
\end{align*}

\textbf{Theorem 6.34. (Hansen)} \emph{Let \(\xi,\nu,\nu_1,\nu_2,...\) be probability measures on \((\mathbb{R},\mathbb{B})\). It holds that}
\begin{align*}
    \nu_n\stackrel{\text{wk}}{\to} \nu\hspace{10pt}\Rightarrow\hspace{10pt} \nu_n*\xi\stackrel{\text{wk}}{\to} \nu *\xi.
\end{align*}

\textbf{Definition 6.35. (Hansen)} \emph{A probability measure \(\nu=f\cdot m\) on \((\mathbb{R},\mathbb{B})\) with density \(f\) with respect to Lebesgue measure is of \textbf{Polya class}\index{Polya class} if \(f\in C_b(\mathbb{R})\) and if the Fourier transform \(\hat{f}\) is \(m\)-integrable.}

\textbf{Lemma 6.39. (Hansen)} \emph{Let \(\nu\) and \(\xi\) be two probability measures on \(\mathbb{R}\). If \(\xi\) is of Polya class then the convolution \(\nu *\xi\) is also of Polya class.}

\textbf{Theorem 6.40. (Hansen)} \emph{(Inversion theorem)\index{Inversion theorem} Let \(\nu=f\cdot m\) be a probability measure on \((\mathbb{R},\mathbb{B})\) of Polya class. It holds that}
\begin{align*}
    f(x)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{-i\theta x}\hat{f}(\theta)\ d\theta,\ x\in\mathbb{R}.\tag{6.39}
\end{align*}

\textbf{Theorem 6.41. (Hansen)} \emph{Let \(\nu_1\) and \(\nu_2\) be two probability measures on \((\mathbb{R},\mathbb{B})\) with characteristic functions \(\phi_1\) and \(\phi_2\). If}
\begin{align*}
    \phi_1(\theta)=\phi_2(\theta),\ \forall \theta \in\mathbb{R}
\end{align*}
\emph{then \(\nu_1=\nu_2\).}

\textbf{Lemma 6.42. (Hansen)} \emph{Let \(f : \mathbb{R}\to\mathbb{R}\) be bounded and uniformly continuous. For every \(\varepsilon>0\) there is a probability measure \(\xi\) of Polya class with the property that}
\begin{align*}
    \Big\vert f(x)-\int f(x+y)\ d\xi(y)\Big\vert<\varepsilon,\ \forall x\in\mathbb{R}.\tag{6.43}
\end{align*}

\textbf{Theorem 6.43. (Hansen)} \emph{(Continuity theorem)\index{Continuity theorem} Let \(\nu,\nu_1,\nu_2,...\) be probability measures on \((\mathbb{R},\mathbb{B})\) with characteristic functions \(\phi,\phi_1,\phi_2,...\). If}
\begin{align*}
    \phi_n(\theta)\to \phi(\theta),\ \theta\in\mathbb{R},\tag{6.45}
\end{align*}
\emph{then it holds that \(\nu_n\stackrel{\text{wk}}{\to}\nu\).}

\textbf{Definition 6.44. (Hansen)} \emph{A sequence of probability measures \(\nu_1,\nu_2,...\) on \(\big(\mathbb{R}^k,\mathbb{B}_k\big)\) is said to \textbf{converge weakly} to a limit probability measure \(\nu\) if}
\begin{align*}
    \int f(x)\ d\nu_n(x)\to\int f(x)\ d\nu(x), \forall f\in C_b(\mathbb{R}^k).\tag{6.46}
\end{align*}
\emph{We will write \(\nu_n\stackrel{\text{wk}}{\to}\nu\) to denote weak convergence.}

\textbf{Theorem 6.45. (Hansen)} \emph{(Continuity theorem)\index{Continuity theorem} Let \(\nu,\nu_1,\nu_2,...\) be probability measures on \(\big(\mathbb{R}^k,\mathbb{B}_k\big)\) with characteristic functions \(\phi,\phi_1,\phi_2,...\). If}
\begin{align*}
    \phi_n(\theta)\to \phi(\theta),\ \theta\in\mathbb{R}^k,\tag{6.47}
\end{align*}
\emph{then it holds that \(\nu_n\stackrel{\text{wk}}{\to}\nu\).}

\textbf{Lemma 6.46. (Hansen)} \emph{Let \(\mathbf{X}\) be an \(\mathbb{R}^k\)-valued random variable with characteristic function \(\phi_\mathbf{X}\), and let \(\mathbf{Y}\) be an \(\mathbb{R}^m\)-valued random variable with characteristic function \(\phi_\mathbf{Y}\). If \(\mathbf{X} \perp \!\!\! \perp \mathbf{Y}\) then the bundle \((\mathbf{X},\mathbf{Y})\) is an \(\mathbb{R}^{k+m}\)-valued random variable with}
\begin{align*}
    \phi_{(\mathbf{X},\mathbf{Y})}(\theta_1,\theta_2)=\phi_\mathbf{X}(\theta_1)\phi_\mathbf{Y}(\theta_2),\ \theta_1\in\mathbb{R}^k,\theta_2\in\mathbb{R}^m.\tag{6.49}
\end{align*}

\textbf{Theorem 6.47. (Hansen)} \emph{(Continuous mapping theorem)\index{Continuity mapping theorem} Let \(\mathbf{X}_1,\mathbf{X}_2,...\) and \(\mathbf{X}\) be random variables with values in \(\mathbb{R}^k\), and let \(h : \mathbb{R}^k\to\mathbb{R}^m\) be continuous. If \(\mathbf{X}_n\stackrel{\mathcal{D}}{\to} \mathbf{X}\), then it holds that \(h(\mathbf{X}_n)\stackrel{\mathcal{D}}{\to} h(\mathbf{X})\).}

\textbf{Theorem 6.48. (Hansen)} \emph{(Cramer-Wold's device)\index{Cramer-Wold's device} Let \(\mathbf{X}_1,\mathbf{X}_2,...\) and \(\mathbf{X}\) be random variables with values in \(\mathbb{R}^k\). If}
\begin{align*}
    \mathbf{v}^\top\mathbf{X}_n\stackrel{\mathcal{D}}{\to} \mathbf{v}^\top\mathbf{X},\tag{6.51}
\end{align*}
\emph{for any fixed vector \(\mathbf{v}\in\mathbb{R}^k\), then it holds that \(\mathbf{X}_n\stackrel{\mathcal{D}}{\to} \mathbf{X}\).}

\textbf{Lemma 6.49. (Hansen)} \emph{Let \(\mathbf{X},\mathbf{X}_1,\mathbf{X}_2,...\) be random variables with values in \(\mathbb{R}^k\), let \(\mathbf{Y}_1,\mathbf{Y}_2,...\) be random variables in \(\mathbb{R}^m\), and let \(\mathbf{y}\) be a vector in \(\mathbb{R}^m\). If it holds that}
\begin{align*}
    \mathbf{X}_n\stackrel{\mathcal{D}}{\to} \mathbf{X},\hspace{15pt}\mathbf{Y}_n\stackrel{\text{P}}{\to} \mathbf{y}
\end{align*}
\emph{then the bundle \((\mathbf{X}_n,\mathbf{Y}_n)\) in \(\mathbb{R}^{k+m}\) will satisfy that}
\begin{align*}
    (\mathbf{X}_n,\mathbf{Y}_n)\stackrel{\mathcal{D}}{\to} (\mathbf{X},\mathbf{y}).
\end{align*}

\textbf{Corollary 6.50. (Hansen)} \emph{(Slutsky's lemma)\index{Slutsky's lemma} Let \(\mathbf{X},\mathbf{X}_1,\mathbf{X}_2,...\) and \(\mathbf{Y}_1,\mathbf{Y}_2,...\) be random variables with values in \(\mathbb{R}^k\). It holds that}
\begin{align*}
    \mathbf{X}_n\stackrel{\mathcal{D}}{\to} \mathbf{X},\hspace{10pt}\mathbf{Y}_n\stackrel{\text{P}}{\to} \mathbf{0}\hspace{10pt}\Rightarrow\hspace{10pt} \mathbf{X}_n+\mathbf{Y}_n\stackrel{\mathcal{D}}{\to}\mathbf{X}.
\end{align*}

\textbf{Corollary 6.51. (Hansen)} \emph{Let \(\mathbf{X}_1,\mathbf{X}_2,...\) and \(\mathbf{X}\) be random variables with values in \(\mathbb{R}^k\) and let \(Y_1,Y_2,...\) be real-valued random variables. It holds that}
\begin{align*}
    \mathbf{X}_n\stackrel{\mathcal{D}}{\to} \mathbf{X},\hspace{10pt}Y_n\stackrel{\text{P}}{\to} 1\hspace{10pt}\Rightarrow\hspace{10pt} Y_n\mathbf{X}_n\stackrel{\mathcal{D}}{\to}\mathbf{X}.
\end{align*}

\textbf{Corollary 6.52. (Hansen)} \emph{Let \(\mathbf{X}_1,\mathbf{X}_2,...\) and \(\mathbf{X}\) be random variables with values in \(\mathbb{R}^k\) and let \(Y_1,Y_2,...\) be real-valued random variables. If}
\begin{align*}
    \mathbf{X}_n\stackrel{\mathcal{D}}{\to} \mathbf{X},\hspace{10pt}Y_n\stackrel{\text{P}}{\to} 0\hspace{10pt}\Rightarrow\hspace{10pt} Y_n\mathbf{X}_n\stackrel{\text{P}}{\to} \mathbf{0}.
\end{align*}

\textbf{Definition 6.53. (Hansen)} \emph{The \(\mathbb{R}^k\)-valued random variable \(\mathbf{Z}=(Z_1,...,Z_k)\) has a \textbf{multivariate Gaussian distribution}\index{multivariate Gaussian distribution} if and only if the real-valued random variable \(\sum_{j=1}^kc_jZ_j\) has a one-dimensional Gaussian distribution for every choice of \(c_1,...,c_k\in\mathbb{R}\).}

\textbf{Theorem 6.54. (Hansen)} \emph{Let \(\mathbf{Z}=(Z_1,...,Z_k)\) have a multivariate Gaussian distribution with \(E\ \mathbf{Z}=\xi\) and \(V\ \mathbf{Z}=\Sigma\). Then the characteristic function is}
\begin{align*}
    \phi_\mathbf{Z}(\theta)=e^{i\theta^\top\xi}\exp\left(-\frac{1}{2}\theta^\top\Sigma\theta\right),\ \theta\in\mathbb{R}^k.\tag{6.53}
\end{align*}
\emph{Conversly, if \(\mathbf{Z}\) has characteristic function given by (6.53) for some \(\xi\in\mathbb{R}^k\) and some symmetric, positive semi-definite \(k\times k\) matrix \(\Sigma\) then \(\mathbf{Z}\) has a multivariate Gaussian distribution with \(E\ \mathbf{Z}=\xi\) and \(V\ \mathbf{Z}=\Sigma\).}

\textbf{Corollary 6.55. (Hansen)} \emph{Let \(\mathbf{Z}\) be an \(\mathbb{R}^k\)-valued random variable, let \(\mathbf{a}\in\mathbb{R}^m\) and let \(B\) be an \(m\times k\) matrix. It holds that}
\begin{align*}
    \mathbf{Z}\sim \mathcal{N}(\xi,\Sigma)\hspace{10pt}\Rightarrow\hspace{10pt} \mathbf{a}+B\mathbf{Z}\sim \mathcal{N}\left(\mathbf{a}+B\xi,B\Sigma B^\top\right).
\end{align*}

\textbf{Lemma 6.56. (Hansen)} \emph{Let \(\mathbf{X}=(X_1,...,X_k)\) and \(\mathbf{Y}=(Y_1,...,Y_m)\) be random variables with values in \(\mathbb{R}^k\) respectively \(\mathbb{R}^m\). If both \(\mathbf{X}\) and \(\mathbf{Y}\) have multivariate Gaussian distributions and if \(\mathbf{X}\) and \(\mathbf{Y}\) are independent, then the \(\mathbb{R}^{k+m}\)-valued bundle \((\mathbf{X},\mathbf{Y})\) has a multivariate Gaussian distribution.}

\textbf{Lemma 6.58. (Hansen)} \emph{Let \(\mathbf{X}=(X_1,...,X_k)\) and \(\mathbf{Y}=(Y_1,...,Y_m)\) be random variables with values in \(\mathbb{R}^k\) respectively \(\mathbb{R}^m\). If the \(\mathbb{R}^{k+m}\)-valued bundle \((\mathbf{X},\mathbf{Y})\) has a multivariate Gaussian distribution, and if}
\begin{align*}
    \text{Cov}(X_j,Y_l)=0,\ \forall j\text{ and }l
\end{align*}
\emph{then \(\mathbf{X}\perp \!\!\! \perp\mathbf{Y}\).}

\textbf{Definition 6.59. (Hansen)} \emph{Let \(\mathbf{X}_1,\mathbf{X}_2,...\) be \(\mathbb{R}^k\)-valued random variables. Let \(\xi\in\mathbb{R}^k\) be a vector and let \(\Sigma\) be a symmetric, positive semi-definite \(k\times k\) matrix.}
\emph{We sat that \(\mathbf{X}_n\) has an \textbf{asymptotic normal distribution}\index{asymptotic normal distribution} with parameters \(\big(\xi,\frac{1}{n}\Sigma\big)\), written}
\begin{align*}
    \mathbf{X}_n\stackrel{\text{a.s.}}{\sim}\mathcal{N}\left(\xi,\frac{1}{n}\Sigma\right),
\end{align*}
\emph{if it holds that}
\begin{align*}
    \sqrt{n}(\mathbf{X}_n-\xi)\stackrel{\mathcal{D}}{\to} \mathcal{N}(0,\Sigma).
\end{align*}

\textbf{Lemma 6.60. (Hansen)} \emph{Let \(\mathbf{X}_1,\mathbf{X}_2,...\) and \(\mathbf{Y}\) be random variables with values in \(\mathbb{R}^k\). If it holds that \(\mathbf{X}_n\stackrel{\text{a.s.}}{\sim} \mathcal{N}(\xi,\frac{1}{n}\Sigma)\) then it follows that \(\mathbf{X}_n\stackrel{\text{P}}{\to}\xi\).}

\textbf{Lemma 6.61. (Hansen)} \emph{Let \(\mathbf{X}_1,\mathbf{X}_2,...\) and \(\mathbf{Y}\) be \(\mathbb{R}^k\)-valued random variables, and assume that \(\sqrt{n}\mathbf{X}_n\stackrel{\mathcal{D}}{\to} \mathbf{Y}\). Let \(g : \mathbb{R}^k\to \mathbb{R}^m\) be a measurable map. Assume that \(g(\mathbf{0})=\mathbf{0}\) and that \(g\) is differentiable in \(\mathbf{0}\) with deriviate \(Dg(\mathbf{0})=A\). Then it holds that}
\begin{align*}
    \sqrt{n}g(\mathbf{X}_n)\stackrel{\mathcal{D}}{\to} A\ \mathbf{Y}.
\end{align*}

\textbf{Lemma 6.62. (Hansen)} \emph{(Delta method)\index{Delta method} Let \(\mathbf{X}_1,\mathbf{X}_2,...\) be \(\mathbb{R}^k\)-valued random variables, and let \(f : \mathbb{R}^k\to \mathbb{R}^m\) be measurable. If \(f\) is differentiable in \(\xi\), then it holds that}
\begin{align*}
    \mathbf{X}_n\stackrel{\text{a.s.}}{\sim} \mathcal{N}\left(\xi,\frac{1}{n}\Sigma\right)\hspace{10pt}\Rightarrow\hspace{10pt} f(\mathbf{X}_n)\stackrel{\text{a.s.}}{\sim} \mathcal{N}\left(f(\xi),\frac{1}{n}Df(\xi)\Sigma Df(\xi)^\top\right).
\end{align*}

\hypertarget{central-limit-theorems}{%
\subsection{Central Limit Theorems}\label{central-limit-theorems}}

\textbf{Lemma 7.1. (Hansen)} \emph{Let \(z_1,...,z_n\) and \(w_1,...,w_n\) be complex numbers. If \(\vert z_i\vert \le 1\) and \(\vert w_i\vert\le 1\) for all \(i=1,...,n\) then it holds that}
\begin{align*}
    \left\vert\prod_{i=1}^n z_i-\prod_{i=1}^n w_i\right\vert\le \sum_{i=1}^n \vert z_i-w_i\vert.\tag{7.1 }
\end{align*}

\textbf{Theorem 7.2. (Hansen)} \emph{(Basic CLT)\index{Basic CLT} Let \(X_1,X_2,...\) be independent and identically distributed real-valued random variables. Assume that \(E\ X_1=0\) and \(E\ X_1^2=1\). Then it holds that}
\begin{align*}
    \frac{1}{\sqrt{n}}\sum_{i=1}^nX_i\stackrel{\mathcal{D}}{\to} \mathcal{N}(0,1)\tag{7.3}
\end{align*}

\textbf{Theorem 7.3. (Hansen)} \emph{(Laplace's CLT)\index{Laplace's CLT} Let \(X_1,X_2,...\) be independent and identically distributed real-valued random variables with \(E\ X_1^2<\infty\). It holds that}
\begin{align*}
    \frac{1}{n}\sum_{i=1}^nX_i\stackrel{\mathcal{D}}{\to} \mathcal{N}\left(E\ X_1,\frac{V\ X_1}{n}\right)\tag{7.4}
\end{align*}

\textbf{Theorem 7.7. (Hansen)} \emph{(Laplace's CLT, multivariate version)\index{Laplace's CLT, multivariate version} Let \(\mathbf{X}_1,\mathbf{X}_2,...\) be independent and identically distributed random variables with values in \(\mathbb{R}^k\). Assume that \(E\vert \mathbf{X}_1\vert^2<\infty\). It holds that}
\begin{align*}
    \frac{1}{n}\sum_{i=1}^n\mathbf{X}_i\stackrel{\mathcal{D}}{\to} \mathcal{N}\left(E\ \mathbf{X}_1,\frac{1}{n}V\ \mathbf{X}_1\right)\tag{7.7}
\end{align*}

\textbf{Definition 7.10. (Hansen)} \emph{Let \((X_{nm})\) be a centralized array of real-valued random variables.}
a. \emph{The array satisfies the \textbf{vanishing variance condition}\index{vanishing variance condition} if}
\begin{align*}
      \max_{m=1,...,n}E\ X_{nm}^2\to 0.\tag{7.8}
  \end{align*}
b. \emph{The array satisfies \textbf{Lindeberg's condition}\index{Lindeberg's condition} if}
\begin{align*}
      \forall c>0:\hspace{15pt}\sum_{m=1}^n\int_{(\vert X_{nm}\vert>c)}X_{nm}^2\ dP\to 0.\tag{7.9}
  \end{align*}
c.~\emph{The array satisfies \textbf{Lyapounov's condition}\index{Lyapounov's condition} of order \(\alpha>2\) if}
\begin{align*}
      \sum_{m=1}^nE\ \vert X_{nm}\vert ^\alpha\to 0.\tag{7.10}
  \end{align*}

\textbf{Lemma 7.11. (Hansen)} \emph{Lyapounov's condition of order \(\alpha>2\) implies Lindeberg's condition.}

\textbf{Lemma 7.12. (Hansen)} \emph{Lindeberg's condition implies the vanishing variance condition.}

\textbf{Theorem 7.14. (Hansen)} \emph{(Lindeberg's CLT)\index{Lindeberg's CLT} Let \((X_{nm})\) be a centralized array of real-valued random variables with \(E\ X_{nm}^2<\infty\). Assume that the array satisfies that}
\begin{align*}
    E\ X_{nm}=0,\ \forall n,m,
\end{align*}
\emph{and that}
\begin{align*}
    \sum_{m=1}^n E\ X^2_{nm}=1.\tag{7.13}
\end{align*}
\emph{Assume that the array has independence within rows i.e.~\(X_{i1}\perp \!\!\! \perp ... \perp \!\!\! \perp X_{ii}\) for all \(i=1,...,n\) and satisfies Lindeberg's condition. Then it holds that}
\begin{align*}
    \sum_{m=1}^nX_{nm}\stackrel{\mathcal{D}}{\to} \mathcal{N}(0,1).
\end{align*}

\textbf{Theorem 7.19. (Hansen)} \emph{(Lindeberg's CLT, multivariate version)\index{Lindeberg's CLT, multivariate version} Let \((\mathbf{X}_{nm})\) be a triangular array of random variables with values in \(\mathbb{R}^k\) with \(E\ \vert \mathbf{X}_{nm}\vert^2<\infty\) for all \(n\), \(m\). Assume that}
\begin{align*}
    E\ \mathbf{X}_{nm}=\mathbf{0},\ \forall n,m
\end{align*}
\emph{and}
\begin{align*}
    \sum_{m=1}^n V\ \mathbf{X}_{nm}\to \Sigma
\end{align*}
\emph{for a fixed \(k\times k\) matrix \(\Sigma\). Assume that the array has independence within rows, and assume that the associated real-valued array \((\vert\mathbf{X}_{nm}\vert )\) satisfies Lindeberg's condition. Then it holds that}
\begin{align*}
    \sum_{m=1}^n\mathbf{X}_{nm}\stackrel{\mathcal{D}}{\to} \mathcal{N}(\mathbf{0},\Sigma)
\end{align*}

\hypertarget{continuous-time-stochastic-processes}{%
\chapter{Continuous Time Stochastic Processes}\label{continuous-time-stochastic-processes}}

\hypertarget{brownian-motion}{%
\section{Brownian Motion}\label{brownian-motion}}

\textbf{Definition 4.1. (Bjork)} \emph{A stochastic process \(W\) is called a \textbf{Brownian motion}\index{Brownian motion} or \textbf{Wiener process}\index{Wiener process} if the following conditions hold}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(W_0=0\).
\item
  \emph{The process \(W\) has independent increments, i.e.~if \(r<s\le t< u\) then \(W_u-W_t\) and \(W_s-W_r\) are independent random variables.}
\item
  \emph{For \(s<t\) the random variable \(W_t-W_s\) has the Gaussian distribution \(\mathcal{N}(0,t-s)\).}
\item
  \emph{\(W\) has continuous trajectories i.e.~\(s\mapsto W(s;\omega)\) i continuous for all \(\omega \in\Omega\).}
\end{enumerate}

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.75\textwidth]{figures/BM_sim.png}
  \end{center}
\end{figure}

As one can see from the simulated sample path on the right, the Brownian motion is rather irratic. In fact, the process varies infinitely on any interval with length greater than 0. This gives some of the characteristics of the process including that: \(W\) is continuous and \(W\) is non-differential everywhere. This irratic behaviour is summed up in the theorem.

\textbf{Theorem 4.2. (Bjork)} \emph{A Brownian motions trajectory \(t\mapsto W_t\) is with probability one nowhere differential, and it has locally infinite total variation.}

\newpage

\hypertarget{filtration}{%
\section{Filtration}\label{filtration}}

Filtrations\index{Filtration} is widely used in stochastic processes, as they allow for the concept of knowledge/information. This is useful when considering mean-values of future states but in an increasing information setting. For this we introduce the term adapted processes.

\textbf{Definition B.17. (Bjork)} \textbf{(Adapted process)}\index{Adapted process} \emph{Let \((\mathcal{F}_t)_{t\ge 0}\) be a filtration on the probability space \((\mathcal{F}_t)_{t\ge 0}\). Furthermore, let \((X_t)_{t\ge 0}\) be a stochastic process on the same space. We say that \(X_t\) is adapted to the filtration \(\mathbf{F}\) if}

\[X_t\ \text{ is }\ \mathcal{F}_t-\text{measurable},\hspace{20pt}\forall t\ge 0.\]

Obviously, we may introduce the \textbf{natural filtration}\index{natural filtration} \(\mathcal{F}^X_t\) given by the tragetory of the process \(X_t\):

\[\mathcal{F}^X_t=\sigma(\{X_s,\ s\le t\}).\]

Indeed, \(X_t\) is adapted to this filtration.

\newpage

\hypertarget{martingale}{%
\section{Martingale}\label{martingale}}

\textbf{Definition.} \emph{Let \(M_t\) be a stochastic process defined on a background space \((\Omega,\mathcal{F},P)\). Let \((\mathcal{F}_t)_{t\ge 0}\) be a filtration. If \(M_t\) is adapted to the filtration \(\mathcal{F}_t\), \(E\vert M_t\vert <\infty\) and}

\[E[M_t\vert \mathcal{F}_s]=M_s,\hspace{20pt}P-\text{a.s.}\]

\emph{holds for any \(t>s\) we say that \(M_t\) is a martingale\index{martingale} (\(\mathbf{F}\)-martingale). If the above has \(\le\) or \(\ge\) we say that \(M_t\) is either a \textbf{submartingale}\index{submartingale} or \textbf{supermartingale}\index{supermartingale} respectively.}

Naturally, this defintions may easily be extended to discrete models and we have the trivial equality:

\[E[M_t-M_s\ \vert\ \mathcal{F}_s]=0.\]

Martingales is useful, when proofing probalistic statements as the posses tractable properties. A useful technique often include the construction of the martingale

\[M_t=E[X\ \vert\ \mathcal{F}_t].\]

\newpage

\hypertarget{stochastic-calculus}{%
\section{Stochastic calculus}\label{stochastic-calculus}}

\hypertarget{stochastic-integrals}{%
\subsection{Stochastic Integrals}\label{stochastic-integrals}}

We want to formulate financial markets in continuous time and the most elegant theory is obtained from processes that can be defined in terms of \textbf{stochastic differential equations}\index{stochastic differential equations, SDE} or in other words by their dynamics. We may call them \textbf{diffusion processes}\index{diffusion processes}, as they may be approximated by a stochastic difference equation:

\[
X_{t+\Delta t}-X_t=\mu(t,X_t)\Delta t+\sigma(t,X_t)Z_t.\tag{4.1}
\]

Above \(Z_t\) is a normally distributed random variable (a disturbance). In this formulation we say that \(S_t\) is driven by two forces: on one hand a locally deterministic velocity or drift \(\mu(t,X_t)\) and on the other hand a Gaussian term amplified by the deterministic factor \(\sigma(t,X_t)\).

\hypertarget{information}{%
\subsubsection{Information}\label{information}}

We consider a primary process \(X_t\) and we introduce the notion of information generated by \(X_t\) in terms of the natural filtration. The idea can be summed up in the following definition.

\textbf{Definition 4.3. (Bjork)} \emph{The symbol \(\mathcal{F}^X_t\subseteq\mathcal{F}\) denotes ``the information generated by \(X_t\) on the interval \([0,t]\)'', or alternatively ``what has happened to \(X_t\) over ther interval \([0,t]\)''.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{If, based upon observations of the trajectory \(\{X_s;\ 0\le s\le t\}\), it is possible to decide whether a given event \(A\) has occurred or not, then we write this as}
  \[
    A\in\mathcal{F}^X_t
    \]
  \emph{or say that ``\(A\) is \(\mathcal{F}^X_t\)-measurable''.}
\item
  \emph{If the value of a given random variable \(Z\) can be completely determined given observations of the tragectory \(\{X_s;\ 0\le s\le t\}\), then we also write}
  \[
    Z\in\mathcal{F}^X_t.\ \text{(}Z\text{ is }\mathcal{F}^X_t\text{-measurable)}
    \]
  3.\_ If \(Y\) is a stochastic process such that we have\_
  \[
    Y_t\in\mathcal{F}^X_t
    \]
  \emph{for all \(t\ge0\) then we say that \(Y_t\) is \textbf{adapted} to the \textbf{filtration} \(\{\mathcal{F}^X_t\}_{t\ge 0}\). For brevity of notation, we will sometimes write the filtration as \(\{\mathcal{F}^X_t\}_{t\ge 0}=\mathbf{F}\).}
\end{enumerate}

\hypertarget{stochastic-integrals-1}{%
\subsubsection{Stochastic Integrals}\label{stochastic-integrals-1}}

We will now formulate the theory of stochastic integrals, that is, processes written in terms of stochastic processes with stochastic integrator and/or stochastic integrant. We will consider some given standard Brownian motion \(W_t\) and another stochastic process \(X_t\). We need som integrability condition on \(X_t\) in order to do the calculations. We therefore determine a selection of suitable stochastic processes \(X\) must be contained in.

\textbf{Definition 4.4. (Bjork)} \emph{Let \(X_t\) be a stochastic process, then}

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  \emph{We say that \(X_t\) belongs to the class \(\mathcal{L}^2[a,b]\) if \(X_t\) is adapted to the filtration \(\mathcal{F}^X_t\) and the following holds}
  \[\int_a^bE[X_s^2]\ ds<\infty\]
\item
  \emph{We say that \(X_t\) belongs to the class \(\mathcal{L}^2\) if} \(X_t\in\mathcal{L}^2[0,t]\) for all \(t>0\).
\end{enumerate}

We now want to define what we mean by

\[
\int_a^bX_t\ dW_s
\]

for some process \(X_t\in\mathcal{L}^2\). We see that a way to go about this problem is to start by defining the concept for a simple stochastic process \(X_t\). By \emph{simple} we mean a process \(X_t\) that is constant on between some deterministic points in time \(a=t_0<t_1<\cdots<t_n=b\). In that case we may define the integral as

\[
\int_a^bX_s\ dW_s = \sum_{k=0}^{n-1}X_{t_k}[W_{t_{k+1}}-W_{t_k}].\tag{4.8}
\]

In the more general setting we may follow the following approach:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Approximate \(X\) with a sequence \(\{X^n\}_{n\in\mathbb{N}}\) of simple processes such that the following convergence criteria hold
\end{enumerate}

\[
  \int_a^bE[(X_s^n-X_s)^2]\ ds\to 0,\ n\to\infty
  \]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  For each \(n\) the integral \(\int_a^b X_s^n\ dW_s:=Z^n\) is well defined and it is possible to prove, using DCT, that a variable \(Z\) exists such that \(Z^n\to Z\) that is in \(L^2\).
\item
  We now define the stochastic integral by the limit
\end{enumerate}

\[
  \int_a^b X_s\ dW_s=\lim_{n\to \infty}\int_a^b X_s^n\ dW_s.\tag{4.9}
  \]

Obviously the hardest step is finding the processes \(X^n\). This stochastic har some properties we will use.

\textbf{Proposition 4.5. (Bjork)} \emph{Let \(X_t\in\mathcal{L}^2\), then}
\begin{align*}
&E\left[\int_a^b X_s\ dW_s\right]=0.\tag{4.12}\\
&E\left[\left(\int_a^b X_s\ dW_s\right)^2\right]=\int_a^b E[ X_s^2]\ dW_s.\tag{4.13}\\
&\int_a^b X_s\ dW_s\ \text{ is }\mathcal{F}_b^W\text{-measurable.}\tag{4.14}
\end{align*}

\hypertarget{martingales}{%
\subsubsection{Martingales}\label{martingales}}

\textbf{Definition 4.7. (Bjork)} \emph{Let \(M_t\) be a stochastic process defined on a background space \((\Omega,\mathcal{F},P)\). Let \((\mathcal{F}_t)_{t\ge 0}\) be a filtration. If \(M_t\) is adapted to the filtration \(\mathcal{F}_t\), \(E\vert M_t\vert <\infty\) and}

\[E[M_t\vert \mathcal{F}_s]=M_s,\hspace{20pt}P-\text{a.s.}\]

\emph{holds for any \(t>s\) we say that \(M_t\) is a martingale (\(\mathbf{F}\)-martingale). If the above has \(\ge\) or \(\le\) we say that \(M_t\) is either a \textbf{submartingale} or \textbf{supermartingale} respectively.}

\textbf{Proposition 4.8. (Bjork)} \emph{For any process \(X_t\in\mathcal{L}^2[s,t]\) the following hold:}

\[
E\left[\left.\int_s^t X_s\ dW_s\right\vert\mathcal{F}_s^W\right]=0
\]

\textbf{Corollary 4.9. (Bjork)} \emph{For any process \(X_t\in\mathcal{L}^2\) the process}

\[
M_t=\int_s^t X_s\ dW_s,
\]

\emph{is an \((\mathcal{F}_t^W)\)-martingale. In other words, modulo an integrability condition, \textbf{every stochastic integral is a martingale}.}

\textbf{Lemma 4.10. (Bjork)} \emph{Let \(M_t\) be a stochastic process with stochastic differential, then \(M_t\) is a martingale if and only if the stochastic differential has the form \(dM_t=X_t\ dW_t\) i.e.~\(M_t\) as no \(dt\)-term.}

\hypertarget{stochastic-calculus-and-the-ito-formula}{%
\subsubsection{Stochastic Calculus and the Ito Formula}\label{stochastic-calculus-and-the-ito-formula}}

Given this breif introduction to stochastic integrals we may formulate som simple calculus revolving around Ito's formula. We consider the stochastic process \(X_t\) and we suppose that there exist a real number \(X_0\) and adapted processes \(\mu\) and \(\sigma\) wrt. \(\mathcal{F}_t^W\) such that for all \(t\ge0\) we have

\[
X_t=X_0+\int_0^t\mu_s\ ds+\int_0^t\sigma_s\ dW_s,\tag{4.16}
\]

where \(W_t\) is a standard Brownian motion. We know from earlier courses that the above may be written in terms of the dynamics (pure notation):

\[
\left\{\begin{matrix}dX_t=\mu_t\ dt+\sigma_t\ dW_t,\tag{4.17/18}\\ X_0=X_0.\end{matrix}\right.
\]

Here we intepret the above as \(X_t\) has boundary condition \(X_0\) and evolves with a drift \(\mu_t\ dt\) amplified and distorted by the drift \(\sigma_t\ dW_t\). We say that \(X_t\) has \textbf{stochastic differential}\index{stochastic differential} \(dX_t\) and initial condition \(X_0\).

We want to understand how transformation of such an integral behaves and therefore we introduce some calculus which will tell how for instance \(f(t,X_t)\) (for some \(C^{1,2}\)-function) behaves. This insight is given by the important Ito's formula.

\textbf{Thoerem 4.11. (Bjork)} \textbf{(Ito's formula, one-dimensional)}\index{Ito's formula} \emph{Assume that the process \(X\) has a stochastic differential form given by}

\[
dX_t=\mu_t\ dt + \sigma_t\ dW_t,\tag{4.28}
\]

\emph{where \(\mu\) and \(\sigma\) are adapted processes, and let \(f:\mathbb{R}_+\times\mathbb{R}\to\mathbb{R}\) be a \(C^{1,2}\)-function. Define the process \(Z\) by \(Z_t=f(t,X_t)\). Then \(Z\) has stochastic differential given by}

\[
df(t,X_t)=\left(\frac{\partial f}{\partial t}(t,X_t) + \mu_t\frac{\partial f}{\partial x}(t,X_t) + \frac{1}{2}\sigma^2_t\frac{\partial^2 f}{\partial x^2}(t,X_t)\right)\ dt+\sigma_t\frac{\partial f}{\partial x}(t,X_t)\ dW_t.\tag{4.29}
\]

\textbf{Proposition 4.12. (Bjork)} \textbf{(Ito's formula, one-dimensional)} \emph{With assumptions as in theorem 4.11, \(df\) is given by}

\[
df=\frac{\partial f}{\partial t}\ dt + \frac{\partial f}{\partial x}\ dX_t + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\ (dX_t)^2,\tag{4.31}
\]

\emph{where we use the following table}

\[
\left\{\begin{matrix}(dt)^2=0,\\ dt\cdot dW_t=0,\\ (dW_t)^2=dt.\end{matrix}\right.
\]

\textbf{Lemma 4.18. (Bjork)} \emph{Let \(\sigma(t)\) be deterministic function of time and define the process \(X\) by}

\[
X_t=\int_0^t \sigma(s)\ dW_s.\tag{4.37}
\]

\emph{Then}

\[
X_t\sim\mathcal{N}\left(0,\int_0^t\sigma^2(s)\ ds\right).
\]

\hypertarget{the-multidimensional-ito-formula}{%
\subsubsection{The multidimensional Ito Formula}\label{the-multidimensional-ito-formula}}

Consider a vector process \(X=(X^1,...,X^n)^\top\) where each component \(X^i\) has stochastic differential

\[
d X_t^i=\mu_t^i\ dt+\sum_{j=1}^d\sigma^{ij}_t\ dW_t^j
\]

where \(W^1,...,W^d\) is independent Brownian motions. Then we have respectively the drift vector process \(\mu_t\) in \(n\) dimensions, the vector Brownian motion in \(d\) dimensions and a \(n\times d\)-dimensional \textbf{diffusion matrix} \(\sigma_t\) given as below

\[
\mu_t=\begin{bmatrix}\mu^1_t\\ \vdots\\ \mu^n_t\end{bmatrix},\hspace{10pt}W_t=\begin{bmatrix}W^1_t\\ \vdots\\ W^d_t\end{bmatrix},\hspace{10pt}\sigma_t=\begin{bmatrix}\sigma^{11}_t & \cdots & \sigma^{1d}_t \\ \vdots & \ddots & \vdots\\ \sigma^{n1}_t &\cdots& \sigma^{nd}_t\end{bmatrix}.
\]

Given this we may write the dynamics of \(X\) as

\[
d X_t=\mu_t\ dt+\sigma_t\ dW_t\in\mathbb{R}^n.
\]

Consider now a function \(f:\mathbb{R}_+\times \mathbb{R}^n\to\mathbb{R}\) which is a \(C^{1,2}\)-mapping. We want to study the dynamics of the process

\[
Z_t=f(t,X_t).
\]

The dynamics is given in the multidimensional version of Ito's formula.

\textbf{Thoerem 4.19. (Bjork)} \textbf{(Ito's formula, multi-dimensional)}\index{Ito's formula, multi-dimensional} \emph{Let \(X\) be given as above. Then the following holds:}

\begin{itemize}
\tightlist
\item
  \emph{The process \(f(t,X_t)\) has the stochastisc differential given by}
  \[
    df(t,X_t)=\left(\frac{\partial f}{\partial t}(t,X_t) + \sum_{i=1}^n\mu^i_t\frac{\partial f}{\partial x^i}(t,X_t) + \frac{1}{2}\sum_{i,j=1}^nC_t^{ij}\frac{\partial^2 f}{\partial x^i\partial x^j}(t,X_t)\right)\ dt+\sum_{i=1}^n\sigma^i_t\frac{\partial f}{\partial x^i}(t,X_t)\ dW_t.
    \]
  \emph{Here the row vector \(\sigma^i_t\) is the \(i\)'th row of the matrix \(\sigma_t\) and the matrix \(C\) is defined by \(C=\sigma\sigma^\top\).}
\item
  \emph{Alternatively, the differential is given by the formula}
  \[
    df(t,X_t)=\frac{\partial f}{\partial t}(t,X_t)\ dt + \sum_{i=1}^n\frac{\partial f}{\partial x^i}(t,X_t)\ dX^i_t + \frac{1}{2}\sum_{i,j=1}^n\frac{\partial^2 f}{\partial x^i\partial x^j}(t,X_t)\ dX^i_tdX^j_t,
    \]
  \emph{with the formal multiplication table}
  \[
    \left\{\begin{matrix}(dt)^2=0,\\  dt\cdot dW_t^i=0, & i = 1,...,d,\\ (dW_t^i)^2=dt, & i=1,...,d, \\ dW_t^i\cdot dW_t^i =0, & i\ne j.\end{matrix}\right.
    \]
\end{itemize}

Obviously, one can write the differential in Ito's formula in many other ways including a matrix-wise version using the Hessian matrix \(H_{ij}=\frac{\partial^2 f}{\partial x^i\partial x^j}\).

\hypertarget{correlated-brownian-motions}{%
\subsubsection{Correlated Brownian motions}\label{correlated-brownian-motions}}

In the previous section the \(d\)-dimensional Brownian was assumed to have independent Brownian motions. However we may instead consider a variation where we have some dependence between the Brownian motions.

This section has not been finished.
\pagebreak

\hypertarget{discrete-stochastic-integrals}{%
\subsection{Discrete Stochastic Integrals}\label{discrete-stochastic-integrals}}

This section has not been finished.
\pagebreak

\hypertarget{stochastic-differential-equations}{%
\subsection{Stochastic Differential Equations}\label{stochastic-differential-equations}}

We start the chapter by formalising some used objects. We consider the following objects.

\begin{itemize}
\tightlist
\item
  \(M(n,d)\) denotes the class of \(n\times d\)-matrices.
\item
  \(W\) is a \(d\)-dimensional Brownian motion
\item
  \(\mu\) is a \(\mathbb{R}^n\)-valued function with arguments \((t,X_t)\) with \(X_t\) being a \(n\)-dimensional stochastic process.
\item
  \(\sigma\) a \(M(n,d)\)-valued function with arguments as in \(\mu\).
\item
  \(x_0\) a \(\mathbb{R}^n\)-valued vector.
\end{itemize}

We want then to understand when the following has a solution

\[
dX_t=\mu(t,X_t)\ dt + \sigma(t,X_t)\ dW_t,\ \ X_0=x_0.\tag{5.1/2}
\]

We call such an equation the \textbf{stochastic differential equation}\index{stochastic differential equation, SDE} or simply SDE. We know that the above is loosely notation for the integral form as

\[
X_t=x_0+\int_0^t\mu(s,X_s)\ ds +\int_0^t\sigma(s,X_s)\ dW_s,\tag{5.3}
\]

for all \(t\ge 0\). The following proposition tells us when an solution exist to the problem above. In the below \(\Vert \cdot \Vert\) is usual euclidian norm

\[
\Vert x\Vert=\sqrt{\sum_{i=1}^nx_i^2}.
\]

\textbf{Proposition 5.1. (Bjork)} \emph{Suppose that there existis a constant \(K\) such that the following conditions are satisfied for all \(x,y\) and \(t\).}
\begin{align*}
\Vert \mu(t,x) - \mu(t,y) \Vert &\le K\Vert x-y\Vert,\tag{5.6}\\
\Vert \sigma(t,x) - \sigma(t,y) \Vert &\le K\Vert x-y\Vert,\tag{5.7}\\
\Vert \mu(t,x) \Vert +\Vert \sigma(t,x) \Vert&\le K(1+\Vert x\Vert).\tag{5.8}
\end{align*}
\emph{Then there exists a unique solution to the SDE above. Furthermore, the solution has the properties}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{\(X\) is \(\mathcal{F}_t^W\)-adapted.}
\item
  \emph{\(X\) has continuous trajectories.}
\item
  \emph{\(X\) is a Markov process.}
\item
  \emph{There exists a constant \(C\) such that}
  \[
    E[\Vert X_t\Vert^2]\le Ce^{Ct}(1+\Vert x_0\Vert^2).\tag{5.9}
    \]
\end{enumerate}

In genereal the solution to an SDE is so complicated, that it in practical terms is unsolvable and may only be approximated on a finely subdividet grid as jumps. There does however exist som nontrivial cases where we may infer a analytical solution. One is the rather important \textbf{Geometric Brownian motion}\index{Geometric Brownian motion}.

\textbf{Proposition 5.2. (Bjork)} \emph{Consider the SDE}

\[
dX_t=\alpha X_t\ dt+\sigma X_t\ dW_t,\tag{5.13}
\]

\emph{with \(X_0=x_0\). Then the solution is given as}

\[
X_t=x_0\cdot \exp\left\{\left(\alpha- \frac{\sigma^2}{2}\right)t+\sigma W_t\right\}.\tag{5.15}
\]

\emph{The expected value of \(X\) is given as \(E[X_t]=x_0e^{\alpha t}\) (eq. 5.16).}

One other generalisation that is analytically solvable is the Linear SDE\index{Linear SDE}.

\textbf{Proposition 5.3. (Bjork)} \emph{Consider the SDE}

\[
dX_t=(A X_t + b_t)\ dt+ \sigma_t\ dW_t,\tag{5.19}
\]

\emph{with \(X_0=x_0\) and \(A\in M(n,n)\) and \(b_t\) being a real-valued function. Then the solution is given as}

\[
X_t=e^{At}x_0+\int_0^te^{A(t-s)}b_s\ ds+\int_0^te^{A(t-s)}\sigma_s\ dW_s.\tag{5.20}
\]

\emph{Where we define the exponential of a matrix as below}

\[
e^{At}=\sum_{k=0}^\infty A^k\frac{1}{k!}t^k.
\]

In general with the SDE we have a partial differential operator \(\mathcal{A}\) called the \textbf{infinitesimal operator}\index{infinitesimal operator} of \(X\) which has some interesting analytical properties regarding \(X\).

\textbf{Definition 5.4. (Bjork)} \emph{Consider the SDE}

\[
dX_t=\mu(t,X_t)\ dt+\sigma(t,X_t)\ dW_t.\tag{5.21}
\]

\emph{The partial differential operator \(\mathcal{A}\) is defined, for any function \(h\in C^2(\mathbb{R}^n)\), by}

\[
\mathcal{A}h(t,x)=\sum_{i=1}^n\mu_i(t,x)\frac{\partial h}{\partial x_i}(x) + \frac{1}{2}\sum_{i,j=1}^n (\sigma(t,x)\sigma(t,x)^\top)_{ij}\frac{\partial^2h}{\partial x_i\partial x_j}(x).
\]

We see that in terms of Ito's formula the operator is included as such

\[
df(t,X_t)=\left\{\frac{\partial f}{\partial t}(t,X_t)+\mathcal{A}f(t,x)\right\}\ dt+[\nabla_xf](t,X_t)\sigma(t,X_t)\ dW_t,
\]

where \(\nabla_x\) is the gradient for function \(h\in C^1(\mathbb{R}^n)\) as

\[
\nabla_xh(x)=\left[\frac{\partial h}{\partial x_1}(x),...,\frac{\partial h}{\partial x_n}(x)\right].
\]

\hypertarget{partial-differential-equations}{%
\subsection{Partial differential equations}\label{partial-differential-equations}}

\textbf{Proposition 5.5. (Bjork)} \textbf{(Feynmann-Kac)}\index{Feynmann-Kac} \emph{Assume that \(F\) is a solution to the boundary value problem}

\[
\frac{\partial F}{\partial t}(t,x)+\mu(t,x)\frac{\partial F}{\partial x}(x,t)+\frac{1}{2}\sigma^2(t,x)\frac{\partial^2 F}{\partial x^2}(t,x)=0,
\]

\emph{with boundary condition \(F(T,x)=\Phi(x)\). Assume furthermore that the process}

\[
\sigma(s,X_s)\frac{\partial F}{\partial x}(s,X_s) \in \mathcal{L}^2
\]

\emph{as per definition 4.4, where \(X\) is defined below. Then \(F\) has the representation}

\[
F(t,x)=E_{t,x}[\Phi(X_T)]=E[\Phi(X_T)\ \vert\ X_t=x],\tag{5.29}
\]

\emph{where \(X\) satisfies the SDE}

\[
dX_s=\mu(s,X_s)\ ds+\sigma(s,X_s)\ dW_s,\tag{5.30}
\]

\emph{with boundary condition \(X_t=x\).}

\textbf{Proposition 5.6. (Bjork)} \textbf{(Feynmann-Kac)} \emph{Assume that \(F\) is a solution to the boundary value problem}

\[
\frac{\partial F}{\partial t}(t,x)+\mu(t,x)\frac{\partial F}{\partial x}(x,t)+\frac{1}{2}\sigma^2(t,x)\frac{\partial^2 F}{\partial x^2}(t,x)-rF(t,x)=0,\tag{5.34}
\]

\emph{with boundary condition \(F(T,x)=\Phi(x)\). Assume furthermore that the process}

\[
e^{-rs}\sigma(s,X_s)\frac{\partial F}{\partial x}(s,X_s) \in \mathcal{L}^2
\]

\emph{as per definition 4.4, where \(X\) is defined below. Then \(F\) has the representation}

\[
F(t,x)=e^{-r(T-t)}E_{t,x}[\Phi(X_T)]=e^{-r(T-t)}E[\Phi(X_T)\ \vert\ X_t=x],\tag{5.36}
\]

\emph{where \(X\) satisfies the SDE}

\[
dX_s=\mu(s,X_s)\ ds+\sigma(s,X_s)\ dW_s,\tag{5.37}
\]

\emph{with boundary condition \(X_t=x\).}

\textbf{Proposition 5.8. (Bjork)} \textbf{(Feynmann-Kac)} \emph{Assume that \(F\) is a solution to the boundary value problem}

\[
\frac{\partial F}{\partial t}(t,x)+\sum_{i=1}^n\mu_i(t,x)\frac{\partial F}{\partial x}(x,t)+\frac{1}{2}\sum_{i,j=1}^n C_{ij}(t,x)\frac{\partial^2 F}{\partial x^2}(t,x)-rF(t,x)=0,
\]

\emph{with boundary condition \(F(T,x)=\Phi(x)\) and \(C_{ij}=\sigma \sigma^\top\). Assume furthermore that the process}

\[
e^{-rs}\sum_{i=1}^n\sigma_i(s,X_s)\frac{\partial F}{\partial x}(s,X_s) \in \mathcal{L}^2
\]

\emph{as per definition 4.4, where \(X\) is defined below. Then \(F\) has the representation}

\[
F(t,x)=e^{-r(T-t)}E_{t,x}[\Phi(X_T)],\tag{5.39}
\]

\emph{where \(X\) satisfies the SDE}

\[
dX_s=\mu(s,X_s)\ ds+\sigma(s,X_s)\ dW_s,\tag{5.40}
\]

\emph{with boundary condition \(X_t=x\).}

\textbf{Proposition 5.9. (Bjork)} \emph{Consider as given a vector process \(X\) with generator \(\mathcal{A}\), and a function \(F(t,x)\). Then, modulo some integrability condition, the following hold:}

\begin{itemize}
\tightlist
\item
  \emph{The process \(F(t,X_t)\) is a martingale relative to the filtration \(\mathcal{F}^X\) if and only if \(F\) satisfies the PDE}
  \[
    \frac{\partial F}{\partial t}+\mathcal{A}F=0.
    \]
\item
  \emph{The process \(F(t,X_t)\) is a martingale relative to the filtration \(\mathcal{F}^X\) if and only if, for every \((t,x)\) and \(T\ge t\), we have}
  \[
    F(t,x)=E_{t,x}[F(T,X_T)].
    \]
\end{itemize}

\hypertarget{the-product-integral}{%
\subsection{The Product Integral}\label{the-product-integral}}

Consider the (stochastic) function \(Y : \mathbb{R} \to \mathbb{R}^{n\times n}\), where \(\mathbb{R}^{n\times n}\) is the set of all \(n\times n\) real-valued matrices, that is \(Y\) has the matrix-representation

\[
Y(t)=\begin{bmatrix}
Y_{11}(t) & \cdots & Y_{1n}(t)\\
Y_{21}(t) & \cdots & Y_{2n}(t)\\
\vdots & \ddots & \vdots \\
Y_{n1}(t) & \cdots & Y_{nn}(t)
\end{bmatrix}.
\]

Assume furthermore that for each coordinate function \(Y_{ij}(t)\) the equation

\[
\frac{dY_{ij}}{dt}(t)=Y_{i1}(t)A_{1j}(t)+\cdots+Y_{in}(t)A_{nj}(t),\hspace{15pt}Y_{ij}(s)=C_{ij},
\]

is satisfied. That is on matrix form the linear system of differential equation

\[
\frac{dY}{dt}(t)=Y(t)A(t),\hspace{15pt}Y(s)=C,
\]

for some function \(A : \mathbb{R}\to\mathbb{R}^{n\times n}\) and initial condition \(C\in \mathbb{R}^{n\times n}\). If \(A\) is continuous we know that \(Y\) is well-defined and absolutely continuous. We may then state the following theorem regarding uniqueness and existence of such a function above.

\textbf{Theorem 1.1. (Bladt)} \textbf{(Uniqueness)} \emph{Consider the homogeneous system of linear differential equations}

\[
\mathbf{Y}'(t)=\mathbf{Y}(t)\mathbf{A}(t),\hspace{15pt} \mathbf{Y}(s)=\mathbf{C},\tag{1}
\]

\emph{where \(\mathbf{Y}(t)\), \(\mathbf{A}(t)\) and \(\mathbf{C}\) are \(n\times n\)-matrices and \(\mathbf{A}(t)\) is continuous on \([s,t]\). Then (1) has at most one solution.}

\textbf{Theorem 1.2. (Bladt)} \textbf{(Existance)} \emph{The matrix function}

\[
\mathbf{Y}(t)=\sum_{k=0}^\infty \mathbf{Y}_k(t),\tag{3}
\]

\emph{converges uniformly and absolutely on finite intervals, and solves the differential equation}

\[
\mathbf{Y}'(t)=\mathbf{Y}(t)\mathbf{A}(t),\hspace{15pt} \mathbf{Y}(s)=\mathbf{C}.
\]

Now, we know that for any system as in (1) with a continuous function \(\mathbf{A}(t)\) we can always construct the solution as some converging series as per theorem 1.2 then theorem 1.1 gives that the solution is unique. We can then with a piece of mind define a symbol for such a solution, without care for the \emph{exact} solution.

\textbf{Definition 1.3. (Bladt)} \textbf{(The Product Integral)} \emph{For any continuous matrix function \(\mathbf{A}(t)\) we define the product integral as}

\[
\prod_{s}^t(\mathbf{I}+\mathbf{A}(x)\ dx)
\]

\emph{as the unique solution \(\mathbf{Y}(t)\) to}

\[
\mathbf{Y}'(t)=\mathbf{Y}(t)\mathbf{A}(t),\hspace{15pt} \mathbf{Y}(s)=\mathbf{I}.
\]

From a simple integral argument we may construct \emph{a} converging series not containing \(\mathbf{Y}\) itself. We see that
\begin{align*}
\prod_{s}^t(\mathbf{I}+\mathbf{A}(x)\ dx)&=\mathbf{I}+\int_s^t\mathbf{Y}'(x)\ dx=\mathbf{I}+\int_s^t\mathbf{Y}(x)\mathbf{A}(x)\ dx\\
&=\mathbf{I}+\int_s^t\left[ \mathbf{I}+\int_s^{x_1}\mathbf{Y}'(x_2)\ dx_2\right]\mathbf{A}(x_1)\ dx_1\\
&=\mathbf{I}+\int_s^t\left[ \mathbf{I}+\int_s^{x_1}\mathbf{Y}(x_2)\mathbf{A}(x_2)\ dx_2\right]\mathbf{A}(x_1)\ dx_1\\
&=\mathbf{I}+\int_s^t\mathbf{A}(x_1)\ dx_1+\int_s^t\int_s^{x_1}\mathbf{Y}(x_2)\mathbf{A}(x_2)\mathbf{A}(x_1)\ dx_2\ dx_1.
\end{align*}
One can continue indefinitely and see that we have the following representation.

\textbf{Corollary 1.4. (Bladt)} \textbf{(Peano Representation)} \emph{The prduct integral has series representation given by}
\begin{align*}
\prod_{s}^t(\mathbf{I}+\mathbf{A}(x)\ dx)&=\mathbf{I}+\sum_{i=1}^\infty\int_s^t\int_s^{x_1}\cdots\int_s^{x_n}A(x_n)\cdots\mathbf{A}(x_2)\mathbf{A}(x_1)\ dx_n\cdots dx_2\ dx_1\\
&=\mathbf{I}+\int_s^t\mathbf{A}(x_1)\ dx_1+\sum_{i=2}^\infty\int_s^t\int_s^{x_1}\cdots\int_s^{x_n}A(x_n)\cdots\mathbf{A}(x_2)\mathbf{A}(x_1)\ dx_n\cdots dx_2\ dx_1.
\end{align*}

\hypertarget{linear-algebra}{%
\chapter{Linear Algebra}\label{linear-algebra}}

\hypertarget{invertible-matrices}{%
\section{Invertible matrices}\label{invertible-matrices}}

This sections study some fundamental properties of the \emph{invertible matrix}\index{invertible matrix}. We start by defining what an invertible matrix is.

\textbf{Definition.} \emph{Let \(A\) be an \(n\times m\) matrix and \(B\) be an \(m\times n\) matrix. We say that \(A\) is invertible if}

\[
AB=I_{\min(m,n)}.
\]

\emph{In general, we only consider square matrices. If the above holds we say that \(B\) is \(A\)'s inverse and we write \(B=A^{-1}\).}

We can consider some equivalence statements regarding invertible matrices.

\textbf{Theorem.} \textbf{(The Invertible Matrix Theorem)} \emph{Let \(A\) be a \(n\times n\) matrix over a field \(K\) (\(\mathbb{R}^n\)), then the following statements are equivalent}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{There exists an \(n\times n\) matrix \(B\) such that \(AB=I_n=BA\).}
\item
  \emph{There exist either a left inverse \(B\) or a right invers \(C\) i.e.~\(BA=I_n=AC\). In this case, \(B=C\).}
\item
  \emph{\(A\) has an inverse and is nonsingular and is nondegenerate.}
\item
  \emph{\(A\) is row-equivalent to \(I_n\).}
\item
  \emph{\(A\) is column-equivalent to \(I_n\).}
\item
  \emph{\(A\) has \(n\) pivot positions.}
\item
  \emph{\(A\) has full rank i.e.~\(\text{rank}(A)=n\) (spans \(K\)).}
\item
  \emph{The equation \(Ax=0\) (\(x\in K\)) has only the trivial solution \(x=0\).}
\item
  \emph{The equation \(Ax=b\) has only one solution \(x\).}
\item
  \emph{The kernal of \(A\) is trivial i.e.~\(\text{ker}(A)=\{0\}\).}
\item
  \emph{The columns of \(A\) are linearly independent.}
\item
  \emph{The columns of \(A\) span \(K\).}
\item
  \emph{\(\text{span}(A)=K\).}
\item
  \emph{The columns of \(A\) form a basis of \(K\).}
\item
  \emph{The linear transformation \(Ax\) is a bijection from \(K\) to \(K\).}
\item
  \emph{\(A\) has non-zero determinant i.e.~\(\text{det}(A)\ne 0\).}
\item
  \emph{\(A\) has not 0 as an eigenvalue.}
\item
  \emph{The transpose of \(A\) is invertible.}
\item
  \emph{\(A\) can be expressed as a finite product of elemtary matrices.}
\end{enumerate}

We futhermore have some properties.

\textbf{Proposition.} \textbf{(Properties)} \emph{Let \(A\) be an \(n\times n\) invertible matrix. Then}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \((A^{-1})^{-1}=A\)
\item
  \((kA)^{-1}=k^{-1}A^{-1}\) \emph{with \(k\ne 0\).}
\item
  \((Ax)^+=x^+A^{-1}\) \emph{if \(A\) has orthonormal columns. \((\cdot)^+\) denotes the Moore-Penrose inverse and \(x\) is a vector.}
\item
  \emph{If \(B\) is an \(n\times n\) invertible matrix then \((AB)^{-1}=B^{-1}A^{-1}\).}
\item
  \(\text{det}(A^{-1})=(\text{det}(A))^{-1}\)
\end{enumerate}

The property 2 is especially useful in some settings. Consider for instance

\[
A=
\begin{bmatrix}
\sigma & 0\\
\sigma & \sigma
\end{bmatrix}=\sigma\begin{bmatrix}
1 & 0\\
1 & 1
\end{bmatrix}=\sigma \tilde{A}.
\]

Then we simply find the inverse of \(\tilde{A}\) and multiply by \(\sigma^{-1}\). That is,

\[
A^{-1}=\frac{1}{\sigma}\tilde{A}^{-1}=\frac{1}{\sigma}
\begin{bmatrix}
1 & 0\\
-1 & 1
\end{bmatrix}=
\begin{bmatrix}
1/\sigma & 0\\
-1/\sigma & 1/\sigma
\end{bmatrix}.
\]

We have an easy propositions regarding diagonal matrices.

\textbf{Proposition.} \emph{If \(A\) is an diagonal matrix\index{diagonal matrix}, then \(A\) is invertible. In particular,}

\[
A^{-1}=\text{diag}(A_{11}^{-1},...,A_{nn}^{-1}).
\]

\newpage

\printindex

  \bibliography{book.bib,packages.bib}

\end{document}
