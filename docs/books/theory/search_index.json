[["index.html", "Complete Theory Msc in Actuarial Mathematics Chapter 1 Introduction ", " Complete Theory Msc in Actuarial Mathematics Joakim Bilyk April 07, 2023 Abstract This document contain a comprehensive outline of theory on probability theory and mathematical statistics applied in finance, life insurance and non-life insurance. Chapter 1 Introduction "],["abbreviations.html", "1.1 Abbreviations", " 1.1 Abbreviations Below is given the abbreviations used when referencing to books: Chapter Abbreviation Source Basic Life Insurance Mathematics Stochastic Processes in Life Insurance Mathematics Topics in Life Insurance Mathematics Asmussen Risk and Insurance: A Graduate Text by Soren Asmussen and Mogens Steffensen (2020). Bladt Notes from lectures in Liv2. Continuous Time Finance Bjork Arbitrage Theory in Continuous Time (Fourth edition) by Thomas Bjork, Oxford University Press (2019). Basic Non-Life Insurance Mathematics Stochastic Processes in Life Insurance Mathematics Topics in Non-Life Insurance Mathematics Probabilistic Machine Learning None Slides from lectures. Quantative Risk Management Measure Theory Bjork Arbitrage Theory in Continuous Time (Fourth edition) by Thomas Bjork, Oxford University Press (2019). Protter Probability Essentials (2. edition) by Jean Jacod and Philip Protter (2004). Random Variables Bjork Arbitrage Theory in Continuous Time (Fourth edition) by Thomas Bjork, Oxford University Press (2019). Hansen Stochastic Processes (2. edition) by Ernst Hansen (2021). Discrete Time Stochastic Processes Hansen Stochastic Processes (2. edition) by Ernst Hansen (2021). Continuous Time Stochastic Processes Bjork Arbitrage Theory in Continuous Time (Fourth edition) by Thomas Bjork, Oxford University Press (2019). Stochastic Calculus Bjork Arbitrage Theory in Continuous Time (Fourth edition) by Thomas Bjork, Oxford University Press (2019). Bladt Notes from lectures in Liv2. Linear Algebra Wiki Wikipedia "],["to-do-reading.html", "1.2 To-do reading", " 1.2 To-do reading Chapter Note Progress Liv2 Matrix representation 4 and later Chapter 6 Remember handouts Chapter 7 Remember handouts ML Make outlines for exam QRM Finish risk measures StokLiv Week 1 StokLiv Week 2 SkadeStok Week 1 SkadeStok Week 2 Liv1 Week 1 Liv1 Week 2 Nonlife1 Week 1 Nonlife1 Week 2 "],["basic-life-insurance-mathematics.html", "Chapter 2 Basic Life Insurance Mathematics", " Chapter 2 Basic Life Insurance Mathematics Noget indhold "],["stochastic-processes-in-life-insurance-mathematics.html", "Chapter 3 Stochastic Processes in Life Insurance Mathematics", " Chapter 3 Stochastic Processes in Life Insurance Mathematics Noget indhold "],["topics-in-life-insurance-mathematics.html", "Chapter 4 Topics in Life Insurance Mathematics ", " Chapter 4 Topics in Life Insurance Mathematics "],["markov-jump-processes.html", "4.1 Markov Jump Processes", " 4.1 Markov Jump Processes Life insurance mathematics revolves arrow Markov processes in continuous time on some discrete (or at least countable) state space. This may in the simplest model be the life-death model or it can be a generalized life-death model with interest levels. In any case, we use Markov processes to determine the possible payments that may occur in the future or have occurred in the past. Let us start by define the Markov jump process as follows. Definition 2.1. (Bladt) A continuous-time stochastic process \\(\\{X_t\\}_{t\\ge 0}\\) taking values in a countable space \\(E\\) is called a Markov jump process with state space \\(E\\) if for all \\(t_n&gt;t_{n-1}&gt;\\cdots &gt; t_1&gt;0\\) and \\(i_n,i_{n-1},...,i_0\\in E\\) it holds that \\[ P(X_{t_n}=i_n \\ \\vert\\ X_{t_{n-1}}=i_{n-1},...,X_{t_1}=i_1,X_0=i_0)=P(X_{n-1}=i_n\\ \\vert\\ X_{n-1}=i_{n-1}). \\] It is seen that process have some property of “forgetability” in the sense that the jump to a fixed state at some future time only depends on the current state, not the sample path of \\(X\\). We may then define the transition probabilities as \\[ p_{ij}(s,t)=P(X_t=j\\ \\vert\\ X_s=i), \\] being the probability of the process being in state \\(j\\) at time \\(t\\) given that the process is in state \\(i\\) at time \\(s\\). This gives the natural transition matrix \\[ \\mathbf{P}(t,s)=\\big\\{p_{ij}(s,t)\\big\\}_{i,j\\in E},\\hspace{10pt} s\\le t. \\] In general, we study the time in-homogeneous models where the above is possibly different for any pair \\(s\\le t\\). There does however exist the special case, where the probabilities only depends on the time \\(h=t-s\\). We call these time homogeneous Markov jump processes and they have the form \\[ p_{ij}(t,s)=p_{ij}(t-s). \\] I.e. the probability of jumping from \\(i\\) to \\(j\\) is the same so long the interval is the same length. Let us now define the transition intensities as below. Definition 2.2. (Bladt) Assume that the limit \\[ \\mathbf{M}(s)=\\big\\{\\mu_{ij}(s)\\big\\}_{i,j\\in E}=\\lim_{h\\to 0_+}\\frac{\\mathbf{P}(s,s+h)+\\mathbf{I}}{j} \\] exist for all \\(s\\ge 0\\). The matrices \\(\\mathbf{M}(s)\\) are called intensity matrices. The the special case of time-homogenous processes we have \\(\\mathbf{M}(s)=\\mathbf{M}\\) is a constant matrix. We may interpret the intensity matrices in the infinitesimal interpretation as \\[ \\mathbf{P}(s,s+h) = \\mathbf{I}+h\\mathbf{M}(s)+o(h), \\] i.e. \\[ p_{ij}(s,s+h)=\\delta_{ij}+\\mu_{ij}(s)h+o(h), \\] where \\(\\delta_{ij}=1_{i=j}\\) is the Kronecker delta. We furthermore see, that the matrix \\(\\mathbf{M}(s)\\) has non-positive diagonal and non-negative upper and lower triangle. Furthermore, the matrix has zero row sums and so it follows that \\[ \\mu_{ii}(s)=-\\sum_{j\\ne i}\\mu_{ij}(s):=-\\mu_{i}(t), \\] for all \\(i\\in E\\). Returning to the infinitesimal interpretation we may write the dynamics of \\(\\mu_{ij}\\) in the following manner \\[ d\\mu_{ij}(t)=\\mu_{ij}(t)\\ dt=P(X_{t+dt}=j\\ \\vert\\ X_t=i) \\] i.e. the probability of a jump from state \\(i\\) to state \\(j\\) during \\([t,t+dt)\\). On the other hand we have \\[ 1-\\mu_{ii}(t)\\ dt=P(X_{t+dt}=i\\ \\vert\\ X_t=i), \\] i.e. the probability that no jumps occur during \\([t,t+dt)\\). We can then define the conditional probability that a jump, if one occurs, is to state \\(j\\) from \\(i\\) as \\[\\begin{align*} q_{ij}(t)&amp;=P(X_{t+dt}=j\\ \\vert\\ X_t=i,X_{t+dt}\\ne i)=\\frac{P(X_{t+dt}=j\\ \\vert\\ X_t=i)}{P(X_{t+dt}\\ne i\\ \\vert\\ X_t=i)}\\\\ &amp;=\\frac{\\mu_{ij}(t)}{\\mu_i(t)}=-\\frac{\\mu_{ij}(t)}{\\mu_{ii}(t)}. \\end{align*}\\] We can now look at the time until the next jump as the random variable \\[ T(s)=\\inf\\left\\{u\\ge 0\\ :\\ X_{s+u}\\ne X_s\\right\\}, \\] i.e. \\(T(s)\\) is a positive random variable. If we condition on \\(X_s=i\\) then \\(T(s)\\ \\vert\\ X_s=i\\) is the time until \\(X\\) jumps out of state \\(i\\) into some other state \\(j\\ne i\\). If we set \\(S_i(t)=P(T(s)&gt;t\\ \\vert\\ X_s=i)\\) and \\(f_i(t)\\) is the density of \\(T(s)\\ \\vert\\ X_s=i\\), then we have \\[ \\mu_i(u)=\\frac{f_i(u)}{S_i(u)}=-\\frac{S&#39;_i(u)}{S_i(u)}=-\\frac{d}{du}\\log(S_i(u)) \\] which is solved for \\[ S_i(t)=P(T(s)&gt;t\\ \\vert\\ X_s=i)=\\exp\\left(-\\int_s^t \\mu_i(u)\\ du\\right) \\] and then \\[ f_i(t)=\\exp\\left(-\\int_s^t \\mu_i(u)\\ du\\right)\\mu_i(t). \\] Theorem 2.3. (Bladt) (Kolmogorov’ Differential Equations) Relation between \\(\\mathbf{M}(s)\\) and \\(\\mathbf{P}(s,t)\\) are given by Kolmogorov’ forward and backward differential equations: \\[ \\frac{\\partial}{\\partial s}\\mathbf{P}(s,t)=-\\mathbf{M}(s)\\mathbf{P}(s,t) \\] and \\[ \\frac{\\partial}{\\partial t}\\mathbf{P}(s,t)=\\mathbf{P}(s,t)\\mathbf{M}(t). \\] the soluction of which is given by \\[ \\mathbf{P}(s,t)=\\prod_s^t(\\mathbf{I}+\\mathbf{M}(x)\\ dx). \\] In the time-homogeneous case we have that \\[ \\frac{d}{d t}\\mathbf{P}(t)=\\mathbf{M}\\mathbf{P}(t)=\\mathbf{P}(t)\\mathbf{M} \\] with solution \\[ \\mathbf{P}(t)=\\exp(\\mathbf{M}t). \\] Proof. We may subdivide the interval \\([s,t]\\) into \\([s,u]\\) and \\([u,t]\\) and use theorem 1.5 from the chapter on Product Integrals to obtain the Chapman-Kolmogorov’s equation \\[ \\mathbf{P}(s,t)=\\mathbf{P}(s,u)\\mathbf{P}(u,t) \\] and in the time homogeneous case we have \\[ \\mathbf{P}(s+t)=\\mathbf{P}(s)\\mathbf{P}(t). \\] Finally, let us define a stopping time and introduce the strong markov property. Definition 2.4. (Bladt) (Stopping time) Let \\(X\\) be a Markov jump process and let \\(\\mathcal{F}_t=\\sigma(X_s\\ :\\ s\\le t)\\). A nonnegative random variable \\(\\tau\\) is called a stopping time for \\(X\\) of \\(\\{\\tau \\le t\\}\\in\\mathcal{F}_t\\) for all \\(t\\).\\ The \\(\\sigma\\)-algebra of the process up to a stopping time \\(\\tau\\), \\(\\mathcal{F}_\\tau\\), is defined by the measurable sets \\(A\\) for which \\[ \\forall t\\ge 0 : A\\cap \\{\\tau\\le t\\}\\in\\mathcal{F}_t. \\] We see that a stopping time in an intuitive sense is a random variable which at any time \\(t\\) we know either that the stopping time is in the future i.e. \\(\\tau &gt; t\\) or that the stopping time already occured and so \\(\\tau\\) becomes known at time \\(t\\). Furthermore, we see that \\(\\{\\tau \\le t\\}\\) is \\(\\mathcal{F}_t\\) measurable and so we only need information from the Markov jump process in order to determine the above. Theorem 2.5. (Bladt) (Strong Markov property) Every Markov jump process satisfies the strong Markov property, i.e., for all \\(0\\le h_1\\le h_2\\le \\cdots \\le h_n\\), we have that \\[ P(X_{\\tau +h_1}=i_1,...,X_{\\tau + h_n}=i_n\\ \\vert\\ \\mathcal{F}_t)=P(X_{\\tau +h_1}=i_1,...,X_{\\tau + h_n}=i_n\\ \\vert\\ X_\\tau) \\] on \\(\\{\\tau &lt;\\infty\\}\\). For time-homogeneous processes this further reduces to \\[ P(X_{\\tau +h_1}=i_1,...,X_{\\tau + h_n}=i_n\\ \\vert\\ \\mathcal{F}_t)=P_{X_\\tau}(X_{\\tau +h_1}=i_1,...,X_{\\tau + h_n}=i_n) \\] also on \\(\\{\\tau &lt;\\infty\\}\\). "],["phase-type-distributions.html", "4.2 Phase-type distributions", " 4.2 Phase-type distributions In life insurance models we often, that is always, have a model with one absorbing state, namely, death. This means that the state space \\(E\\) will have the form of \\(p\\ge 1\\) transient states (states that may interact in both ways) and one absorbing state (a state that is never moved away from). In this case we would often like to study the distribution of \\[ \\inf\\{t\\ge 0 : X(t)=p+1\\}, \\] where \\(p+1\\) is the absorbing state and \\(1,...,p\\) are the transient states. We will assume that \\(P(X(0)=p+1)=0\\) i.e. the above is at least zero and never \\(-\\infty\\). Let us now formalise this setup. Consider a time-inhomogeneous Markov jump process \\(\\{X_t\\}_{t\\ge 0}\\) on the finite state-space \\(E=\\{1,...,p,p+1\\}\\) where the states \\(1,...,p\\) are transient states and \\(p+1\\) is the only absorbing state. This implies that the intesity matrix of \\(\\{X_t\\}_{t\\ge 0}\\) take the form \\[ \\Lambda(t)=\\begin{bmatrix} \\mathbf{T}(t) &amp; \\mathbf{t}(t)\\\\ \\mathbf{0} &amp; 0 \\end{bmatrix}. \\] In the above \\(\\mathbf{T}(t)\\) is a \\(p\\times p\\) matrix function and \\(\\mathbf{t}(t)\\) is a \\(p\\times 1\\) matrix function. We now define the initial distribution of \\(X_0\\) as \\[ \\{P(X_0=i)\\}_{i\\in E\\setminus \\{p+1\\}}=\\mathbf{\\pi}=\\begin{pmatrix}\\pi_1\\\\ \\vdots\\\\ \\pi_p\\end{pmatrix}^\\top. \\] By assumption we have \\(P(X_0=p+1)=0\\) and so \\(\\sum_{i=1}^p\\pi_i=1\\) and \\(\\mathbf{\\pi}\\) is a proper distribution. Definition 2.10. (Bladt) (Phase-Type distribution) Let \\[ \\tau = \\inf\\{t\\ge 0 : X(t)=p+1\\} \\] denote the time until absorption of \\(X\\). The distribution of \\(\\tau\\) is then said to be an inhomogeneous phase-type distribution with representation \\((\\mathbf{\\pi},\\mathbf{T}(t))\\) and we write \\(\\tau \\sim IPH(\\mathbf{\\pi},\\mathbf{T}(t))\\). Lemma 2.11. (Bladt) We have the following decomposition: \\[ \\mathbf{P}(s,t)=\\prod_s^t(\\mathbf{I}+\\mathbf{\\Lambda}(u)\\ du)= \\begin{bmatrix} \\prod_s^t(\\mathbf{I}+\\mathbf{T}(u)\\ du) &amp; \\mathbf{e} -\\prod_s^t(\\mathbf{I}+\\mathbf{T}(u)\\ du)\\mathbf{e}\\\\ \\mathbf{0} &amp; 1 \\end{bmatrix}, \\] where \\(\\mathbf{e}=(1,1,...,1)^\\top\\). Theorem 2.12. (Bladt) Assume that \\(\\tau \\sim IPH(\\mathbf{\\pi},\\mathbf{T}(t))\\). Then the density \\(f\\) and the distribution function \\(F\\) of \\(\\tau\\) are given by \\[\\begin{align*} f(x)&amp;=\\mathbf{\\pi}\\prod_0^x(\\mathbf{I}+\\mathbf{T}(u)\\ du)\\mathbf{t}(x),\\\\ F(x)&amp;=1-\\mathbf{\\pi}\\prod_0^x(\\mathbf{I}+\\mathbf{T}(u)\\ du)\\mathbf{e}. \\end{align*}\\] Proof. Theorem 2.13. (Bladt) If \\(\\tau \\sim IPH(\\mathbf{\\pi},\\mathbf{T}(t))\\) then \\[ P(\\tau &gt;s+t\\ \\vert\\ \\tau &gt;s)=\\frac{\\mathbf{\\pi}\\prod_0^s(\\mathbf{I}+\\mathbf{T}(u)\\ du)}{\\mathbf{\\pi}\\prod_0^s(\\mathbf{I}+\\mathbf{T}(u)\\ du)\\mathbf{e}}\\prod_s^t(\\mathbf{I}+\\mathbf{T}(u)\\ du)\\mathbf{e} \\] so that \\[ \\tau-s\\ \\vert\\ \\{\\tau&gt;s\\}\\sim IPH(\\mathbf{\\alpha},\\mathbf{S}(\\cdot)), \\] where \\(\\mathbf{S}(u)=\\mathbf{T}(s+u)\\) and \\[ \\mathbf{\\alpha}=\\frac{\\mathbf{\\pi}\\prod_0^s(\\mathbf{I}+\\mathbf{T}(u)\\ du)}{\\mathbf{\\pi}\\prod_0^s(\\mathbf{I}+\\mathbf{T}(u)\\ du)\\mathbf{e}}. \\] Corollary 2.14. (Bladt) If \\(\\tau\\sim IPH(\\mathbf{\\alpha},\\mathbf{T}(t))\\) and \\(\\mathbf{T}(t_1)\\) and \\(\\mathbf{T}(t_2)\\) commute for all \\(t_1,t_2\\ge 0\\), then the density \\(f\\) and the distribution function \\(F\\) of \\(\\tau\\) are given by \\[\\begin{align*} f(x)&amp;=\\mathbf{\\pi}\\exp\\left(\\int_0^x\\mathbf{T}(u)\\ du\\right)\\mathbf{t}(x),\\\\ F(x)&amp;=1-\\mathbf{\\pi}\\exp\\left(\\int_0^x\\mathbf{T}(u)\\ du\\right)\\mathbf{e}. \\end{align*}\\] Example (Approximation in time-inhomogeneous case). Consider the Markov jump process on the state space \\(E=\\{1,2,3\\}\\) where 1 is the state “alive” (working), 2 is “disabled” and 3 is the state “death”. We assume that \\(\\Lambda(t)\\) has the structure: \\[\\begin{align*} \\Lambda(t)&amp;=\\begin{bmatrix} \\mu_{11}(t) &amp; \\mu_{12}(t) &amp; \\mu_{13}(t)\\\\ \\mu_{21}(t) &amp; \\mu_{22}(t) &amp; \\mu_{23}(t)\\\\ \\mu_{31}(t) &amp; \\mu_{32}(t) &amp; \\mu_{33}(t) \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} -\\mu_{12}(t)-\\mu_{13}(t) &amp; 0.000015 + 10^{(4.6-10+0.015\\cdot t)} &amp; 0.00005 + 10^{(4.6-10+0.05\\cdot t)}\\\\ 0.000005 + 10^{(4.6-10+0.015\\cdot t)} &amp; -\\mu_{21}(t)-\\mu_{23}(t) &amp; 0.0001 + 10^{(4.6-10+0.05\\cdot t)}\\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\end{align*}\\] We can now implement this intensity matrix as a function in R: mu12 &lt;- function(t) { 0.000015 + 10^(4.6-10+0.015*t) } mu13 &lt;- function(t) { 0.00005 + 10^(4.6-10+0.05*t) } mu11 &lt;- function(t) { -mu12(t)-mu13(t) } mu21 &lt;- function(t) { 0.000005 + 10^(4.6-10+0.015*t) } mu23 &lt;- function(t) { 0.0001 + 10^(4.6-10+0.05*t) } mu22 &lt;- function(t) { -mu21(t)-mu23(t) } mu31 &lt;- function(t) {0} mu32 &lt;- function(t) {0} mu33 &lt;- function(t) {0} M &lt;- function(t) { matrix( c(mu11(t),mu21(t),mu31(t), mu12(t),mu22(t),mu32(t), mu13(t),mu23(t),mu33(t)), ncol = 3 ) } M(0) ## [,1] [,2] [,3] ## [1,] -7.296214e-05 1.898107e-05 5.398107e-05 ## [2,] 8.981072e-06 -1.129621e-04 1.039811e-04 ## [3,] 0.000000e+00 0.000000e+00 0.000000e+00 We see that the intensities is choosen such that the following holds for any time interval \\([t,t+dt)\\) The transition \\(1\\to 3\\) is less likely than \\(2\\to 3\\), The transition \\(2\\to 1\\) is less likely than \\(1\\to 2\\), The transition \\(1\\to 2\\) is less likely than \\(1\\to 3\\). We now wish to compute the density and distribution of \\[ \\tau =\\inf\\{t\\ge 0\\ :\\ X(t)=3\\}. \\] We assume that \\(\\pi=(1,0)\\) i.e. the person is alive. Calculating the distribution of \\(\\tau\\) is then the distribution of the length of a newborn child lifespan. From theorem 2.12 this ammount to calculating the product integral of \\[ \\mathbf{T}(t)=\\{\\mu_{ij}(t)\\}_{i,j\\in \\{1,2\\}} \\] i.e.  \\[ \\prod_0^x(\\mathbf{I}+\\mathbf{T}(u)\\ du). \\] To du this we simply approximate with stepsize \\(h=1/n\\) (for some \\(n\\ge 1\\)) by \\[ \\prod_0^{x+h}(\\mathbf{I}+\\mathbf{T}(u)\\ du)=h\\prod_0^x(\\mathbf{I}+\\mathbf{T}(u)\\ du)\\mathbf{T}(x)+\\prod_0^x(\\mathbf{I}+\\mathbf{T}(u)\\ du), \\] and \\[ \\prod_0^0(\\mathbf{I}+\\mathbf{T}(u)\\ du)=\\mathbf{I}. \\] This is done in the code below n &lt;- 12 #monthly h &lt;- 1/n N &lt;- 120 #max number years T_matrix &lt;- function(t) { matrix( c(mu11(t),mu21(t), mu12(t),mu22(t)), ncol = 2 ) } T_matrix(0) ## [,1] [,2] ## [1,] -7.296214e-05 1.898107e-05 ## [2,] 8.981072e-06 -1.129621e-04 t_matrix &lt;- function(t) { matrix( c(mu13(t),mu23(t)), ncol=1 ) } t_matrix(0) ## [,1] ## [1,] 5.398107e-05 ## [2,] 1.039811e-04 library(rlist) #Initial condition t=0 T_product_integral &lt;- list(diag(c(1,1))) pi &lt;- matrix(c(1,0),ncol = 1) f &lt;- t(pi) %*% diag(c(1,1)) %*% t_matrix(0) F &lt;- 1-t(pi) %*% diag(c(1,1)) %*% matrix(c(1,1),ncol = 1) for (i in 1:(N*n)) { x_1 &lt;- i/n x_0 &lt;- (i-1)/n T_0 &lt;- T_matrix(x_0) M_0 &lt;- T_product_integral[[i]] M_1 &lt;- h*M_0 %*% T_0 + M_0 T_product_integral &lt;- list.append(T_product_integral,M_1) f &lt;- c(f,t(pi) %*% M_1 %*% t_matrix(x_1)) F &lt;- c(F,1-t(pi) %*% M_1 %*% matrix(c(1,1),ncol = 1)) } We see that by calculating \\[ E[\\tau\\ \\vert\\ X_0=1]=\\int_0^\\infty \\tau f(\\tau)\\ d\\tau\\approx\\int_0^{120}\\tau f(\\tau)\\ d\\tau\\approx\\frac{1}{n}\\sum_{i=0}^{120\\cdot n} i/n\\cdot f(i/n). \\] that the life expectation is 84.01 years. We have that we may approximate \\(f\\) and \\(F\\) with an arbitrary precision by choosing \\(n\\) appropriately. \\(\\square\\) I practice we may have alot complications in calculating the product integral of \\(\\mathbf T\\) as the matrix may have large dimensions, be non-cummative, encounter alternating sums with growing terms and obviously being time in-homogeneous. So one has to be smart when constructing numerical methods in computing the integral. We will briefly consider some considerations an implementer may use in calculating the product integral. Applying differential equation. Assume that \\(X\\) is a time in-homogeneous markov jump process then we may always calculate \\(\\prod_s^t(\\mathbf I + \\mathbf T(x)\\ dx)\\) using a stepwise argument: \\[ \\prod_s^t(\\mathbf I + \\mathbf T(x)\\ dx)=\\prod_s^{s+1\\cdot (t-s)/n}(\\mathbf I + \\mathbf T(x)\\ dx)\\prod_s^{s+2\\cdot (t-s)/n}(\\mathbf I + \\mathbf T(x)\\ dx)\\cdots\\prod_s^{t}(\\mathbf I + \\mathbf T(x)\\ dx) \\] for some \\(n\\ge 1\\). In particular one can simply choose \\(n\\) large enough such that the increments is approximately linear and so this is a brute force methods (see example above). Piece wise constant matrix. Since data is scarce it often occur that mortality rates are constant over at least a monthly timeline hence one may assume that \\(\\Lambda\\) is piecewise constant for some fine grid. This in particular means that if the grid has size \\(1/n\\) (for instance \\(n=12\\) or \\(n=4\\)) we have \\[ \\prod_s^{s+1/n}(\\mathbf I + \\mathbf T(x)\\ dx)=\\mathbf{I}e^{\\mathbf{T}(s)\\frac{1}{n}}=e^{\\mathbf{T}(s)\\frac{1}{n}}. \\] and so the above reduces to \\[ \\prod_s^t(\\mathbf I + \\mathbf T(x)\\ dx)=e^{\\mathbf{T}(s)\\frac{1}{n}}e^{\\mathbf{T}(s+1/n)\\frac{1}{n}}\\cdots e^{\\mathbf{T}(t-1/n)\\frac{1}{n}}, \\] assuming that \\(s\\) and \\(t\\) are integers (one could make this more general). Diagonalization. Assume that \\(\\mathbf{T}(x)\\) is constant on some interval \\([s,t]\\) and that for the constant matrix \\(\\mathbf{T}:=\\mathbf{T}(s)\\) there exist unique eigenvalues \\(\\lambda_1,...,\\lambda_p\\). Then we can diagonalize \\(\\mathbf{T}\\) as \\[ \\mathbf{T}=\\mathbf{B}\\mathbf{D}\\mathbf{B}^{-1}, \\] where as usual \\(\\mathbf{D}=\\text{diag}(\\lambda_1,...,\\lambda_p)\\) and \\(\\mathbf{B}\\) is a \\(p\\times p\\) matrix with columns of eigenvectors for \\(\\lambda_1,...,\\lambda_p\\). In this case we can calculate \\[\\begin{align*} \\prod_s^t(\\mathbf I + \\mathbf T(x)\\ dx)&amp;=e^{\\mathbf{T}(t-s)}=e^{\\mathbf{B}\\mathbf{D}\\mathbf{B}^{-1}(t-s)}=\\sum_{n=0}^\\infty \\frac{\\left(\\mathbf{B}\\mathbf{D}\\mathbf{B}^{-1}\\right)^n(t-s)^n}{n!}\\\\ &amp;=\\sum_{n=0}^\\infty \\frac{\\mathbf{B}\\mathbf{D}\\mathbf{B}^{-1}\\mathbf{B}\\mathbf{D}\\mathbf{B}^{-1}\\cdots \\mathbf{B}\\mathbf{D}\\mathbf{B}^{-1}\\mathbf{B}\\mathbf{D}\\mathbf{B}^{-1}(t-s)^n}{n!}\\\\ &amp;=\\sum_{n=0}^\\infty \\frac{\\mathbf{B}\\mathbf{D}^n\\mathbf{B}^{-1}(t-s)^n}{n!}=\\mathbf{B}\\left(\\sum_{n=0}^\\infty \\frac{\\mathbf{D}^n(t-s)^n}{n!}\\right)\\mathbf{B}^{-1}\\\\ &amp;=\\mathbf{B}\\left(\\sum_{n=0}^\\infty \\frac{\\text{diag}(\\lambda_1^n(t-s)^n,...,\\lambda_p^n(t-s)^n)}{n!}\\right)\\mathbf{B}^{-1}\\\\ &amp;=\\mathbf{B}\\text{diag}\\left(\\sum_{n=0}^\\infty \\frac{\\lambda_1^n(t-s)^n}{n!},...,\\sum_{n=0}^\\infty \\frac{\\lambda_p^n(t-s)^n}{n!}\\right)\\mathbf{B}^{-1}\\\\ &amp;=\\mathbf{B}\\text{diag}\\left(e^{\\lambda_1(t-s)},...,e^{\\lambda_p(t-s)}\\right)\\mathbf{B}^{-1}. \\end{align*}\\] which is much easier than any approximation. This is however a unrealisitic expectations to have. Univerformization. Assume that \\(\\mathbf{T}(x)\\) is constant on some interval \\([s,t]\\) and define \\(\\mathbf{T}:=\\mathbf{T}(s)\\). We may furthermore define \\(\\lambda= \\max_{i}(-\\lambda_{ii})\\) as the largest diagonal entry in \\(\\mathbf T\\) (\\(\\lambda &lt;0\\)). We now set \\[ \\mathbf P = \\mathbf I + \\lambda^{-1}\\mathbf T, \\] and see that \\(\\mathbf P\\) is a transition matrix i.e. rowsums is 1 and the the diagonals \\(0\\le p_{ii}\\le 1\\). We see this as \\(0\\le \\lambda^{-1}\\lambda_{ii}\\le1\\) as \\(\\lambda_{ii}\\le 0\\) for all \\(i\\) and \\(\\lambda\\ge 0\\) and dominates all diagonal entries. The rowsums is 1 as \\(\\mathbf T\\) is a intensity matrix i.e. rowsums is 0 and so adding one to the diagonal of the scaled matrix gives a rowsum of 1. We can now rearrange and see that \\[\\begin{align*} \\prod_s^t(\\mathbf I + \\mathbf T(x)\\ dx)&amp;=e^{\\mathbf{T}(t-s)}=e^{\\lambda (\\mathbf P - \\mathbf I)(t-s)}=e^{\\lambda \\mathbf P (t-s)}e^{-\\lambda \\mathbf I(t-s)}\\\\ &amp;=e^{\\lambda \\mathbf P (t-s)}\\mathbf Ie^{-\\lambda (t-s)}=e^{-\\lambda (t-s)}e^{\\lambda \\mathbf P (t-s)}\\\\ &amp;=e^{-\\lambda (t-s)}\\sum_{n=0}^\\infty \\frac{(t-s)^n}{n!}\\mathbf{P}^n. \\end{align*}\\] This is nice since \\(\\mathbf P\\) has entries in the interval \\([0,1]\\) and so the series above converges and is monotonic, so we avoid the alternating sums from the negative diagonal in \\(\\mathbf T\\). Scaling and squaring argument. Assuming the setup above. If the interval \\((t-s)\\) is large we may apply a scaling and squaring argument by setting \\[ e^{\\mathbf T(t-s)}=\\mathbf{S}^{2^n},\\hspace{15pt}\\mathbf{S}=e^{\\mathbf T (t-s)2^{-n}}. \\] This ensures small entries in the matrix \\(S\\) and we can simply square \\(\\mathbf S\\) \\(n\\) times after approximating \\(\\mathbf S\\) numerically. "],["interest-rates.html", "4.3 Interest rates", " 4.3 Interest rates 4.3.1 Basic definitions and properties In money markets interest is derived from the short term rate \\(r\\) that is a continuously compounded interest rate i.e. a bank account \\(B(t)\\) is on the form \\[ B(t)=B(0)e^{\\int_0^tr(u)\\ du} \\] where we will by convention assume \\(B(0)=1\\). Let us define what we will require for the process \\(r\\). Definition. (Interest rate process) A stochastic process \\(\\{r(t)\\}_{t\\ge 0}\\) is an interest rate process if and only if \\(r(t)\\) is adapted to some filtration \\(\\mathcal F_t\\) on the probability space \\((\\Omega,\\mathcal F,\\mathbb P)\\). This in particular means that in theory we may construct the rate as a diffusion process, piecewise constant function jumping at random times and so forth. The class of possible rate process is large but few is realistic. We will in the following think of \\(r\\) generally as a given process which we only assume is adapted to some filtration \\(\\mathcal F\\). The most important price process other than the bank account is the zero-coupon bond. Definition. (Zero-Coupon Bond) A zero-coupon bond with maturity \\(T\\) with underlying interest rate process \\(r(t)\\) is a security which at time \\(T\\) pays the holder 1 and the price of the claim at time \\(0\\le t\\le T\\), \\(B(t,T)\\), satisfies \\[ \\left\\{\\frac{B(t,T)}{B_t}\\right\\}_{0\\le t\\le T}, \\] is a \\(\\mathbb Q\\)-martingale measure for some equivalent martingale measure \\(\\mathbb Q\\sim \\mathbb P\\) (including \\(\\mathbb Q = \\mathbb P\\)). We can in particular deduce the price process under a \\(\\mathbb Q\\)-expectation by the equation \\[ E^{\\mathbb Q}\\left[\\left.\\frac{1}{B_T}\\right\\vert \\mathcal F_t\\right]=E^{\\mathbb Q}\\left[\\left.\\frac{B(T,T)}{B_T}\\right\\vert \\mathcal F_t\\right]=\\frac{B(t,T)}{B_t}, \\] since \\(B(T,T)=1\\) and the integrant is a \\(\\mathbb Q\\)-martingale. Rearranging we have \\[ B(t,T)=B_tE^{\\mathbb Q}\\left[\\left.\\frac{1}{B_T}\\right\\vert \\mathcal F_t\\right]=e^{\\int_0^t r(u)\\ du}E^{\\mathbb Q}\\left[\\left.e^{-\\int_0^T r(u)\\ du}\\right\\vert \\mathcal F_t\\right]=E^{\\mathbb Q}\\left[\\left.e^{-\\int_t^T r(u)\\ du}\\right\\vert \\mathcal F_t\\right].\\tag{11} \\] 4.3.1.1 Forward rates and yields There exist alot of different transformations of \\(r(t)\\) which may be useful in simplifying complex equation and/or easing interpretation. Some of the most used include forward rates and yields. Let us start by defining a simple forward rate called the LIBOR forward rate. Definition 3.1. (Bladt) Consider three fixed time-points \\(t\\le S&lt; T\\) and the corresponding self-financing portfolio consisting of one short position in a zero-coupon bond with maturity \\(S\\) and a long position in a zero-coupon bond with maturity \\(T\\) of size \\(B(t,S)/B(t,T)\\). This portfolio gives the payout: -1 at time \\(S\\) and \\(B(t,S)/B(t,T)\\) at time \\(T\\). This in particular results in the investment of 1 dollar at time \\(S\\) with a \\(\\mathcal F_t\\) deterministic payout \\[ \\frac{B(t,S)}{B(t,T)} \\] at time \\(T\\). The LIBOR forward rate \\(L(t,S,T)\\) is thus the average return in the interval i.e. \\[ L(t,S,T)=\\frac{1}{T-S}\\left(\\frac{B(t,S)}{B(t,T)}-1\\right). \\] We could of couse define the rate as a continuously compounding rate by solving \\[ e^{(T-S)R}=\\frac{B(t,S)}{B(t,T)}\\iff R=\\frac{1}{T-S}\\left(\\log B(t,S)-\\log B(t,T)\\right). \\] Definition. (Continuously compounded forward rate) Let \\(t\\le T\\) be a fixed maturity date. The continuously compounded forward rate on \\([S,T]\\) is defined by \\[ f(t,S,T)=-\\frac{\\log B(t,T)-\\log B(t,S)}{T-S} \\] being the continuously compounded rate for a portfolio consisting of \\(B(t,S)/B(t,T)\\) zero-coupon bond with maturity \\(T\\). The yield with maturity \\(T\\) is the defined by. Definition. (Yield) Let \\(t\\le T\\) be a fixed maturity date. The yield on \\([t,T]\\) is defined by \\[ Y(t,T)=-\\frac{\\log B(t,T)}{T-t}, \\] being the continuous rate on the portfolio consisting of 1 zero-coupon bond with maturity \\(T\\). We obviously have that \\(Y(t,T)=f(t,t,T)\\) is the continuous compounded forward rate on \\([S,T]\\) with \\(S=t\\). Furthermore, we call \\(\\{Y(t,T)\\}_{T\\ge t}\\) the yield curve. Definition. (Forward rate) Let \\(t\\le T\\) be a fixed maturity date. The forward rate on \\([t,T]\\) is defined by \\[ f(t,T)=-\\frac{\\partial}{\\partial T}\\log(B(t,T)). \\] We see that the forward rate is simply the continuously compounded forward rate on a infinitesimal interval of time \\([T-d t,T]\\) i.e. \\[ f(t,T)=\\lim_{S\\to T}f(t,S,T)=\\lim_{S\\to T}-\\frac{\\log B(t,T)-\\log B(t,S)}{T-S}=-\\frac{\\partial}{\\partial T}\\log(B(t,T)). \\] Note that this is all in expectation. The forward rate is useful in the sense that we can write \\[ B(t,T)=\\exp\\left[-\\int_t^T f(t,s)\\ ds\\right]=\\mathbb E^{\\mathbb Q}\\left[\\left.\\exp\\left(-\\int_t^T r(s)\\ ds\\right) \\right\\vert \\mathcal{F}_t\\right]. \\] We may use forward rate interchangeably as expectation-weighted discounting factor, in constructing fixed rates and arbitrage free loans. 4.3.2 Phase-type representation of bond prices Consider the interest rate process \\[ r(t)=r_{X(t)}(t), \\] where \\(\\{X(t)\\}_{t\\ge 0}\\) is a time-inhomogeneous Markov jump process on a statespace \\(E=\\{1,...p\\}\\) with intensity matrix \\(\\mathbf M(t)=\\{\\mu_{ij}\\}_{i,j\\in E}\\) and \\(r_i(t)\\) for \\(i=1,...,p\\) are deterministic functions. We call this a Markov-jump representation interest model and we summaries the model below. Definition. (Markov-jump representation interest model) Let \\(\\{X(t)\\}_{t\\ge 0}\\) be a \\(p\\) dimensional time-inhomogeneous Markov jump process. Define \\(\\mathcal F_t=\\sigma(X(s) : 0\\le s\\le t)\\). Then if \\(r_i(t)\\) is bounded from below and deterministic the process \\[ r(t)=r_{X(t)}(t), \\] is an \\(\\mathcal F_t\\) adapted interest rate process. We see that in the context of this interest rate model the zero-coupon bond with maturity \\(T\\) has price process \\[ B(t,T)=\\mathbb E^{\\mathbb Q}\\left[\\left.\\exp\\left(-\\int_t^Tr_{X(s)}(s)\\ ds\\right) \\right\\vert \\mathcal F_t\\right]=\\mathbb E^{\\mathbb Q}\\left[\\left.\\exp\\left(-\\int_t^Tr_{X(s)}(s)\\ ds\\right) \\right\\vert X_t\\right]. \\] We can then make the matrix representation of the discounting factors. Theorem 3.2. (Bladt) For \\(i,j\\in E\\), let \\[ d_{ij}(s,t)=\\mathbb E^{\\mathbb Q}\\left[\\left.1_{\\{X(t)=j\\}}\\exp\\left(-\\int_s^tr_{X(u)}(u)\\ du\\right) \\right\\vert X_s=i\\right], \\] for \\(s\\le t\\). Then the matrix \\(\\mathbf D(s,t)=\\{d_{ij}(s,t)\\}_{i,j\\in E}\\) has representation \\[ \\mathbf D(s,t)=\\prod_s^t\\Big(\\mathbf I+(\\mathbf M(u)-\\Delta (r(u)))\\ du\\Big), \\] with \\(\\Delta(r(u))=\\text{diag}(r_1(u),...,r_p(u))\\). We can then by setting \\(\\rho\\) as \\[ \\rho=\\max\\left(0,-\\min_{i\\in E}\\inf_{x\\ge 0}r_i(x)\\right), \\] use the matrix representation of \\(\\mathbf D(s,t)\\) to represent the zero-coupon bond and make a statement that relate \\(B(t,T)\\) to an IPH distributed random variable. Theorem 3.3. (Bladt) (Phase-type representation of bond prices) Assume the Markov-jump interest model. The price process of the zero-coupon bond satisfies \\[ B(t,T)=\\mathbb E^{\\mathbb Q}\\left[\\left.\\exp\\left(-\\int_t^Tr_{X(s)}(s)\\ ds\\right) \\right\\vert X_t\\right]=\\pi_{X(t)}^\\top\\mathbf D(t,T)\\mathbf e, \\] where \\(\\mathbf e=(1,...,1)^\\top\\) and \\(\\pi_{X(t)}\\) is the distribution of \\(X(t)\\). Let \\(\\tau\\sim IPH(\\pi_{X(t)},\\mathbf M(x+t)-\\Delta (r(x+t))-\\rho \\mathbf I)\\) and define \\(\\overline F(t)\\) as the survival function for \\(\\tau\\). Then we have \\[ B(t,T)=e^{\\rho (T-t)}\\overline F(T). \\] In particular if \\(\\rho = 0\\) then \\(B(t,T)=\\overline F(T)\\) is the survival function of \\(\\tau\\). We furthermore have the following result regarding the forward rate. Corollary 3.4. (Bladt) Assume the Markov-jump interest model and that \\(X(t)=i\\). The forward rate is then the hazard rate at \\(T\\) for the random variable \\(\\tau\\sim IPH(\\pi_{X(t)},\\mathbf M(x+t)-\\Delta (r(x+t))-\\rho \\mathbf I)\\) less \\(\\rho\\) i.e. \\[ f(t,T)=\\frac{f_{\\tau}(T)}{\\overline F(T)}-\\rho. \\] where \\(f_\\tau\\) is the density function of \\(\\tau\\). We can now for explicity write the distribution function of \\(\\tau\\) in 3.3 and 3.4 above. Corollary 3.5. (Bladt) The stopping time \\(\\tau(t)\\sim IPH(\\mathbb \\pi,\\mathbf M(x+t)-\\Delta (r(x+t))-\\rho \\mathbf I)\\) with \\(\\pi_j=1_{\\{j=i\\}}\\) has distribution function \\[ F_{\\tau(t)}(T)=\\mathbb E^{\\mathbb Q}\\left[\\left.\\int_t^T r_{X(y)}(y)\\exp\\left(-\\int_t^yr_{X(u)}(u)\\ du\\right)\\ dy\\ \\right\\vert\\ X(t)=i\\right]. \\] Corollary 3.6. (Bladt) The stopping time \\(\\tau(t)\\sim IPH(\\mathbb \\pi,\\mathbf M(x+t)-\\Delta (r(x+t))-\\rho \\mathbf I)\\), with \\(\\pi\\) as the initial distribution of \\(X(t)\\). Then the following holds \\[\\begin{align*} \\mathbb P(\\tau &gt; T)&amp;=\\mathbb E^{\\mathbb Q}\\left(\\exp\\left(-\\int_0^T r_{X(u)}(u)\\ du\\right)\\right),\\\\ F_{\\tau(t)}(T)&amp;=\\mathbb E^{\\mathbb Q}\\left(\\int_0^T r_{X(y)}(y)\\exp\\left(-\\int_0^y r_{X(u)}(u)\\ du\\right)\\ dy\\right),\\\\ f_{\\tau(t)}(t)&amp;=\\mathbb E^{\\mathbb Q}\\left( r_{X(t)}(t)\\exp\\left(-\\int_0^t r_{X(u)}(u)\\ du\\right)\\right),\\\\ f(0,T)&amp;=\\frac{f_{\\tau(t)}(T)}{1-F_{\\tau(t)}(T)}. \\end{align*}\\] 4.3.3 Term structure models We may introduce another rate model, where \\(r\\) is drifting with a locally deterministic term and a drift given by a Brownian motion. We call these models term structure models. Definition. (Term structure interest model) Let \\(\\mathbb Q\\) be an equivalent martingale measure and let \\(W^{\\mathbb Q}\\) be a \\(\\mathbb Q\\)-Brownian motion. Assume that the process \\(r\\) has dynamics \\[ dr(t)=\\alpha(t,r(t))\\ dt + \\sigma(t,r(t))\\ dW^{\\mathbb Q}(t), \\] with \\(r(0)=r_0\\). Assume that \\(\\alpha(t,r)\\) and \\(\\sigma(t,r)\\) are deterministic functions. Define \\(\\mathcal F_t=\\sigma(W^{\\mathbb Q}(s) : 0\\le s\\le t)\\). Then \\(r\\) is \\(\\mathcal F_t\\)-adapted and we call \\(r\\) a term structure interest rate with local drift \\(\\alpha\\) and volatility \\(\\sigma\\). One easily sees that \\(r\\) is indeed a Markov process and the arbitrage free price of a zero-coupon bond in this model is given by the price process \\[ B(t,T)=p(t,r(t)) \\] i.e. a function of time and the current rate. Furthermore, the pricing is derived from the risk neutral valuation formula: \\[ p(t,r(r))=\\mathbb E^{\\mathbb Q}\\left[\\left.e^{-\\int_t^Tr(\\tau)\\ d\\tau} \\right\\vert \\mathcal{F}_t\\right]=\\mathbb E^{\\mathbb Q}\\left[\\left.e^{-\\int_t^Tr(\\tau)\\ d\\tau} \\right\\vert r(t)\\right], \\] as \\(r\\) is Markov. We then know that the process \\[ e^{-\\int_0^t r(u)\\ du}B(t,T)=\\mathbb E^{\\mathbb Q}\\left[\\left.e^{-\\int_0^Tr(\\tau)\\ d\\tau} \\right\\vert \\mathcal{F}_t\\right]\\tag{23} \\] is a Doob \\(\\mathbb Q\\)-martingale and so this gives rise to the term structure equation below. Theorem 3.9. (Bladt) (Term structure equation) Assume the term structure model. Then the bond price \\(B(t,T)=p(t,r(t))\\) satisfies the PDE \\[ p_t(t,r)=rp(t,r)-\\alpha(t,r)p_r(t,r)-\\frac{1}{2}\\sigma^2(t,r)p_{rr}(t,r) \\] subect to the condition \\[ p(T,r)=1. \\] The result follows from Ito’s formula on the martingale in (23) (simply set the local drift equal to 0). The equation above is generally not solvable, but there exist some restricted models, that does have an explicit solution. One of the models that does have a solution is the affine model. Corollary. (Affine models) Assume that the pricing function \\(p\\) of a zero-coupon bond has representation \\[ B(t,T)=p(t,r(t))=e^{f(t)r(t)+g(t)}, \\] then the term structure equation becomes \\[ r=f&#39;(t)r+g&#39;(t)+\\alpha(t,r)f(t)+\\frac{1}{2}\\sigma^2(t,r)f^2(t), \\] with terminal conditions \\[ f(T)=g(T)=0. \\] Assume furthermore that \\(\\alpha\\) and \\(\\sigma\\) has representation \\[ \\alpha(t,r)=a(t)+b(t)r\\quad \\text{and}\\quad\\sigma(t,r)=\\sqrt{c(t)+d(t)r} \\] then it follows that \\[ f&#39;(t)=1-b(t)f(t)-\\frac{1}{2}d(t)f^2(t)\\quad \\text{and}\\quad g&#39;(t)=-a(t)f(t)-\\frac{1}{2}c(t)f^2(t). \\] Proof. From theorem 3.9 the term structure equation is \\[ p_t(t,r)=rp(t,r)-\\alpha(t,r)p_r(t,r)-\\frac{1}{2}\\sigma^2(t,r)p_{rr}(t,r). \\] Under assumption that the log of the price process is affine we find that \\[ p_t(t,r)=(f&#39;(t)r+g&#39;(t)p(t,r),\\quad p_r(t,r)=f(t)p(t,r),\\quad p_{rr}(t,r)=f^2(t)p(t,r) \\] and so \\[ (f&#39;(t)r+g&#39;(t)p(t,r)=rp(t,r)-\\alpha(t,r)f(t)p(t,r)-\\frac{1}{2}\\sigma^2(t,r)f^2(t)p(t,r) \\] Hence \\[ r=f&#39;(t)r+g&#39;(t)+\\alpha(t,r)f(t)+\\frac{1}{2}\\sigma^2(t,r)f^2(t). \\] This shows the first part. Now assume that \\[ \\alpha(t,r)=a(t)+b(t)r,\\quad\\sqrt{c(t)+d(t)r}. \\] When inserting in the equation before we have \\[ r=f&#39;(t)r+g&#39;(t)+(a(t)+b(t)r)f(t)+\\frac{1}{2}(c(t)+d(t)r)f^2(t). \\] Then by a coefficient matching argument we have \\[ \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}^\\top= \\begin{pmatrix} f&#39;(t)+b(t)f(t)+\\frac{1}{2}d(t)f^2(t)\\\\ g&#39;(t)+a(t)f(t)+\\frac{1}{2}c(t)f^2(t) \\end{pmatrix}^\\top \\] since by multiplying by \\((r,1)^\\top\\) we get the equation above. Isolating \\(f&#39;\\) and \\(g&#39;\\) yields the result. \\(\\blacksquare\\) One popular model is proposed by Vasicek where \\[ \\alpha(t,r)=a-br(t)\\quad \\text{and}\\quad \\sigma(t,r)=\\sigma, \\] with \\(a,b\\in \\mathbb R\\) and \\(\\sigma\\in\\mathbb R_+\\) are constants. Given the initial condition \\(r(0)=r_0\\in\\mathbb R\\) we have the solution as \\[ r(t)=r_0e^{-bt}+\\frac{a}{b}\\left(1-e^{-bt}\\right)+\\sigma\\int_0^t e^{-(t-s)b}\\ dW^{\\mathbb Q}(t). \\] One may achieve the above integration result by differentiating the function \\(h(t,r)=r(t)e^{bt}\\) and integrating the derivative giving \\[\\begin{align*} r(t)e^{bt}-r(0)&amp;=\\int_0^tdh=\\int_0^tbe^{bs}r(s)+e^{bs}(a-br(s))\\ ds+\\int_0^te^{bs}\\sigma\\ dW^{\\mathbb Q}(s)\\\\ &amp;=\\frac{a}{b}(e^{bt}-1)+\\sigma\\int_0^te^{bs}\\ dW^{\\mathbb Q}(s) \\end{align*}\\] where simpel isolation gives the equation. 4.3.4 Estimation of PH bond models Estimation of rate models may be done by observing current spot rates on the market for zero-coupon bonds with a variety of maturity dates. One may for instance consider \\(T=t+1,...,t+30\\) or even up to \\(T=t+100\\). This gives us the corresponding yield curve defined as \\[ T\\mapsto Y(t,T)=-\\frac{\\log p(t,T)}{T-t}=-\\frac{\\log B(t,T)}{T-t}. \\] If we make probabilistic assumptions on \\(r(t)\\) we may in a parametrialized model estimate a parameter \\(\\theta\\) that associate a distribution function for \\(r(t)\\) i.e. \\(\\theta \\mapsto F_{\\theta,t}(r)\\). Let us see how we may estimate a Markov-jump model and a term structure model below. 4.3.4.1 Data for estimation We consider yield for non-zero coupon bonds listed on The Wall Street Journal. The data considered was pulled on the 17th of february 2023. It must be stressed that the rates are not from zero-coupon bonds and so we in general have yields form contracts paying \\(R\\) continuously until maturity where the face value of \\(1\\) is payed. This in particular means that the prices in the data is prices according to \\[ B_R(t,T]=\\mathbb E^{\\mathbb Q}\\left[\\left.\\int_t^T Re^{-\\int_t^s r(u)\\ du}\\ ds+e^{-\\int_0^T r(u)\\ du} \\right\\vert\\mathcal F_t\\right]=R\\ \\mathbb E^{\\mathbb Q}\\left[\\left.\\int_t^T e^{-\\int_t^s r(u)\\ du}\\ ds \\right\\vert\\mathcal F_t\\right]+ B(t,T] \\] This means that the yields are not equivalent to the zero-coupon yields. In fact it must hold that \\[ Y_R(t,T]\\ge Y(t,T] \\] with \\(Y_R(t,T)\\approx Y(t,T]+R\\cdot f(T-t)\\) where \\(f\\to 0\\) for \\(T-t\\to 0\\) and \\(f\\to 1\\) for \\(T-t\\to \\infty\\). We will in any case try to determine estimate a statistical model of the yield curve based on the yields in the dataset. 4.3.4.2 Estimation of Phase-type models We restrict ourselves to the transition matrices on the form \\[ \\mathbf T(x)=\\lambda_\\theta(x)\\mathbf T, \\] where \\(\\mathbf T\\) is a constant \\(p\\times p\\) matrix and \\(\\lambda_\\theta\\) is a real-valued function. The parameter space \\(\\Theta\\subset \\mathbb R^d\\) for some \\(d\\ge 1\\). This then gives a constant base dynamic between the rate states \\(1,...,p\\in E\\) and \\(\\lambda_\\theta\\) homogeneously speeds up or slows down the waiting time in each state by scaling the transitions intensities. "],["survival-and-mortality-rates.html", "4.4 Survival and mortality rates", " 4.4 Survival and mortality rates 4.4.1 Survival probabilities and forward mortality rates We consider a non-negative random variable \\(\\tau\\) on a probability space \\((\\Omega,\\mathcal F, P)\\). The mortality rate of \\(\\tau\\) at time \\(t\\) is defined through the dynamics in the following definition. Definition. (Mortality rate) Let \\(\\tau\\ge 0\\) be a non-negative stochastic variable. The mortality rate function \\(\\mu(t)\\) is a, possibly stochastic, function defined as \\[ \\mu(t)\\ dt=\\mathbb P(\\tau\\in[t,t+dt)\\ \\vert\\ \\tau &gt; t)=\\frac{f(t)}{\\overline F(t)}\\ dt, \\] where \\(\\overline F(t)=1-F(t)=\\mathbb P(\\tau &gt;t)\\) is the survival function of \\(\\tau\\). The definition above shows that we may alternatively define \\(\\mu\\) in terms of the below derivative \\[ \\mu(t)=-\\frac{d}{dt}\\log\\Big(\\overline F(t)\\Big), \\] hence we have that \\[ -\\int_0^t\\mu(s)\\ ds=\\log\\Big(\\overline F(t)\\Big)-\\log\\Big(\\overline F(0)\\Big)=\\log\\Big(\\overline F(t)\\Big), \\] giving the well-known formula \\[ \\overline F(t)=\\exp\\left(-\\int_0^t\\mu(s)\\ ds\\right). \\] This also gives the nice interpretation for the conditional distribution of \\(\\tau\\) given \\(\\tau&gt;t\\) since. \\[ \\mathbb P(\\tau &gt; s\\ \\vert\\ \\tau &gt; t)=\\frac{\\overline F(s)}{\\overline F(t)}=\\exp\\left(-\\int_t^s\\mu(u)\\ du\\right). \\] Corollary. Let \\(\\tau\\ge 0\\) be a non-negative stochastic variable with mortality rate \\(\\mu(t)\\). The probability that \\(\\tau \\in [t,s)\\) in the event \\(\\tau &gt;t\\) is \\[ p(t,s)=\\mathbb P(\\tau &gt; s\\ \\vert\\ \\tau &gt; t)=\\exp\\left(-\\int_t^s\\mu(u)\\ du\\right). \\] How to deal with these random intensities. Either we could model them directly under a probability measure, \\(\\mathbb P\\) say, but since we do not know much about the probabilities anyway, we might as well model the consequences in price (reserve) by changes of the intensities. That is, how much does a change in intensity cost? This might been seen in a financial context, even as a derivative security, and should therefore be evaluated relative to a risk–free asset, i.e. by using an equivalent martingale measure, \\(\\mathbb Q\\) say. We therefore define the mortality forward rates as Definition. (Mortality forward rate) Let \\(\\tau\\ge 0\\) be a non-negative stochastic variable with mortality rate \\(\\mu(t)\\). The mortality forward rate rate \\(m(t,s)\\) is a function defined by \\[ \\exp\\left(-\\int_t^s m(t,u)\\ du\\right)=q(t,s)=\\mathbb E^{\\mathbb Q}\\left[\\left. \\exp\\left(-\\int_t^s\\mu(u)\\ du\\right)\\right\\vert \\mathcal F(t)\\right]. \\] Notice that in particular the function \\(q(t,s)\\) solves the differential equation \\[ \\frac{\\partial q(t,s)}{\\partial s}=-m(t,s)q(t,s),\\qquad q(t,t)=1. \\] When evaluating a payment from insurance company and the insuread we also need to take into account the interest rate \\(r\\) and so bonds would be priced according to the quantity \\[ E^{\\mathbb Q}\\left[\\left. \\exp\\left(-\\int_t^sr(u)+\\mu(u)\\ du\\right)\\right\\vert \\mathcal F(t)\\right]. \\] Notice that the filtration \\(\\mathcal F(t)\\) is now given by the path of \\((\\mu(t),r(t))\\). A payment of 1 in the event the insured dies in the time interval \\([s,s+dt)\\) is given by \\[ E^{\\mathbb Q}\\left[\\left. \\exp\\left(-\\int_t^sr(u)+\\mu(u)\\ du\\right)\\mu(s)\\right\\vert \\mathcal F(t)\\right]. \\] Then means that we may price for instance a contract of a payment of 1 in the event of death as \\[ E^{\\mathbb Q}\\left[\\left. \\int_t^T\\exp\\left(-\\int_t^sr(u)+\\mu(u)\\ du\\right)\\mu(s)\\ ds\\right\\vert \\mathcal F(t)\\right] \\] where the contract matures at time \\(T&gt;t\\). Furhermore if we assume indepence between the mortality and the rates (obviously satisfies) we have the following decompositions: \\[\\begin{align*} (\\text{Forward rates}):\\qquad&amp; E^{\\mathbb Q}\\left[\\left. \\exp\\left(-\\int_t^sr(u)\\ du\\right)\\right\\vert \\mathcal F(t)\\right]=e^{-\\int_t^sf(t,u)\\ du},\\\\ (\\text{Bond prices}):\\qquad&amp; E^{\\mathbb Q}\\left[\\left. \\exp\\left(-\\int_t^sr(u)+\\mu(u)\\ du\\right)\\right\\vert \\mathcal F(t)\\right]=e^{-\\int_t^sf(t,u)+m(t,u)\\ du},\\\\ (\\text{Insurance event}):\\qquad&amp; E^{\\mathbb Q}\\left[\\left. \\exp\\left(-\\int_t^sr(u)\\ du\\right)\\mu(s)\\right\\vert \\mathcal F(t)\\right]=e^{-\\int_t^sf(t,u)+m(t,u)\\ du}m(t,s). \\end{align*}\\] 4.4.2 Forward transistion rates In the above sections we considered the simple Markov process with two states, one real and a absorbing state. In that context we simply model one jump. However in the general case, we would assume a Markov jump on a finite state space \\(E\\) i.e. \\(\\{Z(t)\\}_{t\\ge 0}\\) with random intensities \\(\\mu_{ij}(t)\\) with \\(i,j\\in E\\). We can combine these intensities into the matrix function \\[ \\mathbf M(s)=\\{\\mu_{ij}(s)\\}_{i,j\\in E}. \\] Then the natural definition of forward transition rates are the elements of the matrix, \\(\\mathbf F(t,s)\\), given by the product integral below \\[ \\mathbb E^{\\mathbb Q}\\left(\\left.\\prod_t^s(\\mathbf I+\\mathbf M(u))\\ du\\right\\vert \\mathcal F (t)\\right)=\\prod_t^s(\\mathbf I+\\mathbf F(t,u))\\ du. \\] We in this context define \\[ \\mathbf P(t,s)=\\prod_t^s(\\mathbf I+\\mathbf M(u))\\ du, \\] being the transition probabilities \\(p_{ij}(t,s)\\). We furthemore set the matrix \\(\\mathbf q\\) to \\[ \\mathbf q(t,s)=\\mathbb E^{\\mathbb Q}\\left(\\left.\\mathbf P(t,s)\\right\\vert \\mathcal F (t)\\right)=\\prod_t^s(\\mathbf I+\\mathbf F(t,u))\\ du. \\] If we assume we may differentiate under the \\(\\mathbb Q\\) expectation we get \\[\\begin{align*} \\mathbb E^{\\mathbb Q}\\left(\\left.\\mathbf P(t,s)\\mathbf M(s)\\right\\vert \\mathcal F (t)\\right)&amp;=\\mathbb E^{\\mathbb Q}\\left(\\left.\\frac{\\partial}{\\partial s}\\mathbf P(t,s)\\right\\vert \\mathcal F (t)\\right)=\\frac{\\partial}{\\partial s}\\mathbb E^{\\mathbb Q}\\left(\\left.\\mathbf P(t,s)\\right\\vert \\mathcal F (t)\\right)\\\\ &amp;=\\frac{\\partial}{\\partial s}\\prod_t^s(\\mathbf I+\\mathbf F(t,u))\\ du=\\mathbf q(t,s)\\mathbf F(t,s), \\end{align*}\\] where we simply use the above definition and the definition of the product integral. Lemma. (Mortality forward rate) Let \\(\\{Z(t)\\}_{t\\ge 0}\\) be a Markov jump process on the finite state space \\(E\\). Assume that the intensity matrix function \\(\\mathbf M(t)\\) exists, then \\[ \\mathbf F(t,s)=\\mathbf q(t,s)^{-1}\\mathbb E^{\\mathbb Q}\\left(\\left.\\mathbf P(t,s)\\mathbf M(s)\\right\\vert \\mathcal F (t)\\right) \\] with \\[ \\mathbf q(t,s) = \\mathbb E^{\\mathbb Q}\\left(\\left.\\mathbf P(t,s)\\right\\vert \\mathcal F (t)\\right). \\] 4.4.3 Reserves revisited Consider an life insurance contract of an insured with underlying Markov jump process \\(\\{Z(t)\\}_{t\\ge 0}\\) commencing at time \\(T&gt;0\\). The contract pays continuous rate \\(b^i(t)\\) while \\(Z(t)= i\\) and transition lump sum payments \\(b^{ij}(t)\\) in the event \\(Z\\) jumps from \\(i\\) to \\(j\\) at time \\(t\\). That is the payment has dynamics \\[ dB(t)=dB^{Z(t)}(t)+\\sum_{j\\ne Z(t-)}b^{Z(t-)j}dN_{Z(t-)j}(t), \\] where \\(N_{ij}(t)=\\#\\{s\\le t:Z(s-)=i,Z(s)=j\\}\\) is the number of jumps from \\(i\\) to \\(j\\) in the interval \\([0,t]\\). The above is equivalent with the process \\[ dB(t)=\\sum_{i\\in E}I^i(t)\\ dB^i(t)+\\sum_{i\\in E}\\sum_{j\\in E : j\\ne i}b^{ij}(t)\\ dN_{ij}(t), \\] where obviously \\(I^i(t)=1_{Z(t)=i}\\). We assume that \\(\\{r(t)\\}_{t\\ge 0}\\) is the stochastic interest rate process which is independent of the payment process. The market reserve or third order reserve is defined under the martingale measure \\(\\mathbb Q\\) as \\[ V(t)=\\mathbb E^{\\mathbb Q}\\left[\\left.\\int_t^Te^{-\\int_t^ur(s)\\ ds}\\ dB(u)\\right\\vert \\mathcal F(t)\\right], \\] i.e. the discounted expected value of the payments. The above filtration is the one given by the sample path of both \\(r\\) and \\(Z\\). Using the independence assumption we have \\[\\begin{align*} V(t)&amp;=\\int_t^TE^{\\mathbb Q}\\left[\\left.e^{-\\int_t^ur(s)\\ ds}\\ dB(u)\\right\\vert \\mathcal F(t)\\right]\\\\ &amp;=\\int_t^TE^{\\mathbb Q}\\left[\\left.e^{-\\int_t^ur(s)\\ ds}\\right\\vert \\mathcal F(t)\\right]E^{\\mathbb Q}\\left[\\left.dB(u)\\right\\vert \\mathcal F(t)\\right]\\\\ &amp;=\\int_t^Te^{-\\int_t^uf(t,s)\\ ds}E^{\\mathbb Q}\\left[\\left.dB(u)\\right\\vert \\mathcal F(t)\\right], \\end{align*}\\] by the definition of the forward rate \\(f(t,s)\\). Notice that now we have two terms both not depending on one another. In other words, \\(E^{\\mathbb Q}\\left[\\left.dB(u)\\right\\vert \\mathcal F(t)\\right]\\) only depends on \\(Z\\) not \\(r\\). When evaluating the expected dynamics under \\(\\mathbb Q\\) we have \\[ E^{\\mathbb Q}\\left[\\left.dB(u)\\right\\vert \\mathcal F(t)\\right]=E^{\\mathbb Q}\\left[\\left.dB(u)\\right\\vert Z(t)\\right]=\\sum_{i\\in E}I^i(t)E^{\\mathbb Q}\\left[\\left.dB(u)\\right\\vert Z(t)=i\\right] \\] with \\[\\begin{align*} &amp;E^{\\mathbb Q}\\left[\\left.dB(u)\\right\\vert Z(t)=i\\right]=E^{\\mathbb Q}\\left[\\left.\\sum_{k\\in E}I^k(u)\\ dB^k(u)+\\sum_{k\\in E}\\sum_{j\\in E : j\\ne k}b^{kj}(u)\\ dN_{jk}(u)\\right\\vert Z(t)=i\\right]\\\\ &amp;=\\sum_{k\\in E}E^{\\mathbb Q}\\left[\\left.I^k(u)\\right\\vert Z(t)=i\\right]\\ dB^i(u)+\\sum_{k\\in E}\\sum_{j\\in E : j\\ne k}b^{kj}(u)\\ E^{\\mathbb Q}\\left[\\left.dN_{jk}(u)\\right\\vert Z(t)=i\\right]\\\\ &amp;=\\sum_{k\\in E}q_{ik}(t,u)\\ dB^i(u)+\\sum_{k\\in E}\\sum_{j\\in E : j\\ne k}b^{kj}(u)\\ E^{\\mathbb Q}\\left[\\left.p_{ik}(t,u)\\mu_{kj}(u)\\right\\vert Z(t)=i\\right]\\ du\\\\ &amp;=\\sum_{k\\in E}q_{ik}(t,u)\\ dB^i(u)+\\sum_{k\\in E}\\sum_{j\\in E : j\\ne k}E^{\\mathbb Q}\\left[\\left.p_{ik}(t,u)b^{kj}(u)\\mu_{kj}(u)\\right\\vert Z(t)=i\\right]\\ du. \\end{align*}\\] 4.4.4 Stochastic mortality rates "],["matrix-methods-in-life-insurance.html", "4.5 Matrix methods in life insurance", " 4.5 Matrix methods in life insurance 4.5.1 Basic setup We consider the time-inhomogeneous Markov jump process \\(X=\\{X(t)\\}_{t\\ge 0}\\) with a finite state–space \\(E\\) and intensity matrix \\(\\mathbf \\Lambda(t)=\\{\\lambda_{ij}(t)\\}_{i,j\\in E}\\). We then define the payment process as \\[ dB(t)=\\sum_{i\\in E}1_{X(t-)=i}\\left(b_i(t)\\ dt +\\sum_{j\\in E}b_{ij}(t)\\ dN_{ij}(t)\\right), \\] with \\(b_i(t)\\) are continuous payment rates (negative if premiums) and \\(b_{ij}(t)\\) lump sum payments, which occur according to the counting measure \\(N_{ij}(t)\\). The intensity matrix is decomposed into \\[ \\mathbf \\Lambda(t)=\\mathbf \\Lambda^0(t) + \\mathbf \\Lambda^1(t), \\] where \\(\\mathbf \\Lambda^1(t)\\) is a non–negative matrix and, consequently, \\(\\mathbf \\Lambda^0(t)\\) a sub–intensity matrix, i.e. row sums are non–positive. We choose this decomposition in a way such that \\(\\mathbf \\Lambda^1(t)\\) contains the intensities with a factor \\(l_{ij}(t)\\) being the probability upon jump from \\(i\\) to \\(j\\) at time \\(t\\) of recieving the payment \\(b_{ij}(t)\\). In other words, we have the decomposition \\[ \\lambda_{ij}(t)=\\lambda_{ij}^0(t)+\\lambda_{ij}^1(t)=l_{ij}(t)\\lambda_{ij}^0(t)+(1-l_{ij}(t))\\lambda_{ij}^1(t)\\iff l_{ij}(t)=\\frac{\\lambda_{ij}^1(t)}{\\lambda_{ij}^0(t)+\\lambda_{ij}^1(t)}. \\] In the case \\(i=j\\) then the counting measure \\(N_{ii}\\) denotes a poisson arrival process for lump sum payments arriving during the visit in the state \\(i\\). This in particular means that the payment \\(b_{ii}(t)\\) will be triggered in \\([t,t+dt)\\) with probability \\(\\lambda_{ii}^1\\ dt\\) given that \\(X(t-)=i\\). Finally, we assume that the spot interest rates in state i follow a deterministic function \\(r_i(t)\\). Hence the interest rates follow the model \\[ r(u) = r_{X(u)}(u). \\] Recall that the reserve is defined as \\[ V(t)=\\mathbb E^{\\mathbb Q}\\left[\\left.\\int_t^Te^{-\\int_t^ur(s)\\ ds}\\ dB(u)\\right\\vert \\mathcal F(t)\\right]. \\] We will be using the following matrix functions when computing the reserve and higher order moments. \\[\\begin{align*} \\mathbf B(t)&amp;=\\{b_{ij}(t)\\}_{i,j\\in E},\\\\ \\mathbf R(t)&amp;=\\mathbf \\Lambda^1(t)\\bullet \\mathbf B(t) + \\mathbf \\Delta (\\mathbf b(t)),\\\\ \\mathbf C^{(k)}(t)&amp;=\\mathbf \\Lambda^1(t)\\bullet \\mathbf B^{\\bullet k}(t),\\qquad k\\ge 2, \\end{align*}\\] where \\(\\bullet\\) denotes the Schur product simply defined as \\(\\mathbf A\\bullet\\mathbf B=\\{a_{ij}b_{ij}\\}\\) with \\(\\mathbf A=\\{a_{ij}\\}\\) and \\(\\mathbf B=\\{b_{ij}\\}\\). The notation \\(\\mathbf \\Delta (\\cdot)\\) operator sets \\(\\cdot\\) as the diagonal i.e. \\[ \\mathbf \\Delta (\\mathbf b(t))=\\text{diag}(b_1(t),...,b_p(t)), \\] assuming \\(E=\\{1,...,p\\}\\). We call \\(\\mathbf B(t)\\) the transition payments, \\(\\mathbf R(t)\\) the rewards and \\(\\mathbf C^{(k)}(t)\\) the \\(k\\)th order contributions. The matrix \\(\\mathbf C^{(k)}(t)\\) is mostly usefull notationally rather than intuitively. 4.5.2 Interest rate free analysis In this section, we consider the risk free context where we are not concerned with any discounting of expected payments. This make us introduce the following random process \\[ U^0(s,t) = \\sum_{i\\in E}\\int_s^tb_i(u)1_{X(u-)=i}\\ du+\\sum_{i,j\\in E}\\int_s^tb_{ij}(u)\\ dN_{ij}(u), \\] giving the total reward obtained in the time interval \\([s,t]\\). We are interested in higher order moments of this quantity (representing the rate free reserve). To this goal we introduce the conditional moments with \\[ m_{ij}^{(k)}(s,t)=\\mathbb E\\left.\\Big[1_{X(t)=j}U^0(s,t)^k\\right\\vert X(s)=i\\Big], \\] representing the weighted expected moment upon start in state \\(i\\) weighted with the probability with end in state \\(j\\). This forms the matrix \\[ \\mathbf m^{(k)}(s,t)=\\left\\{m_{ij}^{(k)}(s,t)\\right\\}_{i,j\\in E}. \\] For simplicity we simply for \\(k=1\\) write \\(\\mathbf m^{(1)}(s,t) = \\mathbf m(s,t)\\). Theorem 5.1. (Bladt) With the above assumptions it holds that \\[ \\mathbf m(s,t)=\\int_s^t\\mathbf P(s,u)\\mathbf R(u)\\mathbf P(u,t)\\ du. \\] In particular, by Van Loans formula we have \\[ \\prod_s^t\\left(\\mathbf I + \\begin{pmatrix}\\mathbf \\Lambda(x) &amp; \\mathbf R(t)\\\\ \\mathbf 0 &amp; \\mathbf \\Lambda(x)\\end{pmatrix} dx\\right)= \\begin{pmatrix} \\mathbf P(s,t) &amp; \\mathbf m(s,t)\\\\ \\mathbf 0 &amp; \\mathbf P(s,t). \\end{pmatrix} \\] Proof. 4.5.3 Transform of rewards and higher order moments Recall that for a random variable \\(X\\) the moment generating function \\(M(\\theta)\\) given by \\[ M(\\theta)=\\mathbb E[e^{\\theta X}], \\] gives us a comprehensive insight into the moments of \\(X\\). Take the taylor approximation of \\(e^{\\theta X}\\): \\[ e^{\\theta X}=\\sum_{n=0}^\\infty\\frac{(\\theta X)^n}{n!} \\] Hence taking expectation gives \\[ M_X(\\theta)=\\mathbb E[e^{\\theta X}]=\\mathbb E\\left[\\sum_{n=0}^\\infty\\frac{(\\theta X)^n}{n!}\\right]=\\sum_{n=0}^\\infty\\frac{\\theta^n \\mathbb E[X^n]}{n!}. \\] Now differentiating wrt. \\(\\theta\\) yields the important result \\[ \\frac{\\partial M_X(\\theta)}{\\partial \\theta}(\\theta)=\\sum_{n=1}^\\infty\\frac{n\\theta^{n-1} \\mathbb E[X^n]}{n!}=\\sum_{n=1}^\\infty\\frac{\\theta^{n-1} \\mathbb E[X^n]}{(n-1)!}=\\mathbb E[X]+\\sum_{n=1}^\\infty\\frac{\\theta^{n} \\mathbb E[X^{n+1}]}{n!}, \\] hence by evaluating in \\(\\theta = 0\\) we get \\(M&#39;_X(0)=\\mathbb E[X]\\). In general we have \\[ \\left.\\frac{\\partial^n M_X(\\theta)}{\\partial \\theta^n}(\\theta)\\right\\vert_{\\theta = 0}=\\mathbb E[X^n],\\qquad n\\ge 1. \\] This is really the motivation for examining the moment generating function of the reward obtained in the interval \\([s,t]\\) i.e. the quantity \\(U^0(s,t)\\). To this we define \\[ \\hat F_{ij}(\\theta;s,t)=\\mathbb E\\left[\\left.e^{\\theta U^0(s,t)}1\\{X(t)=j\\}\\ \\right\\vert\\ X(s)=i\\right], \\] for \\(i,j=1,...,p\\). Obviously we have the following identity \\[ \\sum_{j=1}^p\\hat F_{ij}(\\theta;s,t)=\\mathbb E\\left[\\left.e^{\\theta U^0(s,t)}\\ \\right\\vert\\ X(s)=i\\right]. \\] Furthermore, if we define the distribution of \\(X(t),U^0(s,t)\\) given \\(X(s)\\) by \\[ F_{ij}(x; s,t)=\\mathbb P(X(t)=j,U^0(s,t)\\le x\\ \\vert\\ X(s)=i) \\] then we have the integral decomposition \\[ \\hat F_{ij}(\\theta; s,t)=\\int_{\\mathbb R} e^{\\theta x}\\ dF_{ij}(x; s,t). \\] The following theorem gives a way of calculating the object \\(\\hat{\\mathbf F}(\\theta; s,t)=\\{\\hat F_{ij}(\\theta; s,t)\\}\\). Theorem 5.5. (Bladt) Let \\(\\hat{\\mathbf F}(\\theta; s,t)=\\{\\hat F_{ij}(\\theta; s,t)\\}_{i,j=1,...,p}\\) and \\[ \\mathbf A(\\theta;u)=\\mathbf\\Lambda^1(u)\\bullet\\left\\{e^{\\theta b_{kl}(u)}\\right\\}_{k,l}+\\mathbf \\Lambda^0(u)+\\theta \\mathbf \\Delta(\\mathbf b(u)) \\] where \\(\\bullet\\) denotes the Schur matrix product. Then the moment generating function of \\(U^0(s,t)\\) is given by \\[ \\hat{\\mathbf F}(\\theta; s,t)=\\prod_s^t(\\mathbf I + \\mathbf A(\\theta; u)\\ du). \\] Proof. By the above theorem we see that \\[ \\mathbb E\\left[\\left.e^{\\theta U^0(s,t)}\\ \\right\\vert\\ X(s)=i\\right] =e_i^\\top\\hat{\\mathbf F}(\\theta; s,t)e = e_i^\\top\\prod_s^t(\\mathbf I + \\mathbf A(\\theta; u)\\ du)e, \\] in particular \\[ \\prod_s^t(\\mathbf I + \\mathbf A(\\theta; u)\\ du)e= \\begin{pmatrix} \\mathbb E\\left[\\left.e^{\\theta U^0(s,t)}\\ \\right\\vert\\ X(s)=1\\right]\\\\ \\mathbb E\\left[\\left.e^{\\theta U^0(s,t)}\\ \\right\\vert\\ X(s)=2\\right]\\\\ \\vdots\\\\ \\mathbb E\\left[\\left.e^{\\theta U^0(s,t)}\\ \\right\\vert\\ X(s)=p\\right] \\end{pmatrix}. \\] Furthermore, we have from the property above that \\[ \\left.\\frac{\\partial^k}{\\partial \\theta^k}\\left(e_i^\\top\\prod_s^t(\\mathbf I + \\mathbf A(\\theta; u)\\ du)e\\right)\\right\\vert_{\\theta = 0}=m^{(k)}_{i}(s,t). \\] Lemma 5.6. (Bladt) Given the definitions in theorem 5.5 we have \\[\\begin{align*} \\left.\\frac{\\partial^0}{\\partial \\theta^0}\\mathbf A(\\theta; s)\\right\\vert_{\\theta=0}&amp;=\\mathbf A(\\theta; s)\\Big\\vert_{\\theta=0}=\\mathbf \\Lambda^1(s)+\\mathbf\\Lambda^0(s)=\\mathbf\\Lambda(s),\\\\ \\left.\\frac{\\partial}{\\partial \\theta}\\mathbf A(\\theta; s)\\right\\vert_{\\theta=0}&amp;=\\mathbf \\Lambda^1(s)\\bullet \\mathbf B(s)+\\mathbf \\Delta(\\mathbf b(s))=\\mathbf R(s),\\\\ \\left.\\frac{\\partial^k}{\\partial \\theta^k}\\mathbf A(\\theta; s)\\right\\vert_{\\theta=0}&amp;=\\mathbf \\Lambda^1(s)\\bullet \\mathbf B^{\\bullet k} B(s)=\\mathbf C^{(k)}(s),\\qquad k&gt;1. \\end{align*}\\] Recall that we defined the moments in general by \\[ \\mathbf m^{(k)}(s,t)=\\left\\{\\mathbb E\\left[\\left. 1\\{X(t)=j\\}U^0(s,t)^k\\ \\right\\vert\\ X(s)=i\\right]\\right\\}_{i,j=1,...,p}=\\left\\{m^{(k)}_{ij}(s,t)\\right\\}_{i,j=1,...,p}, \\] hence we can define (for ease of notation) the reduced moments by \\[ \\mathbf m_r^{(k)}(s,t)=\\left\\{\\frac{m^{(k)}_{ij}(s,t)}{k!}\\right\\}_{i,j=1,...,p}=\\frac{1}{k!}\\mathbf m^{(k)}(s,t). \\] Furthermore, we define the reduced contributions \\[ \\mathbf C^{(k)}_r(s,t)=\\frac{1}{k!}\\mathbf C^{(k)}(s,t)=\\frac{1}{k!}\\mathbf \\Lambda^1(t)\\bullet \\mathbf B^{\\bullet k}(t). \\] Theorem 5.7. (Bladt) Given the definitions above we have \\[ \\mathbf m^{(k)}(s,t)=\\int_s^t\\mathbf P(s,x)\\mathbf R(x)\\mathbf m^{(k-1)}_r(x,t)\\ dx+\\sum_{m=2}^k\\int_s^t\\mathbf P(s,x)\\mathbf C^{(m)}_r(x)\\mathbf m^{(k-m)}_r(x,t)\\ dx. \\] Using this result we can make a powerful statement regarding how one could calculate all of the \\(k\\)th moments at once. To this we have the result. Theorem 5.8. (Bladt) Let \\(\\mathbf F^{(k)}(x)\\) be given by \\[ \\mathbf F^{(k)}(x)= \\begin{bmatrix} \\mathbf \\Lambda(x) &amp; \\mathbf R(x) &amp; \\mathbf C^{(2)}_r(x) &amp; \\cdots &amp; \\mathbf C^{(k-1)}_r(x) &amp; \\mathbf C^{(k)}_r(x)\\\\ \\mathbf 0 &amp; \\mathbf \\Lambda(x) &amp; \\mathbf R(x) &amp; \\cdots &amp; \\mathbf C^{(k-2)}_r(x) &amp; \\mathbf C^{(k-1)}_r(x)\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ \\mathbf 0 &amp; \\mathbf 0 &amp; \\mathbf 0 &amp; \\cdots &amp;\\mathbf \\Lambda(x) &amp; \\mathbf R(x)\\\\ \\mathbf 0 &amp; \\mathbf 0 &amp; \\mathbf 0 &amp; \\cdots &amp; \\mathbf 0 &amp;\\mathbf \\Lambda(x) \\end{bmatrix}, \\] and \\(\\mathbf H^{(k)}(s,t)\\) defined by \\[ \\mathbf H^{(k)}(x)= \\begin{bmatrix} \\mathbf P(s,t) &amp; \\mathbf m_r^{(1)}(s,t) &amp; \\mathbf m_r^{(2)}(s,t) &amp; \\cdots &amp; \\mathbf m_r^{(k-1)}(s,t) &amp; \\mathbf m_r^{(k)}(s,t)\\\\ \\mathbf 0 &amp; \\mathbf P(s,t) &amp; \\mathbf m_r^{(1)}(s,t) &amp; \\cdots &amp; \\mathbf m_r^{(k-2)}(s,t) &amp; \\mathbf m_r^{(k-1)}(s,t)\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ \\mathbf 0 &amp; \\mathbf 0 &amp; \\mathbf 0 &amp; \\cdots &amp; \\mathbf P(s,t) &amp; \\mathbf m_r^{(1)}(s,t)\\\\ \\mathbf 0 &amp; \\mathbf 0 &amp; \\mathbf 0 &amp; \\cdots &amp; \\mathbf 0 &amp; \\mathbf P(s,t) \\end{bmatrix}. \\] Then we have the result \\[ \\prod_s^t(\\mathbf I + \\mathbf F^{(k)}(x)\\ dx)=\\mathbf H^{(k)}(s,t). \\] For ease of notation we define the Toeplitz matrix as \\[ \\mathcal T(\\mathbf A_1,\\mathbf A_2,...,\\mathbf A_n)= \\begin{bmatrix} \\mathbf A_1 &amp; \\mathbf A_2 &amp; \\mathbf A_3 &amp; \\cdots &amp; \\mathbf A_{n-1} &amp; \\mathbf A_n\\\\ \\mathbf 0 &amp; \\mathbf A_1 &amp; \\mathbf A_2 &amp; \\cdots &amp; \\mathbf A_{n-2} &amp; \\mathbf A_{n-1}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ \\mathbf 0 &amp; \\mathbf 0 &amp; \\mathbf 0 &amp; \\cdots &amp; \\mathbf A_1 &amp; \\mathbf A_2\\\\ \\mathbf 0 &amp; \\mathbf 0 &amp; \\mathbf 0 &amp; \\cdots &amp; \\mathbf 0 &amp; \\mathbf A_1 \\end{bmatrix}. \\] And hence we could write: \\[ \\mathbf F^{(k)}(x)=\\mathcal T(\\mathbf \\Lambda(x), \\mathbf R(x) , \\mathbf C^{(2)}_r(x) , ... , \\mathbf C^{(k-1)}_r(x), \\mathbf C^{(k)}_r(x)), \\] and \\[ \\mathbf H^{(k)}(x)=\\mathcal T(\\mathbf P(s,t) , \\mathbf m_r^{(1)}(s,t) , \\mathbf m_r^{(2)}(s,t), ...,\\mathbf m_r^{(k-1)}(s,t) , \\mathbf m_r^{(k)}(s,t)). \\] 4.5.4 Markovian interest rates We now study how we may calculate the moments of the reserves in the case with rates. To this we start by assuming the Markov-jump interest model in sync with the Markov-jump process for the policy holder. Obviously, we in practice would assume that these two processes are independent, but for generality we will not insist on this for now. We therefore define the process \\[ \\mathbf r(t)=\\{r_i(t)\\}_{i\\in E_r} \\] and the two Markov jump processes \\[ X_r=\\{X_r(t)\\}_{t\\ge 0}\\in E_r=\\{1,...,p\\},\\qquad X_b(t)=\\{X_b(t)\\}_{t\\ge 0}\\in E_b=\\{1,...,q\\}, \\] and naturally we at time \\(t\\) have the interest rate \\(r_{X_r(t)}(t)\\) and \\(X_v(t)\\) governs the state of the policy holder. Using this we define the combined Markov jump process as \\[ X(t)=\\{X_r(t),X_b(t)\\}_{t\\ge 0}\\in E_r\\times E_b. \\] The processes \\(X_b\\) and \\(X_r\\) may or may not be independent, and the payment processes likewise may or may not be independent of \\(X_r\\). In the independent case the processes \\(X_b\\) and \\(X_r\\) are defined on each their state–space, and the common state–space will be the product set of the two. If the processes are sharing states, with the possibility of having simultaneous jumps, then we obtain dependency of the processes. Such a case could, e.g. be a rise in the interest rate causing an increased intensity of jumping to surrender or free–policy states. In the case where we have independence we have that the transition intensities of \\(X\\) is given by \\[ \\mathbf \\Lambda(t)=\\mathbf \\Lambda_b(t)\\oplus \\mathbf\\Lambda_r(t)=\\mathbf \\Lambda_b(t)\\otimes \\mathbf I_p+\\mathbf I_p \\otimes \\mathbf \\Lambda_r(t)=\\{\\lambda_{ij}\\}_{i,j=1,...,pq}, \\] where \\(\\oplus\\) is the Kronecker sum and \\(\\otimes\\) is the Kronecker product given by \\[ \\mathbf A\\otimes \\mathbf B=\\{a_{ij}\\mathbf B\\}=\\begin{bmatrix} a_{1,1}\\mathbf B &amp; \\cdots &amp; a_{1,n}\\mathbf B\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{n,1}\\mathbf B &amp; \\cdots &amp; a_{n,n}\\mathbf B \\end{bmatrix}. \\] 4.5.5 Reserves We now consider the valuation of the payment process \\(B\\). Introduce the matrix of partial state–wise prospective reserves, \\[\\begin{align*} \\mathbf V(s,t)&amp;=\\{V_{ij}(s,t)\\}_{i,j\\in E_b},\\\\ V_{ij}(s,t)&amp;=\\mathbb E\\left[\\left.1\\{X(t)=j\\}\\int_s^te^{-\\int_s^xr_{X(u)}(u)\\ du}\\ dB(x) \\ \\right\\vert\\ X(s)=i\\right]. \\end{align*}\\] From the Markov-jump representation we recall that we defines \\(d_{ij}(s,t)\\) by \\[ d_{ij}(s,t)=\\mathbb E\\left[\\left.1\\{X(t)=j\\}\\exp\\left\\{-\\int_s^tr_{X(u)}(u)\\ du\\right\\} \\ \\right\\vert\\ X(s)=i\\right], \\] and that the matrix \\(\\mathbf D(s,t)=\\{d_{ij}(s,t)\\}\\) has representation \\[ \\mathbf D(s,t)=\\prod_s^t\\Big(\\mathbf I+[\\mathbf \\Lambda(u)-\\mathbf\\Delta (r(u))]\\ du\\Big). \\] Using this we have the theorem. Theorem 5.10. (Bladt) The matrix of partial state-wise prospective reserves \\(\\mathbf V(s,t)\\) has the following integral representation \\[ \\mathbf V(s,t)=\\int_s^t\\mathbf D(s,x)\\mathbf R(x)\\mathbf P(x,t)\\ dx. \\] Using Van loans formula we can avoid the integration and calculate the product integral from below. Corollary 5.11. (Bladt) The matrix of partial state-wise prospective reserves \\(\\mathbf V(s,t)\\) has the following product integral representation \\[ \\prod_s^t\\left(\\mathbf I+ \\begin{pmatrix} \\mathbf \\Lambda(u)-\\mathbf \\Delta (r(u)) &amp; \\mathbf R(u)\\\\ \\mathbf 0 &amp; \\mathbf \\Lambda(u) \\end{pmatrix} \\right)=\\begin{pmatrix} \\mathbf D(s,t) &amp; \\mathbf V(s,t)\\\\ \\mathbf 0 &amp; \\mathbf P(s,t) \\end{pmatrix}. \\] Finally, we state and prove Thiele’s differential equations for partial reserves with stochastic interest rates. Theorem 5.12. (Bladt) The state-wise reserves satisfies the differential equations \\[ \\frac{\\partial}{\\partial s}\\mathbf V(s,t)=-[\\mathbf \\Lambda(s) - \\mathbf\\Delta(r(s))]\\mathbf V(s,t) - \\mathbf R(s)\\mathbf P(s,t) \\] where \\(\\mathbf V(t,t)=0\\). For the conventional state-wise prospective reserves \\(\\mathbf V^{Th}(t)=\\mathbf V(t,T)\\mathbf e\\), this has the form \\[ \\frac{\\partial}{\\partial s}\\mathbf V^{Th}(t)=-[\\mathbf \\Lambda(t) - \\mathbf\\Delta(r(t)]\\mathbf V^{Th}(t) - \\mathbf R(t)\\mathbf e, \\] where \\(\\mathbf V^{Th}(T)=\\mathbf 0\\). 4.5.6 Higher order moments Consider the matrix of partial state-wise higher order moments of future payments, given by, for \\(k\\in\\mathbb N\\), \\[\\begin{align*} \\mathbf V^{(k)}(t,T)&amp;=\\Big\\{V_{ij}^{(k)}(t,T)\\Big\\}_{i,j\\in E}\\\\ V_{ij}^{(k)}(t,T)&amp;=\\mathbb E\\left[\\left.1\\{X(T)=j\\}\\left(\\int_s^te^{-\\int_s^xr_{X(u)}(u)\\ du}\\ dB(x)\\right)^k \\ \\right\\vert\\ X(s)=i\\right], \\end{align*}\\] and introduce what we shall term the reduced partial state-wise higher order moments: \\[ \\mathbf V_r^{(k)}(t,T)=\\frac{\\mathbf V^{(k)}(t,T)}{k!} \\] and denoting \\[ \\mathbf D^{(m)}(s,t)=\\prod_s^t\\Big(\\mathbf I + [\\mathbf\\Lambda(u)-m\\mathbf\\Delta(r(u))]\\ du\\Big),\\qquad m\\in\\mathbb N, \\] we then obtain the following version of Hattendorff’s theorem for partial reserves with stochastic interest rate. Theorem 5.13. (Bladt) The matrix of reduced partial state-wise higher order moments satisfies the integral equation, for \\(k\\in\\mathbb N_0\\), \\[\\begin{align*} \\mathbf V_r^{(k)}&amp;(t,T)\\\\ &amp;=\\int_t^T\\mathbf D^{(k)}(t,x)\\mathbf R(x)\\mathbf V^{(k-1)}_r(x,T)\\ dx+\\sum_{m=2}^k\\int_t^T\\mathbf D^{(k)}(t,x)\\mathbf C^{(m)}_r(x)\\mathbf V^{(k-m)}_r(x,T)\\ dx. \\end{align*}\\] Using Van loan we can define the following matrix \\[ \\mathbf F_U^{(k)}(x)= \\begin{bmatrix} \\mathbf \\Lambda(x)-k\\mathbf \\Delta(r(x)) &amp; \\mathbf R(x) &amp; \\mathbf C^{(2)}_r(x) &amp; \\cdots &amp; \\mathbf C_r^{(k-1)}(x) &amp; \\mathbf C_r^{(k)}(x)\\\\ \\mathbf0 &amp; \\mathbf \\Lambda(x)-(k-1)\\mathbf \\Delta(r(x)) &amp; \\mathbf R(x) &amp; \\cdots &amp; \\mathbf C_r^{(k-2)}(x) &amp; \\mathbf C_r^{(k-1)}(x)\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots\\\\ \\mathbf 0 &amp; \\mathbf 0 &amp; \\mathbf 0 &amp; \\cdots &amp; \\mathbf \\Lambda(x)-\\mathbf \\Delta(r(x)) &amp; \\mathbf R(x)\\\\ \\mathbf 0 &amp; \\mathbf 0 &amp; \\mathbf 0 &amp; \\cdots &amp; \\mathbf 0 &amp; \\mathbf \\Lambda(x) \\end{bmatrix}, \\] then we get the usefull product integral \\[ \\prod_t^T\\Big(\\mathbf I + \\mathbf F_U^{(k)}(x)\\ dx\\Big)= \\begin{bmatrix} * &amp; * &amp; * &amp; * &amp; \\cdots &amp;* &amp; \\mathbf V_r^{(k)}(t)\\\\ * &amp; * &amp; * &amp; * &amp; \\cdots &amp;* &amp; \\mathbf V_r^{(k-1)}(t)\\\\ * &amp; * &amp; * &amp; * &amp; \\cdots &amp;* &amp; \\mathbf V_r^{(k-2)}(t)\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp;\\vdots &amp; \\vdots\\\\ * &amp; * &amp; * &amp; * &amp; \\cdots &amp;* &amp; \\mathbf V_r^{(2)}(t)\\\\ * &amp; * &amp; * &amp; * &amp; \\cdots &amp;* &amp; \\mathbf V(t)\\\\ * &amp; * &amp; * &amp; * &amp; \\cdots &amp;* &amp; \\mathbf P(t,T) \\end{bmatrix}. \\] Then by differentiating we obtain the classical differential equations below. Theorem 5.14. (Bladt) The matrix of reduced partial state-wise higher order moments satisfies the system of differential equations, for \\(k\\in \\mathbb N_0\\), \\[ \\frac{\\partial}{\\partial t}\\mathbf V_r^{(k)}(t,T)=\\Big(k\\mathbf\\Delta(r(t)) - \\mathbf \\Lambda(t)\\Big)\\mathbf V_r^{(k)}(t,T)-\\mathbf R(t)\\mathbf V_r^{(k-1)}(t,T)-\\sum_{i=2}^k\\mathbf C_r^{(i)}(t)\\mathbf V_r^{(k-i)}(t,T), \\] with terminal condition \\(\\mathbf V_r^{(k)}(T,T)=1_{(k=0)}\\mathbf I\\). 4.5.7 Equivalence premium We assume that a policy has payments determined and fixed and that we want to adjust one or more payments such that \\(V^{Th}\\) satisfies the equivalence principle i.e. \\(V_{X(0)}^{Th}(0)=0\\). To this define \\(\\mathbf R(t)=\\mathbf R(t;\\theta)\\) for some payment \\(\\theta\\). Notice that \\(\\theta\\) is rather general and may be thought of as either a single transition payment or payment rate. This in particular gives the mapping \\(\\theta \\mapsto \\mathbf V(t;\\theta)\\) given by \\[ \\mathbf V(t;\\theta) = \\int_t^T\\prod_t^u\\Big(\\mathbf I +[\\mathbf\\Lambda(s)-\\mathbf\\Delta(r(s))]\\ ds\\Big)\\mathbf R(u;\\theta)\\prod_u^T\\Big(\\mathbf I +\\mathbf \\Lambda(s)\\ ds\\Big)\\ du, \\] hence if \\(\\mathbf R(t;\\theta)\\) is continuously differentiable and \\(\\mathbf \\Lambda\\) and \\(\\mathbf r\\) are continuous we have \\[ \\frac{\\partial}{\\partial \\theta}\\mathbf V(t;\\theta) = \\int_t^T\\prod_t^u\\Big(\\mathbf I +[\\mathbf\\Lambda(s)-\\mathbf\\Delta(r(s))]\\ ds\\Big)\\frac{\\partial}{\\partial \\theta}\\mathbf R(u;\\theta)\\prod_u^T\\Big(\\mathbf I +\\mathbf \\Lambda(s)\\ ds\\Big)\\ du. \\] and then by Van loan formula \\[ \\prod_t^T\\left(\\mathbf I+ \\begin{pmatrix} \\mathbf \\Lambda(u)-\\mathbf \\Delta(r(u)) &amp; \\frac{\\partial}{\\partial \\theta}\\mathbf R(u;\\theta)\\\\ \\mathbf 0 &amp; \\mathbf \\Lambda(u) \\end{pmatrix}du \\right)= \\begin{pmatrix} \\mathbf D(t,T) &amp; \\frac{\\partial}{\\partial \\theta}\\mathbf V(t;\\theta)\\\\ \\mathbf 0 &amp; \\mathbf P(t,T) \\end{pmatrix}. \\] Using this framework we can make establish the pricing corollary below. Corollary. (Equivalence premium) Assume that \\(\\mathbf R(t)=\\mathbf R(t;\\theta)\\) for \\(\\theta\\in\\Theta\\). Let \\(\\tilde\\theta\\) b the equivalence premium i.e. \\(\\tilde \\theta\\) solves \\(V^{Th}_{X(0)}(0;\\tilde\\theta)=0\\), then the sequence \\(\\{\\theta_n\\}_{n\\in \\mathbb N_0}\\) defined by \\[ \\theta_0\\in\\Theta,\\qquad \\theta_{n+1}=\\theta_{n}-\\frac{\\mathbf e_{X(0)}^\\top\\mathbf V(0;\\theta_n)\\mathbf e}{\\mathbf e_{X(0)}^\\top\\mathbf V_\\theta(0;\\theta_n)\\mathbf e}, \\] converges to \\(\\tilde\\theta\\). In the above \\(\\mathbf V_\\theta(0;\\theta)\\) denote the first derivative in \\(\\theta\\). In case \\(V^{Th}_{X(0)}(0;\\tilde\\theta)\\) is linear in \\(\\theta\\) then the above converges for \\(n=1\\) and \\[ \\tilde\\theta = -\\frac{\\mathbf e_{X(0)}^\\top\\mathbf V(0;0)\\mathbf e}{\\mathbf e_{X(0)}^\\top\\mathbf V_\\theta(0;0)\\mathbf e}. \\] "],["financial-mathematics-in-life-insurance.html", "4.6 Financial Mathematics in Life Insurance", " 4.6 Financial Mathematics in Life Insurance 4.6.1 Background and Simple Claims We consider a Black-Scholes market consisting of two assets \\((S^0,S)\\) with dynamics \\[\\begin{align*} dS^0(t)&amp;=r(t)S^0(t)\\ dt,\\\\ dS(t)&amp;=\\mu(t)S(t)\\ dt+\\sigma(t) S(t)\\ dW(t)\\\\ &amp;=r(t)S(t)\\ dt + \\sigma(t)S(t)\\ dW^{\\mathbb Q}(t), \\end{align*}\\] for adapted processes \\(r\\), \\(\\mu\\) and \\(\\sigma\\). Note that we for now consider the deterministic and constant case with \\[ r(t)=r,\\quad\\mu(t)=\\mu, \\quad\\sigma(t)=\\sigma. \\] We furthermore there exist a equivalent martingale measure \\(\\mathbb Q\\sim \\mathbb P\\) which as usual is given by the Radon-Nikodym derivative in the Girsanov theorem where \\[ \\frac{d\\mathbb Q}{d\\mathbb P}(t)=\\phi(t)=\\frac{r(t)-\\mu(t)}{\\sigma}, \\] i.e. \\(W^{\\mathbb Q}\\) is a \\(\\mathbb Q\\)-martingale with \\(\\mathbb P\\)-dynamics \\[ dW^{\\mathbb Q}(t)=dW(t)-\\phi(t)\\ dt. \\] By the risk neutral valuation formula we have for the simple claim \\(\\Phi(S(T))\\) the arbitrage free price is given by \\[ \\Pi(t)=\\mathbb E^{\\mathbb Q}\\left[\\left.e^{-\\int_t^Tr(u)\\ du}\\Phi(S(T))\\ \\right\\vert\\ \\mathcal F(t)\\right] \\] remember \\(\\mathbb Q\\) is not necessarily unique if we only assume that the market is arbitrage free. If we want uniqueness the market \\((S^0,S)\\) need to be complete. This happens to hold in the Black-Scholes model even if \\(r\\), \\(\\mu\\) and \\(\\sigma\\) are not constant. 4.6.2 Payment Streams Using the notation from the above section we consider a contract that pays in accordance to the pricing function of the risky asset \\(S\\). We let \\(B\\) denote the accumulated payments on the interval \\([0,t]\\) and assume that \\(B\\) has dynamics \\[ dB(t)=b(t,S(t))\\ dt+\\Delta B(t,S(t)), \\] where \\(b(t,s)\\) denotes a continuous payment at time \\(t\\) priced according til the price of the asset: \\(s\\). Likewise, \\(\\Delta B(t,s)\\) is lump sums payed at time \\(t\\) according to the price \\(s\\). We know that \\(\\mathbb Q\\) exist and is unique in the Black-Scholes model so we may define the prospective reserve by \\[ V(t,s)=\\mathbb E^{\\mathbb Q}\\left[\\left.\\int_t^Te^{-\\int_t^vr(u)\\ du}dB(v)\\ \\right\\vert\\ S(t)=s\\right]=\\mathbb E^{\\mathbb Q}\\left[\\left.\\int_t^Te^{-(v-t)r}dB(v)\\ \\right\\vert\\ S(t)=s\\right]. \\] Notice that we can decompose into discrete and continuous payments by \\[\\begin{align*} V(t,s)&amp;=\\mathbb E^{\\mathbb Q}\\left[\\left.\\int_t^Te^{-(v-t)r}b(v,S(v))\\ dv\\ \\right\\vert\\ S(t)=s\\right]\\\\ &amp;+\\mathbb E^{\\mathbb Q}\\left[\\left.\\sum_{t&lt; v\\le T}e^{-(v-t)r}\\Delta B(v,S(v))\\ dv\\ \\right\\vert\\ S(t)=s\\right], \\end{align*}\\] where the sum is over all lump sums in the interval \\((t,T]\\). We could construct a collection of arrival times, but these will be stochastic anyways. Since the reserve is prices as a contingent claim we have the Black-Scholes PDE in the following proposition. Proposition 2.1. (Asmussen) The reserve \\(V(t,s)\\) fulfills the PDE on \\(t\\in\\{\\Delta B(t,s)=0\\}\\): \\[\\begin{align*} V_t(t,s)&amp;=rV(t,s)-b(t,s)-V_s(t,s)rs-\\frac{1}{2}V_{ss}(t,s)\\sigma^2s^2,\\\\ V(T,s)&amp;=0. \\end{align*}\\] On the events \\(t\\in \\{\\Delta B(t,s)\\ne 0\\}\\) we have the jumps \\[ V(t-,s)=V(t,s)+\\Delta B(t,s). \\] Proof. Consider the martingale \\[ m(t)=\\mathbb E^{\\mathbb Q}\\left[\\left. \\int_0^Te^{-ru}\\ dB(u)\\ \\right\\vert\\ \\mathcal F(t)\\right]. \\] Then we can decompose into \\[\\begin{align*} m(t)&amp;=\\int_0^te^{-ru}\\ dB(u)+\\mathbb E^{\\mathbb Q}\\left[\\left. \\int_t^Te^{-ru}\\ dB(u)\\ \\right\\vert\\ \\mathcal F(t)\\right]\\\\ &amp;=\\int_0^te^{-ru}\\ dB(u)+e^{-rt}V(t,S(t)) \\end{align*}\\] Then we can derive the dynamics by Ito’s formula. \\[\\begin{align*} e^{rt}dm(t)&amp;=dB(t)+e^{rt}d\\Big(e^{-rt}V(t,S(t))\\Big)\\\\ &amp;=b(t,S(t))\\ dt+\\Delta B(t,S(t))-rV(t,S(t))\\ dt\\\\ &amp;+V_t(t,S(t))\\ dt+V_y(t,S(t))\\ dS(t)+\\frac{1}{2}V_{yy}(t,S(t))\\big(dS(t)\\big)^2\\\\ &amp;+V(t,S(t))-V(t-,S(t))\\\\ &amp;=b(t,S(t))\\ dt+\\Delta B(t,S(t))-rV(t,S(t))\\ dt\\\\ &amp;+V_t(t,S(t))\\ dt+V_y(t,S(t))rS(t)\\ dt + V_y(t,S(t))\\sigma S(t)\\ dW^\\mathbb Q(t)\\\\ &amp;+\\frac{1}{2}V_{yy}(t,S(t))\\sigma^2S^2(t)\\ dt\\\\ &amp;+V(t,S(t))-V(t-,S(t)) \\end{align*}\\] Then since \\(m\\) is a martingale we must have a zero drift i.e. \\[\\begin{align*} 0&amp;=b(t,S(t))-rV(t,S(t))\\\\ &amp;+V_t(t,S(t))+V_y(t,S(t))rS(t)\\\\ &amp;+\\frac{1}{2}V_{yy}(t,S(t))\\sigma^2S^2(t) \\end{align*}\\] Hence we have \\[ V_t(t,S(t))=rV(t,S(t))-b(t,S(t))-V_y(t,S(t))rS(t)-\\frac{1}{2}V_{yy}(t,S(t))\\sigma^2S^2(t) \\] and on discontinuity points \\[ V(t-,S(t))=V(t,S(t))+\\Delta B(t,S(t)) \\] as desired. \\(\\blacksquare\\) We also have the reverse statement: Proposition 2.2. (Asmussen) Assume that a function \\(V\\) exist that solve proposition 2.1’s PDE with the gluing condition, then this functions defines the reserve. From finance mathematics we certainly can find a hedging strategy for this portfolio. This is simply done by choosing the portfolio \\(h(t)=(h^0(t),h^1(t))\\) in accordance to the scheme \\[ h^1(t)=V_s(t,S(t)),\\qquad h^0(t)=\\frac{V(t,S(t))-V_s(t,S(t))S(t)}{S^0(t)}. \\] Normally we would adjust the model such that \\(S^0(t)=1\\) (by dividing by \\(S^0\\)) and so we see we simply hold the amount \\(V(t,S(t))-h^1(t)S(t)\\) in the bank account/risk free asset. We can list a few common contracts used in practice: Pure endowment insurance: Consider a person aged \\(x\\) at time \\(0\\). The pure endowment insurance product pays a single payment \\(S(t)\\) at time \\(T&gt;0\\) if and only if the insured is alive. The equivalence premium is identified as: \\[\\tilde\\theta =\\mathbb P(T_x&gt; T)S(0)\\] Term life insurance insurance: The insured heirs are payed \\(S(t)\\) in the event of death at time \\(t\\le T\\). Equivalence premium: \\[\\tilde\\theta(s) =\\int_0^T\\mathbb E^{\\mathbb Q}\\left(\\left.e^{-rT}S(t)\\ \\right\\vert\\ S(0)=s\\right)f_{T_x}(t)\\ dt=S(0)\\mathbb P(T_x\\le T)\\] Pure endowment: The insured is payed \\(N(T)S(T)\\) at time \\(T\\), though at minimum \\(G(T)\\), regardless of state. Equivalence premium: \\[\\tilde\\theta(s) =e^{-rT}\\mathbb E^{\\mathbb Q}\\left(\\left.\\max\\Big(N(T)S(T),G(T)\\Big) \\ \\right\\vert\\ S(0)=s\\right)\\] Term life insurance: The insured heirs are payed \\(N(T)S(T)\\) at time \\(T\\), though at minimum \\(G(T)\\), if insured dies at time \\(t\\le T\\). Equivalence premium: \\[\\tilde\\theta(s) =\\int_0^Te^{-rt}\\mathbb E^{\\mathbb Q}\\left(\\left.\\max\\Big(N(t)S(t),G(t)\\Big) \\ \\right\\vert\\ S(0)=s\\right)f_{T_x}(t)\\ dt\\] 4.6.3 Unit-Link Insurance Let us start with stating the Thiele differential equation and provide a proof. Theorem. (Thiele differential equation) Let \\(B\\) denote the accumulated payments with dynamics \\[\\begin{align*} dB(t)&amp;=dB^{Z(t)}(t)+\\sum_{k:k\\ne Z(t-)}b^{Z(t-)k}(t)\\ dN^k(t)\\\\ &amp;=b^{Z(t)}(t)\\ dt+\\Delta B^{Z(t)}(t)+\\sum_{k:k\\ne Z(t-)}b^{Z(t-)k}(t)\\ dN^k(t) \\end{align*}\\] where \\(Z\\) is the a Markov-jump process on finite state space \\(E=\\{1,...,p\\}\\) that governs the payments. The process \\(N\\) is defined as \\[ N^k(t)=\\#\\{Z(t-)\\ne k,Z(t)=k\\}, \\] that is counting the number of visits in the \\(k\\)th state. Assume that the interest rate \\(r\\) is deterministic and integrable. The reserve is defined as \\[ V(t)=V^{Z(t)}(t)=\\mathbb E\\left[\\left.\\int_t^Te^{-\\int_t^u r(v)\\ dv}\\ dB(u)\\ \\right\\vert\\ Z(t)\\right]. \\] Then \\(V^j\\) has representation \\[ V^j(t)=\\int_t^Te^{-\\int_t^ur(v)\\ dv}\\underbrace{\\left(\\sum_{k\\in E}p_{jk}(t,u)\\left(b^k(u)\\ du+\\Delta B^k(u)+\\sum_{l:l\\ne k}\\lambda_{kl}(u)b^{kl}(u)\\ du\\right)\\right)}_{:= dC^j(u)} \\] and follows the dynamics \\[\\begin{align*} d\\Big(V^{Z(t)}(t)\\Big)&amp;=rV^{Z(t)}(t)\\ dt-b^{Z(t)}(t)\\ dt -\\Delta B^{Z(t)}(t)\\\\ &amp;-\\sum_{k : k\\ne Z(t)}\\lambda_{Z(t)k}(t)\\underbrace{\\Big(b^{Z(t)k}(t)+V^k(t)-V^{Z(t)}(t)\\Big)}_{:=R^{Z(t)k}}\\ dt\\\\ &amp;+\\sum_{k : k\\ne Z(t-)}\\Big(V^k(t)-V^{Z(t-)}(t)\\Big)\\ dN^k(t). \\end{align*}\\] called the Thiele differential equations. Proof. Split the proof into two parts. The first part we show the representation \\[ V^j(t)=\\int_t^Te^{-\\int_t^ur(v)\\ dv}\\ dC^j(t) \\] then we give two profs for Thieles differential equation. The first is direct on the above and the other is an indirect martingale approach. “(Part i)” We assume that \\(r\\) is deterministic. This is rather important, although if stochastic we could ease the computations with assuming \\(Z\\) and \\(r\\) are independent. If \\(r\\) is deterministic it holds that \\[\\begin{align*} V^j(t)&amp;=\\int_t^Te^{-\\int_t^ur(v)\\ dv}\\mathbb E\\left[\\left.dB(u) \\ \\right\\vert\\ Z(t)= j\\right]\\\\ &amp;=\\int_t^Te^{-\\int_t^ur(v)\\ dv}\\mathbb E\\left[\\left. dB^{Z(u)}(u)+\\sum_{k:k\\ne Z(u-)}b^{Z(u-)k}(u)\\ dN^k(u) \\ \\right\\vert\\ Z(t)= j\\right]\\\\ &amp;=\\int_t^Te^{-\\int_t^ur(v)\\ dv}\\mathbb E\\left[\\left. dB^{Z(u)}(u) \\ \\right\\vert\\ Z(t)= j\\right]\\tag{1}\\\\ &amp;+\\int_t^Te^{-\\int_t^ur(v)\\ dv}\\sum_{k:k\\ne Z(u-)}\\mathbb E\\left[\\left. b^{Z(u-)k}(u)\\ dN^k(u) \\ \\right\\vert\\ Z(t)= j\\right]\\tag{2} \\end{align*}\\] where we for ease of notation use \\(dB^{Z(u)}(u)=b^{Z(u)}(u)\\ dt+\\Delta B^{Z(u)}(u)\\). Then we can work on (1) and (2) independently. For (1) we simply have \\[ \\mathbb E\\left[\\left. dB^{Z(u)}(u) \\ \\right\\vert\\ Z(t)= j\\right]=\\sum_{k\\in E}p_{jk}(t,u)d B^k(u), \\] since we need only the transition \\(j\\to u\\) on \\([t,u]\\) which happens with probability \\(p_{jk}(t,u)\\) if \\(Z(t)=j\\). For (2) we do an additional conditioning and obtain \\[\\begin{align*} \\sum_{k:k\\ne Z(u-)}\\mathbb E&amp;\\left[\\left. b^{Z(u-)k}(u)\\ dN^k(u) \\ \\right\\vert\\ Z(t)= j\\right]\\\\ &amp;=\\mathbb E\\left[\\left.\\mathbb E\\left[\\left.\\sum_{k:k\\ne Z(u-)} b^{Z(u-)k}(u)\\ dN^k(u) \\ \\right\\vert\\ Z(t)= j,Z(u-)\\right]\\ \\right\\vert\\ Z(t)= j\\right]\\\\ &amp;\\stackrel{(\\dagger_1)}{=}\\mathbb E\\left[\\left.\\sum_{k:k\\ne Z(u-)} b^{Z(u-)k}(u)\\mathbb E\\left[\\left. dN^k(u) \\ \\right\\vert\\ Z(t)= j,Z(u-)\\right]\\ \\right\\vert\\ Z(t)= j\\right]\\\\ &amp;\\stackrel{(\\dagger_2)}{=}\\mathbb E\\left[\\left.\\sum_{k:k\\ne Z(u-)} b^{Z(u-)k}(u)\\lambda_{Z(u-)k}(u)\\ du\\ \\right\\vert\\ Z(t)= j\\right]\\\\ &amp;=\\sum_{\\ell\\in E}\\sum_{k:k\\ne \\ell}\\mathbb E\\left[\\left.1\\{Z(u-)=\\ell\\} b^{\\ell k}(u)\\lambda_{\\ell k}(u)\\ du\\ \\right\\vert\\ Z(t)= j\\right]\\\\ &amp;=\\sum_{\\ell\\in E}\\sum_{k:k\\ne \\ell} p_{j\\ell}(t,u) b^{\\ell k}(u)\\lambda_{\\ell k}(u)\\ du\\\\ &amp;=\\sum_{\\ell\\in E}p_{j\\ell}(t,u)\\sum_{k:k\\ne \\ell} b^{\\ell k}(u)\\lambda_{\\ell k}(u)\\ du\\\\ &amp;=\\sum_{k\\in E}p_{jk}(t,u)\\sum_{\\ell :\\ell \\ne k} b^{k\\ell }(u)\\lambda_{k\\ell }(u)\\ du \\end{align*}\\] where we in \\((\\dagger_1)\\) used that \\(b^{Z(u-)k}(u)\\) is \\(Z(u-)\\)-measurable, in \\((\\dagger_2)\\) we used that \\(N^k\\) has predictable compensator \\(\\lambda_k\\) \\[\\begin{align*} N^k(t)=M^k(t)+\\int_0^t\\lambda_k(s)\\ ds\\Rightarrow dN^k(t)=dM^k(t)+\\lambda_k(t)\\ dt \\end{align*}\\] hence \\[ \\mathbb E[dN^k(t) \\ \\vert\\ Z(t-)]=\\lambda_{Z(t-)k}(t)\\ dt. \\] The last step is simply a re indexing wich we use in the below \\[\\begin{align*} V^j(t)&amp;=\\int_t^Te^{-\\int_t^ur(v)\\ dv}\\sum_{k\\in E}p_{jk}(t,u)d B^k(u)\\\\ &amp;+\\int_t^Te^{-\\int_t^ur(v)\\ dv}\\sum_{k\\in E}p_{jk}(t,u)\\sum_{\\ell :\\ell \\ne k} b^{k\\ell }(u)\\lambda_{k\\ell }(u)\\ du\\\\ &amp;=\\int_t^Te^{-\\int_t^ur(v)\\ dv}\\sum_{k\\in E}p_{jk}(t,u)\\left(d B^k(u)+\\sum_{\\ell :\\ell \\ne k} b^{k\\ell }(u)\\lambda_{k\\ell }(u)\\ du\\right) \\end{align*}\\] which gives the desired result. “Part 2” We proceed as follows. By the change of variables formula the dynamics in general is given by \\[ d\\Big(V^{Z(t)}(t)\\Big)=V_t^{Z(t)}(t)\\ dt+\\sum_{k: k\\ne Z(t)}\\Big(V^k(t)-V^{Z(t-)}(t)\\Big)\\ dN^k(t) \\] where \\[ \\frac{d}{dt}V^{Z(t)}(t)=V_t^{Z(t)}(t). \\] Hence we want to study the \\(V_t^{Z(t)}(t)\\) term. We give a proof of the structure of this term with two approaches. “Part 2.a” (Direct approach) Using the above we have \\[\\begin{align*} V_t^{Z(t)}(t)&amp;=\\frac{d}{dt}\\int_t^Te^{-\\int_t^ur(v)\\ dv}\\sum_{k\\in E}p_{jk}(t,u)\\left(d B^k(u)+\\sum_{\\ell :\\ell \\ne k} b^{k\\ell }(u)\\lambda_{k\\ell }(u)\\ du\\right)\\\\ &amp;=\\frac{d}{dt}e^{\\int_0^tr(v)\\ dv}\\int_t^Te^{-\\int_0^ur(v)\\ dv}\\sum_{k\\in E}p_{jk}(t,u)\\left(d B^k(u)+\\sum_{\\ell :\\ell \\ne k} b^{k\\ell }(u)\\lambda_{k\\ell }(u)\\ du\\right). \\end{align*}\\] Then using the product rule we have \\[\\begin{align*} V_t^{Z(t)}(t)&amp;=r(t)e^{\\int_0^tr(v)\\ dv}\\int_t^Te^{-\\int_0^ur(v)\\ dv}\\sum_{k\\in E}p_{jk}(t,u)\\left(d B^k(u)+\\sum_{\\ell :\\ell \\ne k} b^{k\\ell }(u)\\lambda_{k\\ell }(u)\\ du\\right)\\\\ &amp;+e^{\\int_0^tr(v)\\ dv}\\left(\\sum_{k\\in E}\\frac{d}{dt}\\left(\\int_t^Te^{-\\int_0^ur(v)\\ dv}p_{jk}(t,u)\\left(d B^k(u)+\\sum_{\\ell :\\ell \\ne k} b^{k\\ell }(u)\\lambda_{k\\ell }(u)\\ du\\right)\\right)\\right)\\\\ &amp;=r(t)V^{Z(t)}(t)\\\\ &amp;-e^{\\int_0^tr(v)\\ dv}\\sum_{k\\in E}e^{-\\int_0^tr(v)\\ dv}p_{jk}(t,t)\\left(b^k(t)+\\sum_{\\ell :\\ell \\ne k} b^{k\\ell }(t)\\lambda_{k\\ell }(t)\\right)\\\\ &amp;+e^{\\int_0^tr(v)\\ dv}\\sum_{k\\in E}\\int_t^T\\frac{d}{dt}e^{-\\int_0^ur(v)\\ dv}p_{jk}(t,u)\\left(d B^k(u)+\\sum_{\\ell :\\ell \\ne k} b^{k\\ell }(u)\\lambda_{k\\ell }(u)\\ du\\right)\\\\ &amp;=r(t)V^{Z(t)}(t)-b^j(t)-\\sum_{k :k \\ne j} b^{jk}(t)\\lambda_{jk }(t)\\\\ &amp;+\\sum_{k\\in E}\\int_t^Te^{-\\int_t^ur(v)\\ dv}\\frac{d}{dt}p_{jk}(t,u)\\left(d B^k(u)+\\sum_{\\ell :\\ell \\ne k} b^{k\\ell }(u)\\lambda_{k\\ell }(u)\\ du\\right)\\\\ &amp;=r(t)V^{Z(t)}(t)-b^j(t)-\\sum_{k :k \\ne j} b^{jk}(t)\\lambda_{jk }(t)\\\\ &amp;+\\sum_{k\\in E}\\int_t^Te^{-\\int_t^ur(v)\\ dv}\\left(\\lambda_j(t)p_{jk}(t,u)-\\sum_{i:i\\ne k}\\lambda_{ji}(t)p_{ik}(t,u)\\right)\\\\ &amp;\\left(d B^k(u)+\\sum_{\\ell :\\ell \\ne k} b^{k\\ell }(u)\\lambda_{k\\ell }(u)\\ du\\right)\\\\ &amp;=r(t)V^{Z(t)}(t)-b^j(t)-\\sum_{k :k \\ne j} b^{jk}(t)\\lambda_{jk }(t)\\\\ &amp;+\\sum_{k\\in E}\\int_t^Te^{-\\int_t^ur(v)\\ dv}\\lambda_j(t)p_{jk}(t,u)\\left(d B^k(u)+\\sum_{\\ell :\\ell \\ne k} b^{k\\ell }(u)\\lambda_{k\\ell }(u)\\ du\\right)\\\\ &amp;-\\sum_{k\\in E}\\int_t^Te^{-\\int_t^ur(v)\\ dv}\\sum_{i:i\\ne k}\\lambda_{ji}(t)p_{ik}(t,u)\\left(d B^k(u)+\\sum_{\\ell :\\ell \\ne k} b^{k\\ell }(u)\\lambda_{k\\ell }(u)\\ du\\right) \\end{align*}\\] using that \\[ \\frac{d}{dt}\\int_t^s g(t,u)\\ du=-g(t,t)+\\int_t^s\\frac{d}{dt}g(t,u)\\ du \\] and Kolmogorov differential equation \\[ \\frac{d}{dt}p_{jk}(t,u)=\\lambda_{j}(t)p_{jk}(t,u)-\\sum_{i:i\\ne k}\\lambda_{ji}(t)p_{ik}(t,u). \\] We notice that the second term is \\(\\lambda_jV^j(t)\\) and the second is a sum over \\(\\lambda_{ji}(t)V^i(t)\\) hence \\[\\begin{align*} V_t^{Z(t)}(t)&amp;=r(t)V^{Z(t)}(t)-b^j(t)-\\sum_{k :k \\ne j} b^{jk}(t)\\lambda_{jk }(t)\\\\ &amp;+\\lambda_j(t)V^j(t)-\\sum_{i:i\\ne j}\\lambda_{ji}(t)V^i(t)\\\\ &amp;=r(t)V^{Z(t)}(t)-b^j(t)-\\sum_{k :k \\ne j} b^{jk}(t)\\lambda_{jk }(t)\\\\ &amp;+\\sum_{k:k\\ne j}\\lambda_{jk}(t)V^j(t)-\\lambda_{jk}(t)V^k(t)\\\\ &amp;=r(t)V^{Z(t)}(t)-b^j(t)-\\sum_{k :k \\ne j}\\lambda_{jk }(t)\\Big(b^{jk}(t)+V^k(t)-V^j(t)\\Big) \\end{align*}\\] as desired. “Part 2.b” (Martingale approach) Consider the Doob-meyer martingale \\[\\begin{align*} m(t)&amp;=\\mathbb E\\left[\\left.\\int_0^Te^{-\\int_0^ur(v)\\ dv}dB(u)\\ \\right\\vert\\ \\mathcal F(t)\\right]\\\\ &amp;=\\mathbb E\\left[\\left.\\int_0^Te^{-\\int_0^ur(v)\\ dv}dB(u)\\ \\right\\vert\\ Z(t)\\right] \\end{align*}\\] We know that if \\(m\\) is a martingale it must have zero drift term. We have at time \\(t\\) the decomposition \\[\\begin{align*} m(t)&amp;=\\int_0^te^{-\\int_0^ur(v)\\ dv}dB(u)+\\mathbb E\\left[\\left.\\int_t^Te^{-\\int_0^ur(v)\\ dv}dB(u)\\ \\right\\vert\\ Z(t)\\right], \\end{align*}\\] since the process \\(B\\) is known on \\([0,t]\\). Notice that the above is \\[\\begin{align*} m(t)&amp;=\\int_0^te^{-\\int_0^ur(v)\\ dv}dB(u)+e^{-\\int_0^tr(v)\\ dv}\\mathbb E\\left[\\left.\\int_t^Te^{-\\int_t^ur(v)\\ dv}dB(u)\\ \\right\\vert\\ Z(t)\\right]\\\\ &amp;=\\int_0^te^{-\\int_0^ur(v)\\ dv}dB(u)+e^{-\\int_0^tr(v)\\ dv}V^{Z(t)}(t) \\end{align*}\\] On the one hand side we have \\[\\begin{align*} d\\left(e^{-\\int_0^tr(v)\\ dv}V^{Z(t)}(t)\\right)&amp;=-r(t)e^{-\\int_0^tr(v)\\ dv}V^{Z(t)}(t)\\ dt+e^{-\\int_0^tr(v)\\ dv}d\\Big(V^{Z(t)}(t)\\Big)\\\\ &amp;=-r(t)e^{-\\int_0^tr(v)\\ dv}V^{Z(t)}(t)\\ dt\\\\ &amp;+e^{-\\int_0^tr(v)\\ dv}\\Big(V_t^{Z(t)}(t)\\ dt+\\Delta B^{Z(t)}(t)+V^{Z(t)}(t)-V^{Z(t-)}(t)\\Big)\\\\ &amp;=e^{-\\int_0^tr(v)\\ dv}\\left(-r(t)V^{Z(t)}(t)+V_t^{Z(t)}(t)\\right)\\ dt\\\\ &amp;+e^{-\\int_0^tr(v)\\ dv}\\left(\\Delta B^{Z(t)}(t)+\\sum_{k: k\\ne Z(t-)}\\left(V^{k}(t)-V^{Z(t-)}(t)\\right)\\ dN^k(t)\\right)\\\\ &amp;=e^{-\\int_0^tr(v)\\ dv}\\left(-r(t)V^{Z(t)}(t)+V_t^{Z(t)}(t)+\\sum_{k: k\\ne Z(t-)}\\left(V^{k}(t)-V^{Z(t-)}(t)\\right)\\ \\lambda_{jk}(t)\\right)\\ dt\\\\ &amp;+e^{-\\int_0^tr(v)\\ dv}\\left(\\Delta B^{Z(t)}(t)+\\sum_{k: k\\ne Z(t-)}\\left(V^{k}(t)-V^{Z(t-)}(t)\\right)\\ dM^k(t)\\right) \\end{align*}\\] and the first time we have \\[\\begin{align*} d\\left(\\int_0^te^{-\\int_0^ur(v)\\ dv}dB(u)\\right)&amp;=e^{-\\int_0^tr(v)\\ dv}dB(t)\\\\ &amp;=e^{-\\int_0^tr(v)\\ dv}\\left(b^{Z(t)}(t)\\ dt + \\Delta B^{Z(t)}(t)+\\sum_{k:k\\ne Z(t-)}b^{Z(t-)k}(t)\\ dN^k(t)\\right)\\\\ &amp;=e^{-\\int_0^tr(v)\\ dv}\\left(b^{Z(t)}(t)\\ +\\sum_{k:k\\ne Z(t-)}b^{Z(t-)k}(t)\\lambda_{Z(t-)k}(t)\\right)\\ dt\\\\ &amp;+e^{-\\int_0^tr(v)\\ dv}\\left( \\Delta B^{Z(t)}(t)+\\sum_{k:k\\ne Z(t-)}b^{Z(t-)k}(t)\\ dM^k(t)\\right) \\end{align*}\\] by using the compensated version of \\(N^k(t)\\) that is \\(M^k(t)+\\int_0^t\\lambda_{k}(t)\\ dt\\). Then cobining we get \\[\\begin{align*} \\frac{d}{dt}m(t)&amp;=e^{-\\int_0^tr(v)\\ dv}\\Big(-r(t)V^{Z(t)}(t)+V_t^{Z(t)}(t)+b^{Z(t)}(t)\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}b^{Z(t-)k}(t)\\lambda_{Z(t-)k}(t)\\\\ &amp;+\\sum_{k: k\\ne Z(t-)}\\left(V^{k}(t)-V^{Z(t-)}(t)\\right)\\lambda_{jk}(t)\\Big)\\ dt \\end{align*}\\] Hence \\(m\\) is a martingale if and only if \\[\\begin{align*} 0&amp;=-r(t)V^{Z(t)}(t)+V_t^{Z(t)}(t)+b^{Z(t)}(t)\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}b^{Z(t-)k}(t)\\lambda_{Z(t-)k}(t)\\\\ &amp;+\\sum_{k: k\\ne Z(t-)}\\left(V^{k}(t)-V^{Z(t-)}(t)\\right)\\lambda_{jk}(t) \\end{align*}\\] giving that \\[\\begin{align*} V_t^{Z(t)}(t)&amp;=r(t)V^{Z(t)}(t)-b^{Z(t)}(t)\\\\ &amp;-\\sum_{k:k\\ne Z(t-)}b^{Z(t-)k}(t)\\lambda_{Z(t-)k}(t)\\\\ &amp;-\\sum_{k: k\\ne Z(t-)}\\left(V^{k}(t)-V^{Z(t-)}(t)\\right)\\lambda_{jk}(t)\\\\ &amp;=r(t)V^{Z(t)}(t)-b^{Z(t)}(t)\\\\ &amp;-\\sum_{k: k\\ne Z(t-)}\\lambda_{jk}(t)\\left(b^{Z(t-)k}(t)+V^{k}(t)-V^{Z(t-)}(t)\\right) \\end{align*}\\] as desired. \\(\\blacksquare\\) We form the following payment stream for the reserve for an insurance contract. \\[\\begin{align*} dA(t)&amp;=dA^{Z(t)}(t)+\\sum_{k:k\\ne Z(t-)}a^{Z(t-)k}(t)d N^k(t),\\\\ dA^{Z(t)}&amp;=a^{Z(t)}(t)\\ dt+\\Delta A^{Z(t)}(t), \\end{align*}\\] where \\(Z(t)\\) is the state of the insured at time \\(t\\) on the state space \\(E=\\{1,...,p\\}\\). For now ignore the use of \\(A\\) instead of \\(B\\), we will soon fix this. The dynamics of the reserve is then given by \\[\\begin{align*} dV^{Z(t)}(t)&amp;=rV^{Z(t)}(t)\\ dt-dA^{Z(t)}(t)-\\sum_{k:k\\ne Z(t-)}\\lambda_{Z(t)k}(t) R^{Z(t)k}(t)\\ dt\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}\\Big(V^k(t)-V^{Z(t-)}(t)\\Big)dN^k(t),\\\\ R^{Z(t)k}(t)&amp;=a^{Z(t)k}(t)+V^{k}(t)-V^{Z(t)}, \\end{align*}\\] where we call \\(R^{ij}\\) the risk sum. By definition the reserve terminates at 0 i.e. \\(V^{Z(T)}(T-)=\\Delta A^{Z(T)}(T)\\) and so the above is a backward differential equation. If we furthermore want equivalence we would insist that the payment stream is constructed such that \\(V^{Z(0)}(0-)=0\\). We can reverse the differential equation and consider a forwards stochastic equation where we define a new reserve, namely \\(X\\), where \\(X\\) is not valued in as the expected value of the future payments, but instead is governed by an initial condition \\(X(0-)=0\\) and dynamics \\[\\begin{align*} dX(t)&amp;=rX(t)\\ dt-dA^{Z(t)}(t,X(t-))-\\sum_{k:k\\ne Z(t-)}\\lambda_{Z(t)k}(t) \\varrho^{Z(t)k}(t,X(t))\\ dt\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}\\Big(\\chi^k(t,X(t-))-X(t-)\\Big)dN^k(t),\\\\ \\varrho^{Z(t)k}&amp;=a^{Z(t)k}(t,X(t))+\\chi^k(t,X(t))-X(t) \\end{align*}\\] where we now let the payments \\(a^i\\), \\(a{ij}\\) and \\(\\Delta A^i\\) depend on the value of \\(X\\) itself. The process \\(\\chi^k\\) is the value of the process \\(X\\) upon a jump into the state \\(k\\). A traditional choice is \\(\\chi^k(t,x)=1_{(k\\ne p)}x\\) if \\(p\\) is the death state. If we also allow for investment of porportion \\(\\pi(t,x)\\) into the risky asset with dynamics given earlier we would have the following dynamics of \\(X\\). \\[\\begin{align*} dX(t)&amp;=\\Big(r+\\pi(t,X(t))(\\mu(t)-r)\\Big)X(t)\\ dt+\\sigma(t)\\pi(t,X(t))X(t)\\ dW(t)\\\\ &amp;-dA^{Z(t)}(t,X(t-))-\\sum_{k:k\\ne Z(t-)}\\lambda_{Z(t)k}(t) \\varrho^{Z(t)k}(t,X(t))\\ dt\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}\\Big(\\chi^k(t,X(t-))-X(t-)\\Big)dN^k(t), \\end{align*}\\] since the investment would yield an additional \\(\\mu(t)-r\\) deterministic more profits than the bank account with an additional volatility \\(\\sigma(t)\\). We then define the payment stream the policy holder is faced with as \\[\\begin{align*} dB(t)&amp;=dB^{Z(t)}(t,X(t-))+\\sum_{k:k\\ne Z(t-)}b^{Z(t-)k}(t,X(t-))\\ dN^k(t),\\\\ dB^{Z(t)}(t,X(t-))&amp;=b^{Z(t)}(t,X(t))\\ dt + \\Delta B^{Z(t)}(t,X(t-)). \\end{align*}\\] where we see that the payments depend on the state transitions in \\(Z\\) and the state of the reserve \\(X\\) at time \\(t\\). We want to evaluate the value of the future payment to present value i.e. the integral \\[ \\int_t^Te^{-\\int_t^u r(v)\\ dv}dB(u), \\] which depends on the the process \\(Z\\) and \\(X\\), where \\(X\\) fundamentally is a process dependent on both \\(X\\) and \\(S\\). In financial markets we know how to evaluate derivative by finding an equivalent martingale measure \\(\\mathbb Q\\), but in regard to the transition risk in \\(Z\\) it is natural to adhere to the objective measure \\(\\mathbb P\\). This is why we choose to evaluate the payment stream wrt. the product measure \\(\\mathbb P\\otimes \\mathbb Q\\). Recall that we have have two probability spaces \\((\\Omega, \\mathcal F,\\mathbb P)\\) and \\((\\Omega, \\mathcal F,\\mathbb Q)\\) where we simply denote \\[ \\sigma(\\Omega\\times\\Omega)=\\mathcal F \\otimes \\mathcal F:=\\mathcal F^{\\otimes2} \\] and measure a set \\(A\\in \\mathcal F^{\\otimes2}\\) in accordance to \\(\\mathbb P\\otimes \\mathbb Q\\) simply by defining \\[\\begin{align*} \\mathbb P\\otimes \\mathbb Q(A):&amp;=\\int_\\Omega \\mathbb P(\\{\\omega_1 \\in \\Omega\\ \\vert\\ (\\omega_1,\\omega_2) \\in A\\})\\ d\\mathbb Q(\\omega_2)\\\\ &amp;=\\int_\\Omega \\mathbb Q(\\{\\omega_2 \\in \\Omega\\ \\vert\\ (\\omega_1,\\omega_2) \\in A\\})\\ d\\mathbb P(\\omega_1) \\end{align*}\\] In the case \\(A=A_1\\times A_2\\) with \\(A_1,A_2\\in \\Omega\\) then the above is \\[ \\mathbb P\\otimes \\mathbb Q(A)=\\mathbb P(A_1)\\mathbb Q(A_2)=\\mathbb P(A_2)\\mathbb Q(A_1). \\] This is not essential to understand for the discussion but we are in simple terms trying to separating the market risk and life risk. We define the reserve under this new measure: \\[ V^{Z(t)}(t,X(t))=\\mathbb E^{\\mathbb P\\otimes\\mathbb Q}\\left[\\left. \\int_t^Te^{-\\int_t^u r(v)\\ dv}\\ dB(u)\\ \\right\\vert\\ X(t),Z(t)\\right]. \\] Here the intensities \\(\\mathbf \\Lambda\\) for \\(Z\\) does not change but the dynamics of \\(S\\) is transformed by the \\(\\mathbb Q\\)-dynamics of \\(W^\\mathbb Q\\) \\[ dW^\\mathbb Q(t)=dW(t)-\\frac{r-\\mu}{\\sigma}dt. \\] The dynamics of the \\(X\\)-reserve under \\(\\mathbb P\\otimes \\mathbb Q\\) then becomes \\[\\begin{align*} dX(t)&amp;=r(t)X(t)\\ dt+\\sigma(t)\\pi(t,X(t))X(t)dW^\\mathbb Q(t)\\\\ &amp;-dA^{Z(t)}(t,X(t-))-\\sum_{k:k\\ne Z(t-)}\\mu_{Z(t)k}(t) \\varrho^{Z(t)k}(t,X(t))\\ dt\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}\\Big(\\chi^k(t,X(t-))-X(t-)\\Big)dN^k(t). \\end{align*}\\] When conditioning on \\((X(t),Z(t))=(x,j)\\) we get the family of functions \\(V^j(t,x)\\) fulfilling the below proposition. Proposition 3.3. (Asmussen) Assuming that \\(V^j(t,x)\\) is sufficiently differentiable, the following set of PDE’s is fulfilled: \\[\\begin{align*} V_t^j(t,x)&amp;=rV^j(t,x)-b^j(t,x)-\\sum_{k:k\\ne j}\\lambda_{jk}(t)R^{jk}(t,x)-\\mathcal D_x V^j(t,x),\\\\ V^t(t-,x)&amp;=\\Delta B^j(t,x)+V^j(t,x-\\Delta A^j(t,x)),\\\\ V^j(T,x)&amp;=0, \\end{align*}\\] where \\(\\mathcal D_x\\) is the operator \\[ \\mathcal D_xV^j(t,x)=V_x^j(t,x)\\left(rx-a^j(t,x)-\\sum_{k:k\\ne j}\\lambda_{jk}(t)\\varrho^{jk}(t,x)\\right)+\\frac{1}{2}V^j_{xx}(t,x)\\pi^2(t,x)\\sigma^2x^2 \\] and \\[ R^{jk}(t,x)=b^{jk}(t,x)+V^k(t,\\chi^k(t,x))-V^j(t,x). \\] Conversely, if there exist a set of functions \\(V^j(t,x)\\) such that the above holds, then \\(V^{Z(t)}(t,X(t))\\) is the reserved defined as \\[ V^{Z(t)}(t,X(t))=\\mathbb E^{\\mathbb P\\otimes\\mathbb Q}\\left[\\left. \\int_t^Te^{-\\int_t^u r(v)\\ dv}\\ dB(u)\\ \\right\\vert\\ X(t),Z(t)\\right]. \\] Proof. “Existence” Consider the martingale \\[ m(t)=\\mathbb E^{\\mathbb Q}\\left[\\left. \\int_0^Te^{-ru}\\ dB(u)\\ \\right\\vert\\ \\mathcal F(t)\\right]. \\] Then we can decompose into \\[\\begin{align*} m(t)&amp;=\\int_0^te^{-ru}\\ dB(u)+\\mathbb E^{\\mathbb Q}\\left[\\left. \\int_t^Te^{-ru}\\ dB(u)\\ \\right\\vert\\ \\mathcal F(t)\\right]\\\\ &amp;=\\int_0^te^{-ru}\\ dB(u)+e^{-rt}V^{Z(t)}(t,X(t)) \\end{align*}\\] Then we can derive the dynamics by Ito’s formula. \\[\\begin{align*} e^{rt}dm(t)&amp;=dB(t)+e^{rt}d\\Big(e^{-rt}V^{Z(t)}(t,X(t))\\Big)\\\\ &amp;=b^{Z(t)}(t,X(t-))\\ dt+\\Delta B^{Z(t)}(t,X(t-))+\\sum_{k:k\\ne Z(t-)}b^{Z(t-)k}(t,X(t-))dN^k(t)\\\\ &amp;-rV^{Z(t)}(t,X(t))\\ dt+dV^{Z(t)}(t,X(t))\\\\ &amp;=b^{Z(t)}(t,X(t-))\\ dt+\\Delta B^{Z(t)}(t,X(t-))+\\sum_{k:k\\ne Z(t-)}b^{Z(t-)k}(t,X(t-))dM^k(t)\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}b^{Z(t-)k}(t,X(t-))\\lambda_{Z(t-)k}\\ dt-rV^{Z(t)}(t,X(t))\\ dt\\\\ &amp;+V_t^{Z(t)}(t,X(t))\\ dt+V_x^{Z(t)}(t,X(t))\\ dX^c(t)+\\frac{1}{2}V_{xx}^{Z(t)}(t,X(t))\\ \\big(dX^c(t)\\big)^2\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}\\left(V^{k}(t,\\chi^k(t,X(t-)))-V^{Z(t-)}(t-,X(t-))\\right)dM^k(t)\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}\\left(V^{k}(t,\\chi^k(t,X(t-)))-V^{Z(t-)}(t-,X(t-))\\right)\\lambda_{Z(t-)k}(t)\\ dt \\end{align*}\\] By the construction of \\(X\\) we have \\[\\begin{align*} dX^c(t)&amp;=rX(t)\\ dt+\\pi(t,X(t-))\\sigma X(t)\\ dW^\\mathbb Q(t)\\\\ &amp;-a^{Z(t)}(t,X(t-))-\\sum_{k:k\\ne Z(t-)}\\lambda_{Z(t-)k}\\varrho^{Z(t-)k}(t,X(t))\\ dt\\\\ &amp;-\\sum_{k:k\\ne Z(t-)}\\lambda_{Z(t-)k}\\varrho^{Z(t-)k}(t,X(t))\\ dt\\\\ \\big(dX^c(t)\\big)^2&amp;=\\pi^2(t,X(t-))\\sigma^2 X^2(t)\\ dt \\end{align*}\\] Hence we have the \\(dt\\) terms \\[\\begin{align*} e^{rt}\\frac{d}{dt}m(t)&amp;=-rV^{Z(t)}(t,X(t))+b^{Z(t)}(t,X(t-))+\\sum_{k:k\\ne Z(t-)}b^{Z(t-)k}(t,X(t-))\\lambda_{Z(t-)k}\\\\ &amp;+V_t^{Z(t)}(t,X(t))\\\\ &amp;+V_x^{Z(t)}(t,X(t))\\left(rX(t)-a^{Z(t)}(t,X(t-))-\\sum_{k:k\\ne Z(t-)}\\lambda_{Z(t-)k}\\varrho^{Z(t-)k}(t,X(t))\\right)\\\\ &amp;+\\frac{1}{2}V_{xx}^{Z(t)}(t,X(t))\\pi^2(t,X(t-))\\sigma^2 X^2(t)\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}\\left(V^{k}(t,\\chi^k(t,X(t-)))-V^{Z(t-)}(t-,X(t-))\\right)\\lambda_{Z(t-)k}(t) \\end{align*}\\] Giving the desired PDE when setting equal to zero and rearranging. On discontinuity points where \\(Z\\) does not jump we have the gluing condition \\[ V^{Z(t)}(t,X(t))=V^{Z(t)}(t,X(t-)+\\Delta A^{Z(t)}(t,X(t-))=\\Delta B^{Z(t)}(t,X(t-))+V^{Z(t-)}(t,X(t-)) \\] as desired. “Uniqueness” Assume that \\(f^{Z(t)}(t,X(t))\\) satisfies the PDE with gluing and terminal condition. Then it holds that \\[\\begin{align*} e^{rt}d\\left(e^{-rt}f^{Z(t)}(t,X(t))\\right)&amp;=-rf^{Z(t)}(t,X(t))\\ dt+d\\left(f^{Z(t)}(t,X(t))\\right)\\\\ &amp;=-rf^{Z(t)}(t,X(t))\\ dt\\\\ &amp;+f_t^{Z(t)}(t,X(t))\\ dt+f^{Z(t)}_x(t,X(t))\\ dX^c(t)\\\\ &amp;+\\frac{1}{2}f_{xx}^{Z(t)}(t,X(t))\\ (dX^c(t))^2\\\\ &amp;+f^{Z(t)}(t,X(t))-f^{Z(t-)}(t,X(t-))\\\\ &amp;=-rf^{Z(t)}(t,X(t))\\ dt+f_t^{Z(t)}(t,X(t))\\ dt\\\\ &amp;+f^{Z(t)}_x(t,X(t))\\left(rX(t) + a^{Z(t)}(t)+\\sum_{k:k \\ne Z(t-)}\\varrho_{Z(t-)k}(t,X(t))\\lambda_{Z(t-)k}(t)\\right)\\ dt\\\\ &amp;+f_x^{Z(t)}(t,X(t))\\sigma \\pi(t,X(t))X(t)\\ dW^\\mathbb Q(t)\\\\ &amp;+\\frac{1}{2}f_{xx}^{Z(t)}(t,X(t))\\sigma^2 \\pi^2(t,X(t))X^2(t)\\ dt\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}\\left(f^k(t,\\chi^k(t,X(t-)))-f^{Z(t-)}(t,X(t-)\\right)\\ dN^k(t)\\\\ &amp;+f^{Z(t)}(t,X(t-)+\\Delta A^{Z(t)}(t,X(t-)))-f^{Z(t)}(t,X(t-)) \\end{align*}\\] Integrating \\(e^{-rt}f^{Z(t)}(t,X(t))\\) on \\([t,T]\\) we have \\[\\begin{align*} e^{-rt}f^{Z(t)}(t,X(t))&amp;=-\\int_t^Te^{-ru}\\left(-rf^{Z(u)}(u,X(u))+f_t^{Z(u)}(u,X(u))\\right)\\ du\\\\ &amp;-\\int_t^Te^{-ru}f^{Z(u)}_x(u,X(u))\\left(rX(u) + a^{Z(u)}(u)+\\sum_{k:k \\ne Z(u-)}\\varrho_{Z(u-)k}(u,X(u))\\lambda_{Z(u-)k}(u)\\right)\\ du\\\\ &amp;-\\int_t^Te^{-ru}f_x^{Z(u)}(u,X(u))\\sigma \\pi(u,X(u))X(u)\\ dW^\\mathbb Q(u)\\\\ &amp;-\\int_t^Te^{-ru}\\frac{1}{2}f_{xx}^{Z(u)}(u,X(u))\\sigma^2 \\pi^2(u,X(u))X^2(u)\\ du\\\\ &amp;-\\int_t^Te^{-ru}\\sum_{k:k\\ne Z(u-)}\\underbrace{\\left(f^k(u,\\chi^k(u,X(u-)))-f^{Z(u-)}(u,X(u-)\\right)}_{=R^{Z(u-)k}(u,X(u-))-b^{Z(u-)k}(u,X(u-))}\\ dN^k(u)\\\\ &amp;-\\sum_{t\\le u\\le T}e^{-ru}\\left(\\underbrace{f^{Z(u)}(u,X(u-)+\\Delta A^{Z(u)}(u,X(u-)))-f^{Z(u)}(t,X(u-))}_{=-\\Delta B^{Z(u)}(u,X(u-))}\\right)\\\\ &amp;=-\\int_t^Te^{-ru}\\underbrace{\\left(-rf^{Z(u)}(u,X(u))+f_t^{Z(u)}(u,X(u))+\\mathcal D_xf^{Z(u)}(u,X(u))\\right)}_{=-b^{Z(u)}(u,X(u-))-\\sum_{k:k\\ne Z(u-)}R^{Z(u-)k}(u,X(u-))\\lambda_{Z(u-)k}(t)}\\ du\\\\ &amp;-\\int_t^Te^{-ru}f_x^{Z(u)}(u,X(u))\\sigma \\pi(u,X(u))X(u)\\ dW^\\mathbb Q(u)\\\\ &amp;-\\int_t^Te^{-ru}\\sum_{k:k\\ne Z(u-)}\\left(R^{Z(u-)k}(u,X(u-))-b^{Z(u-)k}(u,X(u-))\\right)dM^k(u)\\\\ &amp;-\\int_t^Te^{-ru}\\sum_{k:k\\ne Z(u-)}\\left(R^{Z(u-)k}(u,X(u-))-b^{Z(u-)k}(u,X(u-))\\right)\\lambda_{Z(u-)k}(u)\\ du\\\\ &amp;+\\sum_{t\\le u\\le T}e^{-ru}\\Delta B^{Z(u)}(u,X(u-))\\\\ &amp;=-\\int_t^Te^{-ru}\\left(-b^{Z(u)}(u,X(u-))-\\sum_{k:k\\ne Z(u-)}\\color{red}{R^{Z(u-)k}(u,X(u-))}\\lambda_{Z(u-)k}(t)\\right)\\ du\\\\ &amp;-\\int_t^Te^{-ru}\\sum_{k:k\\ne Z(u-)}\\left(\\color{red}{R^{Z(u-)k}(u,X(u-))}-b^{Z(u-)k}(u,X(u-))\\right)\\lambda_{Z(u-)k}(u)\\ du\\\\ &amp;-\\sum_{t\\le u\\le T}e^{-ru}\\Delta B^{Z(u)}(u,X(u-))\\\\ &amp;-\\int_t^Te^{-ru}f_x^{Z(u)}(u,X(u))\\sigma \\pi(u,X(u))X(u)\\ dW^\\mathbb Q(u)\\\\ &amp;+\\int_t^Te^{-ru}\\sum_{k:k\\ne Z(u-)}\\left(R^{Z(u-)k}(u,X(u-))-b^{Z(u-)k}(u,X(u-))\\right)dM^k(u)\\\\ &amp;=-\\int_t^Te^{-ru}\\left(-b^{Z(u)}(u,X(u-))-\\sum_{k:k\\ne Z(u-)}b^{Z(u-)k}(u,X(u-))\\lambda_{Z(u-)k}(t)\\right)\\ du\\\\ &amp;+\\sum_{t\\le u\\le T}e^{-ru}\\Delta B^{Z(u)}(u,X(u-))\\\\ &amp;-\\int_t^Te^{-ru}f_x^{Z(u)}(u,X(u))\\sigma \\pi(u,X(u))X(u)\\ dW^\\mathbb Q(u)\\\\ &amp;-\\int_t^Te^{-ru}\\sum_{k:k\\ne Z(u-)}\\left(R^{Z(u-)k}(u,X(u-))-b^{Z(u-)k}(u,X(u-))\\right)dM^k(u)\\\\ &amp;=\\int_t^Te^{-ru}\\ dB(u)\\\\ &amp;-\\int_t^Te^{-ru}f_x^{Z(u)}(u,X(u))\\sigma \\pi(u,X(u))X(u)\\ dW^\\mathbb Q(u)\\\\ &amp;-\\int_t^Te^{-ru}\\sum_{k:k\\ne Z(u-)}\\left(R^{Z(u-)k}(u,X(u-))-b^{Z(u-)k}(u,X(u-))\\right)dM^k(u) \\end{align*}\\] Multiplying with \\(e^{rt}\\) we have \\[\\begin{align*} f^{Z(t)}(t,X(t))&amp;=\\int_t^Te^{-r(u-t)}\\ dB(u)\\\\ &amp;-\\int_t^Te^{-r(u-t)}f_x^{Z(u)}(u,X(u))\\sigma \\pi(u,X(u))X(u)\\ dW^\\mathbb Q(u)\\\\ &amp;-\\int_t^Te^{-r(u-t)}\\sum_{k:k\\ne Z(u-)}\\left(R^{Z(u-)k}(u,X(u-))-b^{Z(u-)k}(u,X(u-))\\right)dM^k(u) \\end{align*}\\] Taking expectation wrt. to \\(\\mathbb P\\otimes \\mathbb Q\\) gives \\[ f^{Z(t)}(t,X(t))=\\mathbb E^{\\mathbb P \\otimes \\mathbb Q}\\left[\\left.\\int_t^Te^{-r(u-t)}\\ dB(u)\\ \\right\\vert\\ Z(t),X(t) \\right] \\] since \\(W^\\mathbb Q\\) is a martingale wrt. \\(\\mathbb Q\\) and \\(M^k\\) is a martingale wrt. \\(\\mathbb P\\). \\(\\blacksquare\\) 4.6.4 With-Profit Insurance and the Dynamics of the Surplus One popular model for life insurance policies is the with-profit insurance policy where the payments are agreed upon on a prudent basis of deterministic \\((r^*,\\Lambda ^*)\\) and as such there arises a reserve bases on the first order basis market with a \\(*\\). We call the pair \\((r^*,\\Lambda ^*)\\) the technical basis or first-order basis and \\[ V^{j*}(t)=\\mathbb E^*\\left[\\left.\\int_t^Te^{-\\int_t^u r^*(v)\\ dv}\\ dB(u)\\ \\right\\vert\\ Z(t)=j\\right] \\] the technical reserve or first-order reserve. The payment stream \\(B\\) is constructed in such a way that the technical reserve fulfills the equivalence principle i.e. \\(V^{Z(0)*}(0-)=0\\). The technical basis is chosen conservatively such that in general the market value of the reserve is less than the technical reserve that is the real interest rate \\(r&gt;r^*\\) and \\(\\lambda^*&lt;\\lambda\\) i.e. the policy holder is expected to live longer than in reality and the interest obtained is greater than the “guaranteed” \\(r^*\\). The surplus that arises is payed back to the insured by a dividend payment stream \\(D\\) defined by \\[\\begin{align*} dD(t)&amp;=dD^{Z(t)}(t)+\\sum_{k:k\\ne Z(t-)}\\delta^{Z(t-)k}(t)dN^k(t),\\\\ dD^{Z(t)}(t)&amp;=\\delta^{Z(t)}(t)\\ dt+\\Delta D^{Z(t)}(t). \\end{align*}\\] If we assume a general investment strategy with value process \\(G\\) then we can consider the accumulated value of the account \\[ Y_0(t)=\\int_0^t\\frac{G(t)}{G(s)}d(-(B+D)(s)) \\] i.e. a payment at time \\(s\\) bears the interest \\(G(t)/G(s)\\) up until time \\(t\\). Remember that premiums is negative in \\(B\\) and \\(D\\) and benefits positive, hence the factor \\(-1\\). With this in mind we can define the surplus \\[ Y(t)=Y_0(t)-V^{Z(t)*}(t). \\] If we assume that \\(G\\) is a portfolio consisting of \\(q\\) weight in a risk asset and \\(1-q\\) in the risk free asset with weight chosen by \\[ \\pi(t,Y(t))=q(t)\\frac{Y_0(t)}{Y(t)}\\iff q(t)=\\pi(t,Y(t))\\frac{Y(t)}{Y_0(t)}&gt;0. \\] As we will see this in fact leads to the proportion \\(\\pi\\) is invested in the risky asset. This indeed means that the assets of the insurance company have dynamics \\[\\begin{align*} dG(t)&amp;=r(1-q(t))G(t)\\ dt+q(t)(rG(t)\\ dt + \\sigma G(t)\\ dW^{\\mathbb Q}(t)\\\\ &amp;=rG(t)\\ dt+q(t)\\sigma G(t)\\ dW^\\mathbb Q(t). \\end{align*}\\] Notice that we use the actual interest \\(r\\). Proposition 4.2. (Asmussen) Assuming the with-profit model, the surplus increases with \\[\\begin{align*} dY(t)&amp;=rY(t)\\ dt+\\sigma\\pi(t,Y(t)) Y(t)\\ dW^\\mathbb Q(t)+ d(C-D)(t),\\\\ dC(t)&amp;=c^{Z(t)}(t)\\ dt-\\sum_{k:k\\ne j}R^{Z(t-)k*}(t)\\ dM^k(t),\\\\ c^j(t)&amp;=(r-r^*)V^{j*}(t)+\\sum_{k:k\\ne j}\\Big(\\lambda_{jk}^*(t)-\\lambda_{jk}(t)\\Big)R^{jk*}(t),\\\\ R^{jk*}(t)&amp;=b^{jk}(t)+V^{k*}(t)-V^{j*}(t). \\end{align*}\\] In the above \\(C\\) is called the surplus contribution process. Proof. We have the dynamics of \\(Y_0\\) \\[\\begin{align*} dY_0(t)&amp;=\\frac{Y_0(t)}{G(t)}dG(t)+G(t)\\frac{1}{G(t)}d(-(B+D)(s))\\\\ &amp;=d(-(B+D)(s))+rY_0(t)\\ dt+q(t)\\sigma Y_0(t)\\ dW^\\mathbb Q(t)\\\\ &amp;=d(-(B+D)(s))+rY_0(t)\\ dt+\\pi(t,Y(t))\\frac{Y(t)}{Y_0(t)}\\sigma Y_0(t)\\ dW^\\mathbb Q(t)\\\\ &amp;=d(-(B+D)(s))+rY_0(t)\\ dt+\\pi(t,Y(t))Y(t)\\sigma \\ dW^\\mathbb Q(t)\\\\ &amp;=d(-(B+D)(s))+rY_0(t)\\ dt-rV^{Z(t)*}(t)\\ dt+\\pi(t,Y(t))Y(t)\\sigma \\ dW^\\mathbb Q(t)+rV^{Z(t)*}(t)\\ dt\\\\ &amp;=rY(t)\\ dt+rV^{Z(t)*}(t)\\ dt+\\pi(t,Y(t))Y(t)\\sigma \\ dW^\\mathbb Q(t)-d(B+D)(s) \\end{align*}\\] and the dynamics of the technical reserve \\[\\begin{align*} dV^{Z(t)*}(t)&amp;=V_t^{Z(t-)*}(t)\\ dt-\\Delta B^{Z(t)}(t)+\\sum_{k:k\\ne Z(t-)}\\Big(V^{k*}(t)-V^{Z(t-)*}(t)\\Big)\\ dN^k(t)\\\\ &amp;=\\left[r^*(t)V^{Z(t)*}(t)-b^{Z(t)}(t)-\\sum_{k:k\\ne Z(t-)}\\Big(b^{Z(t-)k}(t)+V^{k*}(t)-V^{Z(t-)*}(t)\\Big)\\lambda_{Z(t-)k}^*(t)\\right]\\ dt\\\\ &amp;-\\Delta B^{Z(t)}(t)+\\sum_{k:k\\ne Z(t-)}\\Big(V^{k*}(t)-V^{Z(t-)*}(t)\\Big)\\ dN^k(t)\\\\ &amp;=\\left[r^*(t)V^{Z(t)*}(t)-b^{Z(t)}(t)-\\sum_{k:k\\ne Z(t-)}R^{Z(t-)k*}(t)\\lambda_{Z(t-)k}^*(t)\\right]\\ dt\\\\ &amp;-\\Delta B^{Z(t)}(t)+\\sum_{k:k\\ne Z(t-)}\\Big(V^{k*}(t)-V^{Z(t-)*}(t)\\Big)\\ dN^k(t) \\end{align*}\\] thus the surplus arrises from \\[\\begin{align*} dY(t)&amp;=dY_0(t)-dV^{Z(t)*}(t)\\\\ &amp;=rY(t)\\ dt+rV^{Z(t)*}(t)\\ dt+\\pi(t,Y(t))Y(t)\\sigma \\ dW^\\mathbb Q(t)-d(B+D)(t)\\\\ &amp;-\\left[r^*(t)V^{Z(t)*}(t)-b^{Z(t)}(t)-\\sum_{k:k\\ne Z(t-)}R^{Z(t-)k*}(t)\\lambda_{Z(t-)k}^*(t)\\right]\\ dt\\\\ &amp;+\\Delta B^{Z(t)}(t)-\\sum_{k:k\\ne Z(t-)}\\Big(V^{k*}(t)-V^{Z(t-)*}(t)\\Big)\\ dN^k(t)\\\\ &amp;=-dD(t) + rY(t)\\ dt + \\pi(t,Y(t))Y(t)\\sigma \\ dW^\\mathbb Q(t)\\\\ &amp;+(r-r^*)V^{Z(t)*}(t)\\ dt\\\\ &amp;+b^{Z(t)}(t)\\ dt+\\sum_{k:k\\ne Z(t-)}R^{Z(t-)k*}(t)\\lambda_{Z(t-)k}^*(t)\\ dt+\\Delta B^{Z(t)}(t)-dB(t)\\\\ &amp;-\\sum_{k:k\\ne Z(t-)}\\Big(V^{k*}(t)-V^{Z(t-)*}(t)\\Big)\\ dN^k(t)\\\\ &amp;=-dD(t) + rY(t)\\ dt + \\pi(t,Y(t))Y(t)\\sigma \\ dW^\\mathbb Q(t)+(r-r^*)V^{Z(t)*}(t)\\ dt\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}R^{Z(t-)k*}(t)\\lambda_{Z(t-)k}^*(t)\\ dt-\\sum_{k:k\\ne Z(t-)}b^{Z(t-)k}(t)dN^k(t)\\\\ &amp;-\\sum_{k:k\\ne Z(t-)}\\Big(V^{k*}(t)-V^{Z(t-)*}(t)\\Big)\\ dN^k(t)\\\\ &amp;=-dD(t) + rY(t)\\ dt + \\pi(t,Y(t))Y(t)\\sigma \\ dW^\\mathbb Q(t)+(r-r^*)V^{Z(t)*}(t)\\ dt\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}R^{Z(t-)k*}(t)\\lambda_{Z(t-)k}^*(t)\\ dt-\\sum_{k:k\\ne Z(t-)}R^{Z(t-)k*}(t)\\ dN^k(t)\\\\ &amp;=-dD(t) + rY(t)\\ dt + \\pi(t,Y(t))Y(t)\\sigma \\ dW^\\mathbb Q(t)\\\\ &amp;+\\underbrace{(r-r^*)V^{Z(t)*}(t)\\ dt+\\sum_{k:k\\ne Z(t-)}R^{Z(t-)k*}(t)\\Big(\\lambda_{Z(t-)k}^*(t)-\\lambda_{Z(t-)k}(t)\\Big)\\ dt}_{:=c^{Z(t)}(t)\\ dt}\\\\ &amp;\\underbrace{-\\sum_{k:k\\ne Z(t-)}R^{Z(t-)k*}(t)\\ dM^k(t)}_{dC(t)-c^{Z(t)}(t)\\ dt}\\\\ \\end{align*}\\] as desired.\\(\\blacksquare\\) We will assume that we pay only dividend rates and allow for lump sum dividend at the end of the contract i.e. \\[ dY(t)=rY(t)\\ dt+\\sigma \\pi(t,Y(t))Y(t)\\ dW^\\mathbb Q+(c^{Z(t)}(t)-\\delta^{Z(t)}(t))\\ dt-\\Delta D^{Z(t)}(t)d1_{\\{T\\}}(t). \\] 4.6.5 Cash Dividends and Market Reserve We now turn to evaluating the market value of the reserve i.e. \\[ V^{Z(t)}(t,Y(t))=\\mathbb E^{\\mathbb P\\otimes \\mathbb Q}\\left[\\left.\\int_t^Te^{-\\int_t^u r(v)\\ dt}\\ d(B+D)(u)\\ \\right\\vert\\ Z(t),Y(t)\\right] \\] It happens to hold that we can achieve a PDE the reserve must satisfies by choosing the functions \\(A\\), \\(\\chi\\) and \\(\\varrho\\) and repeating the Unit-link PDE. Notice that we can link the dynamics of the surplus in the with profit case with the unit link case by considering the with-profit dynamics: \\[\\begin{align*} dY(t)&amp;=r(t)Y(t)\\ dt + \\sigma\\pi(t,Y(t)) Y(t)\\ dW^\\mathbb Q(t) + d(C-D)(t)\\\\ &amp;=r(t)Y(t)\\ dt + \\sigma\\pi(t,Y(t)) Y(t)\\ dW^\\mathbb Q(t)\\\\ &amp;+(c^{Z(t)}(t)-\\delta^{Z(t)}(t))\\ dt-\\Delta D^{Z(t)}(t)d1_{\\{T\\}}(t) \\end{align*}\\] and the unit-link dynamics \\[\\begin{align*} dX(t)&amp;=r(t)X(t)\\ dt + \\sigma\\pi(t,X(t)) X(t)\\ dW^\\mathbb Q(t)\\\\ &amp;-dA^{Z(t)}(t,X(t-))-\\sum_{k:k\\ne Z(t-)}\\lambda_{Z(t)k}(t)\\varrho^{Z(t)k}(t,X(t))\\ dt\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}\\Big(\\chi^k(t,X(t-))-X(t-)\\Big)\\ dN^k(t), \\end{align*}\\] Then if we set \\(A=D-C\\), \\(\\varrho=0\\) and \\(\\chi = X\\) then we obtain the with-profit surplus. This in particular gives us the following proposition for free. Proposition 5.1. (Asmussen) Assuming that \\(V^j(t,x)\\) is sufficiently differentiable, the following set of PDE’s is fulfilled: \\[\\begin{align*} V_t^j(t,y)&amp;=rV^j(t,y)-b^j(t,y)-\\delta^j(t,y)-\\sum_{k:k\\ne j}\\lambda_{jk}(t)R^{jk}(t,y)-\\mathcal D_y V^j(t,y),\\\\ V^t(t-,y)&amp;=\\Delta B^j(t,y)+\\Delta D^j(t,y)+V^j(t,y-\\Delta D^j(t,y)),\\\\ V^j(T,y)&amp;=0, \\end{align*}\\] where \\(\\mathcal D_y\\) is the operator \\[ \\mathcal D_yV^j(t,y)=V_y^j(t,y)\\left(ry+c^j(t,y)-\\delta^j(t,y)\\right)+\\frac{1}{2}V^j_{yy}(t,y)\\pi^2(t,y)\\sigma^2y^2 \\] and \\[ R^{jk}(t,y)=b^{jk}(t)+V^k(t,y)-V^j(t,y). \\] Conversely, if there exist a set of functions \\(V^j(t,y)\\) such that the above holds, then \\(V^{Z(t)}(t,Y(t))\\) is the reserved defined as \\[ V^{Z(t)}(t,Y(t))=\\mathbb E^{\\mathbb P\\otimes\\mathbb Q}\\left[\\left. \\int_t^Te^{-\\int_t^u r(v)\\ dv}\\ d(B+D)(u)\\ \\right\\vert\\ Z(t),Y(t)\\right]. \\] 4.6.6 The Pure Case of Cash Dividends We consider as before the payment streams \\[\\begin{align*} dB(t)&amp;=b^{Z(t)}(t)\\ dt + \\Delta B^{Z(t)}+\\sum_{k:k\\ne Z(t-)}b^{Z(t-)k}(t)dN^k(t),\\\\ dD(t)&amp;=\\delta^{Z(t)}(t)\\ dt+\\Delta D^{Z(t)}(t)d1_{\\{T\\}}(t),\\\\ dC(t)&amp;=c^{Z(t)}(t)\\ dt,\\\\ dY(t)&amp;=r(t)Y(t)\\ dt + \\sigma\\pi(t,Y(t)) Y(t)\\ dW^\\mathbb Q(t) + d(C-D)(t)\\\\ &amp;=r(t)Y(t)\\ dt + \\sigma\\pi(t,Y(t)) Y(t)\\ dW^\\mathbb Q(t)\\\\ &amp;+(c^{Z(t)}(t)-\\delta^{Z(t)}(t))\\ dt-\\Delta D^{Z(t)}(t)d1_{\\{T\\}}(t), \\end{align*}\\] We will now consider a contract like the above but where all dividends is held in cash in the insurance company and payed out when the contract is terminated. This is the case where \\[ \\forall t\\ge 0:\\qquad\\delta^j(t)0,\\quad\\Delta D^j(t,y)=y. \\] We then claim that the reserve may be decomposed as \\[ V^j(t,y)=f^j(t)+y \\] i.e. as a pure time component and linear in the surplus. When considering the PDE for the reserve we can put in \\[ V_t^j=f_t^j,\\quad V_y^j=1,\\quad V_{yy}^j=0, \\] hence we optain \\[\\begin{align*} f_t^j(t)&amp;=rf^j(t)+ry-b^j(t)-\\sum_{k:k\\ne j}\\lambda_{jk}(t)\\Big(b^{jk}(t)+f^k(t)-f^j(t)\\Big)\\\\ &amp;-ry-c^j(t)\\\\ &amp;=rf^j(t)-b^j(t)-c^j(t)-\\sum_{k:k\\ne j}\\lambda_{jk}(t)\\Big(b^{jk}(t)+f^k(t)-f^j(t)\\Big),\\\\ f^j(t)+y&amp;=\\Delta B^j(t)+f^j(t-)+y \\end{align*}\\] with terminal condition \\[ f^j(T-)+y=\\Delta D^j(T)+\\Delta B^j(T)=y+\\Delta B^j(T) \\] hence we have the ODE for \\(f\\) given by \\[\\begin{align*} f_t^j(t)&amp;=rf^j(t)-b^j(t)-c^j(t)-\\sum_{k:k\\ne j}\\lambda_{jk}(t)\\Big(b^{jk}(t)+f^k(t)-f^j(t)\\Big),\\\\ f^j(t)&amp;=\\Delta B^j(t)+f^j(t-),\\\\ f^j(T-)&amp;=\\Delta B^j(T) \\end{align*}\\] We can reqognize this as the PDE of the reserve with payment stream \\(B+C\\) hence \\[\\begin{align*} V^j(t,y)&amp;=\\mathbb E^{\\mathbb Q}\\left[\\left.\\int_t^Te^{-\\int_t^u r(v)\\ dv}d(B+C)(u)\\ \\right\\vert\\ Z(t)=j\\right]+y\\\\ &amp;=V_B^j(t)+V_C^j(t)+y. \\end{align*}\\] However, we also know that the reserve is the expected present value of the payment stream \\(B+D\\) and since only \\(D\\) depends on \\(Y\\) we must have \\[ V^j(t,y)=V_B^j(t)+V_D^j(t,y). \\] This of cause gives the equality \\[ V_D^j(t,y)=V_C^j(t)+y. \\] We call \\(V_B\\) the Guaranteed Benefits (or just GB) and \\(V_D\\) is called Future Discretionary Benefits (or just FDB). Only in the case with pure cash dividends and payout at termination is FDB this easy to calculate. This is in general not the case. 4.6.7 Bonus Payments and Market Reserve 4.6.8 The Pure Case of Bonus Payments 4.6.9 Comparison of Products "],["special-studies-in-life-insurance.html", "4.7 Special Studies in Life Insurance", " 4.7 Special Studies in Life Insurance 4.7.1 Survival Probabilities and Forward Mortality Rates 4.7.2 Dependent Interest and Mortality Rates 4.7.3 Stochastic Interest and Mortality Rate Models 4.7.4 Reserves Revisited 4.7.5 Incidental Policy Holder Behavior "],["continuous-time-finance.html", "Chapter 5 Continuous Time Finance", " Chapter 5 Continuous Time Finance This topic revolves around the theory of the Brownian motion and martingale processes. Other main topics are the binomial model and an introduction to financial derivatives. Financial derivatives is contingent on the outcome of a stochastic process at some future time \\(t=T\\) and often is a function \\(\\Phi\\) of some assets price \\(S_t\\). As such the derivative will give a stochastic payout, at time \\(t=T\\) of the size \\(X_T=\\Phi(S_T)\\). Naturally we want to say something about the fair price of the derivative in the form of \\[\\Pi_t(X_T)=\\mathbb{E}\\left[\\Phi(S_T)\\ \\vert\\ \\mathcal{F}_t\\right],\\] where \\(\\mathcal{F}_t\\subset\\mathcal{F}\\) is the available information at time \\(t\\). We will by defualt intepret the times \\(t=0\\) as today and \\(t=T\\) as tomorrow. This indeed require some fundamental understanding of the behaviour of the asset price \\(S_t\\). This lead us over to discussing the process in center of the Black-Scholes model: the Brownian motion. "],["discrete-time-models.html", "5.1 Discrete time models", " 5.1 Discrete time models 5.1.1 One-period time models The study of this course is the European call option (and put option). This financial derivative is an agreement between two parties where the holder of the option has the right to “exercise” the derivative, at a future time \\(t=T\\). Exercising means buying an asset at a certain agreed opon price-strike \\(K\\). In the case of the put-option: the holder has the right (but not obligation) to sell the asset at the strike price \\(K\\). As such the derivative has the payoff \\[\\text{Call}\\ \\text{option:}\\hspace{10pt}\\Phi(S_T)=(S_T-K)^+,\\hspace{20pt}\\text{Put}\\ \\text{option:}\\hspace{10pt}\\Phi(S_T)=(K-S_T)^+.\\] Our objective is to understand when an arbitrage exist and to find the fair price of these derivative. The strategy in pricing is finding a replicating portfolio with the same payoff as the option (with probability one) and then price the derivative accordingly. 5.1.1.1 Model description In the one-period model we consider the simplest possible market. We have two distinct times \\(t=0\\) (today) and \\(t=1\\) (tomorrow) and we may buy any portfolio as a mixture of bonds and one stock. We denote the bonds price by \\(B_t\\) and the stocks price by \\(S_t\\) and we assume the following: \\[ B_0=1,\\ B_1=1+R,\\hspace{20pt}S_0=s,\\ S_1=\\left\\{\\begin{matrix}s\\cdot u, &amp; with\\ probability\\ p_u.\\\\s\\cdot d, &amp; with\\ probability\\ p_d.\\end{matrix}\\right. \\] We may introduce \\(Z\\) as the random variable \\[ Z=u\\cdot (I)+d\\cdot (1-I), \\] for an bernoulli variable \\(I\\) with succes probability \\(p_u\\). Naturally, we assume \\(d\\le (1+R)\\le u\\) (this is imperative to ensure no arbitrage as we will see). 5.1.1.2 Portfolios and arbirtage We study any portfolio on the \\((B,S)\\) market as a vector \\(h=(x,y)\\) where \\(x\\) is the amount of bonds and \\(y\\) is the amount of stock held in the portfolio. Notice that we allow for shorting, that is \\(x&lt;0\\) or \\(y&lt;0\\). As such, we have that \\(h\\in \\mathbb{R}^2\\). In this we have made some unrealistic, but attractable assumptions included in the assumptions: We allow short positions and fractional holding, i.e. \\(h\\in \\mathbb{R}^2\\), We assume no spread between ask and bids, No transaction costs and A completely liquid market i.e. we may borrow and buy as much stock and bonds as wanted. Given that we have chosen a portfolio \\(h\\) we may introduce the value process. Definition 2.1. (Bjork) The value process of the porfolio \\(h\\in\\mathbb{R}^2\\) is the stochastic process \\[V^h_t=xB_t+yS_t,\\ t=0,1.\\] Given this notation we may define what an arbitrage is. Definition 2.2. (Bjork) An arbitrage is a portfolio \\(h\\) with the properties: 1) \\(V^h_0=0\\), 2) \\(P(V^h_1\\ge 0)=1\\) and 3) \\(P(V^h_1&gt;0)&gt;0\\). That is \\(h\\) is an deterministic money-machine where we at least never loose any money. Granted the bonds give a determinictic non-negative return, but an arbitrage does not require any money out of pocket. With the notion of an arbitrage we will show the first proposition regarding the choice of \\(R,u,d\\) as defined above. Proposition 2.3. (Bjork) The one-period binomial model is arbitrage free if and only if the following inequality hold: \\[d\\le (1+R)\\le u.\\tag{2.1}\\] Proof. The statement is proofed by contradiction. Assume that \\(d&gt;1+R\\) holds. Then by definition \\(u&gt;d&gt;1+R\\). Notice that any portfolio satisfying \\(V_0^h=0\\) must satisfy \\[0=xB_0+yS_0=x+ys\\iff x=-ys\\] That is for some choice \\(y\\) the only arbitrage candidate is the portfolio \\(h=(-ys,y)\\). Calculating the value at time \\(t=1\\) we have \\[V_1^h=-ys\\cdot(1+R)+y\\cdot s\\cdot Z=ys(Z-1-R)\\] However since \\(Z\\ge d\\) we have \\(Z-(1+R)\\ge 0\\) and therefore an arbitrage (for \\(y&gt;0\\)). The other inequality \\(1+R&gt;u\\) follows analog steps. Simply choose some \\(y&lt;0\\) and the result follows. \\(\\blacksquare\\) From inequality (2.1) we see that since \\(1+R\\) is between \\(u\\) and \\(d\\) we may find a pair \\(q_d,q_u\\ge 0\\) with \\(q_d+q_u=1\\) such that \\[1+R=q_u\\cdot u+q_d\\cdot d.\\] This yields the important risk neutral valuation formula as summed op in the following definition Definition 2.4. (Bjork) A probability measure \\(Q\\) is called a martingale meausre if the following condition holds: \\[S_0=\\frac{1}{1+R}E^Q[S_1].\\] The above measure \\(Q\\) is the measure \\(Q(Z=d)=q_d\\) and \\(Q(Z=u)=q_u\\) for the binomial model. This does in fact yield the risk neautral valuation formula: \\[\\begin{align*} S_0&amp;=\\frac{1}{1+R}E^Q[S_1]=\\frac{1}{1+R}(Q(Z=d)\\cdot d\\cdot s+Q(Z=u)\\cdot u\\cdot s)\\\\ &amp;=s\\frac{1}{1+R}(q_d\\cdot d+q_u\\cdot u)=s, \\end{align*}\\] where we simply use \\(1+R=q_d\\cdot d+q_u\\cdot u\\). We call this the risk neautral valuation formula because it in some sense gives an expected discounted value of the future stock price. We end this endavour with reformulating the arbitrage proposition and determining the values of the \\(Q\\)-measure. Proposition 2.5. (Bjork) The one-period binomial model is arbitrage free if and only if there exists a martingale measure \\(Q\\). Proposition 2.6. (Bjork) The one-period binomial model has martingale probabilities given by: \\[\\left\\{\\begin{matrix}q_u=\\frac{(1+R)-d}{u-d},\\\\ q_u=\\frac{u-(1+R)}{u-d}.\\end{matrix}\\right.\\] 5.1.1.3 Contingent Claims This chapter revolves around the financial derivative and we start by stating the definition of the financial derivative. Definition 2.7. (Bjork) A contingent claim (financial derivative) is any stochastic variable \\(X\\) of the form \\(\\Phi(Z)\\), where \\(Z\\) is the stochastic varible driving the stock price process. We may also call the function \\(\\Phi\\) the contract function as it states how the contract is resolved once the stochastic variable \\(Z\\) has been realised. Our objective is now to study, what a buyer of said contract would have to pay at any given time \\(t\\). We call the fair price of \\(X\\) at time \\(t\\): \\(\\Pi_t[X]\\). As such it is easy to see that the fair price at the time of maturity \\(T\\) is simply the payout \\(X\\) i.e. \\(\\Pi_T[X]=X\\). Our strategy is to find a replicating portfolio \\(h\\) and determine the price of said portfolio. Definition 2.8. (Bjork) A contingent claim \\(X\\) can be replicated, or said to be reachable if there exist a portfolio \\(h\\) such that \\[ V_1^h=X, \\] with probability one. In that case, we say that the portfolio \\(h\\) is a hedging portfolio or a replicating portfolio. If all claims can be replicated we say that the market is complete. Our pricing strategy is then to determine the value process of the replicating portfolio and then by the first pricing principle below we say that the price is imply the value of the replicating portfolio. Pricing principle 1. If a clain \\(X\\) is reachable with replicating portfolio \\(h\\), then the only reasonable price process for \\(X\\) is given by \\[ \\Pi_t[X]=V_t^h. \\] Notice, that this assumes that a replicating portfolio exist and even so we have a uniqueness statement to solve. We end this section by writing two important results. Proposition 2.9. (Bjork) Suppose that a claim \\(X\\) is reachable with replicating portfolio \\(h\\). Then any price at time \\(t\\ge 0\\) of the claim \\(X\\) other than the value process of \\(h\\) will lead to an arbitrage on the extended market \\((B,S,X)\\). Proposition 2.10. (Bjork) If the one-period binomial model is free of arbitrage, then it is also complete. The hedging portfolio in the one-period binomial model is given by the portfolio \\((x,y)\\) below \\[\\begin{align*} x&amp;=\\frac{1}{1+R}\\cdot\\frac{u\\Phi(d)-d\\Phi(u)}{u-d},\\tag{2.2}\\\\ y&amp;=\\frac{1}{s}\\cdot\\frac{\\Phi(u)-\\Phi(d)}{u-d}.\\tag{2.3} \\end{align*}\\] 5.1.1.4 Risk Neutral Valuation We see that since the one-period model is complete we can price any contingent claim and we see that \\[\\begin{align*} \\Pi_0[X]&amp;=\\frac{1}{1+R}\\cdot\\frac{u\\Phi(d)-d\\Phi(u)}{u-d}+s\\frac{1}{s}\\cdot\\frac{\\Phi(u)-\\Phi(d)}{u-d}\\\\ &amp;=\\frac{1}{1+R}\\left\\{\\frac{u\\Phi(d)-d\\Phi(u)}{u-d}+(1+R)\\frac{\\Phi(u)-\\Phi(d)}{u-d}\\right\\}\\\\ &amp;=\\frac{1}{1+R}\\left\\{\\frac{(1+R)-d}{u-d}\\Phi(u)+\\frac{u-(1+R)}{u-d}\\Phi(d)\\right\\}\\\\ &amp;=\\frac{1}{1+R}E^Q[X]. \\end{align*}\\] i.e. the price at time \\(t=0\\) should simply be the expected discounted payout according to the martingale measure. This leads to the important pricing proposition: Proposition 2.11. (Bjork) If the one-period binomial model is free of arbitrage, then the arbitrage free price of a contingent claim \\(X\\) is given by \\[ \\Pi_0[X]=\\frac{1}{1+R}E^Q[X].\\tag{2.4} \\] Here the martingale measure \\(Q\\) is uniquely determined by the relation \\[ S_0=\\frac{1}{1+R}E^Q[S_1],\\tag{2.5} \\] and the explicit expressions for \\(q_u\\) and \\(q_d\\) are given in proposition 2.6. Furthermore the claim \\(X\\) can be replicated using the portfolio \\[\\begin{align*} x&amp;=\\frac{1}{1+R}\\cdot\\frac{u\\Phi(d)-d\\Phi(u)}{u-d},\\tag{2.6}\\\\ y&amp;=\\frac{1}{s}\\cdot\\frac{\\Phi(u)-\\Phi(d)}{u-d}.\\tag{2.7} \\end{align*}\\] 5.1.2 Multi-period model The one-period binomial model can easily be extended to a multi-period model, by assuming that the bond and stock pricess evolve by the processes: \\[ t\\ge1:\\ B_t=(1+R)B_{t-1}\\hspace{20pt}\\text{and}\\hspace{20pt}B_0=1, \\] \\[ t\\ge1:\\ S_t=Z_{t-1}S_{t-1}\\hspace{20pt}\\text{and}\\hspace{20pt}S_0=s, \\] where we obviously have that \\(B_t=(1+R)^t\\) for \\(t\\ge 0\\). In the above \\(Z_t\\) is \\(u\\) with probability \\(p_u\\) and \\(d\\) with probability \\(p_d\\). In this context, we need to define a portfolio in terms of a strategy. Definition 2.13. (Bjork) A portfolio strategy is a stochastic process on \\(\\{1,...,T\\}\\) \\[ h=\\left\\{h_t=(x_t,y_t);\\ t=1,...,T\\right\\} \\] such that \\(h_t\\) is a function of \\(S_0,S_1,...,S_{t-1}\\). For a given portfolio strategy \\(h\\) we set \\(h_0=h_1\\) by convention. The associated value process corresponding to the portfolio \\(h\\) is defined by \\[ V_t^h=x_t(1+R)+y_tS_t. \\] Given this notation we may define what an arbitrage is, but first we introduce the notion of a self-financing portfolio. A self-financing portfolio in an intuative sense is a portfolio that is not withdrawn from or deposited into. Definition 2.14. (Bjork) A portfolio strategy \\(h\\) is said to be self-financing if the following condition holds for all \\(t=0,...,T-1\\): \\[ x_t(1+R)+y_tS_t=x_{t+1}+y_{t+1}S_t. \\] The above equation says that the portfolio purchased at time \\(t\\) and helt until \\(t+1\\) \\((x_{t+1},y_{t+1})\\) can only be financed by the market value of the portfolio held from \\([t-1,t)\\) i.e. \\((x_{t},y_{t})\\). We now define an arbitrage. Definition 2.15. (Bjork) An arbitrage is a self-financing portfolio \\(h\\) with the properties: 1) \\(V^h_0=0\\), 2) \\(P(V^h_T\\ge 0)=1\\) and 3) \\(P(V^h_T&gt;0)&gt;0\\). The multiperiod binomial model has an just like the oneperiod model a result regarding when an arbitrage exists. Lemma 2.16. (Bjork) If \\(d\\le (1+R)\\le u\\) (eq. 2.8) then the multiperiod model is arbitrage-free. As one can see, the multiperiod model is rather similar to the one period model. We wil in the following summarise equivalent statements for the multiperiod model as the ones in the oneperiod model. Definition 2.17. (Bjork) The martingale probabilities \\(q_u\\) and \\(q_d\\) are defined as the probabilities for which the relation below holds. \\[ s=\\frac{1}{1+R}E^Q[S_{t+1}\\ \\vert\\ S_t]. \\] Proposition 2.18. (Bjork) The martingale probabilities \\(q_u\\) and \\(q_d\\) are given by \\[ \\left\\{\\begin{matrix}q_u=\\frac{(1+R)-d}{u-d},\\\\ q_u=\\frac{u-(1+R)}{u-d}.\\end{matrix}\\right. \\] Definition 2.19. (Bjork) A contingent claim is a stochastic variable \\(X\\) of the form \\[ X=\\Phi(S_T), \\] where the contract function \\(\\mathbf{\\Phi}\\) is some given real valued function. Definition 2.20. (Bjork) A given contingent claim \\(X\\) is said to be reachable if there exists a self-financing portfolio \\(h\\) such that \\[ V_T^h=X, \\] with probability one. In that case we say that the portfolio \\(h\\) is a hedging portfolio or a replicating portfolio. If all claims can be replicated we say that the market is (dynamically) complete. Pricing principle 2. (Bjork) If a claim \\(X\\) is reachable with replicating portfolio \\(h\\), then the only reasonable price process for \\(X\\) os given by \\[ \\Pi_t[X]=V_t^h,\\ t=0,1,...,T. \\] Proposition 2.21. (Bjork) Assume \\(X\\) is reachable by \\(h\\), then any price other than \\(V_t^h\\) for some \\(t\\ge 0\\) leads to an arbitrage opportunity. Proposition 2.22. (Bjork) The multiperiod model is complete, i.e. every claim can be replicated by a self-financing portfolio. Proposition 2.24. (Bjork) (Binomial algorithm) Consider a \\(T\\)-claim \\(X=\\Phi(S_T)\\). Then this claim can be replicated using af self-financing portfolio. If \\(V_t(k)\\) denotes the value of the portfolio at the node \\((t,k)\\) (\\(k\\) referring to \\(k\\) amount of up-moves for the stock), then \\(V_t(k)\\) can be computed recursively by the scheme \\[ \\left\\{\\begin{matrix}V_t(k)=\\frac{1}{1+R}\\left\\{q_uV_{t+1}(k+1)+q_dV_{t+1}(k)\\right\\},\\\\ V_T(k)=\\Phi(su^kd^{T-k}).\\end{matrix}\\right. \\] where the martingale probabilities \\(q_u\\) and \\(q_d\\) are given by \\[ \\left\\{\\begin{matrix}q_u=\\frac{(1+R)-d}{u-d},\\\\ q_u=\\frac{u-(1+R)}{u-d}.\\end{matrix}\\right. \\] With the notation as above, the hedging portfolio is given by \\[ \\left\\{\\begin{matrix}x_t(k)=\\frac{1}{1+R}\\cdot\\frac{uV_t(k)-dV_t(k+1)}{u-d},\\\\ y_t(k)=\\frac{1}{S_{t-1}}\\cdot\\frac{V_t(k+1)-V_t(k)}{u-d}.\\end{matrix}\\right. \\] In particular, the arbitrage free price of the claim at \\(t=0\\) is given by \\(V_0(0)\\). Example. Consider \\(R=0.04\\), \\(s=100\\), \\(u=1.1\\), \\(d=0.9\\), \\(p_u=0.6\\) and \\(p_d=0.4\\). We consider a model of length \\(T=2\\) and we want to evaluate the price of the european call option with srike \\(K=90\\) that is the contingent claim \\[ X=(S_T-K)^+,\\hspace{20pt}\\Phi(s)=(s-K)^+. \\] For each time \\(t\\) we know the replicating portfolio, if we know the payoff the following period. Therefore we start from the leaves of the tree and work towards the root. Since the strike price is \\(K=90\\) the end result will be the following payoffs: \\[\\begin{align*} u^2:\\hspace{20pt}&amp;(121-90)^+=31\\\\ ud:\\hspace{20pt}&amp;(99-90)^+=9\\\\ du:\\hspace{20pt}&amp;(99-90)^+=9\\\\ d^2:\\hspace{20pt}&amp;(81-90)^+=0 \\end{align*}\\] Therefore by the risk neautral valuation formula with \\(q_u=\\frac{(1+R)-d}{u-d}=0.7\\) and \\(q_d=\\frac{u-(1+R)}{u-d}=0.3\\) we have that the cost of the replicating portfolio at time \\(t=1\\) is respectively \\[\\begin{align*} u:\\hspace{20pt}&amp;\\frac{1}{1+R}\\left\\{31\\cdot q_u + 9 \\cdot q_d\\right\\}\\approx 23.46\\\\ d:\\hspace{20pt}&amp;\\frac{1}{1+R}\\left\\{9\\cdot q_u + 0 \\cdot q_d\\right\\}\\approx 6.06 \\end{align*}\\] To replicate this payoff at time \\(t=1\\) we can use the risk neutral valuation formula once more to find the base cost of the replicating portfolio i.e. the price of \\(X\\) at time \\(t=0\\) \\[ \\frac{1}{1+R}\\left\\{23.46\\cdot q_u + 6.06 \\cdot q_d\\right\\}\\approx 17.54. \\] Working from the root to the leaves we can now calculate the hedging portfolio at time \\(t=0,1\\) for each path. For time \\(t=0\\) we calculate \\[\\begin{align*} x=&amp;\\frac{1}{1+R}\\cdot \\frac{u\\cdot 6.06-d\\cdot 23.46}{u-d}\\approx -69.46,\\\\ y=&amp;\\frac{1}{s}\\cdot\\frac{23.46-6.06}{u-d}\\approx0.87 \\end{align*}\\] We see by calculations that this does indeed replicate the payoff at time \\(t=1\\): \\[\\begin{align*} u:\\hspace{20pt}&amp;V_1^h=(1+R)\\cdot x + 110\\cdot y\\approx 23.46,\\\\ d:\\hspace{20pt}&amp;V_1^h=(1+R)\\cdot x + 90\\cdot y\\approx 6.06. \\end{align*}\\] We also see by calculation that the initial portfolio does cost the expected 17.54 as \\[ x\\cdot 1+y\\cdot100=87-69.46=17.54. \\] Following these steps at time \\(t=1\\) the portfolios \\((-86.54,1)\\) (for the up-scenario) and \\((-38.94,0.5)\\) (for the down-scenario) would arise. Notice when calculating \\(y\\) one has to use the current price \\(S_1=S_0\\cdot Z\\) not \\(S_0\\). One should also check by similar calculations as above, that these portfolios does indeed replicate the payoff of the contingent claim \\(X\\). \\(\\square\\) Proposition 2.25. (Bjork) The arbitrage free price at \\(t=0\\) of a \\(T\\)-claim \\(X\\) is given by \\[ \\Pi_0[X]=\\frac{1}{(1+R)^T}E^Q[X] \\] where \\(Q\\) denotes the martingale measure, or more explicitly \\[ \\Pi_0[X]=\\frac{1}{(1+R)^T}\\sum_{k=0}^T\\binom{T}{k}q_u^kq_d^{T-k}\\Phi(su^kd^{T-k}). \\] Example. R &lt;- 0.04 s &lt;- 100 u &lt;- 1.1 d &lt;- 0.9 p_u &lt;- 0.6 p_d &lt;- 0.4 q_u &lt;- (1+R-d)/(u-d) q_d &lt;- (u-1-R)/(u-d) cap_t &lt;- 2 #Test for K=90 K &lt;- 90 pi_0 &lt;- (1+R)**(-cap_t)*sum( choose(cap_t,0:cap_t)*q_u**(0:cap_t)*q_d**(cap_t - 0:cap_t)*pmax(s*u**(0:cap_t)*d**(cap_t - 0:cap_t)-K,0) ) # = 17.53883 pi_0 &lt;- unlist(lapply(0:ceiling(s*u**cap_t), function(K){ (1+R)**(-cap_t)*sum( choose(cap_t,0:cap_t)*q_u**(0:cap_t)*q_d**(cap_t - 0:cap_t)*pmax(s*u**(0:cap_t)*d**(cap_t - 0:cap_t)-K,0) ) })) Figure 5.1: The pricing function of the European call option. We follow an analog example as the one after proposition 2.24. Let \\(K=90\\) and we see that \\[\\begin{align*} &amp;\\Pi_0[X]\\\\ &amp;=\\frac{1}{(1+0.04)^2}\\sum_{k=0}^2\\binom{2}{k}\\cdot0.7^k\\cdot0.3^{2-k}\\cdot\\Phi(100\\cdot 1.1^k\\cdot0.9^{2-k})\\\\ &amp;=0.9245562\\cdot\\left(\\underbrace{1\\cdot 1\\cdot0.09\\cdot0}_{k=0}+\\underbrace{2\\cdot 0.7\\cdot0. 3\\cdot 9}_{k=1}+\\underbrace{1\\cdot 0.49\\cdot1\\cdot31}_{k=2}\\right)\\\\ &amp;=0.9245562\\cdot\\left(0+3.78+15.19\\right)\\\\ &amp;=17.53883 \\end{align*}\\] Since we know that \\(K\\) must meaningfully range in \\([0,121]\\) we could try to calculate the price of the contingent claim at time \\(t=0\\) for all integers in this interval. We see that the price range between \\(S_0\\) and 0 as expected. One can also see that the price changes slope at the prices 99 and 121 as the function is linear in \\(\\Phi\\) and som realisations loose any effect on the price when the strike is higher than the outcome. \\(\\square\\) Proposition 2.26. (Bjork) The condition \\(d&lt;(1+R)&lt;u\\) is necessary and sufficient condition for absence of arbitrage. 5.1.3 Generelised one-period model In the previous we had the simpel model where we only had one stochastic asset \\(S\\) and only one stochastic variable \\(Z\\) determining the future stock price. Now we will generelise this model by introducing \\(N\\) assets and introducing som stochastic behaviour to the system. 5.1.3.1 Model specification We consider the market consisting of a collection of stochastic prices assets \\(i=1,...,N\\) with \\(N\\)-dimensional price process. \\[ S_t=\\begin{bmatrix} S_t^1\\\\ \\vdots\\\\ S_t^N\\end{bmatrix} \\] We now assume that \\(S_t\\) is defined on a background space with finite sample space \\(\\Omega = \\{\\omega_1,...,\\omega_M\\}\\) with associated probabilities \\(p_j=P(\\omega_j)\\), \\(j=1,...,M\\). We can then for eact time \\(t=1,...,T\\) define the \\(N\\times M\\) matrix \\(D_t\\) as such \\[ D_t=\\begin{bmatrix} S_t^1(\\omega_1)&amp;\\cdots &amp;S_t^1(\\omega_M)\\\\ \\vdots &amp;\\ddots &amp; \\vdots\\\\ S_t^N(\\omega_1) &amp;\\cdots&amp;S_t^M(\\omega_M)\\end{bmatrix}. \\] We will assume that \\(S_0^1&gt;0\\) and \\(S_1^1(\\omega_j)&gt;0\\), \\(j=1,...,M\\). 5.1.3.2 Absence of Arbitrage We now define a portfolio as an \\(N\\)-dimensional row vector \\[ h=\\begin{bmatrix} h^1, \\dots,h^N\\end{bmatrix} \\] representing the amount of assets held at time \\(t=0\\) and held until \\(t=1\\). The value process is then \\[ V^h_t=h\\cdot S_t=\\sum_{i=1}^N h^iS_t^i,\\ t=0,1.\\tag{3.1} \\] For a given \\(\\omega_j\\in\\Omega\\) we have the realisation \\[ V_t^h=hS_t(\\omega_j)=hd_j=(hD)_j. \\] Definition 3.1. (Bjork) The portfolio \\(h\\) is an arbitrage portfolio fil it satisfies the conditions: \\(V_0^h=0\\), \\(P(V_1^h\\ge 0)=1\\) and \\(P(V_1^h&gt;0)&gt;0\\). Lemma 3.2. (Bjork) (Farkas’ Lemma) Suppose that \\(d_0,d_1,...,d_M\\) are column vectors in \\(\\mathbb{R}^N\\). Then exactly one of the following problems possesses a solution. Problem 1: There exist \\(\\lambda_1,...,\\lambda_M\\ge0\\) such that \\(d_0=\\sum_{j=1}^M\\lambda_jd_j\\). Problem 2: There exist \\(h\\in\\mathbb{R}^N\\) such that \\(h^\\top d_0&lt;0\\) and \\(h^\\top d_j\\ge 0\\) for \\(j=1,...,M\\). We now investegate this system for any possible arbitrage portfolios. However first we acknowledge that there exist a nominal price system \\(S_t\\) and a normalised price system \\(Z_t\\). The latter we define as the nominel pricess under the numeraire \\(S_t^1\\) that is \\[ Z_t=\\begin{bmatrix} S_t^1/S_t^1\\\\ S_t^2/S_t^1\\\\ \\vdots\\\\ S_t^N/S_t^1\\end{bmatrix}=\\begin{bmatrix} 1\\\\ S_t^2/S_t^1\\\\ \\vdots\\\\ S_t^N/S_t^1\\end{bmatrix}. \\] The reason for introducing the normalized price system is that we can without much effort translate results in this system to the nominal system and the normalised system is easier to analize. For this, however, we need af few results. Lemma 3.3. (Bjork) With notation as above, the following hold. The \\(Z_t\\) value process i related to the \\(S_t\\) value process by \\[ V_t^{h,Z}=hZ_t=\\frac{1}{S_t^1}V_t^h. \\] A portfolio is an arbitrage in the \\(S_t\\) system if and only if there is an arbitrage in the \\(Z_t\\) system. In the \\(Z_t\\) price system, the numeraie asset \\(Z^1\\) has unit constant prices i.e. \\(Z_t^1=1\\) for all \\(t\\ge 0\\). One of the reason that the normalised system is attractable is that the numeraire asset is constant i.e. risk free in the normalised system. Let us formulate our first main result. Proposition 3.4. (Bjork) The market is arbitrage free if and only if there exists strictly positive real numbers \\(q_1,...,q_M\\ge 0\\) with \\(q_1+\\cdots + q_M=1\\) (eq. 3.2) (probability vector) such that the following vector equality holds \\[ \\begin{bmatrix} Z_0^1\\\\ \\vdots\\\\ Z_N^1\\end{bmatrix}=\\begin{bmatrix} Z_1^1(\\omega_1)\\\\ \\vdots\\\\ Z_1^N(\\omega_1)\\end{bmatrix}q_1+\\cdots +\\begin{bmatrix} Z_1^1(\\omega_M)\\\\ \\vdots\\\\ Z_1^N(\\omega_M)\\end{bmatrix}q_M.\\tag{3.3} \\] 5.1.3.3 Martingale Measures Definition 3.5. (Bjork) Given the objective probability measure \\(P\\) on \\((\\Omega,\\mathcal{F},P)\\), we say that another probability measure \\(Q\\) defined on \\(\\Omega\\) is equivalent to \\(P\\) if \\[ \\forall A\\in\\mathcal{F}:P(A)=0\\iff Q(A)=0, \\] or equivalently \\[ \\forall A\\in\\mathcal{F}:P(A)=1\\iff Q(A)=1. \\] Definition 3.7. (Bjork) Consider the market model above and set \\(S^1\\) as the numeraire asset. We say that a probability measure \\(Q\\) defined on \\(\\Omega\\) is a martingale measure if it satisfies the following conditions: \\(Q\\) is equivalent to \\(P\\), i.e. \\(Q\\sim P\\). For every \\(i=1,...,N\\), the normalized asset price process \\[ Z_t^i=\\frac{S_t^i}{S_t^1}, \\] is martingale under the measure \\(Q\\). Theorem 3.8. (Bjork) (First Fundamental Theorem) Given a fixed numeraire, ther market is free of arbitrage possibilities if and only if there exists a martingale measure \\(Q\\). By assuming that the numeraire asset is risk free (i.e. does not depend on \\(\\omega\\)) then by scaling we can derive the short interest rate as \\[ 1+R=\\frac{S_1^1}{S_0^1}. \\] With this in mind we can formulate theorem 3.8 in its more widely used form. Theorem 3.9. (Bjork) (First Fundamental Theorem) Assume that there exist a risk free asset, and denote the corresponding risk free interest rate by \\(R\\). Then the market is arbitrage free if and only if there exist a measure \\(Q\\sim P\\) such that \\[ S_0^i=\\frac{1}{1+R}E^Q[S_1^i],\\hspace{20pt}\\text{for all}\\ i=1,...,N.\\tag{3.9} \\] 5.1.3.4 Martingale Pricing Moving forward we will assume that there exist a risk free asset and we will denote it by \\(B_t\\) (\\(B_t=S^1_t/S^1_0\\)). Definition 3.10. (Bjork) A contingent claim is any random variable \\(X\\), defined on the sample space \\(\\Omega\\). To ensure no arbitrage in the extended market containing the \\(N\\) assets and the contingent claim we can apply the first fundamental pricing theorem on the extended market. Proposition 3.11. (Bjork) Consider a given claim \\(X\\). In order to avoid arbitrage, \\(X\\) must then be priced according to the formula \\[ \\Pi_0[X]=\\frac{1}{1+R}E^Q[X],\\tag{3.10} \\] where \\(Q\\) is a martingale measure for the underlying market \\((\\Pi,S^1,...,S^N)\\). 5.1.3.5 Completeness Given that a market is arbitrage-free we may run into a uniqueness issue when determining the price of a contingent claim. If a martingale measure exist we will very much like it to be unique as this will ensure that the price from the risk neutral valuation formula is unique. To this we need the market to be complete. Definition 3.12. (Bjork) Consider a contingent claim \\(X\\). If there exists a portfolio \\(h\\), based on the underlying assets, such that \\[ V_1^h=X,\\ \\text{with probability 1}\\tag{3.11} \\] i.e. \\[ V_1^h(\\omega_j)=X(\\omega_j),\\ j=1,...,M,\\tag{3.12} \\] then we say that \\(X\\) is replicated, or hedged by \\(h\\). Such a portfolio \\(h\\) is called a replicating, or hedging portfolio. If every contingent claim can be replicated, we say that the market is complete. We can now formulate a proposition on when the market is complete in terms of the matrix \\(D\\). Proposition 3.13. (Bjork) The market is complete if and only if the rows of the matrix \\(D\\) span \\(\\mathbb{R}^M\\), i.e. if and only if \\(D\\) has rank \\(M\\). Now we formulate the second fundamental pricing theorem in terms of the martingale measure \\(Q\\). Proposition 3.14. (Bjork) (Second Fundamental Theorem) Assume that the model is arbitrage free i.e. \\(Q\\) exist. Then the market is unique if and only if the martingale measure is unique. 5.1.3.6 Stochastic Discount Factors Definition 3.16. (Bjork) The random variable \\(L\\) on \\(\\Omega\\) is defined by \\[ L(\\omega_i)=\\frac{q_i}{p_i},\\hspace{20pt} i=1,...,M. \\] Definition 3.17. (Bjork) Assume the absence of arbitrage, and fix a martingale measure \\(Q\\). With notation as above, the stochastic discount factor (or “state price deflator”) is the random variable \\(\\Lambda\\) on \\(\\Omega\\) by \\[ \\mathbf{M}(\\omega)=\\frac{1}{1+R}\\cdot L(\\omega).\\tag{3.19} \\] Proposition 3.18. (Bjork) The arbitrage free price of any claim \\(X\\) is given by the formula \\[ \\Pi_0[X]=E^P[\\mathbf{M}\\cdot X]\\tag{3.20} \\] where \\(\\mathbf{M}\\) is a stochastic discount factor. "],["self-financing-portfolios.html", "5.2 Self-financing portfolios", " 5.2 Self-financing portfolios We move forward in this chapter by first defining a self-financing portfolio in discrete time and then by letting the step length tend to zero obtain the continuous time analogue. 5.2.1 Discrete time SF portfolio We consider \\(N\\) different adapted price processes \\(S^1,...,S^N\\). We use the following definition. Definition 6.1. (Bjork) We use the following definitions. \\(S_n^i\\) is th price of asset \\(i\\) at time \\(n\\), \\(h_n^i\\) is the number of units of asset \\(i\\) held during \\([n,n+1)\\), that is bought at time \\(n\\), \\(d_n^i\\) is the dividends from asset \\(i\\) in the time-interval \\([n-1,n)\\), that is recieved at time \\(n\\), \\(h_n\\) is the portfolio \\((h_n^1,...,h_n^N)\\) held during \\([n,n+1)\\), \\(c_n\\) is the consumption i.e. withdrawel at time \\(n\\) (negative being deposits/saving), \\(V_n\\) is the value of the portfolio just before time \\(n\\) i.e. of the portfolio \\(h_{n-1}\\) at time \\(n\\). We are now ready to define the self-financing portfolio Definition 6.2. (Bjork) A self-financing portfolio supporting the consumption stream \\(\\mathbf{c}\\) is a portfolio adhering to the budget constraint given as \\[ h_{n+1}S_{n+1}+c_{n+1}=h_nS_{n+1}+h_nd_{n+1.} \\] The interpretation being, that we may only use funds obtained from selling the old portfolio \\(h_n\\) and recieved in dividends to buy the new portfolio \\(h_{n+1}\\) and consume the amount \\(c_{n+1}\\). Before studying the self-financing portfolio we define the operator \\(\\Delta\\) (in definition 6.3) as the increment \\(\\Delta x_n=x_{n+1}-x_n\\) of a countable sequence \\((x_n)_{n\\in\\mathbb{N}_0}\\). Notice that we define the increment forward so the increment \\(n\\) is the increment over the time period \\([n,n+1)\\) with the first increment being \\([0,1)\\). Using this notation we can derive the lemma below. Lemma 6.4. (Bjork) For any pair of sequences of real numbers \\((x_n)_{n\\in\\mathbb{N}_0}\\) and \\((y_n)_{n\\in\\mathbb{N}_0}\\) we have the relations \\[\\begin{align*} \\Delta(xy)_n&amp;=x_n\\Delta y_n+y_{n+1}\\Delta x_n,\\tag{6.5}\\\\ \\Delta(xy)_n&amp;=y_n\\Delta x_n+x_{n+1}\\Delta y_n,\\tag{6.6}\\\\ \\Delta(xy)_n&amp;=x_n\\Delta y_n+y_n\\Delta x_n+\\Delta x_n\\Delta y_n.\\tag{6.7} \\end{align*}\\] This is also valid if the sequances are \\(N\\)-dimensional, where we interpret the products above as scalar products (\\(xy^\\top\\)). Using these definitions and the lemma above we see that the dynamics of the self-financing portfolio is given below. Proposition 6.6. (Bjork) The dynamics of any self-financing portfolio supporting the consumption stream \\(c\\) are given by \\[ \\Delta V_n=h_n \\Delta S_n+h_nd_{n+1}-c_{n+1},\\tag{6.11} \\] or, in more detail \\[ \\Delta V_n=\\sum_{i=1}^Nh_n^i(\\Delta S_n^i+d^i_{n+1})-c_{n+1}.\\tag{6.12} \\] We may rewrite the dividends as accumulating dividends \\(D^i_n=\\sum_{k=1}^nd^i_k\\) and see that \\(d_{n+1}^i=\\Delta D^i_n\\) and so the above condition is equivalent with. Proposition 6.8. (Bjork) The dynamics of any self-financing portfolio supporting the consumption stream \\(c\\) are given by \\[ \\Delta V_n=h_n \\Delta S_n+h_n\\Delta D_n-c_{n+1},\\tag{6.15} \\] or, in more detail \\[ \\Delta V_n=\\sum_{i=1}^Nh_n^i(\\Delta S_n^i+\\Delta D^i_n)-c_{n+1}.\\tag{6.16} \\] 5.2.2 Continuous time SF portfolio Formulating the dynamics of the self-financing portfolio in continuous time is easy work given the discrete setup above. However since we now are in continuous time we will change the \\(n\\) with a \\(t\\) and cosider the behavour \\(V_{t+dt}-V_t\\) as we let \\(dt\\to 0\\). First we formulate some basic notation. Definition 6.9. (Bjork) We use the following definitions. \\(S_t^i\\) is th price of asset \\(i\\) at time \\(t\\), \\(h_t^i\\) is the number of units of asset \\(i\\) held at time \\(t\\), \\(D_t^i\\) is the cumulative dividend processs for asset \\(i\\), \\(h_t\\) is the portfolio \\((h_t^1,...,h_t^N)\\) held at time \\(t\\), \\(c_t\\) is the consumption rate at time \\(n\\) (negative being deposits/saving), \\(V_t\\) is the value of the portfolio at time \\(t\\) i.e. of the portfolio \\(h_t\\) at time \\(t\\). Given these definitions we may define a portfolio strategy that is self-financing. Definition 6.10. (Bjork) Let \\(S\\) be and adapted \\(N\\)-dimensional price process. We define the following A portfolio strategy is any adapted \\(N\\)-dimensional process \\(h\\). The value process \\(V^h\\) corresponding to the portfolio \\(h\\) is given by \\[ V_t^h=\\sum_{i=1}^N h_t^iS_t^i.\\tag{6.17} \\] A consumption process is any adapted one-dimensional process \\(c\\). A portfolio-consumption pair \\((h,c)\\) is called self-financing if the value process \\(V^h\\) satisfies the condition \\[ dV_t^h=\\sum_{i=1}^N h_t^i(dS_t^i+d D^i_t)-c_t\\ dt,\\tag{6.18} \\] i.e. if \\[ dV_t^h=h_t\\ dS_t + h_t\\ dD_t -c_t\\ dt. \\] The gain process \\(G\\) is defined by \\[ G_t=S_t+D_t\\tag{6.19} \\] so we can write the self-financing condition as \\[ dV_t=h_t\\ dG_t-c_t\\ dt.\\tag{6.20} \\] The portfolio \\(h\\) is said to be Markovian if it is of the form \\[ h_t=h(t,S_t), \\] for some function \\(h : \\mathbb{R}_+\\times \\mathbb{R}^N\\to\\mathbb{R}^N\\). 5.2.3 Portfolio weights Definition 6.11. (Bjork) For a given portfolio \\(h\\) the corresponding relative portfolio or portfolio weights \\(w\\) are defined by \\[ w_t^i=\\frac{h_t^iS_t^i}{V_t^h},\\ i=1,...,N,\\tag{6.21} \\] so, in particular, we have \\(\\sum_{i=1}^N w_i=1\\). Lemma 6.12. (Bjork) A portfolio-consumption pair \\((h,c)\\) is self-financing if and only if \\[ dV_t^h=V_t^h\\sum_{i=1}^N w_t^i\\frac{dS_t^i+dD_t^i}{S_t^i}-c_t\\ dt\\tag{6.22} \\] or equivalently with the absolute weights \\[ dV_t^h=\\sum_{i=1}^N h_t^i(dS_t^i+dD_t^i)-c_t\\ dt. \\] Lemma 6.13. (Bjork) Consider the case with no dividends. Let \\(c\\) be a consumption process, and assume that there exist a scalar process \\(Z\\) and a vector process \\(q=(q^1,...,q^N)\\) such that \\[ dZ_t=Z_t\\sum_{i=1}^N q_t^i\\frac{dS_t^i}{S_t^i}-c_t\\ dt,\\tag{6.23} \\] and \\(\\sum_{i=1}^Nqq^i=1\\) (eq. 6.24). Now define a portfolio \\(h\\) by \\[ h_t^i=\\frac{q_t^iZ_t}{S_t^i}.\\tag{6.25} \\] Then the value process \\(V^h\\) is given by \\(V^h=Z\\), the pair \\((h,c)\\) is self-financing, and the corresponding relative portfolio \\(w\\) is given by \\(w=q\\). "],["black-scholes-pde.html", "5.3 Black-Scholes PDE", " 5.3 Black-Scholes PDE The Black-Scholes model revolves arround SDE’s as seen above. In this model we have two assets a risk free asset \\(B\\) and a stochastic priced asset \\(S\\). We therefore start by defining what we mean by a quote-on-qoute risk free asset. Definition 7.1. (Bjork) The price process \\(B\\) is the price of a risk free asset if it has the dynamics \\[ dB_t=r_t B_t\\ dt,\\tag{7.1} \\] where \\(r\\) is any \\(\\mathcal{F}_t\\) adapted process. We see from this definition that the meaning of “risk free” is the property, that \\(B\\) is priced locally deterministic in the sence that \\(r\\) is adapted and therefore known at time \\(t\\) and we therefore know the yield on a short term basis. This is also why we may call \\(r\\) the short interest rate. Given the dynamics above, we know that \\(B\\) in fact is represented by the process \\[ B_t=B_0e^{\\int_0^tr_s\\ ds}, \\] for some \\(B_0\\) initial value. We will moving forward assume that \\(B_0=1\\). The stochastic asset \\(S\\) has dynamics. \\[ dS_t=\\mu(t,S_t)\\ dt + \\sigma(t,S_t)\\ dW_t,\\tag{7.2} \\] where as usual \\(\\mu\\) and \\(\\sigma\\) are deterministic functions and \\(W_t\\) is a standard Brownian motion. Note that the risk free asset has a similarly process with \\(\\sigma = 0\\). We may now include this in the definition of the Black-Scholes model. Definition 7.2. (Bjork) The Black-Scholes model consists of two assets with dynamics given by \\[\\begin{align*} dB_t&amp;=rB_t\\ dt,\\tag{7.3}\\\\ dS_t&amp;=\\mu S_t\\ dt+\\sigma S_t\\ dW_t,\\tag{7.4} \\end{align*}\\] where \\(r,\\mu,\\sigma\\) are deterministic constants. Definition 7.3. (Bjork) A zero coupon bond with maturity \\(T\\) (henceforth “\\(T\\)-bond”) is an asset which pays the holder the face value 1 dollar at time \\(T\\). The price at time \\(n\\) of a \\(T\\)-bond is denoted by \\(p(n,T)\\). Definition 7.4. The (possible stochastic) discrete short rate \\(r_n\\), for the period \\([n,n+1]\\), is defined as \\[ p(n,n+1)=\\frac{1}{1+ r_n}.\\tag{7.6} \\] From this short rate we may derive the dynamics of the bank account recieving zero-coupon rates for each distinct time interval. Definition 7.5. (Bjork) The dynamics of the bank account are given by \\[ \\Delta B_n=r_n B_n.\\tag{7.7} \\] 5.3.1 Contingent Claims and Arbitrage Definition 7.6. (Bjork) A European call option with exercise price (or strike price) \\(K\\) and time of maturity (exercise date) \\(T\\) on the underlying asset \\(S\\) is a contract defined by the following clauses: The holder of the option has, at time \\(T\\), the right to buy one share of the underlying stock at the price \\(K\\) dollars from the underwriter of the option. The holder of the option is in no way obliged to buy the underlying stock. The right to buy the underlying stock at the price \\(K\\) can only be exercised at the precise time \\(T\\). Obviously, we also have the european put option which gives the owner the right to sell an asset at price \\(K\\) at time \\(T\\). Let os formally define a contingent claim. Definition 7.7. Consider a financial market with vector price process \\(S\\). A contingent claim with date of maturity \\(T\\), also called a \\(T\\)-claim, is any random variable \\(\\mathcal{X}\\in\\mathcal{F}_T^S\\). A contingent claim \\(\\mathcal{X}\\) is called a simple claim if it is of the form \\(\\mathcal{X} = \\Phi(S_t)\\). The function \\(\\Phi\\) is called the contract function. Definition 7.8. (Bjork) An arbitrage possibility on a financial market is a self-financed portfolio \\(h\\) such that \\[\\begin{align*} V^h(0)&amp;=0,\\tag{7.13}\\\\ P(V_T^h\\ge0)&amp;=1,\\tag{7-14}\\\\ P(V_T^h&gt;0)&amp;&gt;0.\\tag{7.15} \\end{align*}\\] We say that the market is arbitrage free if there are no arbitrage possibilities. Definition 7.9. (Bjork) Suppose that there exists a self-financing portfolio \\(h\\), such that the value process \\(V^h\\) has the dynamics \\[ d V_t^h=k_tV_t^h\\ dt,\\tag{7.16} \\] where \\(k\\) is an adapted process. Then it must hold that \\(k_t=r_t\\) for all \\(t\\), ore there exists an arbitrage possibility. Theorem 7.10. (Bjork) (Black-Scholes equation) Assume that the market is specified by the equations \\[\\begin{align*} dB_t&amp;=rB_t\\ dt,\\tag{7.18}\\\\ dS_t&amp;=\\mu(t,S_t) S_t\\ dt+\\sigma(t,S_t)S_t\\ dW_t,\\tag{7.19} \\end{align*}\\] and that we want to price a contingent claim of the form \\(\\mathcal{X}=\\Phi(S_t)\\) (eq. 7.20). Then the only pricing function of the form \\(\\Pi_t[\\Phi(S_t)]=F(t,S_t)\\) (eq. 7.21) which is consistent with the absence of arbitrage in the market \\([B_t,S_t,\\Pi_t]\\) is when \\(F\\) is the solution of the following boundary value problem in the domain \\([0,T]\\times\\mathbb{R}_+\\): \\[\\begin{align*} F_t(t,s)+rsF_s(t,s)+\\frac{1}{2}s^2\\sigma^2(t,s)F_{ss}(t,s)-rF(t,s)&amp;=0,\\\\ F(T,s)&amp;=\\Phi(s). \\end{align*}\\] 5.3.2 Risk Neutral Valuation Theorem 7.11. (Bjork) (Risk Neutral Valuation) The arbitrage free price of the claim \\(\\Phi(S_t)\\) is given by \\(\\Pi_t[\\Phi]=F(t,S_t)\\), where \\(F\\) is given by the formula \\[ F(t,s)=e^{-r(T-t)}E^Q_{t,s}[\\Phi(S_T)],\\tag{7.43} \\] where the \\(Q\\)-dynamics of \\(S\\) are those of \\[ dS_t=rS_t\\ dt+S_t\\sigma(t,S_t)\\ dW_t^Q.\\tag{7.42} \\] Property 7.12. (Bjork) (The Martingale Property) In the Black-Scholes model, the price process \\(\\Pi_t\\) for every traded asset, be it the underlying or derivate asset, has the property the the normalized price process \\[ Z_t=\\frac{\\Pi_t}{B_t}, \\] (including \\(S_t/B_t\\)) is a martingale under the measure \\(Q\\). 5.3.3 Black-Scholes formula This chapter will center on deriving the famous Black-Scholes formula. We start by laying out the assumptions of the model. We have a market consiting of two assets: a stochastic prices asset \\(S\\) and a risk free asset \\(B\\). The prices processes have dynamics: \\[\\begin{align*} dS_t&amp;=\\mu S_t\\ dt+\\sigma S_t\\ dW_t,\\tag{7.45}\\\\ dB_t&amp;=r B_t\\ dt,\\tag{7.44} \\end{align*}\\] where \\(S_0=s\\) and \\(B_0=1\\) (by assumption). Now from Feymann-Kac and the definition of arbitrage we know that a simple claim \\(\\Phi(S_t)\\) has the arbitrage free price given by the risk neutral valueation formula. \\[ F(t,s)=e^{-r(T-t)}E^Q_{t,s}[\\Phi(S_T)],\\tag{7.46} \\] where \\(Q\\) is a probability measure, namely a Martingale measure, such that the dynamics of \\(S\\) under this measure is \\[ dS_t=r S_t\\ dt+\\sigma S_t\\ dW^Q_t,\\tag{7.47} \\] with \\(W_t^Q\\) being a Brownian motion wrt. to the probability measure \\(Q\\) (not \\(P\\)). The above still has the initial condition \\(S_0=s\\). Given these assumptions we may formulate the Black-Scholes formula. Theorem 7.13. (Bjork) (Black-Scholes formula) The price of the european call option with strikeprice \\(K\\) and maturity \\(T\\) (contract function \\(\\Phi(S_t)=\\left( S_t - K\\right)^+\\)) takes the form \\(\\Pi_t=F(t,s)\\), where \\[ F(t,s)=s N(d_1(t,s))-e^{-r(T-t)}KN(d_2(t,s)),\\tag{7.52} \\] where \\(N\\) is the distribution-function for an \\(\\mathcal{N}(0,1)\\)-distributed random variable and \\[\\begin{align*} d_1(t,s)&amp;=\\frac{1}{\\sigma \\sqrt{T-t}}\\left(\\log\\left(\\frac{s}{K}\\right)+\\left(r+\\frac{1}{2}\\sigma^2\\right)(T-t)\\right),\\tag{7.53}\\\\ d_2(t,s)&amp;=d_1(t,s)-\\sigma\\sqrt{T-t}.\\tag{7.54} \\end{align*}\\] Proof. We let the market be given in terms of the price processes \\(S\\) and \\(B\\) with dynamics. \\[\\begin{align*} dS_t&amp;=\\mu S_t\\ dt+\\sigma S_t\\ dW_t,\\\\ dB_t&amp;=r B_t\\ dt, \\end{align*}\\] with \\(B_t=1\\) and \\(S_t=s\\). We assume that \\(\\mu,\\sigma, r\\) are deterministic real numbers. Consider the contingent claim \\[ \\Phi(S_t)=\\left( S_t - K\\right)^+, \\] that is the European call option. Let \\(Q\\) be a martingale measure such that the dynamics of \\(S\\) may be written as \\[ dS_t=r S_t\\ dt+\\sigma S_t\\ dW^Q_t, \\] then \\(S_t\\) is clearly a GBM wrt. the measure \\(Q\\). Therefore we know the solution given in terms of the increment of the Brownian motion \\(W^Q\\) as follows \\[ S_u=s\\cdot \\exp\\left\\{\\left(r-\\frac{1}{2}\\sigma^2\\right)(u-t)+\\sigma\\left(W_u^Q-W_t^Q\\right)\\right\\}, \\] for some initial condition \\(S_t=s\\). From theorem 7.10 we know that the only pricing function which takes the form \\[ \\Pi_t[\\Phi(S_T)]=F(t,S_t), \\] can only be consistent with the absence of arbitrage if \\(F\\) is the solution the the boundary value problem \\[\\begin{align*} F_t(t,s)+rsF_s(t,s)+\\frac{1}{2}s^2\\sigma^2F_{ss}(t,s)-rF(t,s)&amp;=0,\\\\ F(T,s)&amp;=\\Phi(s). \\end{align*}\\] From Feymann-Kac we then know that the stochastic representation of such a solution take the form \\[ F(t,s)=e^{-r(T-t)}E_{t,s}^Q[\\Phi(S_T)]. \\] Here the superscript refers to taking mean value with respect to the measure \\(Q\\). This gives the solution to the pricing function \\[ F(t,s)=e^{-r(T-t)}\\int \\Phi(S_T)\\ dQ. \\] Under the measure \\(Q\\) we have that for \\(u\\ge t\\): \\[ Z_u=\\log (S_u/s)\\sim \\mathcal{N}\\left(\\left(r-\\frac{1}{2}\\sigma^2\\right)(u-t),\\sigma\\sqrt{u-t}\\right) \\] Hence we may set \\(u=T\\) and observe that \\[\\begin{align*} F(t,s)&amp;=e^{-r(T-t)}\\int_{-\\infty}^\\infty \\Phi(se^z) f(z)\\ dz\\\\ &amp;=e^{-r(T-t)}\\int_{-\\infty}^\\infty (se^z-K)^+ f(z)\\ dz\\\\ &amp;=e^{-r(T-t)}\\int_{\\log\\left(\\frac{K}{s}\\right)}^{\\infty} (se^z-K) f(z)\\ dz\\\\ &amp;=e^{-r(T-t)}\\left(s\\int_{\\log\\left(\\frac{K}{s}\\right)}^{\\infty} e^z f(z)\\ dz-K\\int_{\\log\\left(\\frac{K}{s}\\right)}^{\\infty} f(z)\\ dz\\right), \\end{align*}\\] where we used that \\(f\\) is the distribution function of a normal distributed random variable with mean \\((r-\\sigma^2/2)(T-t)\\) and variance \\(\\sigma\\sqrt{T-t}\\) and that \\[ (se^z-K)^+ \\ge 0\\iff se^z\\ge K\\iff z\\ge \\log\\left(\\frac{K}{s}\\right) \\] Using that the MGF of a \\(X\\sim\\mathcal{N}(\\alpha, \\beta^2)\\) variable is \\[ E[e^{tX}]=e^{\\alpha t+\\frac{1}{2}\\beta ^2t^2}, \\] and the shorthand \\(N(t)\\) for the distribution function of the standard normal distribution, we have \\[\\begin{align*} F(t,s)&amp;=e^{-r(T-t)}\\left(sE\\left[e^{Z_T}1_{Z_T\\ge \\log\\left(\\frac{K}{s}\\right)}\\right]-K P\\left(Z_T\\ge \\log\\left(\\frac{K}{s}\\right)\\right)\\right)\\\\ &amp;=e^{-r(T-t)}s\\exp\\left\\{\\left(r-\\frac{1}{2}\\sigma^2\\right)(T-t)+\\frac{1}{2}\\sigma^2(T-t)\\right\\}E\\left[1_{Z_T\\ge \\log\\left(\\frac{K}{s}\\right)}\\right]\\\\ &amp;-e^{-r(T-t)}K P\\left(X\\ge\\frac{1}{\\sigma\\sqrt{T-t}}\\left( \\log\\left(\\frac{K}{s}\\right)-(r-\\sigma^2/2)(T-t)\\right)\\right)\\\\ &amp;=sE\\left[1_{Z_T\\ge \\log\\left(\\frac{K}{s}\\right)}\\right]-e^{-r(T-t)}K P\\left(X\\le\\frac{1}{\\sigma\\sqrt{T-t}}\\left(\\log\\left(\\frac{s}{K}\\right)+(r-\\sigma^2/2)(T-t)\\right)\\right)\\\\ &amp;=sN(d_1(s,t))-e^{-r(T-t)}K N\\left(d_2(s,t)\\right), \\end{align*}\\] as desired. \\(\\blacksquare\\) "],["completeness-and-hedging.html", "5.4 Completeness and Hedging", " 5.4 Completeness and Hedging We derived the pricing function of the european call option above and introduced the theory around boundary value problems and Feymann-Kac solution to the partial differential stochastic equation. Now we want to see if a portfolio exists such that it gives the payout \\(\\Phi(S_T)\\) with probability one. In order to do this, we return to the concept of hedge and replication. Definition 8.1. (Bjork) We say that a \\(T\\)-claim \\(\\mathcal{X}\\) can be replicated, alternatively the it is reachable or hedgeable, if there exists a self-financing portfolio \\(h\\) such that \\[ V_T^h=\\mathcal{X},\\ P-\\text{a.s.}\\tag{8.1} \\] In this case we say that \\(h\\) is a hedge against \\(\\mathcal{X}\\). Alternatively, \\(h\\) is called a replicating or hedging portfolio. If every contingent claim is reachable we say that the market is complete. If we can find a portfolio \\(h\\) that reaches \\(\\mathcal{X}\\) in value over the time period \\([t,T]\\) it must mean, that holding the portfolio is equivalent with holding the contract itself. We therefore have the natural assumption that the price process must satisfie \\(\\Pi_t[\\mathcal{X}]=V_t^h\\) for all \\(t\\ge 0\\). How this relates to the absence of arbitrage is given below. Proposition 8.2. (Bjork) Suppose \\(\\mathcal{X}\\) is hedged using the portfolio \\(h\\). Then the only price process \\(\\Pi_t[\\mathcal{X}]\\) which is consistent with no arbitrage is given by \\(\\Pi_t[\\mathcal{X}]=V_t^h\\). Furthermore, if \\(\\mathcal{X}\\) can be hedged by both \\(h\\) and \\(g\\) then \\(V_t^g=V_t^h\\) for all \\(t\\) with probability one. 5.4.1 Completeness in Black-Scholes The Black-Scholes model will be investegated in the following. We start by stating the important theorem. Theorem 8.3. (Bjork) Consider the Black-Scholes model given by \\[\\begin{align*} dS_t&amp;=\\mu(t,S_t) S_t\\ dt+\\sigma(t,S_t) S_t\\ dW_t,\\tag{8.2}\\\\ dB_t&amp;=r B_t\\ dt,\\tag{8.3} \\end{align*}\\] The model above is complete. The following lemma gives us replicability of a simple claim (which we will restrict ud to). Lemma 8.4. (Bjork) Suppose that there exist an adapted process \\(V\\) and an adapted process \\(w=[w^B,w^S]\\) with \\(w^B_t+w^S_t=1\\) (eq. 8.4) for all \\(t\\ge 0\\), such that \\[\\begin{align*} dV_t&amp;=V_t(w_t^Br+w_t^S\\mu(t,S_t))\\ dt+V_tw_t^S\\sigma(t,S_t)\\ dW_t,\\tag{8.5}\\\\ V_t&amp;=\\Phi(S_t).\\tag{8.5} \\end{align*}\\] Then the claim \\(\\mathcal{X}=\\Phi(S_t)\\) can be replicated using \\(w\\) as the relative portfolio. The corresponding value process is given by the process \\(V\\) and the absolute portfolio \\(h\\) is given by \\[\\begin{align*} h_t^B&amp;=\\frac{w_t^B V_t}{B_t},\\tag{8.6}\\\\ h_t^S&amp;=\\frac{w_t^S V_t}{S_t}.\\tag{8.7} \\end{align*}\\] Doing some heuristics we come up with some clever weights, which turns on to adhere to the boundary value problem formulated in the Black-Scholes equation. Given that the weights gives rise to the desired value process, we have succesfully found the portfolio weight (see lemma above). Theorem 8.5. (Bjork) Consider the Black-Scholes model given in (8.3)-(8.4), and a simple contingent claim \\(\\mathcal{X}=\\Phi(S_t)\\). Define \\(F\\) as the solution to the boundary value problem \\[\\begin{align*} F_t(t,s)+rsF_s(t,s)+\\frac{1}{2}s^2\\sigma^2F_{ss}(t,s)-rF(t,s)&amp;=0,\\tag{8.17}\\\\ F(T,s)&amp;=\\Phi(s).\\tag{8.17} \\end{align*}\\] Then \\(\\mathcal{X}\\) can be replicated by the relative portfolio \\[\\begin{align*} w_t^B&amp;=\\frac{F(t,S_t)-S_tF_s(t,S_t)}{F(t,S_t)},\\tag{8.18}\\\\ w_t^S&amp;=\\frac{S_tF_s(t,S_t)}{F(t,S_t)}.\\tag{8.19} \\end{align*}\\] The corresponding absolute portfolio is given by \\[\\begin{align*} h_t^B&amp;=\\frac{F(t,S_t)-S_tF_s(t,S_t)}{B_t},\\tag{8.20}\\\\ h_t^S&amp;=F_s(t,S_t),\\tag{8.21} \\end{align*}\\] and the value process \\(V^h\\) is given by \\[ V^h_t=F(t,S_t).\\tag{8.22} \\] Proposition 8.6. (Bjork) Consider the Black-Scholes model given in (8.3)-(8.4), and a contingent claim on the form \\(\\mathcal{X}=\\Phi(S_T,Z_T)\\) (eq. 8.29). We define the process \\(Z_t\\) as \\[ Z_t=\\int_0^tg(u,S_u)\\ du,\\tag{8.30} \\] for some choice of the deterministic function \\(g\\). Then \\(\\mathcal{X}\\) can be replicated using a relative portfolio given by \\[\\begin{align*} w_t^B&amp;=\\frac{F(t,S_t,Z_t)-S_tF_s(t,S_t,Z_t)}{F(t,S_t,Z_t)},\\tag{8.31}\\\\ w_t^S&amp;=\\frac{S_tF_s(t,S_t,Z_t)}{F(t,S_t,Z_t)}.\\tag{8.32} \\end{align*}\\] where \\(F\\) is the solution to the boundary value problem \\[\\begin{align*} F_t(t,s,z)+rsF_s(t,s,z)+\\frac{1}{2}s^2\\sigma^2F_{ss}(t,s,z)-rF(t,s,z)&amp;=0,\\tag{8.33}\\\\ F(T,s,z)&amp;=\\Phi(s,z).\\tag{8.33} \\end{align*}\\] The corresponding value process is given by \\(V_t=F(t,S_t,Z_t)\\), and \\(F\\) has the stochastic representation \\[ F(t,s,z)=e^{-r(T-t)}E^Q_{t,s,z}[\\Phi(S_T,Z_T)],\\tag{8.34} \\] where the \\(Q\\)-dynamics are given by \\[\\begin{align*} dS_u&amp;=rS_u\\ du + S_u\\sigma(u,S_u)\\ dW^Q_u,\\tag{8.35}\\\\ S_t&amp;=s,\\tag{8.36}\\\\ dZ_u&amp;=g(u,S_u)\\ du,\\tag{8.37}\\\\ Z_t&amp;=z.\\tag{8.38} \\end{align*}\\] 5.4.2 Absence of Arbitrage In general we have conflicting forces when evaluating when a certain market is arbitrage free and/or complete. We have in simple terms the non-rigorous theorem below. Meta-theorem 8.3.1. (Bjork) Let \\(N\\) denote the number of underlying traded assets in the model excluding the risk free asset, and let \\(R\\) denote the number of random sources driving the price system. Genericallly we then have the following statements. The model is arbitrage free if and only if \\(N\\le R\\). The model is complete if and only if \\(N\\ge R\\). The model is arbitrage free and complete if and only if \\(N=R\\). 5.4.3 Incomplete Markets We assume a market with a risk free asset and one risky assets with dynamics \\[ dX_t=\\mu(t,X_t)\\ dt+\\sigma(t,X_t)\\ dW_t.\\tag{9.1} \\] We want to find a unique price of a derivative on a functional form of the risky asset. We assume that we cannot invest in the asset representing the process \\(X_t\\) and so we can solely write contracts based on the observation \\(X_T\\). The problem here is that we can only short or long the risk free asset and so no derivative is replicable. The way we solve this problem is by having the market set the price of risk and universally price derivatives based on this given price process. We then have the assumptions Assumption 9.2.1 We have the market given with the only investable asset \\(B\\) with dynamics \\[ dB_t=rB_t\\ dt.\\tag{9.2} \\] We furthermore, have an empirically observable stochastic process \\(X\\) which is not the price process of any traded asset. The \\(P\\)-dynamics of \\(X\\) is given by \\[ dX_t=\\mu(t,X_t)\\ dt+\\sigma(t,X_t)\\ dW_t. \\] Assumption 9.2.2 There is a liquid market for every contingent claim. Assumption 9.2.3 We assume that There is a liquid, frictionless market for each of the contingent claims \\(\\mathcal{Y}\\) and \\(\\mathcal{Z}\\). The market prices of the claims are of the form \\[ \\Pi_t[\\mathcal{Y}] = F(t,X_t),\\] \\[ \\Pi_t[\\mathcal{Z}] = G(t,X_t),\\] where \\(F\\) and \\(G\\) are smooth real valued function. From Ito’s formula we have the dynamics \\[\\begin{align*} dF=\\mu_F F\\ dt+\\sigma_F F\\ dW,\\tag{9.4}\\\\ dG=\\mu_G G\\ dt+\\sigma_G G\\ dW.\\tag{9.5} \\end{align*}\\] Where the processes \\(\\mu_F\\) and \\(\\sigma_F\\) are given by \\[\\begin{align*} \\mu_F&amp;=\\frac{F_t+\\mu F_x+\\frac{1}{2}\\sigma^2 F_{xx}}{F},\\\\ \\sigma_F&amp;=\\frac{\\sigma F_x}{F}. \\end{align*}\\] By forming a portfolio of the two contracts we lead to the relation. \\[ \\frac{\\mu_F-r}{\\sigma_F}=\\frac{\\mu_G-r}{\\sigma_G}. \\] This gives the important insight. Proposition 9.1. (Bjork) Assume that the market for derivatives is free of arbitrage. Then there exists a universal process \\(\\lambda(t,X_t)\\) such that, with probability one, and for all \\(t\\), we have \\[ \\frac{\\mu_F(t,X_t)-r}{\\sigma_F(t,X_t)}=\\mu(t,X_t),\\tag{9.7} \\] regardless of the specific choice of the derivative \\(F\\). Proposition 9.2. (Bjork) Assume absence of arbitrage, the pricing function \\(F(t,x)\\) of the \\(T\\)-claim \\(\\Phi(X_T)\\) solves the following boundary value problem. \\[\\begin{align*} F_t(t,x)+\\mathcal{A}F(t,x)-rF(t,x)&amp;=0,\\hspace{15pt}&amp;(t,x)\\in (0,T)\\times \\mathbb{R},\\tag{9.8}\\\\ F(T,x)&amp;=\\Phi(x), &amp;x\\in\\mathbb{R},\\tag{9.9} \\end{align*}\\] where \\[ \\mathcal{A}F(t,x)=\\left\\{\\mu(t,x)-\\lambda(t,x)\\sigma(t,x)\\right\\}F_x(t,x)+\\frac{1}{2}\\sigma^2(t,x)F_{xx}(t,x). \\] Proposition 9.3. (Bjork) (Risk neutral valuation) Assuming absence of arbitrage, the pricing function \\(F(t,x)\\) of the \\(T\\)-claim \\(\\Phi(X_T)\\) is given by the formula \\[ F(t,x)=e^{-r(T-t)}E^Q_{t,x}[\\Phi(X_T)].\\tag{9.11} \\] The dynamics of \\(X\\) under the martingale measure \\(Q\\) are given by \\[ dX_t=\\left\\{\\mu(t,x)-\\lambda(t,x)\\sigma(t,x)\\right\\}F_x(t,x)+\\sigma(t,x)\\ dW^Q_t, \\] where \\(W^Q\\) is a \\(Q\\)-Brownien motion. "],["parity-relations.html", "5.5 Parity relations", " 5.5 Parity relations 5.5.1 Put-call Parity The notion of continuous rebalancing the replicating portfolio require leads to problems in the real world. Trading does cost some money (typical in fractions) and so contiuous balancing would make the portfolio go to 0 rather quickly. Why? The Brownian motion has unbounded variation and so we would have to sell and buy the portfolio uncountable many time in any interval and the shift in weight is not neglible. Because of this we would like to see which claims we can replicate by buying and holding a combination of assets and derivatives. Proposition 10.1. (Bjork) Let \\(\\Phi\\) and \\(\\Psi\\) be contract functions for the \\(T\\)-claims \\(\\mathcal{X}=\\Phi(S_T)\\) and \\(\\mathcal{Y}=\\Psi(S_T)\\). Then for any real numbers \\(\\alpha\\) and \\(\\beta\\) we have the following price relation: \\[ \\Pi_t[\\alpha \\Phi + \\beta\\Psi]=\\alpha \\Pi_t[\\Phi]+\\beta\\Pi_t[\\Psi].\\tag{10.1} \\] If we consider the basic contract functions \\[\\begin{align*} \\Phi_S(x)&amp;=x,\\tag{10.2}\\\\ \\Phi_B(x)&amp;=1,\\tag{10.3}\\\\ \\Phi_{C,K}(x)&amp;=(x-K)^+,\\tag{10.4}\\\\ \\Phi_{P,K}(x)&amp;=(K-x)^+. \\end{align*}\\] That is a contract paying (respectively): the price of one stock, 1 dollar, one european call and one european put both with strike \\(K\\). It is clear that the following prices are \\[\\begin{align*} \\Pi_t[\\Phi_S]&amp;=S_t,\\tag{10.5}\\\\ \\Pi_t[\\Phi_B]&amp;=e^{-r(T-t)},\\tag{10.6}\\\\ \\Pi_t[\\Phi_{C,K}]&amp;=c(t,S_t;K,T),\\tag{10.7}\\\\ \\Pi_t[\\Phi_{P,K}]&amp;=p(t,S_t;K,T). \\end{align*}\\] Where \\(c(t,s,K,T,r,\\sigma)\\) and \\(p(t,s,K,T,r,\\sigma)\\) are the pricing function of the european call and put option. We see that we can replicate these payouts by: buying the stock today and selling at time \\(T\\), buying a zero coupon \\(T\\)-bond with face value 1, buying the call and put option. Then we can by choosing \\(\\alpha,\\beta,\\gamma_1,...,\\gamma_n\\) form a portfolio consisting of \\(\\alpha\\) stocks, \\(\\beta\\) \\(T\\)-bonds and \\(\\gamma_i\\) call options with maturity \\(T\\) and strike \\(K_i\\). The price is then a linear combination given the choice (se proposition 10.1). The put option is not includet in the above portfolio as we have the put-call parity below Proposition 10.2. (Bjork) (Put-call parity) Consider a European call and a European put, both with strike \\(K\\) and time of maturity \\(T\\). Then we have the relation. \\[ p(t,s) = Ke^{-r(T-t)}+c(t,s)-s.\\tag{10.11} \\] In particular the put option can be replicated by a constant portfolio consisting of \\(K\\) zero coupon \\(T\\)-bonds, a European call option and a single short position in the underlying stock. We now have the pleasing proposition given the class of claims we can reach with the buy-and-hold portfolio with \\(T\\)-bonds, stock and call options Proposition 10.3. (Bjork) Fix an arbitrary continuous contract function \\(\\Phi\\) with compact support. Then the corresponding contract can be replicated with arbitrary precision (in sup-norm) using a constant portfolio consisting only of bonds, call options and the underlying stock. 5.5.2 The Greeks When holding a portfolio we may denote the pricing function by \\(P(t,s)\\). Here we only have one underlying asset with price process \\(S_t\\). We now have two types of risk: Price changes in the underlying asset. Misspecifications in the model parameters. These two risk give rise to “the greeks” as defined below. Definition 10.4. (Bjork) The greeks of a portfolio is given by \\[ \\Delta=\\frac{\\partial P}{\\partial s},\\ \\Gamma=\\frac{\\partial^2 P}{\\partial s^2},\\ \\rho=\\frac{\\partial P}{\\partial r},\\ \\Theta=\\frac{\\partial P}{\\partial t},\\ \\mathcal{V}=\\frac{\\partial P}{\\partial s}. \\] For the call option in particular we have the following derivatives. Proposition 10.5. (Bjork) The greeks of a portfolio consisting of a single European call option with maturity \\(T\\) and strike price \\(K\\) have the following greeks (\\(\\varphi\\) denoting the density function of a \\(\\mathcal{N}(0,1)\\)-variable): \\[\\begin{align*} \\Delta&amp;=N(d_1),\\tag{10.17}\\\\ \\Gamma&amp;=\\frac{\\varphi(d_1)}{s\\sigma\\sqrt{T-t}},\\tag{10.18}\\\\ \\rho&amp;=K(T-t)e^{-r(T-t)}N(d_2),\\tag{10.19}\\\\ \\Theta&amp;=-\\frac{s\\varphi(d_1)\\sigma}{2\\sqrt{T-t}}-rKe^{-r(T-t)}N(d_2),\\tag{10.20}\\\\ \\mathcal{V}&amp;=s\\varphi(d_1)\\sqrt{T-t}\\tag{10.21}. \\end{align*}\\] "],["fundamental-pricing-theorem-i-and-ii.html", "5.6 Fundamental pricing theorem I and II", " 5.6 Fundamental pricing theorem I and II We start by stating the following theorem. Theorem 11.1. (Bjork) If at least on of the assets \\(S^1,...,S^N\\) has diffusion term which is non-zero at all times, and if naive portfolio strategies are admitted, then the model admits arbitrage. We will go as follows. Derive the fundamental pricing theorem 1 and 2 in a setting with zero interest rate. Then we will extend the result in general by choosing a simple numeraire. We start by defining some basic notation. Definition 11.2. (Bjork) Define the process \\(h\\) as \\[ h=[h^0,h^S]:=[h^0,h^1,...,h^N] \\] We define the following. For a process \\(h\\), its value process \\(V_t^h\\) is defined by \\[ V_t^h=h^0_t\\cdot 1+\\sum_{i=1}^Nh_t^iS_t^i,\\tag{11.3} \\] or in compact form \\[ V_t^h=h_t^0\\cdot 1 + h_t^S S_t\\tag{11.4} \\] An adapted process \\(h^S\\) is called admissible if there exists a non-negative real number \\(\\alpha\\) (which may depend on the choice of \\(h^S\\)) such that \\[ \\int_0^th_u^SdS_u\\ge -\\alpha,\\tag{11.5} \\] for all \\(t\\in[0,T]\\). A process \\(h\\), is called an admissible portfolio process if \\(h^S\\) is admissible. An admissible portfolio is said to be self-financing, if \\[ V_t^h=V_0^h+\\int_0^th_u^S\\ dS_u,\\tag{11.6} \\] i.e. if \\[ dV_t^h=h_t^S\\ dS_t.\\tag{11.7} \\] Lemma 11.3. (Bjork) For any adapted process \\(h^S\\) satisfying the admissibility condition above, and for any real number \\(x\\), there exists a unique adapted process \\(h^0\\), such that: The process \\(h\\) defined by \\(h=[h^0,h^S]\\) is self-financing. The value process is given by \\[ V_t^h=x+\\int_0^th_u^S\\ dS_u.\\tag{11.8} \\] In particular, the space \\(\\mathcal{K}_0\\) of portfolio values, reachable at time \\(T\\) by means of a self-financing portfolio with zero initial cost is given by \\[ \\mathcal{K}_0=\\left\\{\\int_0^Th_t^S\\ dS_u :\\ h^S\\ \\text{is}\\ \\text{admissible}\\right\\}.\\tag{11.9} \\] Definition 11.4. (Bjork) A probability measure \\(Q\\) on \\(\\mathcal{F}_T\\) is called equivalent martingale measure for the market model, the numeraire \\(S^0\\), and the time interval \\([0,T]\\), if it has the following properties: \\(Q\\sim P\\) on \\(\\mathcal{F}_T\\), so \\(P\\) and \\(Q\\) are equivalent. All price processes \\(S^0,S^1,...,S^N\\) are martingales under \\(Q\\) on the time interval \\([0,T]\\). An equivalent martingale measure will often be referred to as just “a martingale measure” or as “an EMM”. If \\(Q\\sim P\\) has the property that \\(S^0,S^1,...,S^N\\) are local martingales, then \\(Q\\) is called a local martingale measure. Theorem 11.5. (Bjork) (The First Fundamental Theorem) The model is arbitrage free “essentially” if and only if there existis a (local) martingale measure \\(Q\\). Definition 11.6. (Bjork) With the notation above, we say that the model admits No Arbitrage (NA) if \\[ \\mathcal{C}\\cap L_+^\\infty=\\{0\\},\\tag{11.21} \\] No Free Lunch with Vanishing Risk (NFLVR) if \\[ \\tilde{\\mathcal{C}}\\cap L_+^\\infty=\\{0\\},\\tag{11.22} \\] where \\(\\tilde{\\mathcal{C}}\\) denotes the closure of \\(\\mathcal{C}\\) in \\(L^\\infty\\). Theorem 11.7. (Bjork) (Kreps-Yan Separation Theorem) If \\(\\mathcal{C}\\) is weak* closed, and if \\[ \\mathcal{C}\\cap L_+^\\infty=\\{0\\}, \\] then there exists a random variable \\(L\\in L^1\\) such that \\(L\\) is \\(P\\) almost surely strictly positive, and \\[ E^P[L\\cdot X]\\le 0, \\] for all \\(X\\in\\mathcal{C}\\). Proposition 11.8. (Bjork) If the asset price processes are uniformly bounded, then the condition NFLVR implies that \\(\\mathcal{C}\\) is weak* closed. Theorem 11.9. (Bjork) (First Fundamental Theorem) Assume that the asset price process \\(S\\) is bounded. Then there exists an equivalent martingale measure if and only if the model satisfies NFLVR. Theorem 11.10. (Bjork) (First Fundamental Theorem) Assume that the asset price process \\(S\\) is locally bounded. Then there exists an equivalent martingale measure if and only if the model satisfies NFLVR. Assumption 11.4.1. (Bjork) We assume that \\(S_t^0&gt;0\\) \\(P\\)-a.s. for all \\(t\\ge 0\\). Definition 11.11. (Bjork) The normalized economy (also referred to as the “Z-economy”) is defined by the price vector process \\(Z\\), where \\[ Z_t=\\frac{S_t}{S_t^0}. \\] Definition 11.12. (Bjork) A portfolio stragegy is any adapted \\((N+1)\\)-dimensional process \\[ h_t=[h_t^0,h_t^1,...,h_t^N]. \\] The S-value process \\(V_t^S\\) corresponding to the portfolio \\(h\\) is \\(h_tS_t\\). The Z-value process \\(V_t^Z\\) corresponding to the portfolio \\(h\\) is \\(h_tZ_t\\). A portfolio is said to be admissible if it is admissible as an \\(Z\\) portfolio. An admissible portfolio is S-self-balancing if \\[ dV_t^S=\\sum_{i=0}^Nh_t^i\\ dS_t^i\\tag{11.26} \\] An admissible portfolio is Z-self-balancing if \\[ dV_t^Z=\\sum_{i=0}^Nh_t^i\\ dZ_t^i.\\tag{11.28} \\] Lemma 11.13. (Bjork) (Invariance Lemma) With assumptions as above, the following hold. A portfolio \\(h\\) is S-self-financing if and only if it is Z-self-financing. The value processes \\(V^S\\) and \\(V^Z\\) are connected by \\[ V_t^Z=\\frac{1}{S_t^0}\\cdot V_t^S. \\] A claim \\(\\mathcal{Y}\\) is S-replical if and only if the claim \\[ \\frac{\\mathcal{Y}}{S_T^0} \\] is Z-replicable. The model is S arbitrage free if and only if it is Z arbitrage free. Theorem 11.14. (Bjork) (The First Fundamental Theorem) Consider the market model \\(S^0,S^1,...,S^N\\) where we assume that \\(S^0_t&gt;0\\), \\(P\\)-a.s. for all \\(t\\ge 0\\). Assume furthermore that \\(S^0,S^1,...,S^N\\) are locally bounded. Then the followin conditions are equivalent: The model satisfies NFLVR. There exists a measure \\(Q\\sim P\\) such that the processes \\[ Z^0,Z^1,...,Z^N, \\] are local martingales under \\(Q\\). 5.6.1 Completeness Lemma 11.15. (Bjork) Consider a given \\(T\\)-claim \\(X\\). Fix a martingale measure \\(Q\\) and assume that the normalized claim \\(X/S^0_T\\) is integrable. If the \\(Q\\)-martingale \\(M\\), defined by \\[ M_t=E^Q\\left[\\left. \\frac{X}{S^0_T}\\right\\vert \\mathcal{F}_t\\right],\\tag{11.34} \\] admits an integral representation of the form \\[ M_t=x+\\sum_{i=1}^N\\int_0^th_s^i\\ dZ_s^i,\\tag{11.35} \\] then \\(X\\) can be hedged in the S-economy. Furthermore, the replicating portfolio \\((h^0,h^1,...,h^N)\\) is given by the above for \\(h^i\\), \\(i=1,...,N\\) and \\(h_t^0=M_t-\\sum_{i=1}^Nh_t^iZ_t^i\\). Theorem 11.16. (Bjork) (Jacod) Let \\(\\mathcal{M}\\) denote the convex set of equivalent martingale measures. Then, for any fixed \\(Q\\in\\mathcal{M}\\), the following statements are equivalent: Every \\(Q\\) local martingale \\(M\\) has dynamics of the form \\[ dM_t=\\sum_{i=1}^Nh_s^i\\ dZ_s^i. \\] \\(Q\\) is an extremal point of \\(\\mathcal{M}\\). Theorem 11.17. (Bjork) (The Second Fundamental Theorem) Assume that the market is arbitrage free and consider a fixed numeraire asset \\(S^0\\). Then the market is complete if and onlt if the martingale measure \\(Q\\), corresponding to the numeraire \\(S^0\\), is unique. 5.6.2 Risk Neutral Valuation Formula We have the setting of a market consisting of the assets \\(S^0,...,S^N\\) of \\(N+1\\) assets. We consider the numeraire \\(S^0\\) being a risk free asset. We introduce a price of contingent claim \\(X\\), such that the extended market consisting of the price process of \\(X\\) and the \\(N+1\\) assets is arbitrage free. Alternatively, we can, equivalently, find a replicating portfolio \\(h\\) such that \\(V^h_T=X\\) with probability one. Theorem 11.18. (Bjork) (General Pricing Equation) The arbitrage free price process for the \\(T\\)-claim \\(X\\) is given by \\[ \\Pi_t[X]=S_t^0E^Q\\left[\\left.\\frac{X}{S^0_T}\\right\\vert \\mathcal{F}_t\\right],\\tag{11.41} \\] where \\(Q\\) is the (not necessarily unique) martingale measure for the a priori given market \\(S^0,S^1,...,S^N\\), with \\(S^0\\) as the numeraire. If if we assume that the bank account takes the form \\[ S_t^0=S_0^0e^{-\\int_0^tr(s)\\ ds}, \\] where \\(r\\) is the short rate, then we have the familier risk neutral valuation formula. Theorem 11.19. (Bjork) (Risk Neutral Valuation Formula) Assuming the existence of a short rate, the pricing formula takes the form \\[ \\Pi_t[X]=E^Q\\left[\\left.e^{-\\int_0^Tr(s)\\ ds}X\\right\\vert \\mathcal{F}_t\\right],\\tag{11.42} \\] where \\(Q\\) is the (not necessarily unique) martingale measure with the bank account as the numeraire. Definition 11.20. (Bjork) A zero coupon bond with maturity date \\(T\\), also called a \\(T\\)-bond, is a contract which guarantees the holder one dollar to be paid on the date \\(T\\). The price at time \\(t\\) of a bond with maturity date \\(T\\) is denoted by \\(p(t,T)\\). Proposition 11.21. (Bjork) The price of a zero coupon \\(T\\)-bond is given by \\[ p(t,T)=E^Q\\left[\\left.e^{-\\int_t^Tr(s)\\ ds}\\right\\vert \\mathcal{F}_t\\right],\\tag{11.43} \\] and in particular we have \\(p(T,T)=1\\) for all \\(T\\ge 0\\) (eq. 11.44). 5.6.3 Stochastic Discount Factors Definition 11.22. (Bjork) Assume the existence of a short rate \\(r\\). For any fixed martingale measure \\(Q\\), let the likelihood process \\(L\\) be defined by \\[ L_t=\\frac{dQ}{dP},\\ on\\ \\mathcal{F}_t.\\tag{11.48} \\] The stochastic discount factor (SDF) process \\(\\mathbf{M}\\), corresponding to \\(Q\\), is defined as \\[ \\mathbf{M}_t=e^{-\\int_0^tr(s)\\ ds}L_t\\ \\ \\left(=\\frac{1}{B_t}\\cdot L_t\\right).\\tag{11.49/50} \\] Proposition 11.23. (Bjork) Assume absence of arbitrage. With notation as above, the following hold: For any sufficiently integrable \\(T\\)-claim \\(X\\), the arbitrage free price is given by \\[ \\Pi_t[X]=E^P\\left[\\left. \\frac{\\mathbf{M}_T}{\\mathbf{M}_t} X \\ \\right\\vert\\ \\mathcal{F}_t\\right].\\tag{11.51} \\] For any arbitrage free asset price process \\(S\\) (derivative or underlying) the process \\(\\mathbf{M}_tS_t\\) is a (local) \\(P\\)-martingale. The \\(P\\)-dynamics of \\(\\mathbf{M}\\) are given by \\[ d\\mathbf{M}_t=-r_t\\mathbf{M}_t\\ dt+\\frac{1}{B_t}\\ dL_t.\\tag{11.53} \\] 5.6.4 Summary Theorem 11.24. (Bjork) (First Fundamental Theorem) The market model is free of arbitrage if and only if there exists a martingale measure, i.e. a measure \\(Q\\sim P\\) such that the processes \\[ \\frac{S_t^0}{S_t^0},\\frac{S_t^1}{S_t^0},...,\\frac{S_t^N}{S_t^0} \\] are (local) martingales under \\(Q\\). Proposition 11.25. (Bjork) If the numeraire \\(S^0\\) is the money account, i.e. \\[ S^0_t=e^{\\int_0^t r(s)\\ ds}, \\] where \\(r\\) is the (possibly stochastic) short rate, and if we assume that all processes are Brownian driven, then a measure \\(Q\\sim P\\) is a martingale measure if and only if all assets \\(S^0,S^1,...,S^N\\) have the short rate as their local rates of return, i.e. if the \\(Q\\)-dynamics are of the form \\[ dS_t^i=S_t^ir_t\\ dt+S_t^i \\sigma_t^i\\ dW_t^Q,\\tag{11.54} \\] where \\(W^Q\\) is a (multidimensional) \\(Q\\)-Brownian motion. Theorem 11.26. (Bjork) (Second Fundamental Theorem) Assuming absence of arbitrage, the market model is complete if and only if the martingale measure \\(Q\\) is unique. Proposition 11.27. (Bjork) In order to avoid arbitrage, \\(X\\) must be priced according to the formula \\[ \\Pi_t[X]=S^0_tE^Q\\left[\\left. \\frac{X}{S^0_T}\\ \\right\\vert\\ \\mathcal{F}_t\\right],\\tag{11.55} \\] where \\(Q\\) is a martingale measure for \\([S^0,S^1,...,S^N]\\), with \\(S^0\\) as the numeraire. In particular, we can choose the bank account \\(B_t\\), as the numeraire. Then \\(B\\) has the dynamics \\[ dB_t=r_tB_t\\ dt,\\tag{11.56} \\] where \\(r\\) is the (possibly stochastic) short rate process. In this case the pricing formula above reduces to \\[ \\Pi_t[X]=E^Q\\left[\\left. e^{-\\int_t^T r(s)\\ ds}X\\ \\right\\vert\\ \\mathcal{F}_t\\right].\\tag{11.57} \\] As a special case, the price of a zero coupon \\(T\\)-bond is given by \\[ p(t,T)=E^Q\\left[\\left. e^{-\\int_t^T r(s)\\ ds}\\ \\right\\vert\\ \\mathcal{F}_t\\right].\\tag{11.58} \\] Defining the stochastic discount factor \\(\\mathbf{M}\\) by \\(\\mathbf{M}_t=B_t^{-1}L_t\\) we also have the pricing formula. \\[ \\Pi_t[X]=E^Q\\left[\\left. \\frac{\\mathbf{M}_T}{\\mathbf{M}_t}X\\ \\right\\vert\\ \\mathcal{F}_t\\right].\\tag{11.59} \\] Different choices of \\(Q\\) will generically give rise to different price processes for a fixed claim \\(X\\). However, if \\(X\\) is attainable then all choices of \\(Q\\) will produce the same price process, which then is given by \\[ \\Pi_t[X]=V_t^h,\\tag{11.60} \\] where \\(h\\) is the hedging portfolio. Different choices of hedging portfolios (if such exist) will produce the same price process. In particular, for every replicable claim \\(X\\) it holds that \\[ V_t^Q=E^Q\\left[\\left. e^{-\\int_t^T r(s)\\ ds}X\\ \\right\\vert\\ \\mathcal{F}_t\\right].\\tag{11.61} \\] "],["mathematics-of-the-martingale-approach.html", "5.7 Mathematics of the martingale approach", " 5.7 Mathematics of the martingale approach 5.7.1 Martingale representation theorem Theorem 12.1. (Bjork) (Representation of Brownian Functionals) Let \\(W\\) be a \\(d\\) dimensional Brownian motions, and let \\(X\\) be a random variable such that \\(X\\in\\mathcal{F}^W_T\\), \\(E[\\vert X\\vert]&lt;\\infty\\). Then there exist uniquely determined \\(\\mathcal{F}^W_t\\)-adapted processes \\(h^1,...,h^d\\), such that \\(X\\) has the representation \\[ X=E[X]+\\sum_{i=1}^d\\int_0^Th^i_s\\ dW_s^i.\\tag{12.2} \\] Under the additional assumption \\[ E[X^2]&lt;\\infty, \\] then \\(h^1,...,h^d\\) are in \\(\\mathcal{L}^2\\). Theorem 12.2. (Bjork) (The Martingale Representation Theorem) Let \\(W\\) be a \\(d\\) dimensional Brownian motions, and assume that the filtration \\(\\mathbf{F}\\) is defined as \\[ \\mathcal{F}_t=\\mathcal{F}^W_t,\\hspace{20pt}t\\in[0,T]. \\] Let \\(M\\) be any \\(\\mathcal{F}_t\\)-adapted martingale. Then there exist uniquely determined \\(\\mathcal{F}_t\\)-adapted processes \\(h^1,...,h^d\\) such that \\(M\\) has the representation \\[ M_t=M_0+\\sum_{i=1}^d\\int_0^t h_s^i\\ dW_s^i,\\hspace{20pt}t\\in[0,T].\\tag{12.9} \\] If the martingale \\(M\\) is square integrable, then \\(h^1,...,h^d\\) are in \\(\\mathcal{L}^2\\). 5.7.2 Girsanov theorem Theorem 12.3. (Bjork) (The Girsanov Theorem) Let \\(W\\) be a \\(d\\) dimensional \\(P\\)-Brownian motion on \\((\\Omega,\\mathcal{F},P,\\mathbf{F})\\) and let \\(\\varphi\\) be any \\(d\\)-dimensional adapted column vector process. Choose a fixed \\(T\\) and define the process \\(L\\) on \\([0,T]\\) by \\[\\begin{align*} dL_t&amp;=\\varphi^\\top_t L_t\\ dW_t,\\tag{12.16}\\\\ L_0&amp;=1,\\tag{12.17} \\end{align*}\\] i.e. \\[ L_t = \\exp\\left\\{\\int_0^t \\varphi^\\top_s\\ dW_s - \\frac{1}{2}\\int_0^t \\Vert\\varphi_s\\Vert ^2\\ ds\\right\\}. \\] Assume that \\[ E^P[L_T]=1,\\tag{12.18} \\] and define the new probability measure \\(Q\\) on \\(\\mathcal{F}_T\\) by \\[ L_T=\\frac{dQ}{dP},\\hspace{15pt}on\\ \\mathcal{F}_T.\\tag{12.19} \\] Then \\[ dW_t=\\varphi\\ dt+dW_t^Q,\\tag{12.20} \\] where \\(W^Q\\) is a \\(d\\) dimensional \\(Q\\)-Brownian motion or equivalently \\[ W_t^Q=W_t-\\int_0^t\\varphi_s\\ ds\\tag{12.21} \\] is a standard \\(Q\\)-Brownian motion. We will often refere to \\(\\varphi\\) as the Girsanov kernel of the measure transformation. Furthermore, we have written on component form above and the \\(L\\) dynamics will have the form \\[ dL_t=L_t\\sum_{i=1}^d\\varphi^i_t\\ dW_t^i, \\] and \\(L\\) will have the explicit form \\[ L_t=\\exp\\left\\{\\sum_{i=1}^d\\int_0^t\\varphi^i_s\\ dW_s^i - \\frac{1}{2}\\int_0^d\\sum_{i=1}^d(\\varphi^i_s)^2\\ ds\\right\\}. \\] The conclusion of the Girsanov Theorem is thwn that we can write \\[ dW_t^i=\\varphi_t^i\\ dt+dW_t^{Q,i}, \\] for \\(i=1,...,d\\) where \\(W_t^{Q,1},...,W_t^{Q,d}\\) are independent standard Brownian motions under \\(Q\\). Definition 12.4. (Bjork) For any Brownian motion \\(W\\) and any kernel process \\(\\varphi\\), the Doleans exponential process \\(\\mathcal{E}\\) is defined by \\[ \\mathcal{E}(\\varphi\\bullet W)_t=\\exp\\left\\{\\int_0^t\\varphi^\\top_s\\ dW_s -\\frac{1}{2}\\int_0^t\\Vert \\varphi\\Vert^2_s\\ ds\\right\\}.\\tag{12.24} \\] Lemma 12.5. (Bjork) (The Novikov Condition) Assume that the Girsanov kernel \\(\\varphi\\) is such that \\[ E^P\\left[e^{\\frac{1}{2}\\int_0^T\\Vert \\varphi_t\\Vert^2\\ dt}\\right]&lt;\\infty.\\tag{12.27} \\] Then \\(L\\) is a martingale and in particular \\(E^P[L_T]=1\\). Theorem 12.6. (Bjork) (The Converse of the Girsanov Theorem) Let \\(W\\) be a \\(d\\)-dimensional standard \\(P\\)-Brownian motion on \\((\\Omega,\\mathcal{F},P,\\mathbf{F})\\) and assume that \\[ \\mathcal{F}_t=\\mathcal{F}^W_t,\\ \\forall t. \\] Assume that there exists a probability measure \\(Q\\) such that \\(Q&lt;&lt;P\\) on \\(\\mathcal{F}_T\\). Then there exists an adapted process \\(\\varphi\\) such that the likelihood process \\(L\\) has the dynamics \\[\\begin{align*} dL_t&amp;=L_t\\varphi^\\top_t\\ dW_t,\\\\ L_0&amp;=1. \\end{align*}\\] This gives us a recipe to transform dynamics of Ito processes under the measure \\(Q\\) as we may rewrite the dynamics of the Brownian motion. We therefore have for an Ito process \\(X\\) with dynamics \\[ dX_t=\\mu(t,X_t)X_t\\ dt+\\sigma(t,X_t) X_t\\ dW_t, \\] may be transformed under \\(Q\\) as \\[\\begin{align*} dX_t&amp;=\\mu(t,X_t)X_t\\ dt+\\sigma(t,X_t) X_t\\ dW_t\\\\ &amp;=\\mu(t,X_t)X_t\\ dt+\\sigma(t,X_t) X_t\\ (\\varphi_t\\ dt+dW_t^Q)\\\\ &amp;=\\left(\\mu(t,X_t) + \\varphi_t\\right) X_t\\ dt + \\sigma(t,X_t)X_t\\ dW_t^Q. \\end{align*}\\] This may lead us into deducing that \\[ \\mu(t,X_t)+\\sigma(t,X_t)\\varphi_t=r_t\\iff\\varphi_t=\\frac{r_t-\\mu(t,X_t)}{\\sigma(t,X_t)}. \\] We furthermore have the Levy characterisation of a Brownian motion. Theorem. (Remark FinKont) (Levy Characterisation of Brownian motion) Let \\(X_t\\) be an Ito process with \\(X_0=0\\). Then \\(X_t\\) is a Brownian motion if and only if the two processes \\(X_t\\) and \\(X_t^2-t\\) are continuous martingales. "],["black-scholes-model---martingale-approach.html", "5.8 Black-Scholes model - martingale approach", " 5.8 Black-Scholes model - martingale approach We consider the standard Black-Scholes model with a single risk free asset and risky asset with dynamics \\[\\begin{align*} dS_t &amp;= \\mu S_t\\ dt+\\sigma S_t\\ dW_t,\\tag{13.1}\\\\ dB_t &amp;= r B_t\\ dt.\\tag{13.2} \\end{align*}\\] We want check whether the model is arbitrage free on any time interval \\([0,T]\\), and find a (perhaps unique) martingale measure such that we may apply the fundamental pricing theorem 1 and 2. From Girsanov this endavour is equivalent with searching for a (perhaps unique) Girsanov kernel \\(\\varphi\\). We therefore define as usual the likelihood process \\[ dL_t=\\varphi_ tL_t\\ dW_t, \\] and setting \\(dQ=L_T\\ dP\\) on \\(\\mathcal{F}_T\\), we know from Girsanov theorem that \\[ dW_t=\\varphi_t\\ dt+dW_t^Q. \\] Inserting into the Black-Scholes model we have \\[ dS_t=S_t(\\mu + \\sigma \\varphi_t)\\ dt+\\sigma S_t\\ dW_t^Q. \\] We know that for \\(Q\\) to be a martingale measure we know that the local rate of return under \\(Q\\) of \\(S\\) must be the short rate \\(r\\) i.e. we have \\[ \\mu + \\sigma \\varphi_t=r\\iff \\varphi_t=\\frac{r-\\mu}{\\sigma}=-\\frac{\\mu -r}{\\sigma},\\tag{13.3} \\] and so we see the Girsanov kernel is constant and deterministic. The process has the economic interpretation that the Girsanov kernel is the risk premium per unit volatility. Lemma 13.1. (Bjork) The Girsanov kernel \\(\\varphi\\) is given by \\[ \\varphi = -\\lambda \\] where the market price of risk \\(\\lambda\\) is defined by \\[ \\lambda =\\frac{r-\\mu}{\\sigma}. \\] We therefore have determined a martingale and so we have the result. Theorem 13.2. (Bjork) The Black-Scholes model above is arbitrage free. We could in general have that \\(\\mu,\\sigma,r\\) are adapted processes. If this is the case we would have to show the Nivokov condition. Pricing then of any \\(T\\)-claim \\(X\\) then is given by the risk neatral pricing formula \\[ \\Pi_t[X]=e^{-r(T-t)}E^Q[X\\ \\vert\\ \\mathcal{F}_t],\\tag{13.7} \\] where the \\(Q\\) dynamics of \\(S\\) has local drift \\(r\\) and volatility from the \\(Q\\)-brownian motion \\(W^Q\\). Theorem 13.3. (Bjork) The Black-Scholes model above is complete. This also holds for the more general model where \\(r,\\mu,\\sigma\\) are adapted processes. Hedging is the possible by considiering a \\(T\\) claim with \\[ E^Q\\left[\\frac{X}{B_T}\\right]&lt;\\infty. \\] Notice the numeraire \\(B_t\\) as in the normalized \\(Z\\)-economy. Consider now the \\(Q\\)-martingale \\[ M_t=E^Q\\left[\\left. \\frac{X}{B_T}\\ \\right\\vert\\ \\mathcal{F}_t\\right],\\tag{13.9} \\] and it now follows from lemma 11.15 the the model is complete if we can find a process \\(h_t^1\\) such that \\[ dM_t=h_t^1\\ dZ_t^i.\\tag{13.10} \\] In order to prove existance of such a process \\(h^1\\) we use the Martingale Representation Theorem 12.2, which says that there exists a process \\(g_t\\) such that \\[ dM_t=g_t\\ dW_t^Q.\\tag{13.11} \\] We can now combine these two equation by the following \\(Q\\) dynamics \\[ dZ_t^1=Z_t^1\\sigma\\ dW_t^Q.\\tag{13.12} \\] Hence we have \\[ dM_t=h_t^1Z_t^1\\sigma\\ dW_t^Q=g_t\\ dW_t^Q\\ \\Rightarrow\\ h_t^1=\\frac{g_t}{Z_t^1\\sigma}. \\] Theorem 13.4. (Bjork) In the Black-Scholes model every \\(T\\)-claim \\(X\\) satisfying \\[ E^Q\\left[\\frac{X}{B_T}\\right]&lt;\\infty \\] can be replicated. The replicating portfolio is given by \\[\\begin{align*} h_t^1&amp;=\\frac{g_t}{Z_t^1\\sigma},\\tag{13.13}\\\\ h_t^0&amp;=M_t-h_t^1Z_t^1,\\tag{13.14} \\end{align*}\\] where \\(M\\) is defined by the above and \\(g\\) is defined by above. If the \\(T\\)-claim is simple that is \\(X=\\Phi(S_T)\\) we may solve a boundary value problem with Feymann-Kac to arrive at the familiar result. Proposition 13.5. (Bjork) In the Black-Scholes model every \\(T\\)-claim on the form \\(X=\\Phi(S_T)\\). Then \\(X\\) can be replicated by the portfolio \\[\\begin{align*} h_t^0&amp;=\\frac{F(t,S_t)-S_t\\frac{\\partial F}{\\partial s}(t,S_t)}{B_t},\\tag{13.15}\\\\ h_t^1&amp;=\\frac{\\partial F}{\\partial s}(t,S_t),\\tag{13.15} \\end{align*}\\] where \\(F\\) solves the Black-Scholes equation \\[\\begin{align*} \\frac{\\partial F}{\\partial t}(t,s)+rs\\frac{\\partial F}{\\partial s}(t,s)+\\frac{1}{2}\\sigma^2s^2 \\frac{\\partial^2 F}{\\partial s^2}(t,s)-rF(t,s)&amp;=0,\\tag{13.16}\\\\ F(T,s)&amp;=\\Phi(s).\\tag{13.16} \\end{align*}\\] Furthermore the value process for the replicating portfolio is given by \\[ V_t=F(t,S_t). \\] "],["multidimensional-models.html", "5.9 Multidimensional models", " 5.9 Multidimensional models We specify the general model by the assumptions below. Assumption 14.0.1 We assume the following: There are \\(n\\) risky assets \\(S^1,...,S^n\\). Under the objective probability measure \\(P\\), the \\(S\\)-dynamics are given by \\[ dS_t^i=\\mu_t^iS_t^i\\ dt +S_t^i\\sum_{j=1}^N\\sigma_t^{ij}\\ dW_t^j,\\tag{14.1} \\] for \\(i=1,...,n\\). The coefficients processes \\(\\mu^i\\) and \\(\\sigma^{ij}\\) above are assumed to be adapted. We have a standard risk free asset with price process \\(B\\) with dynamics \\[ dB_t=r_tB_t\\ dt,\\tag{14.2} \\] where the short rate process \\(r\\) is assumed to be an adapted stochastic process. We can use the representation of \\(\\mu^i\\), \\(\\sigma^{ij}\\) and \\(S^i\\) as vectors and matrices on the form. \\[ \\mu = \\begin{bmatrix} \\mu ^1\\\\ \\vdots\\\\ \\mu ^n \\end{bmatrix},\\ \\sigma= \\begin{bmatrix} \\sigma^{1,1}&amp; \\cdots &amp; \\sigma^{i,N}\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\sigma^{n,1}&amp;\\cdots&amp;\\sigma^{n,N} \\end{bmatrix},\\ D(S)= \\begin{bmatrix} S^{1}&amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0&amp;\\cdots&amp;S^n \\end{bmatrix}. \\] And so we have the model on compact form. \\[\\begin{align*} dS_t&amp;= D(S_t)\\mu_t\\ dt+D(S_t)\\sigma_t\\ dW_t,\\tag{14.3}\\\\ dB_t&amp;=r_tB_t\\ dt.\\tag{14.4} \\end{align*}\\] Now using Girsanov Theorem we can define the prospective likelihood process \\(L\\) by \\[\\begin{align*} dL_t&amp;=L_t\\varphi_t^\\top\\ dW_t,\\tag{14.5}\\\\ L_0&amp;=1,\\tag{14.6} \\end{align*}\\] where \\(\\varphi\\) is an adapted \\(N\\)-dimensional process. Then our candidate martingale measure \\(Q\\) is given by \\(dQ=L_t\\ dP\\) on \\(\\mathcal{F}_t\\) and the Girsanov theorem give the dynamics \\[ dW_t=\\varphi_t\\ dt+dW_t^Q,\\tag{14.7} \\] where \\(W^Q\\) is a standard \\(Q\\)-Brownian motion. Inserting into the \\(P\\)-dynamics we obtain. \\[ dS_t=D(S_t)[\\mu_t+\\sigma_t\\varphi_t]\\ dt+D(S_t)\\sigma_t\\ dW_t^Q.\\tag{14.8} \\] The from (11.54) we know that \\(Q\\) is a martingale measure if and only if the local rate of return (the \\(dt\\)-term) is the short interest rate i.e. if and only if \\[ \\mu_t+\\sigma_t\\varphi=\\mathbf{r}_t,\\tag{14.9} \\] where \\(\\mathbf{r}_t=[r,...,r]^\\top\\in\\mathbb{R}^n\\). We then have that \\[ \\sigma_t\\varphi_t = \\mathbf{r}_t-\\mu_t,\\tag{14.11} \\] where we want to solve for \\(\\varphi\\). Thus we may write a condition for the absence of arbitrage in linear algebra terms. Proposition 14.1. (Bjork) A necessary condition for absence of arbitrage is that \\[ \\mathbf{r}_t-\\mu_t\\in Im[\\sigma_t] \\] with probability one for each \\(t\\). A sufficient condition for absence of arbitrage is that there exists a process \\(\\varphi\\) which solves (14.11) and such that \\(L\\) is a martingale. Note that it is not enought for \\(\\varphi\\) to solce (14.11). We also need that \\(L\\) is a martingale. Definition 14.2. (Bjork) A Girsanov kernel \\(\\varphi\\) is said to be admissible if it generates a martingale measure, i.e. it solves (14.11) and \\(L\\) is a true martingale. Definition 14.3. (Bjork) The model above is said to be generically arbitrage free if it is arbitrage free for every choice of \\(\\mu\\). Proposition 14.4. (Bjork) Disregarding integrability problems the model is generically arbitrage free if and only if, for each \\(t\\le T\\) and \\(P\\)-a.s., the mapping \\[ \\sigma_t : \\mathbb{R}^B\\to \\mathbb{R}^n \\] is surjective, i.e. if and only if the volatility matrix \\(\\sigma_t\\) has rank \\(n\\). Proposition 14.5. (Bjork) Assume that the model i generically arbitrage free and that the filtration \\(\\mathbf{F}\\) is defined by \\[ \\mathcal{F}_t=\\mathcal{F}^W_t.\\tag{14.14} \\] Then, disregarding integrability problems, the model is complete if and only if \\(n=N\\) and the volatility matrix \\(\\sigma_t\\) is invertible \\(P\\)-a.s. for each \\(t\\le T\\). Assumption 14.3.1. (Bjork) We assume that the model is generically free of arbitrage, i.e. that \\[ Im[\\sigma_t]=\\mathbb{R}^n,\\tag{14.16} \\] for all \\(t\\) and with probability one. We also assume that the model is purely Brownian driven, i.e. that \\(\\mathcal{F}_t=\\mathcal{F}_t^W\\). Proposition 14.6. (Bjork) Under assumption 14.3.1 the model is complete if and only if \\[ Im[\\sigma_t^\\top]=\\mathbb{R}^N.\\tag{14.23} \\] If the model is complete then, using the notation of chapter 11, the replicating portfolio \\([h^0,h^S]\\) is given by \\[\\begin{align*} h_t^S&amp;=g_t\\sigma_t^{-1}D^{-1}(Z_t),\\tag{14.24}\\\\ h_t^2&amp;=M_t-h_tZ_t.\\tag{14.25} \\end{align*}\\] With \\(M_t\\) defined by \\[ M_t=E^Q\\left[\\left. \\frac{\\mathcal{X}}{B_T}\\ \\right\\vert\\ \\mathcal{F}_t \\right].\\tag{14.17} \\] Theorem 14.7. (Bjork) (The Second Fundamental Theorem) Under assumptions 14.3.1 the model is complete if and only if the martingale measure is unique. This is equivalent with the statements: \\(Ker[\\sigma_t]=\\{0\\}\\), \\(Im[\\sigma_t^\\top]=\\mathbb{R}^N\\) and \\(\\sigma_t^{-1}\\) exists (i.e. \\(\\sigma_t\\) is invertible). Pricing of any \\(T\\)-claim \\(\\mathcal{X}\\) is now given by the risk neutral valuation formula \\[ \\Pi_t[\\mathcal{X}] = E^Q\\left[\\left. e^{-\\int_t^Tr_u\\ du}\\mathcal{X} \\ \\right\\vert\\ \\mathcal{F}_t\\right],\\tag{14.27} \\] where \\(Q\\) is some choice of martingale measure. Alternatively we can write the price as \\[ \\Pi_t[\\mathcal{X}] = E^Q\\left[\\left. \\frac{\\mathbf{M}_T}{\\mathbf{M}_t}\\mathcal{X} \\ \\right\\vert\\ \\mathcal{F}_t\\right],\\tag{14.29} \\] where \\(\\mathbf{M}\\) is the stochastic discount factor, defined by \\[ \\mathbf{M}_t=\\frac{1}{B_t}L_t. \\] If we have a simple claim i.e. of the form \\(\\mathcal{X}=\\Phi(S_t)\\) and if \\(S\\) is Markovian we have \\[ e^{-r(T-t)}E^Q[\\Phi(S_t)\\ \\vert\\ \\mathcal{F}_t]=e^{-r(T-t)}E^Q[\\Phi(S_t)\\ \\vert\\ S_t], \\] and then the pricing process must be of the form \\(\\Pi_t[\\Phi]=F(t,S_t)\\). We then have \\(F\\) to be the solutions to the PDE \\[\\begin{align*} F_t(t,s)+\\sum_{i=1}^nrs_iF_i(t,s)+\\frac{1}{2}\\text{tr}\\left\\{\\sigma^\\top D(S)F_{ss}D(S)\\sigma\\right\\}-rF(t,s)&amp;=0,\\tag{14.31}\\\\ F(T,s)&amp;=\\Phi(s),\\tag{14.31} \\end{align*}\\] where \\(F_i=\\frac{\\partial F}{\\partial s_i}\\) and \\(F_{ss}\\) denotes the Hessian matrix. Furthermore, \\(\\text{tr}(A)\\) denotes the trace of \\(A\\) i.e. the sum of the diagonal. We have that the hedging portfolio has value process \\(V_t^h=F(t,S_t)\\) with dynamics \\[ dV_t^h=\\sum_{i=1}^n F_i(t,S_t)\\ dS_t^i. \\] Then we must gave the solution \\[\\begin{align*} h_t^i&amp;=\\frac{\\partial F}{\\partial s_i}(t,S_t),\\hspace{10pt}i=1,...,n,\\tag{14.32}\\\\ h_t^0&amp;=\\frac{1}{B_t}\\left\\{F(t,S_t)-\\sum_{i=1}^n \\frac{\\partial F}{\\partial s_i}(t,S_t)\\ S_t^i\\right\\}.\\tag{14.33} \\end{align*}\\] Proposition 14.8. (Bjork) With \\(L\\)-dynamics as in \\(dL_t=L_t\\varphi^\\top_t\\ dW_t\\), the \\(\\mathbf{M}\\)-dynamics are \\[ d\\mathbf{M}_t=-r_t\\mathbf{M}_t\\ dt+\\mathbf{M}_t\\varphi_t^\\top\\ dW_t,\\tag{14.39} \\] or alternatively in terms of the market price of risk \\(\\lambda_t=-\\varphi_t\\) \\[ d\\mathbf{M}_t=-r_t\\mathbf{M}_t\\ dt-\\mathbf{M}_t\\lambda_t^\\top\\ dW_t.\\tag{14.40} \\] Proposition 14.9. (Bjork) (The Hansen-Jagannathan Bounds) Assume generic absence of arbitrage. Then the following holds for all assets, underlying or derivative, and for all admissible Girsanov kernels \\(\\varphi\\), and market prices of risk \\(\\lambda\\). \\[ \\frac{\\vert \\mu_t^p - r_t\\vert}{\\Vert \\sigma_t ^p\\Vert}\\le \\Vert \\varphi_t\\Vert,\\hspace{15pt} \\frac{\\vert \\mu_t^p - r_t\\vert}{\\Vert \\sigma_t ^p\\Vert}\\le \\Vert \\lambda_t\\Vert.\\tag{14.42} \\] "],["basic-non-life-insurance-mathematics.html", "Chapter 6 Basic Non-Life Insurance Mathematics", " Chapter 6 Basic Non-Life Insurance Mathematics Noget indhold "],["stochastic-processes-in-non-life-insurance-mathematics.html", "Chapter 7 Stochastic Processes in Non-Life Insurance Mathematics", " Chapter 7 Stochastic Processes in Non-Life Insurance Mathematics Noget indhold "],["topics-in-non-life-insurance-mathematics.html", "Chapter 8 Topics in Non-Life Insurance Mathematics", " Chapter 8 Topics in Non-Life Insurance Mathematics Noget indhold "],["probabilistic-machine-learning.html", "Chapter 9 Probabilistic Machine Learning ", " Chapter 9 Probabilistic Machine Learning "],["supervised-learning.html", "9.1 Supervised Learning", " 9.1 Supervised Learning In this chapter we restrict our selves to the area of Supervised Learning as we use numerical methods and machine learning algorithms to estimate models in a restricted framework. Take for instance the random forest, this algorithm’s estimation method is perfectly capable of being written recursively and so no “leaning” is done in the sense, that the calculations are predetermined from the algorithm. We therefore call the area of study supervised learning instead of the wider area of study machine learning. Let us define what we mean by supervised learning. Definition. (Supervised Learning) Supervised learning is a field in machine learning that works with labeled data, i.e. data consisting of a set of features \\(X\\), and a response \\(Y\\). The goal is to learn a function \\(m^*\\) that maps a given input \\(x\\) to an output \\(y\\). We will in this chapter only use data in the form of a spread sheet e.g. \\[ \\mathcal{D}_n= \\left(X_i, Y_i \\right)_{i=1,...,n}= \\left[ \\begin{array}{cccc|c} X_{11} &amp; X_{12} &amp; \\cdots &amp; X_{1p} &amp; Y_1\\\\ X_{21} &amp; X_{22} &amp; \\cdots &amp; X_{2p} &amp; Y_2\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots\\\\ X_{n1} &amp; X_{n2} &amp; \\cdots &amp; X_{np} &amp; Y_n \\end{array} \\right], \\] we could however consider any data that may be interpreted by computer software. The setting is then; assume that we have \\(n\\) independent copies of the random variable \\(D=(X,Y)\\in \\mathcal{X}\\times \\mathcal{Y}\\), where \\(X\\) is \\(p\\)-dimensional and \\(Y\\) is one-dimensional. We make no assumption on whether \\(X_j\\), \\(j=1,..,p\\) and \\(Y\\) are discrete or continuous, however in concrete cases this will be specified. We combine the sample of the \\(n\\) observations in the matrix \\(\\mathcal{D}_n=(X_i,Y_i)_{i=1,...,n}\\) being a \\(n\\times p+1\\) matrix as in the above. We call \\(\\mathcal{D}_n\\) the training data. This specification does indeed imply that \\(D_i=(X_i,Y_i)\\) are iid. This is actually a bit controversial as we would expect that the distribution of \\(D\\) will shift over time. For instance, the distribution of ages in a population changes over time and have been more right skewed as humanity advances. This may be accounted for by transforming the data such that the distribution becomes the same. This may be done in a variety of ways some example include: 1) transforming to uniform variable with the time dependent distribution \\(F_t\\), 2) normalizing using a price index and so forth. One should therefore start any analysis by ensuring that the data a given algorithm is trained on is iid. The job becomes finding a good estimator such that we may predict \\(Y\\) given \\(X\\) i.e. \\(Y\\ \\vert\\ X\\). Let us define what an estimator is. Definition. (Estimator) Consider a training dataset \\(\\mathcal{D}_n\\). A estimator \\(m\\) is a function-valued mapping that takes \\(\\mathcal{D}_n\\) as input and associates a function \\(m_n : \\mathcal{X}\\to \\mathcal{Y}\\) i.e. \\(m\\) takes the form \\[ m(\\mathcal{D}_n)=m_n:\\mathcal{X}\\to \\mathcal{Y} \\] the class of estimators is called \\(\\mathcal{G}\\). 9.1.1 What is a good estimator? It is easy to construct an estimator \\(\\hat{m}\\) for instance by maximum likelihood esitmation, bayes optimization or simply by taking conditional expectations. We are however interested in two problems: What is the best class of estimators \\(\\mathcal{G}_0\\subset\\mathcal{G}\\), In the subset of estimators \\(m \\in \\mathcal{G}_0\\), what is then the best estimator. We will now discuss the meaning of being the “best” estimator. Obviously, the first type of consideration is regarding the inherent restrictions of some algorithm, where the second is the problem of error comming from the restriction that we do not have infinite observations. We therefore have two problems namely the inductive bias from the class \\(\\mathcal{G}_0\\) and the estimation error from the available data. A typical problem is that the larger a class \\(\\mathcal{G}_0\\) gives low inductive bias we then may not be able to estimate anything and so the estimation error will be large. The converse also applies. Definition. Let \\(D=(X,Y)\\) be a random variable on the background space \\((\\Omega, \\mathcal{F},P)\\). Then we define the following: A decision rule is a deterministic function \\(m:\\mathcal{X}\\to \\mathcal{Y}\\), A loss function is a deterministic function \\(L: \\mathcal{Y}\\times \\mathcal{Y}\\to \\mathbb{R}_+\\), The risk of a decision rule \\(m\\) is given a loss function \\(L\\) is \\(r(m)=E[L(Y,m(X))]\\). Notice that in the definition of risk we see that \\(m\\) is included inside the expectation. This means in particular that the training data \\(\\mathcal{D}_n\\) is also accounted for i.e. by the tower rule we have \\[ r(m)=\\mathbb{E}[L(Y,m(X))]=\\mathbb{E}\\left[\\mathbb{E}\\Big[L(Y,m_n(X))\\ \\Big\\vert\\ \\mathcal{D}_n\\Big]\\right]:=\\mathbb{E}\\left[R(m)\\right]. \\] Some widely used loss function include Quadratic loss function: \\(L(y_1,y_2)=(y_1-y_2)^2\\), Poisson Deviance: \\(L(y_1,y_2)=2\\left(y_1\\log\\frac{y_1}{y_2}-y_1+y_2\\right)\\), Binary loss function: \\(L(y_1,y_2)=1_{y_1\\ne y_2}\\). Given a loss function \\(L\\) we may find a (possibly non-unique) solution \\(m^*\\) that minimizes \\(R(m)\\). We call this the Bayes estimator. The quantity \\(R(m^*)\\) is called the Bayes risk. On some special case loss functions we may determine the unique solution. Lemma. Assume \\(Y\\) is \\(L_2\\) (square integrable), then the decision function that minimized the risk for the quadratic loss function is \\[ m^*=\\underset{m}{\\text{argmin}}\\ \\mathbb{E}[(Y-m(X))^2]=E[Y\\ \\vert\\ X=x] \\] i.e. the conditional expectation. Proof. Consider the loss function \\(L(y_1,y_2)=(y_1-y_2)^2\\) i.e. the \\(L^2\\) loss function. For any estimator \\(m(X)\\in \\mathcal G\\) we have \\[\\begin{align*} R(m(X))&amp;=\\mathbb E[L(m(X),Y)\\ \\vert\\ X]=\\mathbb E[(m(X)-Y)^2\\ \\vert\\ X]\\\\ &amp;=\\mathbb E[m(X)^2\\ \\vert\\ X]+\\mathbb E[Y^2\\ \\vert\\ X]-2\\mathbb E[m(X)Y\\ \\vert\\ X]\\\\ &amp;=m(X)^2+\\mathbb E[Y^2\\ \\vert\\ X]-2m(X)\\mathbb E[Y\\ \\vert\\ X] \\end{align*}\\] We see that the risk is minimized for the \\(m\\) that minimizes \\(m(X)^2-2m(X)\\mathbb E[Y\\ \\vert\\ X]\\). The first order condition is then \\[ \\frac{\\partial}{\\partial m}R(m(X))=2m(X)-2\\mathbb E[Y\\ \\vert\\ X]=0 \\] hence giving that \\[ m(X)=\\mathbb E[Y\\ \\vert\\ X] \\] as desired. \\(\\blacksquare\\) Remarks on the \\(L^2\\)-loss. The loss function \\(L(y_1,y_2)=(y_1-y_2)^2\\) i.e. the \\(L^2\\) loss function gives some nice interpretations. We know that \\(L\\) as a norm on \\(\\mathbb R\\) forms a Hilbert Space. This means, for instance, that we have some nice geometric interpretations, but more importantly we have alot of tools from linear algebra. We know that the projection \\(m^*\\) onto the space \\(\\mathcal G\\) and so any vector \\(\\hat m(X)-m^*(X)\\) is orthogonal to \\(Y-m^*(X)\\). In particular, this gives that \\[\\begin{align*} r(\\hat m(X))&amp;=\\mathbb E[L(\\hat m(X),Y)]=\\mathbb E\\left[(\\hat m(X)-Y)^2\\right]\\\\ &amp;=\\mathbb E\\left[(\\hat m(X)-m^*(X)+m^*(X)-Y)^2\\right]\\\\ &amp;\\stackrel{(\\dagger)}{=}\\mathbb E\\left[(\\hat m(X)-m^*(X))^2\\right]+\\mathbb E\\left[(m^*(X)-Y)^2\\right]\\\\ &amp;=\\mathbb E\\left[(\\hat m(X)-m^*(X))^2\\right]+r(m^*(X)) \\end{align*}\\] Using in \\((\\dagger)\\) that \\(\\hat m(X)-m^*(X)\\) and \\(m^*(X)-Y\\) are orthogonal with the last equation simply being the definition of risk. Rearringing the above gives \\[ r(\\hat m(X))-r(m^*(X))=\\mathbb E\\left[(\\hat m(X)-m^*(X))^2\\right]=\\text{MSE}(\\hat m(x)) \\] We can further write out the Mean Squared Error in terms of variance and bias. \\[\\begin{align*} \\text{MSE}(\\hat m(x))&amp;=\\mathbb E\\left[(\\hat m(X)-m^*(X))^2\\right]\\\\ &amp;=\\mathbb E\\left[(\\hat m(X)-\\mathbb E[\\hat m(X)\\vert X])^2\\right]+\\mathbb E\\left[(\\mathbb E[\\hat m(X)\\vert X]-m^*(X))^2\\right]\\\\ &amp;=\\text{Var}(\\hat m(X))+\\text{Bias}^2(\\hat m(X)). \\end{align*}\\] Lemma. Assume \\(\\mathcal{Y}=\\{1,...,K\\}\\), the decision function that minimizes the risk for the binary loss function satisfies \\[ m^*=\\underset{m}{\\text{argmin}}\\ \\mathbb{E}[1_{Y\\ne m(X)}]=\\underset{m}{\\text{argmin}}\\ \\mathbb{P}(Y\\ne m(X))=\\underset{k=1,..,K}{\\text{argmax}}\\ \\mathbb{P}(Y=k\\ \\vert\\ X=x). \\] We can now define the prediction risk and the generalization error which relates to the balance of a sufficiently large class \\(\\mathcal{G}_0\\) and how effective the optimal estimator is conditional on the class \\(\\mathcal{G}_0\\). Definition. (Conditional risk) Let \\(\\mathcal{D}_n\\) be some training data. Given an estimator \\(\\hat{m}_n\\) we call \\[ R(\\hat{m}_n)=\\mathbb{E}[L(Y,\\hat{m}_n(X))\\ \\vert\\ \\mathcal{D}_n] \\] the prediction risk or conditional generalized error. Definition. (Risk) We call \\[ r(\\hat{m}_n)=\\mathbb{E}[R(\\hat{m}_n)], \\] the prediction risk or generalized error. 9.1.2 Excess risk Definition. (Excess Risk) Consider the set \\(\\mathcal{G}\\) be the set of all measurable estimators. Fix a subset \\(\\mathcal{G}_0\\subset\\mathcal{G}\\). Given some training data \\(\\mathcal{D}_n\\) consider the Bayes estimator restricted to \\(\\mathcal{G}_0\\) denoted by \\(\\hat{m}_n\\) and the unconditional Bayes estimator restricted to \\(\\mathcal{G}\\) we define the quantity \\[ R(\\hat{m}_n)-r(m^*) \\] or \\[ \\mathbb{E}[R(\\hat{m}_n)\\ \\vert\\ X_1,...,X_n]-r(m^*), \\] as the excess risk. This is the difference between the generalization error and the risk obtained by an optimal decision function. In the context of the above definition we can decompose the risk associated with the optimal estimator \\(\\hat{m}_n\\in\\mathcal{G}_0\\) into the estimation error and the inductive bias. \\[ R(\\hat{m}_n)-r(m^*)=\\underbrace{\\left[R(\\hat{m}_n)-\\inf_{m\\in\\mathcal{G}_0}R(m)\\right]}_{\\text{estimation error}}+\\underbrace{\\left[\\inf_{m\\in\\mathcal{G}_0}R(m)-R(m^*)\\right].}_{\\text{inductions bias/approximation error}} \\] where we have to balance the trade-off with a larger \\(\\mathcal{G}_0\\) infer a lower induction bias but larger estimation error and a smaller class \\(\\mathcal{G}_0\\) infer a lower estimation error but larger induction bias. Definition. (Empirical risk and empirical risk minimizer) Given training data \\(\\mathcal{D}_n\\) and a loss function \\(L\\), we call \\[ \\hat{R}_n(m):=\\sum_{i=1}^nL(Y_i,m(X_i)) \\] the empirical risk. Given an additional function class \\(\\mathcal{G}_0\\), \\[ \\underset{m\\in\\mathcal{G}_0}{\\text{argmin}}\\ \\hat{R}_n(m)=\\underset{m\\in\\mathcal{G}_0}{\\text{argmin}}\\ \\sum_{i=1}^nL(Y_i,m(X_i)) \\] is called empirical risk minimizer or (standard leaner). For larger function classes \\(\\mathcal{G}_0\\) the empirical risk minimizer might not be unique and possibly too noisy. In this case one sometimes adds a penalty term \\(J_\\lambda : \\mathcal{G}\\to \\mathbb{R}_+\\) that penalizes the complexity of \\(m\\) and minimizes the penalized empirical risk: \\[ \\underset{m\\in\\mathcal{G}_0}{\\text{argmin}}\\ \\hat{R}_{n,\\lambda}:=\\underset{m\\in\\mathcal{G}_0}{\\text{argmin}}\\ \\sum_{i=1}^nL(Y_i,m(X_i)) + J_\\lambda(m). \\] If \\(J_\\lambda\\) and \\(\\hat{R}_n\\) is convex one can show that \\[ \\underset{m\\in\\mathcal{G}_0}{\\text{argmin}}\\ \\hat{R}_{n,\\lambda}=\\underset{m\\in\\mathcal{G}_\\eta}{\\text{argmin}}\\ \\hat{R}_{n} \\] for the class \\(\\mathcal{G}_\\eta=\\{m\\in \\mathcal{G}_0\\ \\vert\\ J_\\lambda(m)\\le \\eta\\}\\). Some penalty terms could be \\(J_\\lambda(m)=\\lambda\\int m&#39;&#39;(x)\\ dx\\), \\(J_\\lambda(m)=\\lambda \\int\\vert m&#39;(x)\\vert\\ dx\\), \\(J_\\lambda(m)=\\lambda \\int(m(x))^2\\ dx\\). Proposition. (Probability bounds) Let \\(\\tilde{m}=\\underset{m}{\\text{argmin}}\\ r(m)\\). We have \\[ r(\\hat{m}_n)-r(\\tilde{m})\\le 2\\sup_{m\\in\\mathcal{G}_0}\\Big\\vert\\hat{R}_n(m) - r(m) \\Big\\vert, \\] and for all \\(\\lambda \\in \\Lambda\\), \\[ r(\\hat{m}_{n,\\lambda})-r(\\tilde{m})\\le 2\\sup_{m\\in\\mathcal{G}_0}\\Big\\vert\\hat{R}_{n,\\lambda}(m) - r(m) \\Big\\vert + J_\\lambda(\\tilde{m})-J_\\lambda(\\hat{m}_n). \\] Definition. We say \\(\\hat{m}_n\\) is \\(\\varepsilon\\)-accurate with probabiltiy \\(1-\\delta\\), if \\[ P\\Big(R(\\hat{m}_n)-\\inf_{m\\in\\mathcal{G}}r(m)&gt;\\varepsilon\\Big)&lt;\\delta. \\] "],["training-validating-and-testing.html", "9.2 Training, Validating and Testing", " 9.2 Training, Validating and Testing When deciding which method too choose for a given task, one may like to pick the method with the smallest generalization error. However, most machine learning methods depend on hyper parameters and one may first need to decide which hyper parameters are best for the given task. In short: We would like to compare the generalization error of optimally tuned machine learning methods given our data. Definition. (Training and test set) One often randomly divides the given data into training data and test data: \\(\\mathcal{D}_n=(X_i,Y_i)_{i=1,...,n}\\) (training data) \\(\\mathcal{T}_m=(\\tilde{X}_j,\\tilde{Y}_j)_{j=1,...,m}\\) (test data) with \\(n\\in[0.8m,0.95m]\\). Definition. (Training and test error) The empirical risk on the training data is called training error and on the test data test error. Definition. (Validation set) To tune the hyper parameters for an algorithm, one often randomly divides the given training data into training data (yes: also called training data.) and validation data: \\(\\mathcal{D}_{n_1}=(X_i,Y_i)_{i=\\tau(1),...,\\tau(n_1)}\\) (traning data) \\(\\mathcal{V}_{n_2}=(\\tilde{X}_j,\\tilde{Y}_j)_{j=\\tau(n_1+1),...,\\tau(n)}\\) (validation data) where \\(\\tau\\) is a randomly picked permutation of \\(\\{1,...,n\\}\\) and \\(n_1+n_2=n\\) is respectively the size of the training set and the validation set. One can now compare different methods via the following simple algorithm: Split your data into train, validation and test set. For a given method and a rich set of hyper parameter configurations train the method on the training set and compare performance via empirical risk on the validation set. For every method, pick the hyper parameter with the smallest empirical risk. Compare different methods with the chosen hyper parameters on the test set (trained on training+validation set) and pick the method with smallest empirical risk. Notice that the procedure above is stochastic and has bias and variance both for selecting the optimal hyper parameters and for selecting the optimal method. Bias: Bias occurs because the sample sizes used for learning the hyper parameters \\(n_1\\) are smaller than the actual training size \\(n\\) and also the full data size \\(n+m\\). Variance: The results are stochastic because the validation and test set are not of infinite size. Variance can be reduced by repeating steps 1-4 several times. The most popular method for doing so is (nested) cross validation. 9.2.1 Estimating risk We want to estimate the generalization error of a method \\(\\hat{m}_{n,\\lambda}\\) that depends on a fixed hyper parameter \\(\\lambda\\). Definition. (M-fold Cross validation) Given the indices of a data set \\(S=\\{1,...,n\\}\\), M-fold cross validation follows the following steps: Divide the data into \\(M\\) disjoint sets \\(S_1,...,S_M\\) of same size. Define \\(S_{-l}=\\cup_{k\\ne l}S_k\\) being the complement to \\(S_l\\). (\\(\\#S_l=n/M\\) and \\(\\#S_{-l}=(M-1)n/M\\)) For each subdivision \\(l=1,...,M\\) train the algorithm on \\(S_{-l}\\) and denote the estimator \\(\\hat{m}_\\lambda(S_{-l})\\). Calculate the cross validated empirical risk \\[ CV(\\hat{m}_{n,\\lambda})=\\frac{1}{M}\\sum_{l=1}^M\\frac{1}{\\vert S_{-l}\\vert}\\sum_{i\\in S_{-l}}L\\big(Y_i,\\hat{m}_\\lambda(S_{-l})(X_i)\\big) \\] It is a non-trivial discussion what \\(CV(\\hat{m}_{n,\\lambda})\\) is estimating. Consider the heuristic \\[\\begin{align*} CV(\\hat{m}_{n,\\lambda})&amp;=\\frac{1}{M}\\sum_{l=1}^M\\frac{1}{\\vert S_{-l}\\vert}\\sum_{i\\in S_{-l}}L\\big(Y_i,\\hat{m}_\\lambda(S_{-l})(X_i)\\big)\\\\ &amp;\\approx \\frac{1}{M}\\sum_{l=1}^MR\\big(Y,\\hat{m}_\\lambda(S_{-l})(X)\\big)\\\\ &amp;\\approx \\mathbb{E}\\left[R\\big(Y,\\hat{m}_\\lambda(S_{-l})(X)\\big)\\right]. \\end{align*}\\] Where we used the law of large numbers in the first approximation. The last approximation is not so clear because the summands are dependent for \\(M&gt;2\\). The bias is minimal for large \\(M\\) since estimation is based on training the algorithm on \\((M-1)n/M\\) data points. In the case that \\(M=n\\), M-fold cross validation is also known as leave-one-our cross validation. It is however often not practical because of the computational cost. In practice, setting \\(M=5\\) or \\(M=10\\) is common choices. When deciding on an optimal hyper parameter we would like to pick \\[ \\underset{\\lambda \\in \\Lambda}{\\text{argmin}}\\ CV(\\hat{m}_{n,\\lambda}). \\] But the hyper parameter space \\(\\Lambda\\) is often multi-dimensional and partly continuous. Hence it is infeasible to try out all parameters. Common practice are Grid search Random search (e.g. pick 200 parameters uniformly random from the parameter space) Advanced optimization techniques (i.e. techniques that aim to find the minimzer of a function (here: cross validated empirical risk) without the requirement of knowing the analytical form of the function to optimize. Above we have discussed how we can use cross validation to pick an optimal parameter for a given method and data set. But how to choose between different methods? One popular way is nested cross validation comprising an inner loop for hyper parameter selection (tuning) and an outer loop for method comparison. Assume that we want to compare \\(J\\) methods \\(\\hat{m}_{n,j}\\), \\(j=1,...,J\\). then we may use nested cross validation. Definition. (Nested \\(M_1-M_2\\) Cross-validation) Given the indices of a data set \\(S=\\{1,...,n\\}\\), nested \\(M_1-M_2\\) cross validation follows the following steps: Divide the data into \\(M_1\\) disjoint sets \\(S_1,...,S_{M_1}\\) of same size. Define \\(S_{-l}=\\cup_{k\\ne l}S_k\\) being the complement to \\(S_l\\). (\\(\\#S_l=n/M_1\\) and \\(\\#S_{-l}=(M_1-1)n/M_1\\)) For each \\(l=1,...,M_1\\), run \\(M_2\\)-fold cross validation on \\(S_{-l}\\) for all \\(J\\) methods, returning optimal hyper parameters \\(\\hat{\\lambda}(j,l)\\), \\(j=1,...,J\\) and \\(l=1,...,M_1\\). (the one with lowest \\(CV\\) is choosen for each \\((j,l)\\)) Calculate the cross validated empirical risk \\[ CV(\\hat{m}_{n,j})=\\frac{1}{M_1}\\sum_{l=1}^{M_1}\\frac{1}{\\vert S_{-l}\\vert}\\sum_{i\\in S_l}L\\big(Y_i,\\hat{m}_{\\lambda(j,l)}(S_{-l})(X_i)\\big) \\] Pick the method with the smallest risk (and possibly tune again for fitting) "],["linear-models.html", "9.3 Linear Models", " 9.3 Linear Models We may take the linear model as a case study of the methods introduced in the above chapter. As such we consider the squaed loss \\(L(y_1,y_2)=(y_1-y_2)^2\\) for which we already know the Bayes rule: \\[ m^*(x)=\\underset{m}{\\text{argmin}}\\ R(m)=\\mathbb{E}[Y\\ \\vert\\ X=x]. \\] The linear model has the following assumptions. There exists paramaters \\(\\beta_0^*,\\beta_1^*,...,\\beta_p^*\\), with \\[ m^*(x)=\\beta_0^*+\\sum_{j=1}^{p}\\beta_j^*x_j. \\] In other words, we assume that \\(m^*\\) is a linear function, i.e., \\[ m^*\\in\\mathcal{G}=\\{f : \\mathbb{R}^p\\to \\mathbb{R}\\ \\vert\\ f(x)=\\beta^\\top x\\}. \\] Given iid training data \\((X_i,Y_i)_{i=1,...,n}\\) we have an additive noise model \\[ Y_i=\\beta_0^*+\\sum_{j=1}^{p}\\beta_j^*X_{ij}+\\varepsilon_i, \\] with \\(\\varepsilon_i=Y_i-m^*(X_i)\\) and hence iid with \\(\\mathbb{E}[\\varepsilon_i\\ \\vert\\ X_i]=0\\). Notice, that since we are assuming \\(m^*\\in\\mathcal{G}\\) we have by assumption no inductive bias and we therefore only consider estimation error i.e. \\[ R(\\hat{m}_n)-r(m^*). \\] Given the training data we may approximate the coefficients using the following. Lemma. (Coefficients in the linear model) Under the Linear model assumption we have for \\(j=1,...,p\\) \\[ \\beta^*_j=\\frac{\\text{Cov}\\Big(X_{1j},Y_1-\\sum_{k\\in \\{1,...,p\\}\\setminus \\{j\\}} \\beta_k^*X_{1k}\\Big)}{\\text{Var}(X_{1j})} \\] In particular, if the components of \\(X\\) are uncorrelated, we have \\[ \\beta_j^*=\\frac{\\text{Cov}(X_{1j},Y_1)}{\\text{Var}(X_{1j})}. \\] Lemma. (Bayes risk in the linear model) Under the Linear model assumption we have \\(r(m^*)=\\text{Var}(\\varepsilon_i)\\) For \\(m(x)=\\beta_0+\\sum_{j=1}^p \\beta_jx_j\\), \\[ r(m)-r(m^*)=\\Vert\\Sigma^{1/2}(\\beta -\\beta^*) \\Vert^2_2. \\] with \\(\\Sigma = \\mathbb E[\\mathbf X\\mathbf X^\\top]\\). Proof. (1). We have by assumptions that \\(m^*(X)=\\mathbb E[Y\\ \\vert\\ X]=X^\\top\\beta^*\\) is the Bayes estimator. Using that the noise is additive we have \\(Y=m^*(X)+\\varepsilon\\) with \\(\\mathbb E[\\varepsilon]=0\\). \\[\\begin{align*} r(m^*)&amp;=\\mathbb E[(m^*(X)-Y)^2]=\\mathbb E[(X^\\top\\beta ^*-Y)^2]\\\\ &amp;=\\mathbb E[\\varepsilon^2]=\\text{Var}(\\varepsilon)-\\mathbb E[\\varepsilon]^2\\\\ &amp;=\\text{Var}(\\varepsilon). \\end{align*}\\] (2). Take any linear estimor \\(m(X)=X^\\top \\beta\\), then we have \\[\\begin{align*} r(m)&amp;=\\mathbb E[(m(X)-Y)^2]\\\\ &amp;=\\mathbb E[(m(X)-m^*(X)+m^*(X)-Y)^2]\\\\ &amp;=\\mathbb E[(m(X)-m^*(X))^2]+\\mathbb E[(m^*(X)-Y)^2]+2\\mathbb E[(m(X)-m^*(X))(m^*(X)-Y)]\\\\ &amp;=\\mathbb E[(m(X)-m^*(X))^2]+r(m^*) \\end{align*}\\] Using that \\(m^*(X)-Y\\) is orthogonal to \\(m(X)-m^*(X)\\). This gives us the following \\[\\begin{align*} r(m)- r(m^*)&amp;=\\mathbb E[(m(X)-m^*(X))^2]\\\\ &amp;=\\mathbb E[(X^\\top \\beta -X^\\top \\beta^*)^2]\\\\ &amp;=\\mathbb E[XX^\\top(\\beta-\\beta^*)^2]\\\\ &amp;=\\Sigma(\\beta-\\beta^*)^2=\\Vert \\Sigma^{1/2}(\\beta - \\beta^*)\\Vert ^2_2 \\end{align*}\\] as desired. \\(\\blacksquare\\) 9.3.1 Least Squares Estimator Moving forward we will assume \\(\\beta_0^*=0\\) since we can always translate the data and make the centered around 0. We will furthermore use the notation: \\[ \\mathbf{X}= \\begin{bmatrix} X_{11} &amp; \\cdots &amp; X_{1p}\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ X_{n1} &amp; \\cdots &amp; X_{np} \\end{bmatrix},\\hspace{10pt} \\mathbf{Y}= \\begin{pmatrix} Y_1\\\\ \\vdots\\\\ Y_n \\end{pmatrix},\\hspace{10pt} \\varepsilon= \\begin{pmatrix} \\varepsilon_1\\\\ \\vdots\\\\ \\varepsilon_n \\end{pmatrix}. \\] In this cases the empirical risk takes the form \\[ \\hat{R}_n(m)=\\frac{1}{n}\\Vert\\mathbf{Y}-\\mathbf{X}\\beta \\Vert^2_2. \\] Lemma. (Least squares estimator) It holds that \\((\\mathbf{X}^\\top\\mathbf{X})\\hat{\\beta}=\\mathbf{X}^\\top \\mathbf{Y}\\), If \\(\\mathbf{X}\\) has full rank, then \\[ \\hat{\\beta}_n^{LS}=(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y}. \\] Theorem. (Excess risk Least squares estimator) If \\(\\mathbf{X}^\\top\\mathbf{X}\\) is invertible, then \\[ \\mathbb{E}[R(\\hat{m}_n^{LS})\\ \\vert\\ \\mathbf{X}]-r(m^*)=\\frac{\\sigma^2}{n}\\cdot \\text{tr}\\left(\\Sigma\\hat{\\Sigma}^{-1}\\right) \\] with \\(\\Sigma=\\mathbb{E}[\\mathbf{X}^\\top\\mathbf{X}]\\) and \\(\\hat{\\Sigma}=\\frac{1}{n}\\mathbf{X}^\\top\\mathbf{X}\\). Proof. \\[ \\hat{\\beta}^{LS}=\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{Y}=\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{X} \\beta^*+\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{\\varepsilon}=\\beta^*+\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{\\mathbf { \\varepsilon }}. \\] Thus \\[\\begin{align*} R\\left(\\hat{m}^{LS}\\right)-r\\left(m^*\\right) &amp; =\\left\\|\\Sigma^{1 / 2}\\left(\\hat{\\beta}^{L S}-\\beta^*\\right)\\right\\|_2^2=\\left\\|\\Sigma^{1 / 2}\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{\\varepsilon}\\right\\|_2^2=n^{-1}\\left\\|n^{-1/2}\\Sigma^{1 / 2} \\hat{\\Sigma}^{-1} {\\mathbf{X}^T \\mathbf{\\varepsilon}}\\right\\|_2^2 \\\\ &amp; =n^{-1}\\|A \\mathbf{\\varepsilon}\\|_2^2, \\end{align*}\\] where \\(\\hat \\Sigma=n^{-1} \\mathbf{{X^T}X}, A:=n^{-1/2}\\Sigma^{1 / 2} \\hat{\\Sigma}^{-1} {\\mathbf{X}^T}\\). We calculate the excess risk conditional on \\(\\mathbf{X}\\). Note that \\(\\operatorname{tr}(\\cdot)\\) is linear and invariant under cyclic permutations. We have \\[\\begin{align*} \\mathbb{E}\\left[R\\left(\\hat{m}^{LS}\\right) \\mid \\mathbf{X}\\right]-r\\left(m^*\\right) &amp; =n^{-1} \\mathbb{E}\\left[\\|A \\mathbf{\\varepsilon}\\|_2^2 \\mid \\mathbf{X}\\right] \\\\ &amp; =n^{-1} \\mathbb{E}\\left[\\operatorname{tr}\\left(A \\mathbf{\\varepsilon \\varepsilon}^T A^T\\right) \\mid \\mathbf{X}\\right]=n^{-1} \\operatorname{tr}(A \\underbrace{\\mathbb{E}\\left[\\mathbf{\\varepsilon \\varepsilon}^T\\right]}_{=\\sigma^2 I_{p \\times p}} A^T)=\\frac{\\sigma^2}{n}\\|A\\|_F^2 \\\\ &amp; =\\frac{\\sigma^2}{n} \\cdot \\operatorname{tr}\\left(\\Sigma^{1 / 2} \\hat{\\Sigma}^{-1} \\hat{\\Sigma} \\hat{\\Sigma}^{-1} \\Sigma^{1 / 2}\\right) \\\\ &amp; =\\frac{\\sigma^2}{n} \\cdot \\operatorname{tr}\\left(\\Sigma \\hat{\\Sigma}^{-1}\\right) . \\end{align*}\\] as desired. \\(\\blacksquare\\) From the above it follos that if \\(\\hat{\\Sigma}\\approx \\Sigma\\) then \\[ \\mathbb{E}[R(\\hat{m}_n^{LS})\\ \\vert\\ \\mathbf{X}]-r(m^*)\\approx\\frac{\\sigma^2p}{n}. \\] This approximation does not take into account the variation of \\(X\\). Due to the inverse, it is not easily possible to derive an upper bound for the expectation of \\(\\hat{\\Sigma}^{-1}\\). Therefore, we only obtain a result for the excess Bayes risk which holds with high probability and under additional assumptions (which could be relaxed but would lead to much more complicated proofs). Theorem. (PAC Least squares estimator) We do not assume a linear model. Let \\(m(x)=E[Y\\ \\vert\\ X=x]\\) and assume \\(m^*(x)=x\\Sigma^{-1}E[YX]\\) is the best linear approximation. Assume that \\(X\\) has bounded support and sub-Gaussian noise, i.e., there exists a \\(\\sigma\\) such that for all \\(t\\): \\[ E[e^{tx}\\ \\vert\\ X=x]\\le e^{t^2\\sigma^2/2}, \\] then for \\(n\\) big enough, and \\(t&gt;\\max\\{0,2.6-\\log p\\}\\) \\[ P\\left(r(\\hat{m}^{LS})-r(m^*)\\ge \\frac{2 A}{n}(1+\\sqrt{8t})^2+\\frac{\\sigma^2(p+2\\sqrt{pt}+2t)}{n} + o(1/n)\\right)\\le 3e^{-t} \\] where \\(A=\\mathbb{E}\\left[\\Vert\\Sigma^{1/2}X(m(X)-\\beta^\\top X) \\Vert^2\\right]\\) is an approximation error. Up until now we have assumed that \\(\\mathbf{X}\\) has full rank. This is not very realistic for very large \\(p\\) (\\(p&gt; &gt; n\\)). Even if \\(\\mathbf{X}^\\top\\mathbf{X}\\) is invertible, the variance of the estimation error might be too large. This leads us to penalized models that reduce variance by adding some bias. Other alternatives (not discussed here) are dimension reduction, say via PCA, or feature selection, say forward stepwise regression. 9.3.2 Ridge Regression Definition. (Ridge regression) Let \\(\\lambda \\geq 0\\) and \\[ J_\\lambda(\\beta)=\\lambda\\|\\beta\\|_2^2=\\lambda \\sum_{j=1}^p \\beta_j^2. \\] The Ridge estimator is defined as \\[\\begin{align*} \\hat{\\beta}_\\lambda^{\\text {ridge }} &amp; =\\underset{\\beta \\in \\mathbb{R}^p}{\\arg \\min }\\left\\{\\hat{R}_n(\\beta)+J_\\lambda(\\beta)\\right\\} \\\\ &amp; =\\underset{\\beta \\in \\mathbb{R}^p}{\\arg \\min }\\left\\{\\|\\mathbf{Y}-\\mathbf{X} \\beta\\|_2^2+\\lambda\\|\\beta\\|_2^2\\right\\} . \\end{align*}\\] The corresponding algorithm is \\[ \\hat{m}_{n, \\lambda}^{\\text {ridge }}(x)=\\sum_{j=1}^p \\hat{\\beta}^\\text{ridge}_{\\lambda, j} x_j. \\] Lemma. (Ridge regression solution) Let \\(\\lambda&gt;0\\). Then \\[ \\hat{\\beta}^\\text{ridge}_\\lambda=\\left(\\mathbf{X}^T \\mathbf{X}+\\lambda n I_{p \\times p}\\right)^{-1} \\mathbf{X}^T \\mathbf{Y} \\] In ridge regression, the matrix \\(\\mathbf{X}^T \\mathbf{X}\\) is ‘made invertible’ by adding a positive multiple of the identity matrix. Therefore, the ridge estimator also can be used in the case \\(p&gt;n\\). The name ‘ridge’ stems from the fact that the optimization problem is equivalent to \\[ \\min _{ \\beta \\in \\mathbb{R}^p} \\hat{R}_n(X\\beta) \\quad \\text { s.t. } \\quad\\|\\beta\\|_2 \\leq t \\] for some suitable \\(t&gt;0\\). Theorem. (Excess risk for ridge regression estimator) Under the linear model, \\[ \\mathbb E[ R\\left(\\hat{m}^{n,\\text{ridge}}_\\lambda \\right)|X]-r\\left(m^*\\right)=\\frac{\\sigma^2}{n} \\cdot \\operatorname{tr}\\left(\\Sigma\\left(\\hat{\\Sigma}+\\lambda I_{p \\times p}\\right)^{-1} \\hat{\\Sigma}\\left(\\hat{\\Sigma}+\\lambda I_{p \\times p}\\right)^{-1}\\right)+\\lambda^2\\left\\|\\Sigma^{1 / 2}\\left(\\hat{\\Sigma}+\\lambda I_{p \\times p}\\right)^{-1} \\beta^*\\right\\|_2^2. \\] Let \\(\\Sigma=U D U^T\\) be the spectral decomposition of \\(\\Sigma\\) with orthogonal matrix \\(U\\) and diagonal matrix \\(D=\\operatorname{diag}\\left(s_1, \\ldots, s_p\\right.\\) ) (entries are the eigenvalues of \\(\\Sigma\\) ). By assuming \\(\\hat \\Sigma= \\Sigma\\), the excess risk simplifies to \\[ \\frac{\\sigma^2}{n} \\sum_{j=1}^p \\frac{s_j^2}{\\left(s_j+\\lambda\\right)^2}+\\lambda^2 \\cdot \\sum_{j=1}^p \\frac{s_j\\left(U^T \\beta^*\\right)_j^2}{\\left(s_j+\\lambda\\right)^2}. \\] Proof. \\[\\begin{align*} \\hat{\\beta}_\\lambda-\\beta^* &amp; =-\\lambda n\\left(\\mathbf{X}^T \\mathbf{X}+\\lambda n I_{p \\times p}\\right)^{-1} \\beta^*+\\left(\\mathbf{X}^T \\mathbf{X}+\\lambda n I_{p\\times p}\\right)^{-1} \\mathbf{X}^T \\mathbf{\\varepsilon} \\\\ &amp; =-\\lambda\\left(\\hat{\\Sigma}+\\lambda I_{p \\times p}\\right)^{-1} \\beta^*+\\frac{1}{n}\\left(\\hat{\\Sigma}+\\lambda I_{p \\times p}\\right)^{-1} \\mathbf{X}^T \\mathbf{\\varepsilon} \\end{align*}\\]. thus \\[ R\\left(\\hat{\\beta}_\\lambda\\right)-R\\left(\\beta^*\\right)=\\left\\|B-\\frac{1}{\\sqrt{n}} A \\mathbf{\\varepsilon}\\right\\|_2^2=\\|B\\|_2^2-\\frac{2}{\\sqrt{n}}\\langle B, A\\varepsilon \\rangle+\\frac{1}{n}\\|A \\mathbb{\\mathbf { \\varepsilon }}\\|_2^2 \\] where \\(A=\\Sigma^{1 / 2}\\left(\\hat{\\Sigma}+\\lambda I_{p \\times p}\\right)^{-1} \\frac{\\mathbf{X}^T}{\\sqrt{n}}\\) and \\(B:=\\lambda \\Sigma^{1 / 2}\\left(\\hat{\\Sigma}+\\lambda I_{p \\times p}\\right)^{-1} \\beta^*\\). Since \\(\\mathbb{E} \\mathbf{\\varepsilon}=0\\), we have \\[\\begin{align*} \\mathbb{E}\\left[R\\left(\\hat{\\beta}_\\lambda\\right) \\mid \\mathbf{X}\\right]-R\\left(\\beta^*\\right) &amp; =\\frac{\\sigma^2}{n}\\|A\\|_F^2+\\|B\\|_2^2 \\\\ &amp; =\\frac{\\sigma^2}{n} \\cdot \\operatorname{tr}\\left(\\Sigma\\left(\\hat{\\Sigma}+\\lambda I_{p \\times p}\\right)^{-1} \\hat{\\Sigma}\\left(\\hat{\\Sigma}+\\lambda I_{p \\times p}\\right)^{-1}\\right)\\\\ &amp;+\\lambda^2\\left\\|\\Sigma^{1 / 2}\\left(\\hat{\\Sigma}+\\lambda I_{p \\times p}\\right)^{-1} \\beta^*\\right\\|_2^2 \\end{align*}\\] Furthermore, assuming \\(\\hat \\Sigma= \\Sigma\\) the above expression simplifies to \\[\\begin{align*} &amp;\\frac{\\sigma^2}{n} \\cdot \\operatorname{tr}\\left(\\Sigma\\left(\\Sigma+\\lambda I_{p \\times p}\\right)^{-1} \\Sigma\\left(\\Sigma+\\lambda I_{p \\times p}\\right)^{-1}\\right)+\\lambda^2\\left\\|\\Sigma^{1 / 2}\\left(\\Sigma+\\lambda I_{p \\times p}\\right)^{-1} \\beta^*\\right\\|_2^2 \\\\ =&amp;\\frac{\\sigma^2}{n} \\cdot \\operatorname{tr}\\left(D\\left(D+\\lambda I_{p \\times p}\\right)^{-1} D\\left(D+\\lambda I_{p \\times p}\\right)^{-1}\\right)+\\lambda^2\\left\\|D^{1 / 2}\\left(D+\\lambda I_{p \\times p}\\right)^{-1} U^T \\beta^*\\right\\|_2^2 \\\\ =&amp;\\frac{\\sigma^2}{n} \\sum_{j=1}^p \\frac{s_j^2}{\\left(s_j+\\lambda\\right)^2}+\\lambda^2 \\cdot \\sum_{j=1}^p \\frac{s_j\\left(U^T \\beta^*\\right)_j^2}{\\left(s_j+\\lambda\\right)^2} \\end{align*}\\] and the result follows. \\(\\blacksquare\\) If all eigenvalues are equal, that is, \\(s_j=s\\) and if additionally \\(\\left(U^T \\beta^*\\right)_j=b(j=1, \\ldots, p)\\), then the expression of the theorem simplifies to \\[ \\frac{\\sigma^2 p}{n} \\cdot \\frac{s^2}{(s+\\lambda)^2}+\\lambda^2 \\frac{s b^2 p}{(s+\\lambda)^2} \\underset{\\lambda=\\frac{\\sigma^2/n}{b^2}}{\\stackrel{\\min }{\\rightarrow}} \\frac{\\sigma^2 p}{n} \\cdot \\frac{b^2 s}{\\frac{\\sigma^2}{n}+b^2 s} \\leq \\frac{\\sigma^2 p}{n} . \\] We see that for a suitable choice of the penalization parameter \\(\\lambda\\), the excess Bayes risk of the ridge estimator can be smaller than the corresponding upper bound of the LS estimator. 9.3.3 Lasso Regression If we believe that some covariates are pure noise, i.e., unrelated to \\(Y\\), the most obvious choice to penalize \\(\\beta\\) would be of the form \\(\\|\\beta\\|_0=\\#\\{j=\\) \\(\\left.{1, \\ldots, p: \\beta_j} \\neq 0\\right\\}\\). Then, one would simply penalize the number of non-zero entries of \\(\\beta\\). However, this leads to an NP-hard optimization problems whose solutions are not accessible in practice. One therefore uses a different norm which has similar properties but leads to a convex optimization problem. Definition. (Lasso - Least absolute shrinkage and selection operator regression) Let \\(\\lambda \\geq 0\\) and \\[ J_\\lambda(\\beta)=\\lambda \\cdot\\|\\beta\\|_1=\\lambda \\sum_{j=1}^p\\left|\\beta_j\\right| . \\] The LASSO estimator is given by \\[\\begin{align*} \\hat{\\beta}_\\lambda^{\\text {lasso }} &amp; \\in \\underset{\\beta \\in \\mathbb{R}^p}{\\arg \\min }\\left\\{\\hat{R}_n(X\\beta)+J_\\lambda(X\\beta)\\right\\} \\\\ &amp; =\\underset{\\beta \\in \\mathbb{R}^p}{\\arg \\min }\\left\\{\\frac{1}{n}\\|\\mathbf{Y}-\\mathbf{X} \\beta\\|_2^2+\\lambda \\cdot\\|\\beta\\|_1\\right\\} \\end{align*}\\] The corresponding algorithm reads \\[ \\hat{m}_{n, \\lambda}^{l a s s o}(x)=\\sum_{j=1}^p \\hat{\\beta}^{\\text{lasso}}_{\\lambda, j} x_j. \\] There exists no easy closed-form solution for \\(\\hat \\beta^{\\text{lasso}}_{\\lambda}\\) besides some special cases. For \\(\\beta \\in \\mathbb{R}^p\\), define \\[ S(\\beta):=\\left\\{j \\in\\{1, \\ldots, p\\}: \\beta_j \\neq 0\\right\\}. \\] For \\(S \\subset\\{1, \\ldots, p\\}\\) and \\(v \\in \\mathbb{R}^p\\), put \\(v_S:=\\left(v_j \\mathbb{1}_{\\{j \\in S\\}}\\right)_{j=1, \\ldots, p}\\) If \\(p \\ll n\\), then \\(\\hat{\\Sigma}\\) would usually be invertible and the smallest eigenvalue (Rayleigh quotient) would satisfy \\[ \\lambda_{\\min }(\\hat{\\Sigma}):=\\inf _{v \\in \\mathbb{R}^p} \\frac{v^T \\hat{\\Sigma} v}{\\|v\\|_2^2}&gt;0. \\] Then \\(\\hat{\\Sigma}\\) would be one-to-one (injective) and the linear equation system \\(\\hat{\\Sigma} \\beta=\\frac{1}{n} \\mathbf{X}^T \\mathbf{Y}\\) would lead to the (unique) least squares estimator. For \\(p \\gg n\\), one has \\(\\lambda_{\\min }(\\hat{\\Sigma})=0\\). When employing LASSO, we are usually only interested in estimators \\(\\hat{\\beta}\\) whith non-zero entries at the components \\(S\\left(\\beta^*\\right)\\). This means that in principle we only need injectivity of \\(\\hat{\\Sigma}\\) on the set \\[ \\tilde{C}=\\left\\{\\beta \\in \\mathbb{R}^p: S(\\beta)=S\\left(\\beta^*\\right)\\right\\}=\\left\\{\\beta \\in \\mathbb{R}^p:\\left\\|\\beta_{S\\left(\\beta^*\\right)^ c}\\right\\|_1=0\\right\\}, \\] or equivalently, \\(\\inf _{v \\in \\tilde{C}} \\frac{v^T \\hat{\\Sigma} v}{\\|v\\|_2^2}=\\inf _{v \\in \\tilde{C}} \\frac{v^T \\hat{\\Sigma} v}{\\left\\|v_{S\\left(\\beta^*\\right)}\\right\\|_2^2}&gt;0 .\\) Definition. (Restricted eigenvalue property (REP)) We say that the restricted eigenvalue property (REP) is satisfied with \\(\\alpha&gt;0\\) if for \\[ C:=\\left\\{\\beta \\in \\mathbb{R}^p:\\left\\|\\beta_{S\\left(\\beta^*\\right)^c}\\right\\|_1 \\leq \\alpha \\left\\|\\beta_{S\\left(\\beta^*\\right)}\\right\\|_1\\right\\} \\] it holds that \\[ \\Lambda_{\\min }(\\Sigma):=\\inf _{v \\in C} \\frac{v^T \\Sigma v}{\\left\\|v_{S\\left(\\beta^*\\right)}\\right\\|_2^2}&gt;0, \\] Theorem. Let \\(\\varepsilon \\sim N\\left(0, \\sigma^2\\right), X \\sim N(0, \\Sigma)\\) and \\(\\Sigma_{j j}=1(j=1, \\ldots, p)\\). Define \\(s:=\\# S\\left(\\beta^*\\right)\\). Then there exist universal constants \\(c_1, c_2&gt;0\\) such that the condition \\[ n \\geq c_1 \\frac{\\|\\Sigma\\|_2}{\\Lambda_{\\min }(\\Sigma)^2} s \\log (e p / s) \\] implies: For each \\(t \\geq 0\\) and \\[ \\lambda \\geq \\frac{6 \\sqrt{2} \\sigma}{\\sqrt{n}} \\sqrt{\\log (p)+t}, \\] it holds that \\[ \\mathbb{P}\\left(R\\left(\\hat{m}_{n,\\lambda}\\right)-r\\left(\\beta^*\\right)&gt;16 \\lambda^2 \\frac{s}{\\Lambda_{\\min }(\\Sigma)}\\right) \\leq e^{-t}+2 p e^{-c_2 n} . \\] The upper bound for the convergence rate of the excess risk of the LASSO estimator is minimized for \\(\\lambda=6 \\sqrt{2} \\cdot \\frac{\\sigma}{\\sqrt{n}} \\sqrt{\\log (p)}\\). With that choice, \\[ 16 \\lambda^2 \\frac{s}{\\Lambda_{\\min }(\\Sigma)}=\\frac{\\text {c }}{\\Lambda_{\\min }(\\Sigma)} \\cdot \\frac{\\sigma^2 s}{n} \\cdot \\log (p) \\] Interpretation: \\(\\hat{\\beta}_\\lambda\\) behaves like the LS estimator in a model with \\(s\\) instead of \\(p\\) dimensions. The LASSO estimator \\(\\hat{\\beta}_\\lambda\\) has to ‘pay’ with a factor \\(\\log (p)\\) for the missing insight which components are non-zero. This is a rather small price to pay even if \\(p\\) is large. One can prove similar theoretical statements without the conditions \\(\\varepsilon \\sim N\\left(0, \\sigma^2\\right)\\) and \\(X \\sim N(0, \\Sigma)\\) and still can preserve the small \\(\\log (p)\\) term. Regarding the REP: The smallest eigenvalue \\(\\lambda_{\\min }(\\Sigma)\\) measures how strongly the components of \\(X\\) are correlated. Note that a strong correlation of \\(X\\) is a problem for estimation of \\(\\beta^*\\), but not for the excess risk itself: In the extreme case \\(X_1=X_2\\), it is clear that \\(\\hat{\\beta}\\) cannot distinguish the values of \\(\\beta_1^*\\) and \\(\\beta_2^*\\), but it can still provide good predictions through \\(X \\hat{\\beta}\\). Unfortunately, the proof technique underlying the theorem transfers the estimation quality of \\(\\beta^*\\) to an upper bound of the excess risk, therefore this fact is not adequately represented in the result. The assumption \\(\\Sigma_{j j}=1\\) is only to provide an easier result. In practice, this normalization can be obtained by standardizing \\(X_1, \\ldots, X_n\\) before computing the LASSO estimator (that is, center \\(X_i\\) and divide by the empirical standard deviation). Theorem. We do not assume that the linear model holds. Assume that \\(X\\) and \\(Y\\) have bounded support (bounded by \\(B&gt;0\\)). Let \\[ \\beta_*=\\underset{\\|\\beta\\|_1 \\leq \\eta}{\\operatorname{argmin}}\\ r(X\\beta) \\] Then for any \\(\\xi&gt;0\\), \\[ \\mathbb P\\left (r(\\widehat{\\beta})- r\\left(\\beta_*\\right)\\geq\\sqrt{\\frac{2(\\eta+1)^4 B^2}{n} \\log \\left(\\frac{2p^2}{\\xi}\\right)}\\right) \\leq \\xi . \\] Proof. Set \\(Z=(Y, X)\\) and \\(Z_i=\\left(Y_i, X_i\\right)\\), \\(\\gamma = \\gamma(\\beta)=(-1, \\beta)\\). Then \\[ r(X\\beta)=\\mathbb{E}\\left(Y-\\beta^T X\\right)^2=\\gamma^T \\Lambda \\gamma \\] where \\(\\Lambda=\\mathbb{E}\\left[Z Z^T\\right]\\). Note that \\(\\|\\gamma\\|_1=\\|\\beta\\|_1+1\\). Let \\(\\mathcal{B}=\\left\\{\\beta:\\|\\beta\\|_1 \\leq \\gamma\\right\\}\\). \\[ \\hat{R}_n( X\\beta)=\\frac{1}{n} \\sum_{i=1}^n\\left(Y_i-X_i^T \\beta\\right)^2=\\gamma^T \\widehat{\\Lambda} \\gamma \\] where \\(\\widehat{\\Lambda}=\\frac{1}{n} \\sum_{i=1}^n Z_i Z_i^T\\). For any \\(\\beta \\in \\mathcal{B}\\) \\[\\begin{align*} |\\hat{R}_n(\\mathbf X\\beta)-r(X\\beta)| &amp; =\\left|\\gamma^T(\\widehat{\\Lambda}-\\Lambda) \\gamma\\right| \\\\ &amp; \\leq \\sum_{j, k}|\\gamma_j||\\gamma_k||\\widehat{\\Lambda}_{j k}-\\Lambda_{jk}| \\\\ &amp; \\leq(\\eta+1)^2 \\max_{jk} |\\widehat{\\Lambda}_{j k}-\\Lambda_{jk}| \\end{align*}\\] Note that \\(|\\Lambda_{jk}| \\leq B^2\\). By Hoeffding’s inequality, \\[ \\mathbb{P}\\left( |\\widehat{\\Lambda}_{j k}-\\Lambda_{jk}|\\geq \\epsilon\\right) \\leq 2 e^{-2n \\epsilon^2 / B^2} \\] and so, by the union bound, \\[ \\mathbb{P}\\left(\\max _{j, k}|\\widehat{\\Lambda}_{j k}-\\Lambda_{j k}| \\geq \\epsilon\\right) \\leq 2 p^2 e^{-2n \\epsilon^2 / B^2}=\\xi, \\] if we choose \\(\\epsilon=\\sqrt{\\frac{B^2} {2 n} \\log \\left(\\frac{{2} p^2}{{ \\xi}}\\right)}\\). Hence, with probability \\(1-\\xi\\) (see slide 14, lecture 1), \\[ r({X \\hat\\beta}) - r\\left(X\\beta^*\\right)\\leq 2(\\eta+1)^2 \\varepsilon. \\] as desired. \\(\\blacksquare\\) Definition. If \\(P(S(\\widehat{\\beta})=S(\\beta)) \\rightarrow 1\\) we call \\(\\hat \\beta\\) sparsistent. We call \\(\\hat{\\beta}\\) weakly sparsistent if, for every \\(\\beta\\) as \\(n \\rightarrow \\infty\\) \\[ P_\\beta\\left(I\\left(\\widehat{\\beta}_j=1\\right) \\leq I\\left(\\beta_j=1\\right) \\text { for all } j\\right) \\rightarrow 1 \\] In the above \\(S(\\cdot)\\) represent the covariates with non-zero covariates. Therefore we can interpret a sparsistent estimator as an estimator which for increasing information converges to choosing the correct explanatory variables, where the weak condition only ensure that we may choose the right covariates but at least not the wrong. Suppose that \\(p\\) is fixed. Then the least squares estimator \\(\\widehat{\\beta}_n\\) is minimax and satisfies \\[ \\sup _\\beta E_\\beta\\left(n\\left\\|\\widehat{\\beta}_n-\\beta\\right\\|^2\\right)=O(1) . \\] But sparsistent estimators have larger risk: Theorem. We don’t assume the linear model. Suppose that the following condiitons hold: \\(p\\) is fixed. The covariariates are nonstochastic and \\(n^{-1} \\mathbf{X}^T \\mathbf{X} \\rightarrow \\Sigma\\) for some positive definite matrix \\(\\Sigma\\). The errors \\(\\epsilon_i\\) are independent with mean 0, finite variance \\(\\sigma^2\\) and have a density \\(f\\) satisfying \\[ 0&lt;\\int\\left(\\frac{f^{\\prime}(x)}{f(x)}\\right)^2 f(x) d x&lt;\\infty \\] If \\(\\widehat{\\beta}\\) is weakly sparsistent, then \\[ \\sup _\\beta \\mathbb E_\\beta\\left(n\\left\\|\\widehat{\\beta}_n-\\beta\\right\\|^2\\right) \\rightarrow \\infty . \\] More generally, if \\(\\ell\\) is any loss function \\(\\ell: \\mathbb R \\mapsto \\mathbb R_{\\geq 0}\\).then \\[ \\sup _\\beta \\mathbb E_\\beta\\left(\\ell\\left(n^{1 / 2}\\left(\\widehat{\\beta}_n-\\beta\\right)\\right)\\right) \\rightarrow \\sup _s \\ell(s) . \\] Proof. Choose any \\(s \\in \\mathbb{R}^d\\) and let \\(\\beta_n=-s / \\sqrt{n}\\). Then, \\[\\begin{align*} \\sup _\\beta \\mathbb E_\\beta\\left(\\ell\\left(n^{1 / 2}(\\widehat{\\beta}-\\beta)\\right)\\right. &amp; \\geq \\mathbb E_{\\beta_n}\\left(\\ell\\left(n^{1 / 2}(\\widehat{\\beta}-\\beta)\\right) \\geq \\mathbb E_{\\beta_n}\\left(\\ell\\left(n^{1 / 2}(\\widehat{\\beta}-\\beta)\\right) I(\\widehat{\\beta}=0)\\right)\\right. \\\\ &amp; =\\ell\\left(-\\sqrt{n} \\beta_n\\right) \\mathbb P_{\\beta_n}(\\widehat{\\beta}=0)=\\ell(s) P_{\\beta_n}(\\widehat{\\beta}=0) . \\end{align*}\\] Now, \\(\\mathbb P_0(\\widehat{\\beta}=0) \\rightarrow 1\\) by assumption. It can be shown (via contiguity) that we also have \\(\\mathbb P_{\\beta_n}(\\widehat{\\beta}=0) \\rightarrow 1 .\\) Hence, with probability tending to 1, \\[ \\sup _\\beta E_\\beta\\left(\\ell\\left(n^{1 / 2}(\\widehat{\\beta}-\\beta)\\right) \\geq \\ell(s) .\\right. \\] Since \\(s\\) was arbitrary the result follows. \\(\\blacksquare\\) 9.3.4 Conclusion Lasso and Ridge regression aim for different things. Ridge regression reduces variance of the estimator by restricting the function space. Heuristically, the smaller \\(||\\beta^\\ast||_2\\) the more helpful is ridge regression. Lasso is useful in sparse setting, i.e, if one wishes to eliminate components/features. Example. Assume that \\(\\rm{corr}(X_{1},X_2)=1\\) and \\(Y=0.5X_1+0.5X_2\\). While Lasso will probably estimate \\(\\beta_1=1, \\beta_2=0\\), Ridge will probably estimate correctly \\(\\beta_1=0.5, \\beta_2=0.5\\). Definition. (Elastic net) For \\(\\lambda&gt;0, \\alpha \\in (0,1)\\), and \\(J^{elastic.net}(\\beta)= \\sum_{j=1}^p \\alpha |\\beta_j| + (1-\\alpha) \\beta_j^2\\), we call \\[\\begin{align*} \\hat{\\beta}_\\lambda^{\\text {elastic.net }} &amp; \\in \\underset{\\beta \\in \\mathbb{R}^d}{\\arg \\min }\\left\\{\\hat{R}_n(\\beta)+J^{elastic.net}_\\lambda(\\beta)\\right\\} \\\\ \\end{align*}\\] elastic net estimator. "],["nonparametric-regression.html", "9.4 Nonparametric Regression", " 9.4 Nonparametric Regression Linear models are quite restrictive and one may ask how one can achieve more flexibility. In this chapter we will look at the nonparametric regression problem with squared loss \\(L(y_1,y_2)=(y_1-y_2)^2\\). We already know that the Bayes-rule is \\(m^\\ast(x)=\\mathbb E[Y|X=x]\\). 9.4.1 Linear Smoothers Definition. (k-nearest-neighbor) The k-nearest-neighbor estimator is \\[ \\hat m^{knn}(x)= \\frac 1 k \\sum_{i \\in \\mathcal N_k(x)}Y_i, \\] where \\(\\mathcal N_k(x)\\) contains the indices of the \\(k\\) closest points of \\(\\{X_1, \\dots X_n\\}\\) to \\(x\\). Definition. (Linear Smoother) An estimator is called linear smoother if it can be written as \\[ \\hat m(x)= \\sum_i w_i(x)Y_i, \\] where the weight function \\(w_i\\) can depend on \\(\\{X_1, \\dots, X_n\\}\\). Example. The k-nearest-neighbor estimator is a linear smoother: \\[ \\hat m^{knn}(x)= \\sum_i^n w_i(x) Y_i, \\] with \\[ w_i(x)=\\begin{cases}\\frac 1 k &amp; X_i \\ \\text{belongs to the}\\ k \\ \\text{closest points to}\\ x \\\\ 0 &amp; else\\end{cases} \\] In the below proposition the definition of Lipschitz continuity is used. We recall that a real-valued function \\(f:\\mathbb R\\to\\mathbb R\\) is \\(L\\)-Lipschitz continuous if and only if \\[ f(x_1)-f(x_2)\\le L\\Vert x_1-x_2\\Vert_2 \\] for all \\((x_1,x_2)\\in\\mathbb R^2\\). This in particulat means that \\(f(x)\\in[f(x_1)-L\\vert x_1-x\\vert,f(x_1)+L\\vert x_1-x\\vert]\\) i.e. does not on any interval grow faster that a linear function with slope \\(L\\). Proposition. (MSE k-nearest-neighbor) Assume that \\[ E[Y|X=x]=m^\\ast(x)\\in \\mathcal G_L = \\{m: \\mathbb R^p \\mapsto \\mathbb R\\ |\\ m \\ \\text{is L-Lipschitz continuous}\\}, \\] and \\(\\textrm{Var}(Y|X=x)=\\sigma^2(x)\\leq \\sigma^2.\\) Then \\[ \\mathbb E[(\\hat m^{knn}(x)-m^\\ast(x))^2]\\leq (cL)^2 \\left(\\frac k n \\right)^{2/p}+\\frac {\\sigma^2}k. \\] In particular, for \\(k_n=O_p( n^{2/(2+p)})\\), we get \\[ \\mathbb E[(\\hat m^{knn}(x)-m^\\ast(x))^2]=O_p(n^{-2/(2+p)}). \\] Proof. Write \\(\\mathbf X=(X_1,\\dots, X_n)\\) and denote by \\(Y_i^{(x)}\\) the \\(i\\)th closest \\(Y\\) to \\(x\\) among \\(Y_1,\\dots Y_n\\). \\[\\begin{align*} \\mathbb E[(\\hat m^{knn}(x)-m^\\ast(x))^2]&amp; \\stackrel{(\\dagger_1)}{=}\\mathbb E \\Big [{\\left(\\mathbb{E}[\\hat m^{knn}(x)|\\mathbf X]-m^\\ast(x)\\right)^2}\\Big]+{\\mathbb{E}\\Big[(\\hat m^{knn}(x)-\\mathbb{E}[\\hat m^{knn}(x)|\\mathbf X])^2\\Big]}\\\\ &amp;\\stackrel{(\\dagger_2)}{=}\\mathbb E\\left[ \\Big \\{\\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(x)}\\left(m^\\ast\\left(X_i\\right)-m^\\ast(x)\\right)\\Big \\}^2\\right] \\\\ &amp;+ \\frac 1 {k^2} \\mathbb E \\left[ \\sum_i^k \\sum_j^k \\{Y_i^{(x)}- \\mathbb E[Y_i^{(x)}| \\mathbf X]\\}\\{Y_j^{(x)}- \\mathbb E[Y_j^{(x)}| \\mathbf X]\\} \\right] \\\\ &amp;\\stackrel{(\\dagger_3)}{\\leq} \\mathbb E\\left[ \\left(\\frac{L}{k} \\sum_{i \\in \\mathcal{N}_k(x)}\\left\\|X_i-x\\right\\|_2\\right)^2 \\right]+\\frac{\\sigma^2}{k} \\\\ &amp;\\stackrel{(\\dagger_4)}{\\leq} L^2 c \\left(\\frac k n \\right)^{2/p}+\\frac {\\sigma^2}k \\end{align*}\\] where we as usual in \\((\\dagger_1)\\) use that \\(m^*(x)-Y\\) is orthogonal to any element \\(m(x)-m^*(x)\\). In the \\((\\dagger_2)\\) we use the assumption that \\(m^*(x)=\\mathbb E[Y\\vert X=x]\\) and hence \\[ \\mathbb E\\left[\\left. \\hat m^{knn}(x)\\right\\vert X\\right]=\\frac{1}{k}\\sum_{i\\in \\mathcal N_k(x)}\\mathbb E\\left[\\left. Y_i\\right\\vert X \\right]=\\frac{1}{k}\\sum_{i\\in \\mathcal N_k(x)}m^*(X_i). \\] Furthermore, the second term is derived from the below \\[\\begin{align*} \\mathbb{E}\\left[(\\hat m^{knn}(x)-\\mathbb{E}[\\hat m^{knn}(x)|\\mathbf X])^2\\right]&amp;=\\mathbb{E}\\left[\\left(\\frac{1}{k}\\sum_{l\\in \\mathcal N_k(x)}Y_l-\\frac{1}{k}\\sum_{i\\in \\mathcal N_k(x)}\\mathbb E\\left[\\left. Y_i\\right\\vert X \\right]\\right)^2\\right]\\\\ &amp;=\\frac 1 {k^2} \\mathbb E \\left[ \\sum_i^k \\sum_j^k \\{Y_i^{(x)}- \\mathbb E[Y_i^{(x)}| \\mathbf X]\\}\\{Y_j^{(x)}- \\mathbb E[Y_j^{(x)}| \\mathbf X]\\} \\right] \\end{align*}\\] The \\((\\dagger_3)\\) is derived by the definition of a \\(L\\)-Lipschitz function and the assumption regarding the variance of the conditional variable \\(Y\\ \\vert\\ X=x\\). \\((\\dagger_4)\\) is a result taken from Gyorfi et al. 2002, vol. 1, chap. 6.3. \\(\\blacksquare\\) Note that this is slower than the “parametric” MSE of \\(n^{-1}\\). In particular the rate depends on \\(p\\). Even worse: It grows exponentially in \\(p\\). We have learned that the knn estimator can be written as \\[ \\hat m^{knn}(x)= \\frac 1 k w_i(x_i) Y_i, \\] Note that \\(w_i(x)\\) is not smooth as a function of \\(x\\). This also makes the estimator not smooth. Given that we assume that \\(m^\\ast\\) is smooth, this may not be desirable. An alternative are kernel smoothers. Definition. (Kernel Smoother) The kernel smoother is a linear smoother with \\[ \\hat m^{ks}(x)= \\sum_i w_i(x_i) Y_i, \\] where \\[ w_i(x)=\\frac{K\\left(\\frac{||x-X_i||}{h}\\right)}{\\sum_j K\\left(\\frac{||x-X_j||}{h}\\right)} \\] Often \\(K=\\prod_j k_j\\), such that \\(K\\left(\\frac{||x-X_i||}{h}\\right)=\\prod_j k(x-X_{ij})\\). The function \\(k: \\mathbb R \\mapsto \\mathbb R\\) is usually a symmetric density function. The general idea is to fit a smooth function to the data points \\((X,Y)\\) such that \\(m(X)\\) is somewhat centered between the data points. The choice of a symmetric function \\(k\\) ensures that the estimator weighs the datapoints closer to \\(x\\) higher than further away data point. In the case with \\(X,Y\\in \\mathbb R\\) we can choose for instance Gaussian kernel: \\(k_j(z)=\\exp\\left(\\frac{z}{2b^2}\\right)\\) with \\(b&gt;0\\), Uniform kernel: \\(k_j(z)=1_{[0,1]}(z)/2\\in \\{0,1\\}\\), Triangular kernel: \\(k_j(z)=\\big(1-z\\big)^+\\), Epanechnikov kernel: \\(k_j(z)=\\frac{(1-z^2)^a}{2^{2a+1}\\Gamma(a+1)^2\\Gamma(2a+2)^{-1}}1_{[0,1]}(z)\\) with \\(a=1\\). Biweight kernel: Above with \\(a=2\\). where of course the argument is \\(z=\\vert x-X_{ij}\\vert/h\\) with \\(h&gt;0\\) being the bandwidth. Notice that the uniform kernel in fact is the Epanechnikov kernel with \\(a=0\\). Proposition. (MSE Kernel Smoother) Assume that \\(E[Y|X=x]=m^\\ast(x)\\in \\mathcal G_L\\) with \\[ \\mathcal G_L = \\{m: \\mathbb R^p \\mapsto \\mathbb R\\ |\\ m \\ \\text{is L-Lipschitz continuous}\\}, \\] and \\(\\textrm{Var}(Y|X=x)=\\sigma^2(x)\\leq \\sigma^2.\\) Then \\[ \\mathbb E[(\\hat m^{ks}(x)-m^\\ast(x))^2]= O_p\\left( \\frac{1}{nh^p} + h^2 \\right) \\] In particular, for \\(h_n=O_p(n^{-1/(2+p)})\\), we get \\[ \\mathbb E[(\\hat m^{ks}(x)-m^\\ast(x))^2]=O_p(n^{-2/(2+p)}). \\] 9.4.2 Curse of dimensionality We have seen that under a Lipschitz condition, both kernel smoother and knn have an asymptotic mean squared error of order \\(n^{-2/(2+p)}\\). One can show that under the assumption that \\(m^\\ast\\) is twice continuously differentiable, the rate for both methods can be improved to \\[ n^{-4/(4+p)}. \\] But this rate is still exponentially decreasing in \\(p\\). Furthermore, it has been shown that no method can do better under the given assumptions. A new observation \\(x_0\\) will have very few or no observations in its neighborhood. This leads to high variance and high bias when increasing the size of the neighborhood. Under the model in the previous section, the setting \\(n=50,p=1\\) has the same expected amount of observations in a neighborhood as the setting \\(n=7.5\\times10^{110}, p=100\\). There are two ways to tackle the curse of dimensionality. Sparsity: Assume that the intrinsic dimension is lower. E.g. Not all variables are relevant. Or feature engineer a few highly predictive variables. Structure: Interactions are limited and structure can be exploited e.g. an additive structure \\(m(x)=m_1(x_1)+m_2(x_2)\\). Remember that structure is essential for interpretability. 9.4.3 Splines We want to establish a framework to estimate additive regression functions. To this end, assume \\(p=1\\) until further notice. Definition. (Splines) A \\(k\\)th-order spline with \\(l\\) knotpoints \\(x_1 &lt;\\dots&lt;x_l\\) is a polynomial of degree \\(k\\) on each interval \\((-\\infty,x_1],[x_1,x_2]\\dots,[x_l,\\infty)\\) has continuous derivatives of orders \\(0,...,k-1\\) on the knotpoints \\(x_1 &lt;\\dots&lt;x_l\\) Example. A \\(k\\)th-order spline \\(m\\) with \\(l\\) knotpoints can an be uniquely written as \\[ m(x)=\\sum_{j=1}^{k+1+l}\\theta_jg_j(x) \\] truncated power basis For \\(j=1,\\dots, k+1\\): \\(g_{j}=x^{j-1}\\) For \\(j=1,\\dots,l:\\) \\(g_{k+1+j}=(x-x_j)^k_+\\) \\((x)_+:= \\max(x,0)\\) B-splines more complicated, but computationally more robust and faster to compute. Splines have high variance at the boundaries. Solution: Let the piecewise polynomial function have a lower degree at \\((-\\infty,x_1],[x_l,\\infty)\\). Definition. (Natural Splines) A \\(k\\)th-order natural spline (\\(k=\\) odd number) with knotpoints \\(x_1 &lt;\\dots&lt;x_l\\) is a polynomial of degree \\(k\\) on the intervals \\([x_1,x_2]\\dots,[x_{l-1},x_l)\\) is a polynomial of degree \\((k-1)/2\\) on \\((-\\infty,x_1],[x_l,\\infty)\\) has continuous derivatives of orders \\(0,...,k-1\\) on the knotpoints \\(x_1 &lt;\\dots&lt;x_l\\) Note that natural splines have dimension \\(l\\) which is in particular independent of the order \\(k\\) (compare to dimension \\(k+l\\) for splines). There is also a truncated power basis and a B-splines basis for natural splines. 9.4.4 Linear regression with splines. We still assume the one-dimensional case, \\(p=1\\). Instead of looking at observations \\((X_i,Y_i)_{i=1,\\dots,n}\\) we can consider a natural splines basis and look at observations \\((g_1(X_i), \\dots, g_l(X_i), Y_i)_{i=1,\\dots,n}.\\) By doing so we are able to approximate any natural spline in \\(x\\) instead of just linear functions in \\(x\\), while still being in a linear regression framework, i.e., \\[ \\hat \\beta = {\\arg \\min }_\\beta \\sum_i (Y_i - { G_i^T}\\beta )^2= (\\mathbf G^T\\mathbf G)^{-1}\\mathbf G^T\\mathbf Y \\] where \\(G_i^T=(g_1(X_i), \\dots, g_l(X_i))\\), the rows of \\(\\mathbf G\\). Problem: How do we choose the number of knotpoints \\(l\\) and their position? First thought: Cross validation. But that would be quite expensive to run. Let’s look at the following minimization problem: \\[ \\hat m= \\arg\\min_{m} \\sum_i (Y_i-m(X_i))^2+\\lambda\\int_a^b m&#39;&#39;(x)^2\\mathrm dx, \\] where minimization runs over all twice times differentiable functions \\(m\\) and observations \\(X_i\\) are in \\([a,b]\\) for all \\(i\\). Theorem (Smoothing splines) If \\(m\\) is twice differentiable and the solution to \\[ \\arg\\min_{m} \\sum_i (Y_i-m(X_i))^2+\\lambda\\int_a^b m&#39;&#39;(x)^2\\mathrm dx, \\] then \\(m\\) is a natural spline of order 3 (natural cubic spline). Proof. Given a minimizer \\(\\tilde m\\), we construct the unique natural cubic spline \\(m\\) on \\([a,b]\\) with knotpoints \\(X_1,\\dots, X_n\\) and \\(m(X_i)=\\tilde m(X_i)\\). We will show that \\(\\tilde m=m\\). Define \\(h=\\tilde m-m\\). Note that \\(h(X_j)=0 (j=1,\\dots,n)\\), \\(m&#39;&#39;&#39;(x)=0\\) for \\(x&lt;X_1\\) and \\(x&gt;X_n\\) as well as \\(m^{(4)}=0\\). Hence by applying integration by parts twice we get \\[\\begin{align*} \\int_a^b m&#39;&#39;(x)h&#39;&#39;(x)\\mathrm dx &amp;= -\\int_{X_1}^{X_n}m&#39;&#39;&#39;(x)h&#39;(x)\\mathrm dx \\\\ &amp;= -\\sum_{j=1}^{n-1}m&#39;&#39;&#39;(x)h(x)\\Big|_{X_j}^{X_{j+1}} \\\\ &amp;=0 \\end{align*}\\] This implies \\[ \\int_a^b \\tilde m&#39;&#39;(x)^2\\mathrm dx= \\int_a^b \\{m&#39;&#39;(x)+h(x)\\}^2\\mathrm dx=\\int_a^b m&#39;&#39;(x)^2+h(x)^2\\mathrm dx. \\] meaning \\[ \\int_a^b m&#39;&#39;(x)^2\\mathrm dx \\leq \\int_a^b \\tilde m&#39;&#39;(x)^2\\mathrm dx. \\] with equality only for \\(h&#39;&#39;=0\\), i.e, \\(h\\) linear. Since \\(h(X_i)=0 (i=1,\\dots n)\\), we have \\(h=0\\) and so \\(\\tilde m=m\\). \\(\\blacksquare\\) We can conclude the following: Let \\(\\mathbf G\\) be the matrix with with rows \\(G_i=(g_1(X_i), \\dots, g_l(X_i))\\). \\(\\{g_j\\}_{j=1,\\dots,l}\\) is a basis for natural cubic splines. Define \\[ \\hat \\beta = \\arg\\min_{\\beta} \\sum_i (Y_i- G_i^T\\beta)^2+\\lambda\\int_0^1 \\left\\{{\\sum_j \\beta_jg_j&#39;&#39;}(x)\\right\\}^2\\mathrm dx. \\] Then, \\[ \\sum_j \\hat \\beta_j g_j=\\arg\\min_{m} \\sum_i (Y_i-m(X_i))^2+\\lambda\\int_0^1 m&#39;&#39;(x)^2\\mathrm dx. \\] Smoothing splines can be seen as a special case of generalized ridge regression: Write \\(\\mathbf W_{ij}=\\int_0^1g_i&#39;&#39;(x)g_j&#39;&#39;(x)\\mathrm dx,\\) then \\[\\begin{align*} &amp;\\arg\\min_{\\beta} \\sum_i (Y_i- G_i^T\\beta)^2+\\lambda\\int_0^1 \\left\\{{\\sum_j \\beta_jg_j&#39;&#39;}(x)\\right\\}^2\\mathrm dx\\\\= &amp;\\arg\\min_{\\beta} \\sum_i (Y_i- G_i^T\\beta)^2+\\lambda \\beta^T \\mathbf W \\beta. \\end{align*}\\] Hence, \\[ \\hat \\beta= (\\mathbf G^T\\mathbf G+\\lambda \\mathbf W)^{-1}\\mathbf G^T\\mathbf Y. \\] It can be shown the smoothing splines are asymptotically equivalent to kernel smoothers with varying bandwidth and a specific choice of kernel, see Silverman (1984) or Wang, Du, and Shen (2013) for a more recent contribution. In general, smoothing splines are more practical since they can be efficiently calculated while kernel smoothers are much easier to analyze theoretically. We have seen that fully nonparametric methods suffer from the curse of dimensionality: the optimal rate of convergence for twice continuously differentiable functions is \\(n^{-4/(4+p)}\\). One solution is to restrict oneself to the class of additive functions \\[ \\mathcal G=\\{m\\ |\\ m(x)=m_1(x_1)+\\cdots +m_p(x_p)\\} \\] Stone (1985) showed that the components \\(m_k\\), if twice continuously differentiable, can be estimated with one-dimensional rate of \\(n^{-4/(4+p)}\\). The components \\(m_j\\) are usually estimated via the so called backfitting algorithm (Hastie and Tibshirani 1990). Backffitting comprises the following two steps: Definition. (Backfitting Algorithm) Intialize: \\(\\hat m_j^{[0]}=0, j=1,\\dots,p\\) Iterate for \\(r=1,\\dots\\) Residuals: \\(r_{ij}^{[r]}=Y_i-\\sum_{k &lt; j} \\hat{m}_{k}^{[r]}(x_{ik})-\\sum_{k &gt; j} \\hat{m}_{k}^{[r-1]}(x_{ik})\\). Smooth: \\(\\hat{m}_j^{[r]}=\\operatorname{Smooth}\\left(\\left\\{X_{ij},r_{ij}^{[r]}\\}_{i=1,\\dots,n}\\right\\}\\right).\\) Center: \\(\\hat{m}_j^{[r]}=\\hat{m}_j^{[r]}-\\frac{1}{n} \\sum_{i=1}^n \\hat{m}_j^{[r]}\\left(X_{i j}\\right)\\). Note that Smooth is a one-dimensional regression problem. In practice Smooth is most often a smoothing spline. It has been shown backfitting via smoothing splines achieve optimal rate of \\(n^{-4/(4+p)}\\) for each component and \\(pn^{-4/(4+p)}\\) for the p-dimensional additive regression function. Problem: Additive methods are still not optimal in the case of sparsity (i.e. some features being not relevant) and interactions between features. "],["trees-and-forests.html", "9.5 Trees and forests", " 9.5 Trees and forests In this chapter we will look at the nonparametric regression problem with squared loss \\(L(y_1,y_2)=(y_1-y_2)^2\\) and also the classification problem with Binary loss function \\(L(y_1,y_2)= 1(y_1\\neq y_2)\\) or squared loss. We already know that the Bayes-rule is \\[ m^\\ast(x)=\\mathbb E[Y|X=x] \\] for the squared loss and \\[ m^\\ast(x)=\\underset{k=1,\\dots,K}{\\operatorname{argmax}}\\ \\mathbb P(Y=k|X=x) \\] for the binary loss. Definition. (Decision trees) A decision tree is an estimator, that partition the feature space \\(\\mathcal X\\) into sections \\(T=\\{R_1,R_2,...,R_k\\}\\) such that \\(R_i\\cap R_j=\\emptyset\\) (pairwise disjoint) for all \\(i\\ne j\\) with \\(1\\le i,j\\le k\\) and \\[ \\bigcup_{i=1}^kR_i=\\mathcal X \\] The algorithm associated with the decision tree is then \\[ m(T)(x)=\\sum_{i=1}^k m_i 1_{R_i}(x). \\] From the above we notice that the tree estimate is a piecewise constant function. We call the constant areas of the tree estimate leaves or terminal nodes. The leaves partition the feature space \\(\\mathcal X\\). Given the data \\(\\mathcal D_n\\) the value within a leave is given by averaging the responses (regression with \\(L_2\\) loss) or majority vote (classification with binary loss). In particular we have \\[ m_i=\\left(\\sum_{j=1}^n1_{R_i}(X_j)\\right)^{-1}\\left(\\sum_{j=1}^n1_{R_i}(X_j)X_j\\right),\\qquad (\\text{continuous case}) \\] i.e. the simple estimated conditional mean \\(\\mathbb {\\hat E}[Y\\ \\vert\\ X\\in R_i]\\) and \\[ m_i=\\underset{y\\in \\mathcal Y}{\\operatorname{argmax}}\\left\\{\\sum_{j=1}^n1_{R_i}(X_j)1_{y}(Y_j)\\right\\}.\\qquad (\\text{discrete case}) \\] i.e. the mode. Decision trees are popular because the decision making (estimate) is nicely visualizable/interpretable if not too deep. One can easily draw out the subsetting tree starting with the entire feature space \\(\\mathcal X\\) and then spitting in two all the way down to the terminal notes. The interior edges on this representation are called nodes and can also be interpreted as subset of \\(\\mathcal X\\). Decision trees can be quite efficient in classification tasks where we are interested in 0-1 (binary) decisions. This is the case in many application but is often not the case in insurance since it is impossible and frankly not usefull having a 0/1 estimate of whether a claim may arrive. Instead the insurance company is interested in the probability that a claim may arrive during a timespan. 9.5.1 CART A decision tree is uniquely defined by its leaves. Given a tree \\(T\\), we write \\(m(T)\\) for the corresponding regression or classification function. The naive approach of looking for leaves \\(R_1,\\dots, R_l\\) that minimize a certain loss is practically not feasible because of the computational cost. Instead: top-down greedy approach, called CART (classification and regression trees). We now describe how to construct nodes via the CART algorithm. Definition. (CART) Start with \\(\\mathcal X\\) as initial node for splitting. Follow the process below for all subnotes made from splitting. Let \\(R\\subseteq \\mathcal X\\) be the input note. (In initial run \\(R=\\mathcal X\\)) For every dimension \\(j=1,\\dots,p\\) and every point \\(s_j\\in R(j)\\) where \\[R(j)= \\{x_j\\ |\\ \\exists\\ x_1,\\dots,x_{j-1},x_{j+1},\\dots,x_p\\ \\text{with}\\ (x_1,\\dots,x_p)\\in R\\}\\] i.e. the support of \\(x_j\\) under the node \\(R\\). We define the following: \\[R_1(j,s_j)=\\{(x_1,\\dots,x_p)\\in R| \\ x_j \\leq s_j\\}, \\quad R_2(j,s_j)=\\{(x_1,\\dots,x_p)\\in R| \\ x_j &gt; s_j\\}.\\] being a subdivision of \\(R=R_1(j,s_j)\\cup R_2(j,s_j)\\) since \\(R_1(j,s_j)\\cap R_2(j,s_j)=\\emptyset\\). Notice that one intepret \\(R_1(j,s_j)\\) is the lower section of \\(R\\) through the splitter \\(s_j\\) in dimension \\(j\\) and \\(R_2(j,s_j)\\) being the upper section of the note. We pick the following minimizer \\[(j^\\ast,s^\\ast)= \\arg\\min_{j,s} Q_n(R_1(j,s_j)) + Q_n(R_2(j,s_j))\\] and split the note \\(R\\) into \\(R_1(j^*,s^*)\\) and \\(R_1(j^*,s^*)\\). If either of \\(R_1(j^*,s^*)\\) and \\(R_1(j^*,s^*)\\) satisfy some predertermined stopping criterion the note is no longer splittet. If not it is further split as long as stopping criterion does not apply. Definition. (CART loss) For regression the most common loss function is the squared loss: \\[ Q_n(R_l)= \\sum_{i: X_i \\in R_l} (Y_i - \\overline Y_i(R_l))^2 \\] with \\[ \\overline Y_i(R_l)= \\frac 1 {|R_l|}\\sum_{i: X_i \\in R_l}Y_i\\ \\text{and}\\ |R_j|=\\#\\{i: X_i\\in R_j\\} \\] In the classification case, define \\[ \\hat p_{lk}= \\frac 1 {|R_l|} \\sum_{i: X_i \\in R_l} 1(Y_i=k) \\] i.e. the proportion of class \\(k\\) observation in \\(R_l\\). Other commom loss functions include: Brier score: \\(Q_n(R_l)= \\sum_{i: X_i \\in R_l} (Y_i - \\overline Y_i(R_l))^2\\) (=squared loss) Misclassification error: \\(Q_n(R_l)= 1 - \\max_{k}\\hat p_{lk}\\) Gini index: \\(Q_n(R_l)= \\sum_{k=1}^K \\hat p_{lk}(1-\\hat p_{lk})\\) Entropy : \\(Q_n(R_l)=-\\sum_{k=1}^K \\hat p_{lk}\\log\\hat p_{lk}\\) Note: In classification, even if binary loss is the ultimate goal, Gini index and entropy might be the better choices for splitting. The most common stopping criterion is specifying a min-node size, i.e. stop if \\(|R_j|&lt;c\\). A common choice is \\(c=5\\). An alternative stopping criteria is the depth of \\(R_j\\), i.e., the number of parent nodes of \\(R_j\\). The process described so far will probably lead to an overfit. One may think that one way out is to stop growing the tree early enough.n This is not advisable because stopping the growing early because the current split does not lead to great improvement in the loss does not mean that a split afterwards might not turn very effective. 9.5.2 Pruning The idea of pruning is to let a tree grow very deep first (= small min node size) and then select a subtree (pruning) as final fit. Idea: We can compare different subtrees via a penalized loss. Definition. (Subtree) We call \\(T&#39;\\) a subtree of \\(T\\) if the nodes of \\(T&#39;\\) is a subset of the nodes of \\(T\\). \\(T&#39;\\) and \\(T\\) have the same root. If \\(T&#39;\\) is a subtree of \\(T\\), we also write \\(T&#39;\\subseteq T\\). We can derive a subtree, by pruning a tree at any non-terminal node (i.e. by deleting all descendants of that node) Definition. (\\(\\alpha\\)-pruned tree) Consider a tree \\(\\hat T_n\\) with corresponding estimator \\(\\hat m_n(\\hat T_n)\\). Given a parameter \\(\\alpha&gt;0\\), the \\(\\alpha\\)-pruned tree is \\[ \\hat T_{n,\\alpha} := \\arg \\min_{T\\subseteq \\hat T_n}\\{\\hat R_n(\\hat m_n) + \\alpha |T|\\}, \\] where \\(|T|\\) is the number of leaves of \\(T\\). How do we find an optimal \\(\\alpha\\) (and also the optimal subtree for a fixed \\(\\alpha\\))? Brute force cross-validation will be too computationally intensive even if we know the answer to the second question. The answer to both questions is the weakest link algorithm. Proposition (weakest link) The \\(\\alpha\\)-pruned tree \\(\\hat T_{n,\\alpha}\\) is unique. Furthermore, there exist (unique) trees \\(T_0\\supset \\cdots\\supset T_l\\) with \\(\\{T_0,\\dots, T_l\\}=\\{\\hat T_{n,\\alpha} : \\alpha&gt;0\\}:=\\hat{ \\mathbb T}_n\\); in particular the set \\(\\{\\hat T_{n,\\alpha} : \\alpha&gt;0\\}\\) is finite. (Weakest link algorith): Let \\(\\widetilde Q_n\\) be the loss function corresponding to \\(\\hat R_n\\). The trees \\(T_0,\\dots, T_l\\) can be found with the following algorithm Let \\(t_L,t_R\\) be any two terminal nodes in \\(\\hat T_n\\) resulting from a split of the immediate ancestor node t. If \\(\\widetilde Q_n(t) = \\widetilde Q_n(t_L)+\\widetilde Q_n(t_R)\\), prune off \\(t_L\\) and \\(t_R\\). Continue this process until no more pruning is possible. The resulting tree is \\(T_0\\). \\(k=0\\) Go through all non-terminal nodes \\(t\\) of \\(T_k\\) and calculate \\[g(t)=\\frac{\\widetilde Q_n(t)-\\widetilde Q_n(T_t)}{|T_t|-1},\\] where \\(T_t\\) is the tree (branch) with root \\(t\\) and nodes consisting of \\(t\\) and the descendants of \\(t\\) in \\(T_k\\). Furthermore \\(\\widetilde Q_n(T_t)=\\sum_{t \\ \\text{leaf of}\\ T_t} \\widetilde Q_n(t)\\). \\(\\alpha= \\min_{t \\ \\text{ non-terminal node of} \\ T_k}g(t)\\). From top to bottom in \\(T_k\\) prune all non-terminal nodes with \\(g(t)=\\alpha\\) and call the resulting tree \\(T_{k+1}\\). If \\(T_{k+1}\\) has more than one node, set \\(k \\leftarrow k+1\\) and go to step 3. The above proposition gives a surprising result, that for all \\(\\alpha\\in \\mathbb R_+\\) the mapping \\(\\alpha \\mapsto\\hat T_{n,\\alpha}\\in\\hat{ \\mathbb T}_n\\) is well defined and the set \\(\\hat{ \\mathbb T}_n\\) is finite. Intuitively one can come to this conclusion by pre-assuming that the mapping before is unique and considering that the mapping \\(f : \\alpha \\mapsto \\vert \\hat T_{n,\\alpha}\\vert\\) is monotonic and that \\(f(\\alpha)\\to \\vert \\hat T_n\\vert\\) for \\(\\alpha \\to 0_+\\) and \\(f(\\alpha)\\to 1\\) for \\(\\alpha \\to \\infty\\). In particular, the tree is pruned more and more harshly for larger \\(\\alpha\\) and for sufficiently large \\(\\alpha\\) the tree \\(\\hat T_{n,\\alpha}\\to\\mathcal X\\) for \\(\\alpha \\to \\infty\\). Given \\(l\\) trees \\(\\{T_0,\\dots, T_l\\}=\\{\\hat T_{n,\\alpha} : \\alpha&gt;0\\}\\), we can pick the optimal tree via cross validation. Problem with decision trees. There are (at least) two major problems with decision trees Instability: Small variations in the data can lead to a very different tree Performance: The performance (measured via test error) is usually much weaker compared to other learning algorithms 9.5.3 Bagging Bagging stands for Bootstrap aggregation. Idea: Generate artificial data via bootstrap, build a decision tree for each data-set and average. Hope: It reduces the variance of decision trees. In the following recall that Bootstrap sampling involves drawing \\(m\\) samples \\((\\tilde X_i,\\tilde Y_i)\\) from the original dataset with replacement. This gives a new dataset \\(\\tilde {\\mathcal D}_m\\) of new data which may be analised. The process of drawing alot of dataset will reduce the variance by simply averaging over all boostrap samples \\(\\tilde {\\mathcal D}_m^{(1)},...,\\tilde {\\mathcal D}_m^{(B)}\\). Definition. (Bagging) Given data \\({(X_1,Y_1),\\dots, (X_n,Y_n)}\\), draw \\(B\\) bootstrap samples \\(\\tilde {\\mathcal D}^{(b)}_n={(\\tilde X^{(b)}_1,\\tilde Y^{(b)}_1),\\dots, (\\tilde X^{(b)}_n,\\tilde Y^{(b)}_n)}\\), for \\(b=1,\\dots, B\\), i.e., \\(\\tilde {\\mathcal D}^{(b)}_n\\) arises from \\(\\mathcal D_n\\) by drawing \\(n\\) times with replacement. The bagging estimator is defined as the following. In the squared loss case (average): \\[ \\hat m^{bagg}_n=\\frac 1 B \\sum_{b=1}^B \\hat m_n(\\tilde {\\mathcal D}^{(b)}_n) \\] and in the binary loss case (majority vote): \\[ \\hat m^{bagg}_n(x)=\\arg \\max_{k\\in {1,\\dots K}} \\#\\{b: m_n(\\tilde {\\mathcal D}^{(b)}_n) (x)=k\\} \\] Note that \\(\\hat m^{bagg}_n\\) is a stochastic estimator, even given \\(\\mathcal D_n\\). Note: Usually, bagging is applied on non-pruned trees. (See it as an alternative way to reduce variance) While bagging improves performance of decision trees, interpretability is worsened. If interpretability is not a concern, then bagging is still inferior compared to other learning techniques. Random forests are a modification on bagging estimators. 9.5.4 Random Forests As bagging random forest are an ensemble of decision trees. They add the following modification to bagging: - \\(\\texttt{mtry}\\): Each time a node is split it is only done on a subset of size \\(\\texttt{mtry}\\) of viable variables. I.e., each time you want to split a node, turn on a random generator and draw \\(1\\leq\\texttt{mtry}&lt;p\\) viable variables from \\(\\{1,\\dots,p\\}\\). Only those variables drawn are allowed to be considered for the next split. - We denote the random forest estimator by \\(\\hat m^{rf}_n\\). Note that \\(\\hat m^{rf}_n\\), as \\(\\hat m^{bagg}_n\\), is a stochastic estimator, even given \\(\\mathcal D_n\\). Why is \\(\\texttt{mtry}\\) so useful? Assume that we are given \\(l\\) estimators \\(m_n(\\mathcal D_n, U_1), \\dots m_n(\\mathcal D_n, U_l)\\). Here, \\(U_1,\\dots, U_l\\) are iid variables inducing the additional stochasticity of the estimators (e.g. through random forest or bagging). Assume that \\(\\hat m_n(\\mathcal D_n, U_1), \\dots \\hat m_n(\\mathcal D_n, U_l)\\) have pairwise correlation \\(\\rho\\) and variance \\(\\sigma^2\\). We get \\[\\begin{align*} \\textrm{Var}\\left (\\frac 1 l \\sum_{j=1}^l \\hat m_n(\\mathcal D_n, U_j)\\right)&amp;= \\frac 1 {l^2} \\sum_{jk} \\textrm{Cov}(m_n(\\mathcal D_n, U_j),m_n(\\mathcal D_n, U_k))\\\\&amp;=\\frac 1 {l^2} \\left( l\\sigma^2+ (l^2-l)\\rho\\sigma^2\\right)\\\\&amp;= \\rho \\sigma^2 + \\frac{1-\\rho}{l}\\sigma^2 \\end{align*}\\] Meaning: Variance of an ensemble of trees is small the smaller the correlation between the trees. The \\(\\texttt{mtry}\\) parameter aims to make trees less alike and hence more uncorrelated. Random forest are competitive to many state of the art learners. While not having “best performance” in terms off accuracy they are not too far behind. Advantages of random forests are Default hyperparameters already lead to very strong results (i.e. without tuning hyperparameter) \\(\\texttt {mtry}=\\lfloor \\sqrt{p}\\rfloor\\) min node size= 1 for classification, 5 for regression Can be implemented (and is implemented) very fast Random forest (and also trees) don’t perform well for additive functions Decision trees are not good with additive functions (example). Consider \\(m^\\ast(x) = \\sum_j^p 1(x_j \\leq 0)\\) for large \\(p\\). For a perfect fit, a tree algorithm would have to grow a tree with depth \\(p\\), where each leaf is the result of splitting once with respect to each covariate. Hence, we end up with \\(2^p\\) leaves, which on average contain \\(n/(2^p)\\) data points. Even if all splits are optimal for \\(2^p&gt;n\\), the tree will not be able to lead to a perfect decision rule. While predictions of random forests are relatively smooth, they are not monotonic. e.g. in car insurance, if everything else is the same, more mileage should lead to higher insurance price. Random forests deal well with sparsity, i.e., when the number of features \\(p\\) is large, but the number of relevant features is rather small: \\(s&lt;&lt;p\\). "],["boosting-and-additive-trees.html", "9.6 Boosting and additive trees", " 9.6 Boosting and additive trees In the previous section we have discussed that random forests are not good in estimating additive structures. In this section we will see two tree-based estimators that can deal better with additive structures. 9.6.1 Gradient Boosting Machines The general idea of boosting is to construct an estimator of the form \\[ \\hat m_n(x)=\\sum_{j=1}^B \\hat m_{n,j}(x), \\] where \\(\\hat m_{n,b}\\) is the result of improving on the estimator \\(\\hat m_n^{(b-1)}(x)=\\sum_{j=1}^{b-1} \\hat m_{n,j}(x)\\). Definition. (Forward Stagewise Additive Modeling) The function \\(\\hat m_n(x)=\\sum_{j=1}^B \\hat m_{n,j}(x)\\) is called Forward Stagewise Additive Modeling estimator if \\[ m_{n,0}(x)=\\arg \\min_{\\eta\\in \\mathcal G} \\sum_{i=1}^N L\\left(Y_i, \\eta\\right) \\] and for \\(b=1,\\dots,B\\) \\[ \\hat m_{n,b} = \\arg \\min_{\\eta\\in \\mathcal G} \\hat R_n\\left( \\hat m_n^{(b-1)}(x)+\\eta(x)\\right), \\] where \\(\\hat m_n^{(b-1)}=\\sum_{j=1}^{b-1}\\hat m_{n,j}.\\). The set \\(\\mathcal G\\) is chosen to be very restrictive/small. The component \\(m_{n,b}\\) is called weak learner. Problem: Finding the exact minimizer is only feasible (not harder than than the usual minimisation) for very specific loss functions \\(\\hat R_n\\). In the case of squared loss, minimization in step \\(b\\) boils down to minimizing the empirical squared loss with respect to the residuals \\(Y_i- m_n^{(b-1)}(X_i)\\). \\[ L\\left(Y_i, \\hat m_n^{(b-1)}(X_i)+\\eta(X_i)\\right)=L\\left(Y_i-\\hat m_n^{(b-1)}(X_i),\\eta(X_i)\\right) \\] In the classification case with \\(\\{-1,1\\}\\) valued response and an exponential loss \\((L(y_1,y_2)=e^{-y_1y_2})\\), minimization in step \\(b\\) boils down to minimizing the weighted empirical exponential loss with observation \\(i\\) receiving weight \\(e^{-Y_im_n^{(b-1)}(X_i)}\\). \\[ L\\left(Y_i, \\hat m_n^{(b-1)}(X_i)+\\eta(X_i)\\right)=e^{Y_im_n^{(b-1)}(X_i)}L\\left(Y_i,\\eta(X_i)\\right) \\] This algorithm is also known as AdaBoost.M1. Adaboost.M1 was introduced in Freund and Schapire (1997) and was only later (J. Friedman, Hastie, and Tibshirani 2000) identified as Forward Stagewise Additive Modeling with exponential loss. Definition. (Gradient Boosting Machine v1) (J. H. Friedman 2001) Gradient Boosting approximates Forward Stagewise Additive Modeling. The idea is to choose to hyperparameters: the function space \\(\\mathcal G\\) and the learning rate \\(\\eta\\). The algorithm for chosen hyperparameters is. Step 0: Initialize \\[ \\hat m_{n}^{(0)}(x)=\\arg \\min_{m \\in \\mathcal G} \\sum_{i=1}^N L\\left(Y_i, m\\right). \\] Step \\(1,..., B\\): For \\(b=1,\\dots,B\\) do: For each observation \\(i=1,2, \\ldots, n,\\) calculate the gradient: \\[g_{ib}=-\\frac{\\partial L(Y_i,y)}{\\partial y} \\Bigr\\rvert_{y=m_n^{(b-1)}(X_i)}.\\] Improve the estimator by finding: \\[(\\xi_b, \\tilde m_{n,b})= \\arg \\min_{\\xi\\in \\mathbb R_+,m \\in \\mathcal G}\\sum_{i=1}^n(g_{ib}-\\xi m(X_i))^2\\] Note: \\(\\xi_b\\) is only necessary if \\(\\mathcal G\\) is not closed under multiplication by constants. (i) Search for \\(\\tilde m_{n,b}\\) by line search: \\[\\alpha_b= \\arg \\min_{\\alpha} \\hat R_n(m_n^{(b-1)}+\\alpha \\tilde m_{n,b})\\] We call \\(\\hat m_{n,b}(x)= \\alpha_b \\tilde m_{n,b}(x)\\) the weak learner. Improve the estimator by setting: \\[\\hat m^{(b)}_n(x)=\\hat m^{(b-1)}_n(x)+\\eta \\hat m_{n,b}(x).\\] One may tune the hyperparameters, max depth \\(J\\), \\(\\mathcal G=\\{\\text{trees of max depth} \\ J\\}\\) and learning rate \\(\\eta\\), by: Initialize \\(\\hat m_{n}^{(0)}(x)=\\arg \\min_{m \\in \\mathcal G} \\sum_{i=1}^N L\\left(Y_i, m\\right)\\). For \\(b=1,\\dots,B\\) : For \\(i=1,2, \\ldots, n,\\) calculate the gradient: \\[ g_{ib}=-\\frac{\\partial L(Y_i,y)}{\\partial y} \\Bigr\\rvert_{y=m_n^{(b-1)}(X_i)} . \\] Fit a regression tree with squared loss and max depth \\(J\\) to the targets \\(g_{ib}\\). \\(\\rightarrow\\) leaves \\(R_{bj}, j=1,2, \\ldots, J_b\\). For \\(j=1,2, \\ldots, J_b\\) calculate the value for leaf \\(R_{bj}\\): \\[ v_{bj}=\\arg \\min_{ v_{bj} \\in \\mathbb R} \\sum_{i: X_i \\in R_{kj}} L\\left(Y_i, m_n^{(b-1)}(X_i)+ v_{bj}\\right) . \\] \\(\\hat m_{n,b}(x)=\\sum_{j=1}^{J_b} v_{bj} 1\\left(X_i \\in R_{b j}\\right)\\) Update: \\(\\hat m^{(b)}_n(x)=\\hat m^{(b-1)}_n(x)+ \\eta \\hat m_{n,b}(x)\\). The main difference between the tree gradient booster and general gradient boosting is: Least squares problem is not solved directly but greedy step-wise by growing a tree via CART-algorithm. Instead of a line search we minimize the empirical loss in each leaf. More recently Chen and Guestrin (2016) proposed to optimize a regularized objective in the Forward Stagewise Additive Modeling with the following further changes: 1. Replace \\(\\hat R_n\\) by \\(\\hat R_{n,\\gamma,\\lambda}(\\hat m_n^{(b-1)}+m_{n,b}) =\\hat R_n(\\hat m_n^{(b-1)}+m_{n,b}) + J_{\\gamma,\\lambda}(m_{n,b})\\) - \\(J_{\\gamma,\\lambda}(m_{n,b})= \\gamma J_b+ \\frac 1 2 \\lambda ||v_b||_2^2\\) - \\(J_b\\)= number of leaves of tree in iteration \\(b\\) - \\(v_b=\\) leaf values of tree in iteration \\(b\\) 2. Use second order approximation instead of gradient descent (see next slide) - i.e. gradient descent in 2b,c is replaced by a Newton-Raphson type approximation The algorithm is implemented in the \\(\\texttt{xgboost}\\) package. Main hyperparameters: - Penalties: \\(\\gamma\\), \\(\\lambda\\) - learning rate: \\(\\eta\\) - max tree depth Gradient Boosting (Second version) XGBoost Use a second order approximation of \\(\\hat R_{n,\\gamma,\\lambda}\\) \\[\\begin{align} &amp;\\hat R_{n,\\gamma,\\lambda}(\\hat m_n^{(b-1)}(x))+\\hat m_{n,b}(x))\\\\ \\\\&amp;\\approx\\sum_{i=1}^n \\left\\{ L(Y_i,\\hat m_n^{(b-1)}(X_i))+ g_i\\hat m_{n,b}(X_i)+ \\frac 1 2 h_i \\hat m_{n,b}(X_i)^2\\right \\}+ J_{\\gamma,\\lambda}(m_{n,b})\\\\ &amp;= \\sum_{j=1}^{J_b}\\left \\{ \\left[\\sum_{i: X_i \\in R_{jb}}g_i\\right]v_{jb} + \\frac 1 2 \\left[\\sum_{i: X_i \\in R_{jb}}h_i+\\lambda\\right]v_{jb}^2 \\right\\}+\\gamma J_b \\quad \\text{(ignoring constants)} \\end{align}\\] with \\(g_i= \\frac{\\partial L(Y_i,y_i)}{\\partial y_i} \\Bigr\\rvert_{y_i=m_n^{(b-1)}(X_i)}\\) and \\(h_i= \\frac{\\partial^2 L(Y_i,y_i)}{\\partial^2 y_i} \\Bigr\\rvert_{y_i=m_n^{(b-1)}(X_i)}\\). In the last expression, for a fixed tree, the optimal leaf values \\(v_{jb}\\) are given by \\[ v_{jb}=-\\frac{\\sum_{i: X_i \\in R_{jb}} g_i }{\\sum_{i: X_i \\in R_{jb}} h_i+\\lambda} \\] This leads to the objective function \\[ -\\frac 1 2\\sum_{j=1}^{J_b} \\frac {(\\sum_{i: X_i \\in R_{jb}}g_i)^2}{\\sum_{i: X_i \\in R_{jb}}h_i+\\lambda}+\\gamma J_b \\] Hence, in step \\(b\\), when growing a tree the loss reduction of splitting \\(R_{jb}\\) in \\(R_{jb,L},R_{jb,R}\\) is given by \\[ \\frac 1 2 \\left\\{\\frac {(\\sum_{i: X_i \\in R_{jb,L}}g_i)^2}{\\sum_{i: X_i \\in R_{jb,L}}h_i+\\lambda}+\\frac {(\\sum_{i: X_i \\in R_{jb,R}}g_i)^2}{\\sum_{i: X_i \\in R_{jb,R}}h_i+\\lambda}-\\frac {(\\sum_{i: X_i \\in R_{jb}}g_i)^2}{\\sum_{i: X_i \\in R_{jb}}h_i+\\lambda}\\right\\}-\\gamma \\] In step \\(b\\) the \\(j\\)th node is split a dimension and position that maximises the loss reduction, given that the maximising loss reduction is positive and the current node has depth smaller \\(J\\). Otherwise the node is not further split. Gradient boosting machines are known to often provide the strongest predictive performance. \\(\\texttt{xgboost}\\) and \\(\\texttt{lightgbm}\\) are popular implementations. They are quite fast, but not as fast as random forests and also more reliant on optimal parameter tuning. The most relevant parameters are tree depth and learning rate Why are gradient boosting machines so powerful? A contributing factor may be that a small max depth restricts the number of interactions fitted. max dept=\\(1\\) corresponds to an additive model etc…. Gradient boosting machines, in contrast to random forests have also, no problem to fit additive functions. Different additive components can be fitted in different iterations \\(b\\). 9.6.2 Bayesian additive regression trees Another powerful algorithm is BART Definition. (Bayesian additive regression trees (BART) (for least squares)) (Chipman, George, and McCulloch 2010) Let \\(\\hat{m}^1(x)=\\sum_{k=1}^K \\hat{m}_k^1(x)\\), \\(\\hat{m}_1^1(x)=\\hat{m}_2^1(x)=\\cdots=\\hat{m}_K^1(x)=\\frac{1}{n K} \\sum_{i=1}^n Y_i\\). For \\(b=2, \\ldots, B\\) : For \\(k=1,2, \\ldots, K\\) : For \\(i=1, \\ldots, n\\), calculate residuals \\(r_i=y_i-\\sum_{k^{\\prime}&lt;k} \\hat{m}_{k^{\\prime}}^b\\left(X_i\\right)-\\sum_{k^{\\prime}&gt;k} \\hat{m}_{k^{\\prime}}^{b-1}\\left(X_i\\right)\\) Consider a new tree, \\(\\hat{m}_k^b\\) by making ONE of the following changes to \\(\\hat{m}_k^{b-1}\\) (every change has a fixed probability). GROW: split one leaf PRUNE: prune sister leaves CHANGE: change the decision rule of one node SWAP: swap decision rule of two nodes The change is accepted by throwing a coin with the probabilities of the coin given by comparing posterior of old tree and proposed tree. Otherwise, tree is kept: \\(\\hat{m}_k^{b}=\\hat{m}_k^{b-1}\\) Update leaf values for \\(\\hat{m}_k^{b}\\) by sampling from a posterior distribution. Set \\(\\hat{m}^b(x)=\\sum_{k=1}^K \\hat{m}_k^b(x)\\). Compute the mean after \\(L\\) burn-in samples, \\(\\hat{m}(x)=\\frac{1}{B-L} \\sum_{b=L+1}^B \\hat{m}^b(x)\\) Only a heuristic of BART is presented here. Many packages differ in the exact implementation. In general BART can be seen as a mix of random forest, boosting, additive models. BART is very expensive to train. But if one is able to tune the hyperparameters, BART has often shown to provide unmatched accuracy. "],["some-practical-considerations.html", "9.7 Some practical considerations", " 9.7 Some practical considerations In this lecture we will discuss some practical issues that may help for your projects. Categorical data. One point often ignored is that in many applications a significant part of features are categorical. All algorithms we have introduced so far implicitly assume, however, that features are numerical. The most “famous” ways of dealing with categorical features are one-hot encoding or dummy encoding. Definition. (One-hot encoding) Given a feature with \\(X\\) that can take \\(k\\) differen categorical values, one-hot encoding entails transforming the feature into \\(k\\) binary features where \\[ X^{(l)}=1(X=l), \\quad l=1,\\dots,k. \\] Definition. (Dummy encoding) Given a feature with \\(X\\) that can take \\(k\\) differen categorical values, dummy encoding entails transforming the feature into \\(k-1\\) binary features where, given a reference \\(l^\\ast\\), \\[ X^{(l)}= 1(X=l), \\quad l=1,\\dots,l^\\ast-1,l^\\ast+1,\\dots,k. \\] Dummy encoding solves the problem of collinearity when one-hot encoding. Both methods can easily make the dimension of the problem rapidly increase if a feature has many categories e.g. car brand, country, or postcode. Grouping variables with similar effect on the response might reduce variance. This could e.g. be done via expert knowledge. Trees are an automated alternative. There are \\(B_k = \\sum_{j=0}^k \\binom k j\\) ways to partition a categorical feature with \\(k\\) values (Bell number). \\(B_2=2\\), \\(B_3=5\\), \\(B_4=15\\), \\(B_5=52\\), \\(B_6=203\\), \\(B_7=877\\), \\(B_8=4140\\). In a CART algorithm, we would only partition a current node into two child-nodes. In this case there are \\(2^{k - 1}-1\\) possible partitions. This is still not feasible for large \\(k\\). It is well known Wright and König (2019) that if optimal partition is with respect to gini index or squared loss, then the optimal partition can be found by considering only \\(k-1\\) partitions: Order feature values by mean response. The optimal partition is a contiguous partition. - Search for optimal partition treating the categorical features as numerical, i.e., split into a left and right partition. Wright and König (2019) propose to order categorical variables only once when fitting a random forest. This is the default in \\(\\texttt{ranger}\\). \\(\\texttt{lightgbm}\\) and very recently (April 2022) \\(\\texttt{xgboost}\\) offer the option to order variables before every split. Trees with different loss functions. In many insurance applications it is common to assume that \\(Y|X\\) has a certain (known) parameterized distribution. In this case the loss function can correspond to the negative log likelihood or deviance. When fitting a tree, leaf values then correspond to the maximum likelihood parameter estimate for observations conditioned on that leaf. One advantage is that if the distribution is chosen well this can lead to more robust/better results and additionally it provides estimates of the full distribution. The disadvantage is that: It leaves room for misspecification. If one is only interested in the mean response, then estimating the distribution first may lead to a biased estimate of the mean. Computational time might be an issue if the likelihood function is too complicated. Example. (Poisson loss with given exposure) Assume that the response \\(Y\\) given \\(X\\) is Poisson distributed with mean \\(\\lambda(X)E\\). Given iid observations \\((X_i,Y_i,E_i)_{i=1,\\dots,n}\\), the log likelihood in leaf \\(R_1\\) is given by \\[ l(\\lambda)= \\sum_{i: X_i\\in R_1} -\\lambda E_i+ Y_i\\log(\\lambda E_i)-log(Y_i!) \\] The minimzer is given by \\[ \\hat \\lambda = \\frac{\\sum_{i: X_i\\in R_1} Y_i}{\\sum_{i: X_i\\in R_1} ^n E_i} \\] Which has the same minimizer as when minimizing the empirical risk of the Poisson Deviance \\[ \\hat R_n(\\lambda E_i)=\\sum_{i=1}^n 2\\left(Y_i\\log {\\frac {Y_i}{\\lambda E_i }}-Y_i+\\lambda E_i \\right). \\] \\(\\texttt{distRforest}\\) is a recent attempt to add more distributions to random forest. In boosting algorithms we don’t directly work with the loss function but only with its gradient (boosting v1) or with a second order approximation (boosting v2). Assume \\(Y|X\\) follows a distributions from an exponential dispersion family, i.e., the density (for fixed \\(X=x\\)) can be written as \\[ f\\left(Y ; \\theta, E^{-1} \\phi\\right)=\\exp \\left\\{ \\frac{\\theta Y-A(\\theta)}{\\phi/E}+c\\left(Y ; \\phi/E\\right)\\right\\}. \\] \\(\\phi\\) is dispersion parameter. \\(E\\) is exposure. \\(A\\) is twice continuously differentiable, gradient of \\(A\\) one-to-one. We ignore the dispersion parameter \\(\\phi\\) since it does not change point prediction if it does not change over different observations \\(i=1,\\dots,n\\). Then, the negative log likelihood up to additive terms that do not depend on \\(\\theta\\) is \\[ - E\\times(\\theta Y - A(\\theta)) \\] We can translate this to a generalized framework: We could say we wish to estimate the distribution \\(\\tilde g (\\mathbb E[Y])\\), given canonical link function \\(\\tilde g\\). \\(\\Rightarrow \\tilde g(\\mathbb E(Y))=\\theta\\). Meaning we wish to minimize the loss \\(L(Y,\\theta)= - E\\times(\\theta Y - A(\\theta)).\\) Hence, when employing a gradient boosting machine algorithm we can use the gradient: \\[ g_i= \\frac{\\partial L(Y_i,\\theta)}{\\partial \\theta} \\Bigr\\rvert_{\\theta=m_n^{(b-1)}(X_i)}=-E_i\\times \\left(Y- \\frac{\\partial A(\\theta)}{\\partial \\theta} \\Bigr\\rvert_{\\theta=m_n^{(b-1)}(X_i)}\\right), \\] and the second derivative: \\[ h_i= \\frac{\\partial^2 L(Y_i,\\theta)}{\\partial^2 \\theta} \\Bigr\\rvert_{\\theta=m_n^{(b-1)}(X_i)}=E_i\\times \\left( \\frac{\\partial^2 A(\\theta)}{\\partial^2 \\theta} \\Bigr\\rvert_{\\theta=m_n^{(b-1)}(X_i)}\\right). \\] \\(\\texttt{xboost}\\) and \\(\\texttt{lightgbm}\\) come with many pre-configured loss functions. Imbalanced data. When considering individual insurance claims data, one will find that most policies do not have a claim registered. In other words: The number of observed zeros will be much higher than observations of other numbers when looking at claim frequencies or claim amounts. This can be a problem when modelling under a Poisson assumptions and zero inflated distributions have been suggested. It is usually not problematic when modelling under a Bernoulli assumption (logistic regression). It can be a problem if we are instead interested in 0/1 decisions (e.g. in fraud detection). In this case, there have been loss modifications suggested that reward more detecting 1s than detecting 0s. A prominent example is focal loss (Lin et al. 2017). In pricing we are rather interested in the probability and not in 0/1 decisions. Claim amounts are often modeled via a frequency severity approach modelling frequency and severity separately (assuming independence between frequency and severity): \\[ \\pi=\\mathbb{E}\\left(\\frac{L}{E}\\right) = \\mathbb{E}\\left(\\frac{N\\times Y}{E}\\right)= \\mathbb{E}\\left(\\frac{N}{E}\\right) \\times \\mathbb{E}\\left(Y\\right)=\\mathbb{E}(F) \\times \\mathbb{E}(Y). \\] where \\(\\pi\\)= technical price, \\(L\\)= loss, \\(E\\)= exposure, \\(N\\)= frequency and \\(Y\\)= severity. Note that it is important here how to define a claim (is a claim with claim size zero a claim?). If it is is not counted as a claim, then modelling \\(E[Y]\\) is easier. "],["neural-networks.html", "9.8 Neural Networks", " 9.8 Neural Networks In this lecture we will introduce neural networks. Neural networks with multiple layers are also known as deep learners. We will only consider feed forward neural networks. Usually used for tabular (=unstructrued) data. The recent popularity of neural networks stems mostly from modification more suitable for structured data, like Convolutional neural networks for image classification Transformers in natural learning processing As usual, we assume that we are given an iid dataset \\(\\mathcal D_n=\\{X_i,Y_i\\}_{i=1,\\dots,n}\\). Single Layer Feed Forward. We start with a single layer neural network (= 1 hidden layer). The architecture of a single layer neural network corresponds to the function class \\(\\mathcal G\\) such that \\(\\hat m(D_n)(x)=\\hat m_n(x): \\mathcal X \\mapsto \\mathcal Y\\) can be written as \\[\\begin{align} \\hat m_n(x)&amp;=g\\left(\\beta_0 + \\sum_{k=1}^{K} \\beta_kH_k(x)\\right)\\\\ &amp;= g\\left(\\beta_0 + \\sum_{k=1}^{K} \\beta_k\\phi\\left(w_{k0} + \\sum_{j=1}^p w_{kj}x_j\\right)\\right). \\end{align}\\] In the the previous slide the illustration showed \\(p=3\\), \\(K=4\\). To ease notation we will be ignoring the intercepts, here: \\(\\beta_0, w_{k0}\\). In matrix notation, the hidden layer is \\(H(x)=\\phi(x^tw)\\) and the output is \\[ \\hat m_n(x)=g \\left( H(x)^T\\beta \\right)=g\\left(\\phi(x^Tw)^T\\beta\\right). \\] with \\(w \\in \\mathbb R^{p\\times K}\\), \\(\\beta \\in \\mathbb R^{K}\\). The functions \\(\\phi\\) and \\(g\\) are called activation functions and are applied element-wise. The activation functions make the estimator non-linear. Popular activation functions. Sigmoid: \\(\\phi(z)=\\frac{e^z}{1+e^z}\\) maps to \\([0,1]\\) Hyperbolic tangent: \\(\\phi(z)=\\textrm{tanh}(z)= \\frac{e^z-e^{-z}}{e^x+e^{-z}}\\) maps to \\([-1,1]\\) Rectified Linear Unit (ReLU): \\(\\phi (z)=\\max(z,0)\\) “standard” activation function Leaky ReLU: \\(\\phi (z)=\\max(z,0)+ \\alpha \\min(0,z)\\) Backpropagation. Neural networks are fitted via gradient descent with small step sizes. We are hence interested in \\(\\frac{\\partial L(Y_i,m(\\beta,w))}{\\partial w}, \\frac{\\partial L(Y_i,m(\\beta,w))}{\\partial \\beta}.\\) Calculating the gradient via the chain rule is called backpropagation. Note that the loss is calculated via the following composition: \\[ X_i \\stackrel{w}{\\rightarrow } Z_1 \\stackrel{\\phi}{\\rightarrow } H \\stackrel{\\beta}{\\rightarrow }Z_2\\stackrel{g}{\\rightarrow } m \\stackrel{}{\\rightarrow } L(Y_i,m) \\] Hence, by the chain rule we have \\[ \\frac{\\partial L(Y_i, \\partial m)}{\\partial \\beta}=\\frac{\\partial L(Y_i,m)}{m}\\frac{\\partial m}{Z_2} \\frac{\\partial Z_2}{\\partial \\beta}, \\] as well as \\[ \\frac{\\partial L(Y_i,m)}{\\partial w}=\\frac{\\partial L(Y_i,m)}{\\partial m}\\frac{\\partial m}{\\partial Z_2} \\frac{\\partial Z_2}{\\partial H} \\frac{\\partial H}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial w}. \\] In step \\(r\\) and given a learning rate \\(\\gamma\\) the weights are updated as \\[\\begin{align*} \\beta^{(r+1)}= \\beta^{(r)} -\\gamma \\frac 1 n \\sum_{i=1}^n\\frac{\\partial L(Y_i,\\hat m_n^{(r)}(X_i))}{\\partial \\beta^{(r)}}, \\\\ w^{(r+1)}= w^{(r)} -\\gamma \\frac 1 n \\sum_{i=1}^n \\frac{\\partial L(Y_i,\\hat m_n^{(r)}(X_i))}{\\partial w^{(r)}}. \\end{align*}\\] Feed foreward Neural Network. Usually one works with multiple hidden layers and possibly mutlipe outputs. the \\(l\\)th hidden layer arises from the \\(l-1\\)th hidden layer from (ignoring intercepts) \\[ H^{(l)}=\\phi_l(H^{(l-1)}w^{(l)}). \\] The fitting is analogue to the single layer case. To avoid overfitting one can imploy penalty terms, e.g. lasso and/or ridge. In the machine learning community this is also known as weight decay. Especially ridge penalty is popular. Ridge penalty means penalizing every entry of \\(\\beta, w\\) by its square. Stochastic Gradient Descent and Early Stopping. In “deterministic” gradient descent, weights are updated via \\[ \\beta^{(r+1)}= \\beta^{(r)} -\\gamma \\sum_{i=1}^n\\frac{\\partial L(Y_i,\\hat m_n^{(r)}(X_i))}{\\partial \\beta^{(r)}}, \\quad w^{(r+1)}= w^{(r)} -\\gamma \\sum_{i=1}^n \\frac{\\partial L(Y_i,\\hat m_n^{(r)}(X_i))}{\\partial w^{(r)}}. \\] For large data sets this may not be computationally feasible/too expensive and an alternative is Stochastic Gradient Descent (SGD). Definition. Stochastic Gradient Descent (SGD) Input: \\(\\texttt{batch-size}\\), early stopping criteria (most often whether improvement is better than some treshold on the validation set) For \\(j=1,\\dots\\)STOP (=epochs) For \\(k=1,\\dots, n/\\texttt{batch-size}\\) Sample without replacement \\(\\texttt{batch-size}\\) from \\(\\{1,\\dots,n\\}\\setminus \\cup_{l=1}^{k-1}B_j^j\\), resulting in the \\(k\\)th batch, \\(B_k^j\\). Perform gradient descent with using only observations \\(i \\in B_k^j\\) Dropout learning. An alternative/additional way to perform regularization is dropout, introduced in Srivastava et al. (2014). It is an analogue to random forest, but the obvious idea of averaging the outputs of many separately trained nets is prohibitively expensive. Instead: In every mini-batch, for every \\(i\\), randomly remove a fraction \\(p\\) of the units in a layer when calculating the gradient. The surviving units get an additional weight of \\(1/(1 − p)\\). I.e. in epoch \\(j\\) and mini-batch \\(k\\), for individual \\(i\\) and current unit \\(H(j,k)\\), \\[ \\widetilde H(j,k,i)=\\begin{cases}0 &amp; \\text{with probability}\\ p \\\\ H(k,j)/(1 − p) &amp;\\text{else}\\end{cases} \\] Note: \\(\\mathbb E[\\widetilde H(j,k,i)]=H(k,j)\\). The heuristic is that many thinned networks are trained in parallel. More Hyperparameters. An alterantive to a ridge penalty is max-norm regularization, with parameter \\(c\\). I.e. in every learning step enforce \\(||\\beta||^2_2, ||w||^2_2\\leq c\\), where \\(||\\cdot||^2_2\\) denotes the squared sum of all entries. Instead of standard SGD, alternative updates can be performed, e.g., momentum: \\[ w^{(r+1)}=w^{(r)} +\\alpha \\Delta w^{(r)} - \\gamma \\sum_{i=1}^n \\frac{\\partial L(Y_i,\\hat m_n^{(r)}(X_i))}{\\partial w^{(r)}} \\] \\[ \\Delta w^{(r)}= w^{(r)}-w^{(r-1)} \\] or Adam. Further comments. The size of the learning rate can depend on the current epoch, also known as learning rate decay. Initial weights, i.e, starting values for \\(w, \\beta\\), do effect the outcome. Covariates are often standardized via minmax scaling. \\[ \\tilde x = \\frac{x- \\min(x)}{\\max(x)-\\min(x)} \\in [0,1] \\] In R, implementation of neural network is provided via keras/tensorflow https://tensorflow.rstudio.com/. Given the many options to configure, initialize and stop training a neural network, training a neural network is often seen as “art”. For tabular (=unstructured) data tree-based algorithms usually outpeform neural networks with respect to test error. Neural networks are however unchallenged for structured data (especially when also extending the feed forward network accounting for the structure) "],["local-explanations.html", "9.9 Local explanations", " 9.9 Local explanations In this lecture we shift the perspective and we wish to understand what a predictor \\(\\hat m_n(x)\\) is actually doing. 9.9.1 Interpretability Regression: \\(Y_i=m(X_i)+\\varepsilon_i\\) \\[ X\\qquad \\rightarrow \\qquad \\widehat m (X) \\qquad \\rightarrow \\qquad m (X)\\quad \\text{or}\\quad Y\\ \\vert\\ X \\] Interpretability is understanding the relationship between \\(X\\) and \\(\\widehat m (X)\\). This is different to understanding the relationship between \\(X\\) and \\(Y\\). Quality Linear Models Machine Learning interpretable yes no interactions manually yes yes interactions no yes variable selection/sparsity yes yes non-linearity no yes Current machine learning algorithms are often highly flexible and can deal with interaction, non-linearity, sparsity and variable selection; but they are not interpretable. Local explanations. Fix a value \\(x_0 \\in \\mathbb R^p\\). A local approximation at \\(x_0\\) of the function \\(\\hat m_n\\) is given by \\[ \\hat m\\left(x_0\\right)=\\phi_{0}+\\sum_{k=1}^{p} \\phi_k(x_0), \\] where \\(\\phi_0,\\phi_1(x_0),\\dots,\\phi_p(x_0)\\) are constants. Note: the right hand-side is not identified. Local explanations add constraints such that \\(\\phi_k(x_0)\\) is uniquely identified and best reflects the local contribution of feature \\(k\\) to \\(\\hat m_n\\left(x_0\\right)\\). Definition. (Additive Value functions) A value function \\(v\\) assigns a real value \\(v(S)\\) to each subset \\(S \\subseteq \\{1,\\dots p\\}\\). Definition. (Shapley Axioms) Given a vaue function \\(v_{x_0}\\) with \\(v_{x_0}(\\{1,\\dots,p\\})=m(x_0)\\). The four Shapley axioms are Efficiency \\(m\\left(x_0\\right)=\\phi_{0}+\\sum_{k=1}^{p} \\phi_k(x_0)\\), \\(\\phi_{0}=v(\\emptyset)\\). Symmetry: Fix any \\(k,l \\in \\{1,\\dots,p\\}, k\\neq l\\). If \\(v_{x_0}(S\\cup k)=v_{x_0}(S\\cup l)\\), for all \\(S \\subseteq \\{1,\\dots p\\}\\setminus \\{k,l\\}\\), then \\(\\phi_k(x_0)=\\phi_l(x_0).\\) Dummy For \\(k=1,\\dots,p\\): If \\(v_{x_0}(S\\cup k)=v_{x_0}(S)\\), for all \\(S \\subseteq \\{1,\\dots p\\}\\setminus \\{k\\}\\), then \\(\\phi_k=0.\\) Linearity For \\(k=1,\\dots,p\\): If \\(m(x_0)=m^1(x_0)+m^2(x_0)\\), then \\(\\phi_k(x_0)=\\phi^1_k(x_0)+\\phi^2_k(x_0)\\), where \\(\\phi^l\\) is the explanation corresponding to the function \\(m^l\\). Theorem. (Shapley Axioms) Given a value function \\(v_{x_0}\\), there exist unique constants \\(\\phi_0,\\phi_1(x_0),\\dots,\\phi_p(x_0)\\) such that the four Shapley axioms are satisfied. They are given by \\[\\begin{align} \\phi_{k}(x_0)&amp;=\\frac{1}{p !} \\sum_{\\pi \\in \\Pi_p} \\Delta_{v_{x_0}}\\left(k, \\{\\pi(1),\\dots,\\pi(k-1)\\}\\right)\\\\ &amp;=\\frac 1 {p!}\\sum_{S: S \\subseteq \\{1,\\dots,p\\} \\setminus\\{k\\}} {|S| !(p -|S|-1) !}\\Delta_{v_{x_0}}(k, S), \\end{align}\\] where \\(\\Delta_{v_{x_0}}(k, S)=v_{x_0}(S \\cup k)-v_{x_0}(S)\\) and \\(\\Pi_p\\) is the set of permutations of \\(\\{1,\\dots,p\\}\\). Proof. For \\(R\\subseteq\\{1,\\dots,p\\}\\), and a value fucntion \\(v\\), define the value function \\[ v_R(S)=\\begin{cases}m(x_0) &amp; \\text{if}\\ R\\subseteq S \\\\ 0 &amp; \\text{else} \\end{cases} \\] By the symmetry axiom, if \\(j,k\\in R\\), then \\(\\phi_j(v_R)=\\phi_k(v_R).\\) By the dummy axiom, if \\(k\\notin R\\), then \\(\\phi_k(v_R)=0\\). Hence, by the efficiency axiom \\((v_R(\\{1,\\dots,p\\})=m(x_0))\\), for \\(k\\in R\\), \\[ \\phi_k(v_R)= \\frac{m(x_0)}{|R|}. \\] We will (later) show that \\[ \\tag{1} v(U)=\\sum_{T: T\\subseteq\\{1,\\dots,p\\}}\\sum_{S: S\\subseteq T} (-1)^{|T-S|}\\frac{v(S)}{m(x_0)}v_T(U). \\] Then, by the linearity axiom \\[\\begin{align} \\phi_k(v)&amp;=\\sum_{T: T\\subseteq\\{1,\\dots,p\\}}\\sum_{S: S\\subseteq T} (-1)^{|T-S|} \\frac{v(S)}{m(x_0)} \\phi_k(v_T)\\\\ &amp;=\\sum_{T: T\\subseteq\\{1,\\dots,p\\}, k \\in T}\\sum_{S: S\\subseteq T} (-1)^{|T-S|} \\frac{v(S)}{|T|}\\\\ &amp;=\\sum_{S: S\\subseteq\\{1,\\dots,p\\}}\\sum_{T: S \\cup \\{k\\} \\subseteq T} (-1)^{|T-S|} \\frac{v(S)}{|T|} . \\end{align}\\] We can write \\[ \\phi_k(v)=\\sum_{S: S\\subseteq\\{1,\\dots,p\\}}\\gamma_i(S) v(S) , \\quad \\gamma_k(S)=\\sum_{T: S\\cup \\{k\\}\\subseteq T} (-1)^{|T-S|} \\frac{1}{|T|} \\] Observe that for \\(S_1\\neq S_2, S_2=S_1\\cup\\{k\\}\\), \\(\\gamma_k(S_1)=-\\gamma_k(S_2)\\). Hence, \\[\\begin{align} \\phi_k(v)&amp;=\\sum_{S: S\\subseteq\\{1,\\dots,p\\}}\\gamma_k(S) v(S) \\\\ &amp;= \\sum_{S: S\\subseteq\\{1,\\dots,p\\}, k\\in S}\\gamma_k(S) v(S) +\\sum_{S: S\\subseteq\\{1,\\dots,p\\}, k\\notin S}\\gamma_k(S) v(S) \\\\ &amp;= \\sum_{S: S\\subseteq\\{1,\\dots,p\\}, k\\notin S}-\\gamma_k(S) v(S\\cup\\{k\\}) +\\sum_{S: S\\subseteq\\{1,\\dots,p\\}, k\\notin S}\\gamma_k(S) v(S) \\\\ &amp;= \\sum_{S: S\\subseteq\\{1,\\dots,p\\}, k\\notin S}-\\gamma_k(S) \\left [ v(S\\cup\\{k\\}) - v(S) \\right] \\end{align}\\] The proof follows by showing that \\[ \\tag{2} \\gamma_k(S)=-\\frac{|S|!(p-|S|-1)!}{p!}, \\qquad k \\notin S, \\] as well as equation (1). We first show \\((2)\\). We will use that \\[ \\frac 1 l = \\int_0^1 x^{l-1}\\mathrm dx. \\] Such that for \\(k \\notin S\\), we have \\[\\begin{align} \\gamma_k(S)&amp;= \\sum_{l=0}^{p-|S|-1} (-1)^{l+1}\\binom{p-|S|-1}{l}\\frac 1 {|S|+1+l} \\\\ &amp;= -\\int_0^1x^{|S|}\\sum_{l=0}^{p-|S|-1} (-1)^{l}\\binom{p-|S|-1}{l}x^{l}\\mathrm dx. \\end{align}\\] By the binomial theorem, this simplifies to \\[\\begin{align} \\gamma_k(S)&amp;= - \\int_0^1x^{|S|}(1-x)^{p-|S|-1}\\mathrm dx \\\\ &amp;= -\\frac{|S|!(p-|S|-1)!}{p!}. \\end{align}\\] The last equality follows from \\[ \\int_0^1 x^a (1-x)^b\\mathrm dx= \\frac{a!b!}{(a+b+1)!}, \\qquad a,b\\geq0 \\] which can be proven by induction over \\(b\\). It remains to show (1). We have \\[\\begin{align} &amp; \\quad \\sum_{T: T\\subseteq\\{1,\\dots,p\\}}\\sum_{S: S\\subseteq T} (-1)^{|T-S|}\\frac{v(S)}{m(x_0)}v_T(U) \\\\ &amp;=\\sum_{T: T\\subseteq U}\\sum_{S: S\\subseteq T} (-1)^{|T-S|}v(S)\\\\ &amp;= \\sum_{S: S\\subseteq U}\\left[\\sum_{D: D \\subseteq \\{U\\setminus S\\}} (-1)^{|D|} \\right]v(S)\\\\ &amp;= v(U), \\end{align}\\] where the last equation follows because the expression in the square bracket is zero for \\(U\\neq S\\). That is because a non-empty set has an equal number of subsets with an odd number of elements as subsets with an even number of elements. Note that we will slightly abuse notation by ignoring ordering in the input of the functions below. Lundberg and Lee (2017) proposed to use Shapley values with value function \\[ v(S)=\\mathbb E[ \\hat m_n(X_S,X_{-S})| X_S=x_S] = \\int \\hat m_n(x_1,\\dots,x_p) p_X(x_S,X_{-S}) \\mathrm dx_{-S} \\] for model explanation. And called it SHAP (SHapley Additive exPlanations). To simplify calculations, Lundberg and Lee (2017) proposed to calculate \\[ v(S)= \\mathbb E[\\hat m_n(x_S,X_{-S})]= \\int \\hat m_n(x_1,\\dots,x_p) p_{X_{-S}}(X_{-S}) \\mathrm dx_{-S} \\] Janzing, Minorics, and Blöbaum (2020) argue that the latter value function should actually be the preffered value function. Chen et al. (2020) coin it interventional SHAP (and the original SHAP above observational SHAP). In the next, we give an example why interventional SHAP values might be preferred compared to observational SHAP values. Example. (Observational SHAP vs Interventional SHAP) Assume \\[ \\hat m_n(x_1,x_2)=x_1 \\] Let \\(X_1\\), \\(X_2\\) be binary with \\[ p(x_1,x_2)= \\begin{cases} \\frac 1 2 &amp; \\text{if} \\ x_1=x_2 \\\\ 0 &amp; \\text{else} \\end{cases} \\] For observational SHAP, we have \\(v_x (\\emptyset)=\\mathbb E[\\hat m_n(X_1,X_2) ]=0.5\\) \\(v_x (\\{1\\})=\\mathbb E[\\hat m_n(X_1,X_2) |X_1=x_1 ]=x_1\\) \\(v_x (\\{2\\})=\\mathbb E[\\hat m_n(X_1,X_2) |X_2=x_2 ]=x_2\\) \\(v_x (\\{1,2\\})=\\hat m_n(x_1,x_2) =x_1\\) Hence, \\(\\Delta(2,\\emptyset)= v_x (\\{2\\})-v_x (\\emptyset)=x_1-0.5\\) \\(\\Delta(2,\\{1\\})= v_x (\\{1,2\\})-v_x (\\{1\\})=x_1-x_1=0\\) Such that \\(\\phi_2= \\frac 1 2 (x_1-0.5)\\neq 0\\) For interventional SHAP, we have \\(v_x (\\emptyset)=\\mathbb E[\\hat m_n(X_1,X_2) ]=0.5\\) \\(v_x (\\{1\\})=\\mathbb E[\\hat m_n(x_1,X_2) ]=x_1\\) \\(v_x (\\{2\\})=\\mathbb E[\\hat m_n(X_1,x_2) ]=0.5\\) \\(v_x (\\{1,2\\})=[\\hat m_n(x_1,x_2) =x_1\\) Hence, \\(\\Delta(2,\\emptyset)= v_x (\\{2\\})-v_x (\\emptyset)=0\\) \\(\\Delta(2,\\{1\\})= v_x (\\{1,2\\})-v_x (\\{1\\})=0\\) Such that \\(\\phi_2= 0\\) We conclude, that if one uses observational SHAP for model explanation, then the contribution of a feature is a combination of its own contribution and the contribution of features it is correlated with. Discussion Point. If \\(\\mathbb P(X_1=X_2)\\), then \\(\\tilde m_n(x)=x_2\\) is as good in estimating the response as \\(\\hat m_n(x)=x_2\\). In particular the Bayes rule is not unique. With that regard, it could make sense to attribute \\(x_1\\) and \\(x_2\\) the same importance. An argument against assigning \\(x_2\\) any importance is that if \\(\\phi\\) is to explain the behaviour of \\(\\hat m_n(x)=x_2\\), then it is not directly affected by \\(x_2\\). We will later see that observational SHAP has one further issue: One may need to extrapolate in order to calculate it. How do we calculate/estimate Shapley values? Rember, Shapley values are: \\[\\begin{align} \\phi_{k}(x_0)&amp;=\\frac{1}{p !} \\sum_{\\pi \\in \\Pi_p} \\Delta_{v_{x_0}}\\left(k, \\{\\pi(1),\\dots,\\pi(k-1)\\}\\right)\\\\ &amp;=\\frac 1 {p!}\\sum_{S: S \\subseteq \\{1,\\dots,p\\} \\setminus\\{k\\}} {|S| !(p -|S|-1) !}\\Delta_{v_{x_0}}(k, S), \\end{align}\\] To calculate Shapley values exact, one would need to evaluate \\(p!\\) or (second equation) \\(2^p\\) summands. For large \\(p\\) that may be infeasible. Definition. (Estimate Shapley values via permutation sampling) The Shapley value \\(\\phi_k\\) can be approximated by the following algorithm: For \\(r=1,\\dots\\), Sample a permutation \\(\\pi\\) of \\(\\{1,\\dots,p\\}\\) Approximate \\(\\Delta_{v_{x_0}}\\left(k, \\{\\pi(1),\\dots,\\pi(k-1)\\}\\right)\\) (call the result \\(\\Delta^{(r)}\\)) I.e approximate \\(v(\\{\\pi(1),\\dots,\\pi(k-1)\\}\\cup \\{k\\})\\) and \\(v(\\{\\pi(1),\\dots,\\pi(k-1)\\})\\) \\(\\Delta^{(r)}\\) is the difference If \\(v(S)=\\mathbb E[ \\hat m_n(X_S,X_{-S})| X_S=x_S]\\), (observational SHAP) Not clear what to do. One would need to estimate the distribution of \\(X\\) first. Which is a high dimensional problem… If \\(v(S)=\\mathbb E[ \\hat m_n(x_S,X_{-S})]\\) (interventional SHAP) Draw \\(m\\) individuals from \\(\\{1,\\dots,n\\}\\), say \\(I\\subseteq n\\), \\(v(S)\\approx \\frac 1 I \\sum_{i \\in I} \\hat m_n(x_S,x_{i,-S})\\) \\(\\hat \\phi_k\\) is the average of all \\(\\Delta^{(r)}\\) Problem: many samples needed to get accurate approximation. Hence, slow. Charnes et al. (1988) discussed that Shapley values can be re-written as the solution of the following constraint minimisation problem: \\[ (\\phi(x_0))_{0,\\dots,p} = \\arg\\min_{(\\phi_k)_{k=1,\\dots,p}} \\mu(S) \\sum_{S: S\\subseteq\\{1,\\dots,p\\}} \\left(v_{x_0}(S)- \\left [\\phi_0 + \\sum_{k \\in S} \\phi_k\\right]\\right)^2, \\] \\[ \\mu(S) =\\frac{p-1} {\\binom{p}{|S|} |S| (p-|S|)}. \\] Note that \\(\\mu(\\emptyset)=\\mu(\\{1,\\dots,p\\})=\\infty\\) and so the minimisation is not well defined. The infinite weight practically enforces \\(\\phi_0=v_{x_0}(\\emptyset)\\), \\(\\sum_{k=0}^p \\phi_k=v_{x_0}(\\{1,\\dots,p\\})\\). One can hence, exclude the two sets \\((\\emptyset, \\{1,\\dots,p\\})\\) from the minimisation and add the two constraints, \\(\\phi_0=v_{x_0}(\\emptyset)\\), \\(\\sum_{k=0}^p \\phi_k =v_{x_0}(\\{1,\\dots,p\\})\\), to the minimisation. This is a quadratic programing problem (good), but estimating \\(v_{x_0}(S)\\) for all \\(2^p\\) subsets might economically not be feasible. Introduced in Lundberg and Lee (2017), Covert and Lee (2021) give a detailed explanation on how Kernel SHAP is implemented. Instead of estimating \\(v_{x_0}(S)\\) for all \\(2^p\\) subsets they only evaluate \\(n\\) subsets. The \\(m\\) subsets are drawn from the set of all subsets of \\(\\{1,\\dots,p\\}\\) minus the empty set and the full set, where the probability of drawing set \\(S\\) is proportional to \\(\\mu(S)\\) Hence the final minimisation reads \\[\\begin{align} \\min_{(\\phi_k)_{k=1,\\dots,p}} \\frac 1 m \\sum_{i=1}^m \\left(v_{x_0}(S_i)- \\left [v_{x_0}(\\emptyset)+ \\sum_{k \\in S_i} \\phi_k\\right]\\right)^2, \\\\ \\text{subject to} \\sum_{k=1}^p \\phi_k =v_{x_0}(\\{1,\\dots,p\\})-v_{x_0}(\\emptyset), \\end{align}\\] where \\(S_i\\) is the subset from draw \\(i\\). The value function used in Kernel SHAP is interventional SHAP, i.e., \\(v_{x_0}(S)=\\mathbb E[\\hat m_n(x_S,X_{-S})]\\). Lundberg and Lee (2017) propose a method to estimate interventional SHAP that is fast and exact (exact in the sense that it calculates the exact plug-in estimates). It is specific for tree based algoirthms. The algorithm is called TreeSHAP and it makes use of the binary tree structures that leads to a partitioning of the feature space. For calculating \\(\\mathbb E[\\hat m_n(x_S,X_{-S})]\\), TreeSHAP recursively follows the decision path for \\(x_0\\) if the split feature is in \\(S\\), and takes the weighted average of both branches if the split feature is not in \\(S\\). For efficient caulculation, TreeSHAP does not go down the tree for every \\(S\\) separately but only once and while doing so keeps track of all possible \\(S\\). "],["causality.html", "9.10 Causality", " 9.10 Causality This lecture covers a selected topic on causaliy leading to the so called adjustment formula. We mostly follow Peters, Janzing, and Schölkopf (2017). Definition. (Graph Terminology) We are given a random variables \\(X = (X_1,...,X_p)\\) with index set \\(V := \\{1,...,p\\}\\). A graph \\(G = (V,\\mathcal E)\\) consists of nodes or vertices \\(V\\) and edges \\(\\mathcal E \\subseteq V^2\\) with \\((v, v) \\notin \\mathcal E\\) for any \\(v \\in V\\). A node \\(k\\) is called a parent of \\(j\\) if \\((k,j)\\in \\mathcal E\\) and \\((j,k)\\notin \\mathcal E\\) The set of parents of \\(k\\) is denoted by \\(pa_G(k)\\) a child if \\((j,k) \\in \\mathcal E\\) and \\((k, j) \\notin\\mathcal E\\). The set of childrens of \\(k\\) is denoted by \\(ch_G(k)\\) Two nodes \\(k\\) and \\(j\\) are adjacent if either \\((k,j)\\in \\mathcal E\\) or \\((j,k) \\in \\mathcal E\\). We say that there is an undirected edge between two adjacent nodes \\(k\\) and \\(j\\) if \\((k, j) \\in \\mathcal E\\) and \\(( j, k) \\in \\mathcal E\\). An edge between two adjacent nodes \\((k,j)\\) is directed if \\((k,j)\\in \\mathcal E\\) and \\((j,k) \\notin \\mathcal E\\) or vice versa. We write \\(k \\rightarrow j\\) for \\((k, j) \\in \\mathcal E\\), \\((j, k) \\notin \\mathcal E\\) and \\(j \\rightarrow k\\) for \\((j, k) \\in \\mathcal E\\), \\((k, j) \\notin \\mathcal E\\) \\(G\\) is called directed if all its edges are directed. A path in \\(G\\) is a sequence of (at least two) distinct vertices \\(k_1,\\dots ,k_m\\), such that there is an edge between \\(k_l\\) and \\(k_{l+1}\\) for all \\(l = 1,\\dots ,m − 1\\). If \\(k_{l-1} \\rightarrow k_l\\) and \\(k_{l+1} \\rightarrow k_l\\) (\\(k_{l-1} \\rightarrow k_l\\leftarrow k_{l+1}\\)), \\(k_l\\) is called a collider relative to this path. If \\(k_l \\rightarrow k_{l+1}\\) for all \\(l\\), we speak of a directed path from \\(k_1\\) to \\(k_m\\) In this case, We call \\(k_1\\) an ancestor of \\(k_m\\) and \\(k_m\\) a descendant of \\(k_1\\). \\(G\\) is called a directed acyclic graph (DAG) if all edges are directed and there is no pair \\((j,k)\\) with directed paths from \\(j\\) to \\(k\\) and from \\(k\\) to \\(j\\). Definition. (Pearl’s d-separation) In a DAG \\(G\\), a path between nodes \\(k_1\\) and \\(k_m\\) is blocked by a set \\(S\\) (with neither \\(k_1\\) nor \\(k_m\\) in \\(S\\) ) if there is a node \\(k_l\\) fulfilling one of the two points: \\(k_l \\in S\\) and \\[\\begin{align} k_{l-1} &amp; \\rightarrow k_l \\rightarrow k_{l+1} \\\\ \\text { or } \\ k_{l-1} &amp; \\leftarrow k_l \\leftarrow k_{l+1} \\\\ \\text { or }\\ k_{l-1} &amp; \\leftarrow k_l \\rightarrow k_{l+1} \\end{align}\\] neither \\(k_l\\) nor any of its descendants is in \\(S\\), and \\[ k_{l-1} \\rightarrow k_l \\leftarrow k_{l+1} \\] In a DAG \\(G\\), we say that two disjoint subsets of vertices \\(A\\) and \\(B\\) are \\(d\\)-separated by a third (also disjoint) subset \\(S\\) if every path between nodes in \\(A\\) and \\(B\\) is blocked by \\(S\\). - We then write \\[ A \\perp\\!\\!\\!\\!\\perp_G B \\mid S \\] Definition. (Structural causal models) A structural causal model (SCM) \\(\\mathfrak{C}:=\\left(S, P_N\\right)\\) consists of a collection \\(S\\) of \\(p\\) (structural) assignments \\[ X_j:=f_j\\left(pa(j), N_j\\right), \\quad j=1, \\ldots, p, \\] and a distribution \\(P_N=\\bigtimes_{j=1}^p P_{N_j}\\) of jontly independent noise variables \\(N_1\\dots,N_p\\). Note: An SCM \\(\\mathfrak{C}\\) defines a unique graph \\(G\\) and a unique distribution, \\(P^{\\mathfrak{C}}_X\\), over the variables \\(X_1,...,X_p\\). The reverse is not true. Example. (Structural causal models with acyclic graph structure) \\[\\begin{align*} &amp; X_1:=f_1\\left(X_3, N_1\\right) \\\\ &amp; X_2:=f_2\\left(X_1, N_2\\right) \\\\ &amp; X_3:=f_3\\left(N_3\\right) \\\\ &amp; X_4:=f_4\\left(X_2, X_3, N_4\\right) \\end{align*}\\] \\(N_1, \\ldots, N_4\\) jointly independent Definition. (Interventional distribution/ do-operator) Consider an SCM \\(\\mathfrak C := (S,P_N)\\) and its entailed distribution \\(P^{\\mathfrak C}_X\\). We deifine a new SCM, \\(\\tilde {\\mathfrak C}\\), by replacing the assignment for \\(X_k\\) by \\[ X_k = \\tilde f(\\tilde {pa}(k), \\tilde N_k). \\] The resulting entailed distribution of the new SCM is called interventional distribution. We write short: \\[ do(X_k = \\tilde f(\\tilde {pa}(k), \\tilde N_k)), \\quad P_X^{\\tilde {\\mathfrak C}}=P^{\\mathfrak C, do(X_k = \\tilde f(\\tilde {pa}(k), \\tilde N_k))}_X \\] An intervention can also simply assign a fixed value \\(a\\) (i.e., \\(\\tilde N_k=a\\) (deterministic) and the set of parents is empty). This is called an atomic or deterministic intervention and can be denoted by \\(do(X_k = a)\\). Definition. (Total causal effect) Given an SCM \\(\\mathfrak{C}\\), we say there is a total causal effect from \\(X\\) to \\(Y\\) if \\[ X \\perp\\!\\!\\!\\!\\perp Y \\quad \\text { in } P_{{X}}^{\\mathfrak{C} ; d o\\left(X=\\tilde{N}_X\\right)} \\] for some random variable \\(\\tilde{N}_X\\). Note: A directed path from \\(X\\) to \\(Y\\) is a necessary but not sufficient condition for a total causal effect (effects can cancel out). In contrast, a directed path from \\(X\\) to \\(Y\\) or \\(Y\\) to \\(X\\) is NOT necessary for \\(X\\) and \\(Y\\) to be correlated. Definition. (Markov property) Given a DAG \\(G\\) and an entailed joint absolutely continuous distribution \\(P_{{X}}\\), this distribution is said to satisfy the global Markov property with respect to the DAG \\(G\\) if \\[ {A} {\\perp\\!\\!\\!\\!\\perp }_{\\mathcal{G}} {B}|{C} \\Rightarrow {A} {\\perp\\!\\!\\!\\!\\perp } {B}| {C} \\] for all disjoint sets of nodes \\({A}, {B}, {C}\\). the local Markov property with respect to the DAG \\({G}\\) if each variable is independent of its non-descendants given its parents, and the Markov factorization property with respect to the DAG \\({G}\\) if \\[ p({x})=p\\left(x_1, \\ldots, x_d\\right)=\\prod_{j=1}^d p\\left(x_j \\mid {pa}(j)\\right). \\] Theorem. (Equivalence of Markov properties, see e.g. Lauritzen (1996)) If \\(P_X\\) is absolutely continuous, then all three Markov properties above are equivalent. Remark (SCM induced Graph is Markov): Note that every SCM induced Graph satisfies the markovian properties. Proposition. (Adjustment formula) Consider an SCM over variables \\({V}\\) with \\(X, Y \\in {X}\\) and \\(Y \\notin {pa(X)}\\). \\(Z\\) is called valid adjustment set if it fulfills one of the three following conditions “parent adjustment”: \\[ {Z}=pa(X) \\] “backdoor criterion”: \\({Z} \\subseteq {V} \\backslash\\{X, Y\\}\\) with \\({Z}\\) contains no descendant of \\(X\\) AND \\({Z}\\) blocks all paths from \\(X\\) to \\(Y\\) entering \\(X\\) through the backdoor \\((X \\leftarrow \\ldots)\\). “toward necessity”: \\({Z} \\subseteq {V} \\backslash\\{X, Y\\}\\) with \\({Z}\\) contains no descendant of any node on a directed path from \\(X\\) to \\(Y\\) (except for descendants of \\(X\\) that are not on a directed path from \\(X\\) to \\(Y\\) ) AND \\({Z}\\) blocks all non-directed paths from \\(X\\) to \\(Y\\) If \\(Z\\) is a valid adjustment set, then \\[ p^{\\mathfrak{C}, d o(X=x)}(y)=\\int p^{\\mathfrak{C}}(y \\mid x, {z}) p^{\\mathfrak{C}}({z}) \\mathrm dz. \\] Proof. We only proof “parent adjustment” and “backdoor criterion”. “towards necessity” can be looked up in Shpitser, VanderWeele, and Robins (2010) We call the interventional density \\(\\tilde p\\) and the original density \\(p\\). For the parent adjustment, \\(Z=pa(X)\\), note that \\[\\begin{align} \\tilde p (y|z)&amp;= \\tilde p (y|x,z)= p (y|x,z) \\\\ \\tilde p(z)&amp;= p(z) \\end{align}\\] Hence, \\[ p(y|do(X=x))= \\tilde p(y)= \\int \\tilde p(y|z) \\tilde p(z) \\mathrm dz = \\int p(y|x,z) p(z) \\mathrm dz \\] - For the backdoor adjustment, let \\(Z\\) fulfill (i) and (ii) and let \\(S=pa(X)\\). We have \\[\\begin{align} p(y|do(X=x))&amp;\\stackrel{\\text{parent adjustment}}{=} \\int p(y|x,s) p(s) \\mathrm ds\\\\ &amp;=\\int p(s)\\int p(y,z|x,s) \\mathrm ds \\mathrm dz \\\\ &amp;\\stackrel{\\text{Bayes formula}}{=}\\int p(s)\\int p(y|x,s,z) p(z|x,s) \\mathrm ds \\mathrm dz \\\\ &amp;\\stackrel{\\text{(ii):} Y {\\perp\\!\\!\\!\\!\\perp } {S}| {X,Z}}{=}\\int p(s)\\int p(y|x,z) p(z|x,s) \\mathrm ds \\mathrm dz \\\\ &amp;\\stackrel{\\text{(i):} X {\\perp\\!\\!\\!\\!\\perp } {Z}| {S}}{=}\\int p(s)\\int p(y|x,z) p(z|s) \\mathrm ds \\mathrm dz \\\\ &amp;\\stackrel{p(z)= \\int p(s)p(z|s) \\mathrm ds}{=}\\int p(y|x,z) \\mathrm dz \\end{align}\\] Intuition behind backdoor criterion: Backdoor paths carry spurious associations from \\(X\\) to \\(Y\\). Blocking all backdoor paths ensures that measured assoziation is causal. We don’t include descendants of \\(𝑋\\) that are also ancestors of \\(𝑌\\) because this would block a causal path. We don’t include descendants of \\(𝑋\\) that are also descendants of \\(𝑌\\) because this would introduce collider bias. Questions. Do we need to observe \\(Z_1\\) to be able to calculate \\(P(Y|do(X=x))\\)? What are all valid adjustment sets for calculating \\(P(Y|do(X=x))\\)? Solution. All valid adjustment sets are \\[ \\{Z_1,Z_3\\},\\{Z_2,Z_3\\},\\{Z_1,Z_2,Z_3\\}. \\] In particular, \\(Z_1\\) is not needed need to calculate \\(P(Y\\vert do(X=x))\\). Measuring Total causal effect. How can we calculate \\(\\mathbb E[Y\\vert do(X =x)]-\\mathbb E[Y]\\), i.e. the total causal effect of \\(X=x\\) on \\(Y\\). If we have a valid adjustment set \\(Z\\), then \\[\\begin{align} E[Y\\vert do(X =x)]&amp;= \\int y p^{d o(X=x)}(y) \\ dy \\\\ &amp;= \\int \\int y p(y\\vert x,z)p(z) \\ dy \\ dz \\\\ &amp;= \\int p(z) \\int y p(y\\vert x,z)\\ dy \\ dz \\\\ &amp;= \\int p(z) \\mathbb E[ Y\\vert X=x,Z=z]\\ dz . \\end{align}\\] Which can be estimated from iid observations of \\((X,Z,Y)\\). Counterfactual fairness. Assume \\(U\\) is a set of protected features. For example \\(U=\\{\\text{gender, ethnicity}\\}\\). Let \\(U \\cup V=\\{1,\\dots, p\\}, U\\cap V=\\emptyset\\). Can we debias an algorithm predicting \\(\\mathbb E[Y\\vert X=x]\\) such that it does not use information contained in \\(X_U\\); neither directly nor indirectly. Kusner et al. (2017) introduced the notion of counterfactual fairness. We here present a sufficient condition: Given a structural causal model of \\((X,Y)\\), an estimator is counterfactual fair if it is a function of the non-descendents of \\(X_U\\). A counterfactual estimator is in particular given by \\(\\hat m_n^{debiased}(x_v)=E[\\hat m_n(X)\\vert do(X_v =x_v)]\\) "],["local-and-global-explanations.html", "9.11 Local and Global Explanations", " 9.11 Local and Global Explanations In this lecture we will introduce some global explanations and see how they are connected to local explanations. We will also see how these explanations can be used for de-biasing. 9.11.1 Interpretability Reminder: Why interpretability. Algorithmic accountability Are risk estimates transparent? EU regulation GDPR: “Individuals have the right to an explanation of the logic behind the decision.” EU AI Act If process of decision making is transparent \\(\\rightarrow\\) Biases easier to detect \\(\\rightarrow\\) more robustness a unmeasured confounders can hunt you later under distributional shifts 9.11.2 Partial dependence plots Partial dependence plots are popular post-hoc global explanations Definition. (Partial dependence plots Friedman 2001) Given an estimator \\(\\hat m_n\\) and a target subset \\(S\\subset \\{1,\\dots,p\\}\\), the partial dependence plot , \\(\\xi_S\\), is defined as \\[ \\xi_S(x_S)= \\int \\hat m_n(x) \\hat p_{-S}(x_{-S})\\mathrm dx_{-S}. \\] In particular, a partial dependence plot for feature \\(k\\) is \\[ \\xi_k(x_k)= \\int \\hat m_n(x) p_{-k}(x_{-k})\\mathrm dx_{-k}. \\] Partial dependence can be misleading because they ignore interaction effects. Approximation is highly non-trivial and can be very unstable trough extrapolatio. 9.11.3 A functional decomposition Assume a data set with \\(p\\) features. Also assume that we can approximate the regression function \\(m\\) by a \\(q-th\\) order functional decomposition: \\[m(x) \\approx m_0+\\sum_{k=1}^p m_k(x_{k}) + \\sum_{k_1&lt;k_2} m_{k_1k_2}(x_{k_1},x_{k_2}) + \\cdots +\\sum_{k_1&lt;\\cdots &lt;k_q} m_{k_1,\\dots,k_q} (x_{k_1},\\dots,x_{k_q}).\\] Optimal rates of convergence under the assumption that \\(m\\) has two continuous partial derivatives: Model general general \\(p\\) \\(p=6\\) Comparable sample sizes for p=6 Full model \\(O_p(n^{-2/(p+4)})\\) \\(O_p(n^{-1/5})\\) 1 000 000 Interaction (q) \\(O_p(n^{-2/(p+4)})\\) \\(O_p(n^{-2/(p+4)})\\) 1 000 - 1 000 000 Interaction (q=2) \\(O_p(n^{-1/3})\\) \\(O_p(n^{-1/3})\\) 4 000 Additive (q=1) \\(O_p(n^{-2/5})\\) \\(O_p(n^{-2/5})\\) 1 000 Generalized ANOVA is probably the most considered functional decomposition identification. Generalized ANOVA: For every\\(S\\subseteq \\{1,\\dots,d\\}\\) and \\(k\\in S\\), \\[ \\int m_{S}\\left(x_{S}\\right) \\int w(x) \\mathrm dx_{-S} \\ \\mathrm d x_{k}=0 \\] Common choices for \\(w\\) are \\(w \\equiv 1\\) \\(w(x)=p(x),\\ \\) \\(p=\\) density of \\(X\\). \\(w(x)=\\prod p_j(x_j),\\ \\) \\(p_j=\\) density of \\(X_j\\). Generalized ANOVA, however, has drawbacks for interpretability \\(w=1\\) ignores the distribution of \\(X\\) \\(w=p\\) uses the correlation structure (analog to observational SHAP) \\(w=\\prod p_j\\) Assumes that features are independent Definition. (Marginal Identification) For every \\(S\\subseteq \\{1,\\dots,d\\}\\), \\[ \\sum_{T: T \\cap S \\neq \\emptyset} \\int \\hat m_T(x_T) p_{S}(x_{S}) \\mathrm dx_S=0. \\] Theorem. Given any initial estimator \\(\\hat m^{(0)}=\\{\\hat m^{(0)}_S | S\\subseteq \\{1,\\dots,d\\}\\}\\), there exists exactly one set of functions \\(\\hat m^\\ast=\\{\\hat m^\\ast_S | S\\subseteq \\{1,\\dots,d\\}\\}\\) satisfying the marginal identification with \\(\\sum_S \\hat m^\\ast_S = \\sum_S \\hat m^{(0)}_S\\). The functions are given by \\[ \\hat m^\\ast_S (x_S)= \\sum_{T \\supseteq S} \\sum_{T\\setminus S \\subseteq U\\subseteq T}(-1)^{|S|-|T\\setminus U|} \\times \\int \\hat m^{(0)}_T(x_T) \\hat p_U(x_U)\\mathrm dx_U. \\notag \\] In particular \\(\\hat m^\\ast\\) does not depend on the particular identification of \\(\\hat m^{(0)}\\). The are three reasons why the marginal identification is particularly interesting Partial dependence plots Interventional SHAP values Fairness/ discrimination-free pricing Remember: partial dependence plot, \\(\\xi_S\\), is defined as \\[\\xi_k(x_S)= \\int \\hat m(x) p_{-S}(x_{-S})\\mathrm dx_{-S}.\\] Corollary. If \\(\\hat m^{\\ast}\\) satisfies the marginal identification, then \\[ \\xi_S=\\sum_{U \\subseteq S} \\hat m_U^{\\ast}. \\] In particular if \\(S\\) is only one feature, i.e., \\(S=\\{k\\}\\), we have \\[ \\xi_k(x_k)= \\hat m_0^{\\ast} + \\hat m_k^\\ast(x_k). \\] Theorem. \\(\\hat m^{\\ast}\\) satisfies the marginal identification Then, interventional SHAP values are weighted averages components interaction component is equally split between involved features: \\[ \\phi_k(x)= \\hat m^{\\ast}_k(x_k)+ \\frac 1 2 \\sum_j \\hat m^{\\ast}_{kj}(x_{kj}) + \\cdots + \\frac 1 d \\hat m^{\\ast}_{1,\\dots,d}(x_{1,\\dots,d}). \\] Assume \\(U\\) is a set of protected features. For example \\(U=\\{\\text{gender, ethnicity}\\}\\). Let \\(U \\cup V=\\{1,\\dots, d\\}, U\\cap V=\\emptyset\\). \\(E[m(X) |\\ do(X_V=x_V))\\) does not use information contained in \\(X_U\\); neither directly nor indirectly. Under the assumed causal structure we have \\[ E[m(X) |\\ do(X_V=x_V)]= \\int m(x) p_U(x_U) dx_U. \\] Under marginal identifiaction: \\[ \\int \\hat m_n^\\ast(x) \\hat p_U(x_U) dx_U= \\sum_{S \\subseteq V} \\hat {m}^\\ast_S(x_S), \\] i.e., a “de-biased” estimator can be extracted from \\(\\hat m_n\\) by dropping all components that include features in \\(U\\). Summary (Identification) If \\(m\\) is identified via martginal identification, \\[ \\hat m_n(x)=\\hat m_0^\\ast + \\sum_j \\hat m_j ^\\ast (x_j) + \\sum_{j&lt;k} \\hat m_{j,k} ^\\ast (x_j,x_k)+ \\cdots \\] then Interventional SHAP values are: \\[ \\phi_k(x)= \\hat m^{\\ast}_k(x_k)+ \\frac 1 2 \\sum_j \\hat m^{\\ast}_{kj}(x_{kj}) + \\cdots \\] Partial dependence plots are: \\[ \\xi_k(x_k)= \\hat m_0^{\\ast} + \\hat m_k^\\ast(x_k). \\] Fairness/Discrimination-free pricing: If \\(S\\) are protected variables and all components that contain a subset of \\(S\\) are dropped,we derive a de-biased estimator. What does discrimination free pricing actually mean? Average claim Size Car Male Female Male Car Type $1 $2 Female Car Type $2 $1 Proportions in population \\((p_{X,D})\\) Car Male Female Male Car Type 0.45 0.05 Female Car Type 0.05 0.45 If we use \\(p_D(d) = 0.5\\) for pricing, then the price for Male Car Type 1 and Female Car Type would be $1.50 each. Problem: The prices are not calibrated now: The insurance company is charging more than the expected average claim size of $1.10 . (We ignore risk loading) We could multiply the price by \\(\\frac{1.10}{1.50}\\approx 0.7333\\). Hence charge everyone \\(\\$1.10\\). Problem: Is the price still discrimination free? Note that $1.10 is also the price if we would ignore gender completely, \\(\\mathbb E[Y|X=x]\\) (i.e. allow for indirect discrimination). The answer will most likely be around the question why we wanted to de-bias for gender in the first place; what are the risk-factors we want to use, and what are unmeasured confounders. "],["quantative-risk-management.html", "Chapter 10 Quantative Risk Management ", " Chapter 10 Quantative Risk Management "],["the-loss-variable.html", "10.1 The Loss Variable", " 10.1 The Loss Variable We describe the financial risk with random variables defined on a filtered probability space \\(\\left(\\Omega,\\mathcal{F},P,\\left\\{\\mathcal{F}_t\\right\\}_{t\\in \\mathbb{R}_+}\\right)\\). We assume that the value process \\(V_t\\), denoting the market value of the portfolio at time \\(t\\), is adapted to the filtration i.e. may be determined at time \\(t\\) or from information available at time \\(t\\). We will be considering discrete time jumps of size \\(\\Delta t\\) for simplicity and assume that the portfolio remains fixed over the time horizon \\([t,t+\\Delta t)\\), there are no income or payments made during the time period. This means in particular that the value \\[ \\Delta V_{t+\\Delta t}=V_{t+\\Delta t}-V_t, \\] only depends on the valuation of the components in the portfolio. We will henceforth be using the notation \\(t\\) and \\(t+1\\) and so forth denoting the intervals \\([t+n\\Delta t,t+(n+1)\\Delta t)\\) i.e. \\(t\\) may be any time and the integer representing how many time jumps of size \\(\\Delta t\\) has been made since \\(t\\). Under the assumptions above it is reasonable to assume that \\(V_t\\) is Markovian in the sense that there exist \\(d\\ge 1\\) random sources \\(\\mathbf{Z}_t=(Z_{t,1},...,Z_{t,d})^\\top\\) such that \\[ V_t=f(t,\\mathbf{Z}_t)\\tag{2.2} \\] for some measurable function \\(f : \\mathbb{R}_+\\times \\mathbb{R}^d\\to\\mathbb{R}\\). We will call \\(\\mathbf{Z}_t\\) the risk factors associated with the portfolio. The could be for instance the stockprice of a stock held in the portfolio. We may now define the loss variable as \\(L_{t+1}:=-(V_{t+1}-V_t)\\) and risk-factor changes as \\(\\mathbf{X}_{t+1}:=\\mathbf{Z}_{t+1}-\\mathbf{Z}_t\\) and see that \\[ L_{t+1}=-\\left(f(t+1,\\mathbf{Z}_t+\\mathbf{X}_{t+1})-f(t,\\mathbf{Z}_t)\\right),\\tag{2.3} \\] if one assume differentiability of \\(f\\) we may approximate \\(L_{t+1}\\) as \\[ L_{t+1}^\\Delta:=-\\left(\\frac{\\partial f}{\\partial t}(t,\\mathbf{Z}_t)+\\sum_{i=1}^d \\frac{\\partial f}{\\partial z_i}(t,\\mathbf{Z}_t)\\mathbf{X}_{t+1,i}\\right).\\tag{2.4} \\] This is obviously a nice linear but the approximation error may be large if \\(\\Delta t\\) is large. We are interested in the the distribution of \\(L_{t+1}\\) such that we may determine sufficiently capital holding in realtion to the risk associated with the assets and liabilities held on the firm’s balance sheet. 10.1.1 Risk measures In the general sense, a risk measure is simply a measurable function \\(\\rho : \\mathbb{L} \\to\\mathbb{R}\\) taking a loss variable \\(L\\in \\mathbb{L}\\) as input and associating a number \\(\\rho(L)\\) as output. In this setting we let \\(\\mathbb{L}\\) be the set of all real-valued random variables. We may interpret this as the riskyness of the portfolio with associated loss variable \\(L\\). That is say we have two loss variable from the value processes \\(V\\) and \\(V&#39;\\) i.e. \\(L\\) and \\(L&#39;\\) we say that \\(V\\) is riskier than \\(V&#39;\\) if and only if \\(\\rho(L)\\ge \\rho(L&#39;)\\). We may now consider the fundamental definition of a coherent risk measures as in Coherent Measures of Risk by Artzner, Delbean, Eber and Heath (1999), by first stating the definition of a risk measure. Definition. (Risk Measure) Let \\(\\mathbb{L}\\) be the set of all real valued random variable i.e. \\[ \\mathbb{L}=\\left\\{X\\ \\vert \\ X : (\\Omega,P,\\mathcal{F})\\to (\\mathbb{R},\\mathbb{B})\\right\\} \\] Any mapping \\(\\rho : \\mathbb{L} \\to\\mathbb{R}\\) is called a measure of risk. We now want some properties to be satisfied by the mapping \\(\\rho\\) such that it is a sensable measure of risk. To this we define four axioms as. Definition. (Coherent Risk Measure) Let \\(\\rho\\) be a measure of risk. We say that \\(\\rho\\) is a coherent risk measure if and only if \\(\\rho\\) satisfies the axioms below. Translation invariance: Let \\(\\alpha\\in\\mathbb{R}\\) and \\(r\\) be a predictable process we have \\(\\rho(X+\\alpha\\cdot r)=\\rho(X)+\\alpha\\). Subadditivity: Let \\(X_1,X_2\\in\\mathbb{L}\\), then \\(\\rho(X_1+X_2)\\le \\rho(X_1)+\\rho(X_2)\\). Positive homogeneity: Let \\(c&gt;0\\), then \\(\\rho(cX)=c\\rho(X)\\). Monotonicity: Let \\(X,Y\\in\\mathbb{L}\\) such that \\(X\\le Y\\) \\(P\\)-a.s., then \\(\\rho(X)\\le \\rho(Y)\\). Remark: We see that axiom (1) gives the equation \\(\\rho(X+\\rho(X)\\cdot r)=0\\) i.e. we may hedge the risk by holding \\(\\rho(X)\\) in a asset with rate of return \\(r\\). The axiom (2) ensures that we take into account the correlation of multiple assets i.e. we will in general not experience the maximal loss of two sources at the same time (although this is possible). If \\(X_1\\) and \\(X_2\\) satisfies \\(\\text{Corr}(X_1,X_2)=1\\) then \\(\\rho(X_1+X_2)= \\rho(X_1)+\\rho(X_2)\\). There exist alot of different tangible measures of risk, some being coherent others non-coherent. The most well studied include Value at Risk VaR and Expected Shortfall ES. These two measures are in the realm of the loss distribution approach, however before studying these we introduce a few other worthy mentions: Factor sensitivity measures and scenario based risk measures: Factor sensitivity measures are measures on the form \\(\\rho=g(\\nabla L)\\) where \\(g\\) is some \\(d\\)-dimensional measurable function. In this \\(\\nabla L\\) is the gradient i.e. \\[ \\nabla L=\\nabla \\Big(f(t,\\mathbf{Z}_t)-f(t+1,\\mathbf{Z}_{t+1})\\Big)=\\left(\\frac{\\partial L}{\\partial z_1},...,\\frac{\\partial L}{\\partial z_d}\\right). \\] Scenario based risk measures are measures where one assume that a collection \\(\\mathbf{x}=(x_1,...,x_N)\\) of events \\(x_i\\in \\Omega\\) such that \\(\\sum_{i=1}^N P(x_i)=1-\\varepsilon\\) for some small \\(\\varepsilon&gt;0\\). We may then associate the measures \\(\\psi\\) as \\[ \\psi(L)=\\max\\left\\{w_1L(x_1),...,w_NL(x_N)\\right\\}, \\] with \\(\\mathbf{w}\\ge 0\\) and \\(\\sum_{i=1}^Nw_i=1\\) being weights. That is \\(\\psi\\) gives the largest weighted loss. A natural way of choosing \\(w_i\\) is such that \\(w_i\\approx P(x_i)\\), but one may also weight certain events with a larger weight for a more prudent measure. Notice also that the criteria \\(\\sum_{i=1}^N P(x_i)=1-\\varepsilon\\) does not necessarily have to be satisfied when considering the worst possible outcomes. 10.1.1.1 Value at Risk The Value at Risk, henceforth VaR, is a quantile measure of the loss distribution \\(F_L\\) associated with \\(L\\). We define VaR as such: Definition 2.8. (McNeil) (Value-at-Risk) Let \\(\\alpha\\in (0,1)\\) (\\(\\alpha\\) being large) we define the VaR of a portfolio with loss variable \\(L\\) at the confidence level \\(\\alpha\\) is a given by \\[\\begin{align*} \\text{VaR}_\\alpha(L)&amp;=\\inf\\left\\{ l\\in\\mathbb{R}\\ :\\ P(L&gt;l)\\le 1-\\alpha \\right\\}\\\\ &amp;=\\inf\\left\\{ l\\in\\mathbb{R}\\ :\\ F_L(l)\\ge \\alpha \\right\\}\\\\ &amp;=F^{\\leftarrow}_L(\\alpha), \\end{align*}\\] where \\(F^{\\leftarrow}_L\\) is the generalized inverse of \\(F_L\\). There exist alot of different tangible measures of risk, some being coherent others non-coherent. The most well studied include Value at Risk VaR and Expected Shortfall ES. These two measures are in the realm of the loss distribution approach, however before studying these we introduce a few other worthy mentions: Factor sensitivity measures and scenario based risk measures. Returning to the notion of a coherent risk measure we show the lemma, that VaR is not a coherent risk measure. Lemma. Consider the risk measure VaR (Value-at-Risk) given as above. The VaR is not a coherent risk measure. Proof. Translation invariance: Consider that for all \\(a\\in\\mathbb R\\) we have \\[ F_{L+ar}(\\ell)=\\mathbb P(L+a\\le \\ell)=\\mathbb P(L\\le\\ell - a)=F_L(\\ell-a). \\] Hence we have \\[\\begin{align*} \\text{VaR}_\\alpha(L+a)&amp;=\\inf\\left\\{ \\ell\\in\\mathbb{R}\\ :\\ F_{L+\\alpha}(\\ell)\\ge \\alpha \\right\\}\\\\ &amp;=\\inf\\left\\{ \\ell\\in\\mathbb{R}\\ :\\ F_{L}(\\ell-a)\\ge \\alpha \\right\\}\\\\ &amp;=\\inf\\left\\{ m\\in\\mathbb{R}\\ :\\ F_{L}(m)\\ge \\alpha \\right\\}+a\\\\ &amp;=\\text{VaR}_\\alpha(L)+a, \\end{align*}\\] So VaR adhere to translation. Subadditivity: Positive homogeneity: Monotonicity: "],["measure-theory.html", "Chapter 11 Measure theory ", " Chapter 11 Measure theory "],["axioms-of-probability.html", "11.1 Axioms of Probability", " 11.1 Axioms of Probability Som udgangspunkt betragtes rummet \\((\\Omega,\\mathcal{A})\\) udstyret med en brolægning \\(\\mathcal{A}\\subseteq 2^\\Omega\\), hvor \\(2^\\Omega=\\mathcal{P}(\\Omega)\\) er mængden af alle delmængder af \\(\\Omega\\). Typisk anvendes brolægningerne 1) en algebra eller 2) en \\(\\sigma\\)-algebra. Definition 2.1. (Protter) (Algebra) En brolægning \\(\\mathcal{A}\\subseteq 2^\\Omega\\) kaldes en algebra, hvis \\(\\Omega\\subseteq\\mathcal{A}\\), \\(\\forall A\\in\\mathcal{A} \\Rightarrow A^c=\\Omega\\setminus A\\in\\mathcal{A}\\) (lukket under komplementer), \\(A_1,A_2,...,A_n\\in\\mathcal{A}\\Rightarrow \\cup_{i=1}^n A_i\\in\\mathcal{A}\\) (lukket under endelige foreninger). Definition 2.2. (Protter) (\\(\\mathit{\\sigma}\\)-algebra) En brolægning \\(\\mathcal{A}\\subseteq 2^\\Omega\\) kaldes en \\(\\sigma\\)-algebra, hvis \\(\\Omega\\subseteq\\mathcal{A}\\), \\(\\forall A\\in\\mathcal{A} \\Rightarrow A^c\\in\\mathcal{A}\\) (lukket under komplementer), \\((A_n)_{n\\in\\mathbb{N}}\\subset\\mathcal{A}\\Rightarrow \\cup_{i\\in\\mathbb{N}} A_i\\in\\mathcal{A}\\) (lukket under tællelige foreninger). Egenskaber for en \\(\\sigma\\)-algebra: 1) \\(\\emptyset\\in\\mathcal{A}\\), 2) \\(A,B\\in\\mathcal{A}\\Rightarrow A \\cup B\\in\\mathcal{A}\\), og 3) \\((A_i)_{i\\in\\mathbb{N}}\\subseteq\\mathcal{A}\\Rightarrow\\cap_{i\\in\\mathbb{A}}A_i\\in\\mathcal{A}\\). Eksempel: Borel-sigma-algebraen \\(\\mathcal{B}(\\mathbb{R}^n)=\\sigma(\\mathcal{C})\\), hvor \\(\\mathcal{C}\\) kan være en af følgende frembrigersystem. (\\(\\sigma(\\mathcal{C})\\) benævner den mindste \\(\\sigma\\)-algebra på \\(\\Omega\\), hvor \\(\\mathcal{C}\\in\\mathcal{A}\\)). De åbne mængder dvs. \\(\\mathcal{C}=\\mathcal{O}^n\\) De lukkede mængder dvs. \\(\\mathcal{C}=\\{A^c : A\\in\\mathcal{O}^n\\}\\) Halvåbne mængder/bokse Uendelige intervaller som \\(\\mathcal{C}=\\{(-\\infty,a] : a\\in\\mathbb{R}\\}\\) Definition 2.2. (Protter) (Sandsynlighedsmål) Lad \\(\\mathcal{A}\\subseteq2^\\Omega\\) være en \\(\\sigma\\)-algebra. \\(P : \\mathcal{A}\\to[0,1]\\) kaldes et sandsynlighedsmål hvis \\(P(\\Omega)=1\\), For \\((A_n)_{n\\in\\mathbb{N}}\\) af parvist disjunkte delmængder af \\(\\Omega\\) gælder \\(P\\left(\\bigcup_{n\\in\\mathbb{N}}A_n\\right)=\\sum_{n\\in\\mathbb{N}}P(A_n)\\). Bemærkning: Husk et mål \\(\\mu : \\mathcal{A}\\to [0,\\infty)\\) på \\(\\mathcal{A}\\) opfylder at 1) \\(\\mu(\\emptyset)=0\\) og 2) for en parvis disjunkt familie \\((A_n)_{n\\in\\mathbb{N}}\\) gælder \\(\\mu\\left(\\mathop{\\dot{\\bigcup}}_{n\\in\\mathbb{N}}A_n\\right)=\\sum_{n\\in\\mathbb{N}}\\mu(A_n)\\). Konsekvenser: Umildbare konsekvenser ved definition 2.3 er følgende \\(\\sum_{i=1}^n P(A_i)=P\\left(\\bigcup_{i=1}^nA_i\\right)\\), hvis alle \\(A_i\\) er pavist disjunkte \\(0\\le P(A)\\le 1\\), for alle \\(A\\in\\mathcal{A}\\) \\(P(A^c)=1-P(A)\\), for alle \\(A\\in\\mathcal{A}\\) \\(P(A)\\le P(B)\\) hvis \\(A\\subseteq B\\) Theorem 2.3. (Protter) (Opad- og nedadkontinuitet) Lad \\(P : \\mathcal{A}\\to[0,1]\\) være et ssh. mål på \\((\\Omega,\\mathcal{A})\\). Da gælder \\(P(A_n)\\uparrow P(A)\\) hvis \\(A_n\\uparrow A\\), \\(P(A_n)\\downarrow P(A)\\) hvis \\(A_n\\downarrow A\\) "],["conditional-probability-and-independence.html", "11.2 Conditional Probability and Independence", " 11.2 Conditional Probability and Independence Definition 3.1. (Protter) (Uafhængighed) Lad \\((\\Omega,\\mathcal{A},P)\\) være et ssh. rum. Lad \\(A,B\\in\\mathcal{A}\\) være to hændelser. \\(A\\) og \\(B\\) kaldes uafhængige hvis \\(P(A\\cap B)=P(A)P(B)\\). Lad \\(A_i\\in\\mathcal{A}\\) for en indeksmængde \\(i\\in I\\) (ikke et krav om endelighed eller tællelig). Hændelserne \\(A_i\\) kaldes uafhængige hvis \\(P(\\cap_{i\\in J}A_i)=\\prod P(A_i)\\) for et \\(J\\subseteq I\\) med \\(\\# J&lt; +\\infty\\). Bemærkning. Der gælder at 1) \\(\\emptyset\\) og \\(\\Omega\\) er uafhængige af alle \\(A\\in\\mathcal{A}\\) samt \\(A\\in\\mathcal{A}\\) er uafhængig med sig selv hvis og kun hvis \\(P(A)=\\{0,1\\}\\). Theorem 3.1. (Protter) Lad \\((\\Omega,\\mathcal{A},P)\\) være et ssh. rum. Antag \\(A,B\\in\\mathcal{A}\\) være uafhængige, så er følgende par uafhængige: \\((A^c,B),(A,B^c)\\) og \\((A^c,B^c)\\). Definition 3.2. (Protter) (Betinget sandsynlighed) Lad \\(A,B\\in\\mathcal{A}\\) være hændelser og \\(P(B)&gt;0\\). Den betingede sandsynlighed \\(A\\) givet \\(B\\) er \\(P(A\\vert B)=P(A\\cap B)/P(B)\\). Theorem 3.2. (Protter) Lad \\(A,B\\in\\mathcal{A}\\) være hændelser og \\(P(B)&gt;0\\). \\(A\\) og \\(B\\) er uafhængige hvis og kun hvis \\(P(A\\vert B)=P(A)\\). Funktionen \\(P(\\cdot \\vert B) : \\mathcal{A} \\to [0,1]\\) definerer et nyt ssh. mål på \\(\\mathcal{A}\\), kaldet den betingede sandsynlighed givet \\(B\\). Andet kan tilføjes f.eks. Bayes’. "],["probabilities-on-a-finite-or-countable-space.html", "11.3 Probabilities on a Finite or Countable Space", " 11.3 Probabilities on a Finite or Countable Space Lad \\(\\Omega\\) være et endelig eller tællelig mængde og lad \\(\\sigma\\)-algebraen \\(\\mathcal{A}=2^\\Omega\\). Theorem 4.1. (Protter) (Punktsandsynligheder) Lad \\(A,B\\in\\mathcal{A}\\) være hændelser og \\(P(B)&gt;0\\). En sandsynlighed på en tællelig eller endelig mængde \\(\\Omega\\) er givet ved sandsynlighederne for hvert atom \\(p_\\omega=P(\\{\\omega\\})\\), \\(\\omega\\in\\Omega\\). Hvis en følge af reelle tal \\((p_\\omega)_{\\omega\\in\\Omega}\\) indiceret over elementerne i \\(\\Omega\\) opfylder at \\(p_\\omega\\ge0\\) og \\(\\sum_{k\\in\\Omega}p_k=1\\), så eksisterer et unikt sandsynlighedsmål \\(P\\) givet ved \\(P(\\{\\omega\\})=p_\\omega\\). Bemærkning. Alle sandsynlighedsmål på endelige eller tællelige mængder \\(\\Omega\\) kan således karakteriseres ved punktsandsynlighederne \\(p_\\omega\\). Dvs. et sandsynligedsmål \\(P : 2^\\Omega \\to [0,1]\\) er givet ved summen af punktsandsynligheder \\[ P(A)=\\sum_{k\\in A}p_k,\\ A\\subseteq\\Omega \\] Definition 4.1. (Protter) En ssh. \\(P\\) på en endelig mængde \\(\\Omega\\) er uniform hvis \\(p_\\omega\\) afhænger af \\(\\omega\\). Eksempler: (Den uniforme fordeling.) Det følger direkte af definition 4.1, at den uniforme fordeling er givet ved \\[ P(A)=\\frac{\\#A}{\\#\\Omega} \\] (Binomialfordelingen) Lad \\(\\Omega=\\{0,1,2,...,n\\}\\) og \\(\\mathcal{A}=2^\\Omega\\). Givet et \\(q\\in[0,1]\\) defineres binomialfordelingen ved \\[ p_k={n\\choose k}q^k(1-q)^{n-k},\\ k\\in\\Omega \\] (Geometriske fordeling) Lad \\(\\Omega=\\mathbb{N}_0=\\{0,1,2,...\\}\\) og \\(\\mathcal{A}=2^{\\mathbb{N}_0}\\). Givet et \\(q\\in[0,1]\\) er den geometriske fordeling givet ved \\[ p_k=(1-q)^kq,\\ k\\in\\Omega \\] (Hypergeometrisk fordeling) Lad \\(N,M\\in\\mathbb{N}\\) være givet. (Poisson fordelingen) Lad \\(\\Omega=\\mathbb{N}_0\\) og \\(\\mathcal{A}=2^{\\mathbb{N}_0}\\). Givet et parameter \\(\\lambda&gt;0\\) er poissonfordelingen gived ved \\[ p_n=e^{-\\lambda}\\frac{\\lambda^n}{n!},\\ n\\in\\Omega \\] Desuden er \\(K(n,k)={n\\choose k}\\) givet ved \\[ {n\\choose k}=\\frac{n!}{k!(n-k)!}. \\] "],["construction-of-a-probability-measure-on-mathbb-r.html", "11.4 Construction of a Probability Measure on \\(\\mathbb R\\)", " 11.4 Construction of a Probability Measure on \\(\\mathbb R\\) Sandsynlighedsmålet kan indledelsesvis indføres på følgende vis: Lad \\((\\Omega, \\mathcal{A},\\mu)\\) være et målrum og lad \\(f : (\\Omega,\\mathcal{A}) \\to [0,\\infty]\\) være \\(\\mathcal{A}/\\mathcal{B}\\)-målelig. Definer nu målet \\(\\nu : \\mathcal{A} \\to [0,\\infty]\\) ved \\[ V(A)=\\int_A f d\\mu=\\int 1_A fd\\mu=\\int 1_A(x)f(x)d\\mu(x)=\\int1_A(x)f(x)\\mu(dx),\\ A\\in\\mathcal{A} \\] I situationen hvor \\(V(\\Omega)=1\\) er \\(\\nu\\) et ssh. mål. Især er vi interesseret i målrummet \\((\\mathbb{R}^n,\\mathcal{B}_n)\\). Notationen \\(\\nu =f\\cdot \\mu\\) bruges og betyder “\\(\\nu\\) har tæthed \\(f\\) mht. \\(\\mu\\)”. Eksempler. (Lebesguemålet) Lad \\((\\Omega,\\mathcal{A},\\mu)=(\\mathbb{R},\\mathcal{B},m)\\) været et målrum, hvor \\(m\\) er lebesgue-målet. Lad \\(f : \\mathbb{R}\\to (0,1/\\sqrt{2\\pi}]\\) være givet ved \\[ f(x)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}x^2},\\ x\\in\\mathbb{R}, \\] da er \\(f\\) målelig, da \\(f\\) er kontinuer for alle \\(x\\in\\mathbb{R}\\). Da er \\(\\nu : \\mathcal{B} \\to [0,1]\\), også kaldet normalfordelingen, et ssh. mål givet ved \\[ \\nu(A)=\\int_A f(x) dm(x),\\hspace{20pt} \\nu(\\mathbb{R})=\\int_{\\mathbb{R}}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}x^2}dm(x)=1 \\] (Tællemålet) Lad \\((\\mathbb{N}_0,2^{\\mathbb{N}_0},\\mu)\\) været et målrum og lad \\(\\mu\\) være tællemålet. Lad \\(f : \\mathbb{N}_0 \\to [0,\\infty]\\) være en vilkårlig målelig funktion. Da er \\(\\nu\\) givet ved \\[ \\nu(A)=\\int_A fd\\mu=\\sum_{x\\in A}f(x) \\] hvis \\(\\nu(\\mathbb{N}_0)=\\sum_{x\\in\\mathbb{N}_0}f(x)=1\\) er \\(\\nu\\) et ssh. mål på \\((\\mathbb{N}_0,2^{\\mathbb{N}_0})\\). Definition 7.1. (Protter) (Fordelingsfunktionen/distribution function) Lad \\(P\\) være et ssh. mål på \\((\\mathbb{R},\\mathcal{B})\\), da er fordelingsfunktionen \\(F : \\mathbb{R}\\to [0,1]\\) defineret ved. \\[ F(x)=P((-\\infty,x]). \\] Theorem 7.1. (Protter) (Entydighed) Fordelingsfunktionen \\(F\\) karakteriserer \\(P\\) og er unik. Corollary 7.1. (Protter) Lad \\(F\\) være en fordelingsfunktion for \\(P\\) på \\(\\mathbb{R}\\). Definer da \\(F(x-):=\\lim_{u\\uparrow x}F(u)\\). For vilkårlige \\(x,y\\in\\mathbb{R}\\) gælder \\(P((x,y])=F(y)-F(x)\\) \\(P([x,y])=F(y)-F(x-)\\) \\(P(\\{x\\})=F(x)-F(x-)\\) Theorem 7.2. (Protter) (Egenskaber for fordelingsfunktionen) \\(F : \\mathbb{R} \\to [0,1]\\) er en fordelingsfunktion for et unikt sandsynlighedsmål \\(P\\) på \\((\\mathbb{R},\\mathcal{B})\\), hvis \\(F\\) er ikke-aftagende dvs. \\(F(x)\\le F(y)\\), for \\(x\\le y\\) \\(F\\) er højre kontinuer dvs. \\(F(u)\\downarrow F(x)\\) for \\(u\\downarrow x\\). \\(F(x)\\to 1\\) når \\(x\\to \\infty\\), \\(F(x)\\to 0\\) når \\(x\\to -\\infty\\). Bemærkning. Teorem 7.2 kan fungere som et værktøj til konstruktion af et sandsynligedsmål givet en fordelingsfunktion. "],["random-variables.html", "11.5 Random Variables", " 11.5 Random Variables Indledelsesvis genopfriskes målbare rum. For to rum \\((E,\\mathcal{E})\\) og \\((F,\\mathcal{F})\\), hvor \\(\\mathcal{E}\\) og \\(\\mathcal{F}\\) er \\(\\sigma\\)-algebraer på de respektive rum. Da kaldes en en afbildning \\(X : (E,\\mathcal{E})\\to(F,\\mathcal{F})\\) for en målelig afbildning, hvis og kun hvis for alle delmængder \\(A\\in\\mathcal{F}\\) er \\(X^{-1}(A)\\in\\mathcal{E}\\). Med andre ord for enhver billedmængde, kan vi måle urbilledet, hvorfra funktionsværdierne på billedmængden kunne være kommet fra. Definition. (Stokastisk variabel) En målelig afbildning udstyret med et sandsynlighed mål på domænet dvs. \\(X : (\\Omega,\\mathcal{A},P) \\to (F,\\mathcal{F})\\), kaldes en stokastisk variabel (stochastic/random variable). Bemærkning. Oftest betragtes \\(F=\\mathbb{R}^n\\) og \\(\\mathcal{F}=\\mathcal{B}_n\\) især når \\(n=1\\). Corollary 8.1. (Protter) (Urbilleder) Lad \\(X\\) være en stokastisk variabel er \\(X^{-1}(A):=\\{\\omega\\in\\Omega \\vert X(\\omega)\\in A\\}:=\\{X\\in A\\}\\). Hertil gælde specielt for eksempelvis \\(A=(-\\infty,x]\\) og \\(A=(x,\\infty)\\) følgende urbilleder \\(X^{-1}(A)=\\{X\\le x\\}\\) og \\(X^{-1}(A)=\\{X&gt; x\\}\\). Definition. (Billedmål) For en målbar funktion \\(X : (\\Omega,\\mathcal{A},\\mu) \\to (F,\\mathcal{F})\\), hvor \\(\\mu\\) er et mål på \\(\\mathcal{A}\\) defineres \\(\\mu^X(A)=\\mu(X^{-1}(A))\\) som billedmålet af \\(A\\). Theorem 8.5. (Protter) (Billedmål for stokastiske variable) Lad \\(X : (\\Omega,\\mathcal{A},P) \\to (F,\\mathcal{F})\\) være en stokastisk variabel, så gælder for alle \\(A\\in\\mathcal{F}\\) at \\(P^X(A)=X(P)(A)=P(X^{-1}(A))=P(X\\in A)\\), hvor \\(P^X\\) kaldes fordelingen af \\(X\\) (distribution of \\(X\\)). Specielt er \\(P^X\\) et sandsynlighedsmål.. Bemærkning. Fordelingsfunktionen \\(F_X: \\mathbb{R}\\to [0,1]\\) for \\(X\\) er givet ved \\(F_X(x)=P^X((-\\infty,x])=P(X\\in (-\\infty,x])=P(X\\le x)\\). Definition. (Næsten sikker) For en stokasitsk variabel \\(X:(\\Omega,\\mathcal{A},P) \\to (F,\\mathcal{F})\\) og en delmængde \\(A\\in \\mathcal{F}\\) siges at \\[ X\\in A\\hspace{10pt} P\\text{-n.s}\\hspace{10pt}\\Leftrightarrow\\hspace{10pt} X\\in A\\hspace{10pt} P\\text{-a.s}\\hspace{10pt}\\Leftrightarrow\\hspace{10pt} X\\in A\\hspace{10pt} P\\text{-a.e} \\] gælde hvis \\(P(X\\in A)=1\\) dvs. ækvivalent \\(P(X\\in A^c)=P(X\\in F\\setminus A)=0\\). Ovenstående udtales n.s (næsten sikker), a.s (almost surely) og a.e (almost everywhere). "],["integration-with-respect-to-a-probability-measure.html", "11.6 Integration with Respect to a Probability Measure", " 11.6 Integration with Respect to a Probability Measure Definition 9.1. (Forventet værdi) Lad \\(X : (\\Omega, \\mathcal{A}, P) \\to (F,\\mathcal{F})\\) være en stokastisk variabel. Da defineres forventningen af \\(X\\) som følgende integrale, når dette er veldefineret. \\[ E(X)=E\\{X\\}=\\int X dP=\\int_\\Omega X dP=\\int_\\Omega X(\\omega)dP(\\omega)=\\int_\\Omega X(\\omega)P(d\\omega). \\] Bemærkning. Integralet ovenfor er veldefineret hvis 1) \\(X\\ge 0\\) P-n.s dvs \\(P(X\\ge 0)=1\\) eller, hvis 2) (definition 9.2) \\(X=X^+-X^-\\) er \\(E(X)=E(X^+)-E(X^-)\\), hvis blot \\(E(X^+),E(X^-)&lt;+\\infty\\) (kun en nødvendig). Eksempler. Hvis \\(\\Omega\\) er tællelig, er \\(P\\) en simpel funktion, givet ved singelton mængder. Derved bestemmes \\(E(X)\\) ved \\[ E(X)=\\sum_{\\omega\\in\\Omega} X(\\omega)P(\\{\\omega\\}) \\] Theorem 9.1. (Protter) (Sætninger fra An2) Husk \\(\\mathcal{L}^p\\) defineres som mængden af \\(\\mathcal{A}/\\mathcal{B}\\) målelige funktioner \\(f\\), hvor \\(\\vert f\\vert ^p\\) er integrabel. Det vil sige i konteksten af forventet værdi arbejder vi med mængden \\[ \\mathcal{L}^p=\\{X : (\\Omega,\\mathcal{A},P)\\to(\\mathbb{R},\\mathcal{B})\\ \\vert\\ E\\vert X\\vert^p&lt; +\\infty\\}. \\] \\(\\mathcal{L}^p\\) er et lineært vektorrum dvs. for \\(X\\in \\mathcal{L}^p\\Rightarrow aX\\in\\mathcal{L}^p\\), \\(a\\in\\mathbb{R}\\) og for \\(X,Y\\in\\mathcal{L}^p\\Rightarrow X+Y\\in\\mathcal{L}^p\\). Desuden for \\(0\\le X\\le Y\\) og \\(Y\\in\\mathcal{L}^1\\), så er \\(X\\in\\mathcal{L}^1\\) og \\(E(X)\\le E(Y)\\). For \\(X\\in\\mathcal{L}^p\\) er \\(E(X)\\le E\\vert X\\vert\\). Hvis \\(X=Y\\) \\(P-n.s\\) og \\(X\\in\\mathcal{L}^p\\), så er \\(Y\\in\\mathcal{L}^p\\) og \\(E(X)=E(Y)\\) samt \\(E\\vert X-Y\\vert^p=0\\). (Monotom konvergens teorem) Hvis \\(X_n\\uparrow X\\) og \\(X_n\\) gælder \\(\\lim_{n\\to \\infty} E(X_n)=E(X)\\). (Fatou’s lemma) Hvis \\(X_n\\ge Y\\) \\(P-n.s.\\) (\\(Y\\in\\mathcal{L}^p\\)) eller \\(X_n\\ge 0\\) \\(P-n.s.\\) for alle \\(n\\), så er \\(E(\\liminf_{n\\to\\infty}X_n)\\le \\liminf_{n\\to \\infty} E(X_n)\\). (Lebesgue’s domineret konvergens teorem) Hvis \\(X_n\\uparrow X\\) \\(P-n.s.\\) og hvis \\(\\vert X_n\\vert \\le Y\\in\\mathcal{L}^1\\) \\(P-n.s.\\), så er \\(X_n,X\\in\\mathcal{L}^1\\) og \\(E(X_n)\\to E(X)\\). Theorem 9.2. (Protter) Lad \\(X_n : (\\Omega,\\mathcal{A},P)\\to(F,\\mathcal{F})\\) være en følge af stokastiske variable. Hvis \\(X_n\\ge0\\), så gælder \\(E\\left(\\sum_{n=1}^\\infty X_n\\right)=\\sum_{n=1}^\\infty E(X_n)\\) (begge enten uendelige eller endelige) Hvis \\(\\sum_{n=1}^\\infty E\\vert X_n\\vert &lt;+\\infty\\), så konvergerer \\(\\sum_{n=1}^\\infty X_n\\) \\(P-n.s.\\) og er integrabel. Desuden holder ovenstående lighed. Theorem 9.3. (Protter) (Cauchy-Schwarz ulighed) Lad \\(L^p=\\{X : (\\Omega,\\mathcal{A},P)\\to(\\mathbb{R},\\mathcal{B})\\ \\vert\\ E(X^p)&lt; +\\infty\\}\\) dvs. mængden af \\(p\\)-potens integrable stokastiske variable. Hvis \\(X,Y\\in L^2\\), så er \\(XY\\in L^1\\) og følgende ulighed gælder \\[ \\vert E(XY)\\vert \\le \\sqrt{E(X^2)E(Y^2)}. \\] Der gælder \\(L^2\\subset L^1\\) og hvis \\(X\\in L^2\\), så \\(E(X)^2\\le E(X^2)\\). Rummmet \\(L^2\\) er et lineært vektor rum. Theorem 9.4. (Protter) (Chebyshev’s/Markov’s/Bienaymé-Chebyshev’s ulighed) For en stokastisk variabel \\(X\\) gælder \\[ P(\\vert X\\vert \\ge a)\\le \\frac{E(X^2)}{a^2},\\frac{E\\vert X\\vert}{a} \\hspace{20pt} P\\{\\vert X-E\\{X\\}\\vert\\ge a\\}\\le \\frac{\\sigma^2 E\\{X^2\\}}{a^2} \\] Theorem 9.5. (Protter) (Forventnings reglen) Lad \\(X\\in \\mathbb{R}\\) være en stokastisk variabel, så \\(X : (\\Omega, \\mathcal{A}, P)\\to (E,\\mathcal{E})\\) og med fordeling \\(P^X\\). Lad \\(h : (E,\\mathcal{E})\\to(\\mathbb{R},\\mathcal{B})\\) være en målelig funktion. Der gælder \\(h(X)\\in\\mathcal{L}^1(\\Omega, \\mathcal{A}, P)\\iff h\\in\\mathcal{L}^1(E, \\mathcal{E}, P^X)\\) Hvis (a) er opfyldt eller \\(h\\ge 0\\), så er \\[ E(H(X))=\\int h(x)P^X(dx)=\\int h(x)dP^X(x) \\] "],["independent-random-variables.html", "11.7 Independent Random Variables", " 11.7 Independent Random Variables Definition 10.1. (Protter) (Uafhængige Stokastiske Variable) Brolægninger \\((\\mathcal{A}_i)_{i\\in I}\\subseteq \\mathcal{A}\\) kaldes uafhængige hvis for alle endelige \\(J\\subseteq I\\) og alle \\(A_i\\in\\mathcal{A}_i\\) er \\[ P(\\cap_{i\\in J} A_i)=\\prod_{i\\in J}P(A_i) \\] dvs. for alle brolægning er alle hændelser uafhængige. tokastiske Variable \\((X_i)_{i\\in I}\\), hvor \\(X_i : (\\Omega, \\mathcal{A})\\to (E_i,\\mathcal{E}_i)\\), kaldes uafhængige, hvis brolægningerne i familien givet ved \\(\\sigma(X_i^{-1}(\\mathcal{E}_i))\\subseteq \\mathcal{A}\\) er uafhængige. Theorem 10.1. (Protter) (Ækvivalensudsagn) For to stokastiske variable \\(X : (\\Omega, \\mathcal{A})\\to (E,\\mathcal{E})\\) og \\(Y : (\\Omega, \\mathcal{A})\\to (F,\\mathcal{F})\\) er følgende udsagn ækvivalente. \\(X\\) og \\(Y\\) er uafhængige. \\(P(X\\in A, Y\\in B)=P(X\\in A)P(Y\\in B)\\) for alle \\(A\\in \\mathcal{E}\\) og \\(B\\in\\mathcal{F}\\). \\(P(X\\in A, Y\\in B)=P(X\\in A)P(Y\\in B)\\) for alle \\(A\\in \\mathcal{C}\\) og \\(B\\in\\mathcal{D}\\). Hvor \\(\\mathcal{C}\\) og \\(\\mathcal{D}\\) er fællesmængdestabile mængdesystemer (brolægning) der frembringer hhv. \\(\\mathcal{E}\\) og \\(\\mathcal{F}\\). \\(f(X)\\) og \\(g(Y)\\) er uafhængige for alle par \\((f,g)\\) målbare funktioner. \\(E(f(X)g(Y))=E(f(X))E(g(Y))\\) for alle par \\((f,g)\\) begrænsede og målbar eller positive og målbare funktioner. Hvis \\(E\\) og \\(F\\) er metriske rum med respektive Borel \\(\\sigma\\)-algebraer \\(\\mathcal{E}\\) og \\(\\mathcal{F}\\). \\(E(f(X)g(Y))=E(f(X))E(g(Y))\\) for alle par \\((f,g)\\) begrænsede og kontinuere funktioner. Notation. Ofte ønskes at betragte funktioner af flere variable, hvor domænerummet ønskes konstrueret fra to rum hvorpå \\(x\\) og \\(y\\) lever hhv. \\((E,\\mathcal{E},P)\\) og \\((F,\\mathcal{F},Q)\\). Vi kan da konstruere et målrum givet ved \\((E\\times F, \\sigma(\\mathcal{E}\\times\\mathcal{F}))\\), hvor vi lader \\(\\mathcal{E}\\otimes \\mathcal{F}=\\sigma(\\mathcal{E}\\times\\mathcal{F})\\). Tilsvarende kan vi konstruere produktmålet \\(P\\otimes Q(A)=P(A)Q(A)\\) på \\((E\\times F, \\mathcal{E}\\otimes \\mathcal{F},P\\otimes Q)\\). Theorem 10.2. (Protter) Lad \\(f : (E \\times F, \\mathcal{E} \\otimes \\mathcal{F}) \\to (\\mathbb{R}, \\mathcal{R})\\) være målbar. For hvert \\(x \\in E\\) og \\(y \\in F\\), er de respektive “sektionerne” \\(y \\to f(x,y)\\) og \\(x \\to f(x,y)\\) henholdsvis \\(\\mathcal{F}\\)- og \\(\\mathcal{E}\\) målbare funktioner. Notation. Det kan ønskes at betragte mål med faste \\(x,y\\) og hvordan disse opfører sig. Teoremet fortæller, at man kan vælge tilfældige \\(x \\in E\\) og \\(y \\in F\\) således, at man kan betragte henholdsvis \\(\\mathcal{F}\\)- og \\(\\mathcal{E}\\) målbare funktioner i én variable (enten \\(x\\) eller \\(y\\)) for sig selv. Theorem 10.3. (Protter) (Tonelli-Fubini) Lad \\((E,\\mathcal{E},P)\\) og \\((F,\\mathcal{F},Q)\\) være sandsynlighedsrum. Lad \\(P\\otimes Q(A\\times B)=P(A)Q(B)\\). Dette er et unikt ssh mål, som udvider til sandsynlighedsrummet \\((E\\times F,\\mathcal{E}\\otimes \\mathcal{F},P\\otimes Q)\\). Lad \\(f : (E\\times F,\\mathcal{E}\\otimes \\mathcal{F},P\\otimes Q) \\to (\\mathbb{R}, \\mathcal{R})\\) være en målbar, positiv eller integrabel mht. \\(P\\otimes Q\\). Da er \\(x\\mapsto \\int f(x,y)Q(dy)\\) en \\(\\mathcal{E}\\)-målelig funktion og \\(y\\mapsto \\int f(x,y)P(dy)\\) en \\(\\mathcal{F}\\)-målelig funktion. Specielt er \\[ \\int f d P\\otimes Q=\\int\\left\\{\\int f(x,y) Q(dy)\\right\\}P(dx)=\\int\\left\\{\\int f(x,y) P(dx)\\right\\}Q(dy) \\] "],["probability-distributions-on-mathbb-r.html", "11.8 Probability Distributions on \\(\\mathbb R\\)", " 11.8 Probability Distributions on \\(\\mathbb R\\) Dette kapitel undersøger egenskaberne ved sandsynlighedsmål \\(P\\) på \\((\\mathbb{R},\\mathcal{B})\\), hvor fordelingsfunktionen er karakteriseret ved \\(F(x)=P((-\\infty,x])\\). Definition 11.1. (Protter) Lebesguemålet er mængdefunktion \\(m: \\mathcal{B} \\to[0,\\infty0]\\), der opfylder: For \\(A_1,A_2,A_3,...\\in\\mathcal{B}\\) parvist disjunkte mængder gælder \\(m(\\cup_{i=1}^\\infty A_i)=\\sum_{i=1}^\\infty m(A_i)\\), hvis \\(a&lt;b\\) og \\(a,b\\in\\mathbb{R}\\), så \\(m((a,b])=b-a\\). Theorem 11.1. (Protter) Lebesguemålet er unikt. Theorem 11.2. (Protter) Lebesguemålet eksisterer. Definition 11.2. (Protter) Tætheden af et sandsynlighedsmål \\(P\\) på \\((\\mathbb{R},\\mathcal{B})\\) er en positiv Borel målelig funktion \\(f\\) der opfylder for alle \\(x\\in\\mathbb{R}\\): \\[ F(x)=P((-\\infty,x])=\\int_{-\\infty}^x f(y)dy=\\int f(y)1_{(-\\infty,x]}(y)dm(y) \\] I tilfældet \\(P=P^X\\) (husk thm 8.5. \\(P^X(A)=P(X\\in A)\\)), dvs \\(P\\) er fordelingsmålet af en s.v. \\(X\\), så siger vi at \\(f\\) er tætheden af \\(X\\). Theorem 11.3. (Protter) Lad \\(f\\in \\mathcal{M}_{\\mathbb{R}}^+\\). Da gælder (\\(f\\) karakteriseret fuldkomment tætheden for et sandsynlighedsmål \\(P\\) på \\((\\mathbb{R},\\mathcal{B})\\)) \\(\\Longleftrightarrow\\) (\\(\\int f dm(x)=1\\)). Desuden gælder hvis \\(f&#39;\\) opfylder \\(m(f\\ne f&#39;)=0\\) (\\(f=f&#39;\\) \\(m\\)-\\(n.o.\\)), så er \\(f&#39;\\) og en tæthed for samme sandsynlighedsmål. Omvendt bestemmer et sandsynlighedsmål også den tæthed, når denne eksisterer. Remark 11.1. \\(F\\) er differentiabel \\(m\\)-\\(n.o.\\) uafhængig af \\(f\\) og med \\(f=0\\) ellers. Corollary 11.1. (Protter) (Forventningsreglen) Lad \\(X\\) være en \\(\\mathbb{R}\\)-værdi stokastisk variabel med tæthed \\(f\\). Lad \\(g\\in\\mathcal{M}_{\\mathbb{R}}\\). Så er \\(g\\) integrabel mht. \\(P^X\\), hvis og kun hvis \\(fg\\) er integrabel mht. \\(m\\). Hvis da er \\[ E[g(X)]=\\int g(x)P^X(dx)=\\int g(x)f(x) dm(x) \\] Theorem 11.4. (Protter) Lad \\(X\\) have tæthed \\(f_X\\) og lad \\(g\\in\\mathcal{M}_{\\mathbb{R}}\\). Lad \\(Y=g(X)\\). Så er \\[ F_Y(y)=P(Y\\le y)=P(G(X)\\le y)=\\int_{A_y} f_X(u)dm(u), \\] hvor \\(A_y=\\{u : g(u)\\le y\\}\\). Corollary 11.2. (Protter) Lad \\(X\\) have en kontinuer tæthed \\(f_X\\). Lad \\(g: \\mathbb{R}\\to \\mathbb{R}\\) være \\(C^1\\) og strengt monotom. Lad \\(h(y)=g^{-1}(y)\\) være \\(g\\) inverse også \\(C^1\\). Så har \\(Y=g(X)\\) tæthed \\[ f_Y(y)=f_X(h(y))|h&#39;(y)|. \\] Corollary 11.3. (Protter) Lad \\(X\\) have en kontinuer tæthed \\(f_X\\). Lad \\(g: \\mathbb{R}\\to \\mathbb{R}\\) være stykkevis \\(C^1\\) og strengt monotom på intervallerne \\(I_1,I_2,...,I_n\\) med \\(\\cup_{i\\in I}I_i=\\mathbb{R}\\) (\\(g\\) skal kun være \\(C^1\\) og strengt monotom på det indre af \\(I_i\\)). Lad \\(h_i(y)=g^{-1}(y)\\) for \\(g: I_i\\to \\mathbb{R}\\). Så har \\(Y=g(X)\\) tæthed \\[ f_Y(y)=\\sum_{i=1}^n f_X(h_i(y))|h_i&#39;(y)| 1_{g(I_i)}(y). \\] "],["probability-distributions-on-mathbb-rn.html", "11.9 Probability Distributions on \\(\\mathbb R^n\\)", " 11.9 Probability Distributions on \\(\\mathbb R^n\\) Dette kapitel undersøger fordelinger for stokatiske variable på \\((\\mathbb{R}^n,\\mathcal{B}^n)\\) for \\(n=2,3,...\\). Særligt vil vi have interesse for særtilfældet \\(n=2\\). Mange sætninger i dette kapitler har en analog i kapitel 11. Definition 12.1. (Protter) Lebesgue målet på \\((\\mathbb{R}^n,\\mathcal{B}^n)\\) er defineret for det kartesiske produkt \\(A_1\\times A_2\\times ...\\times A_n\\) ved \\(m_n(\\prod_{i=1}^n A_i)=\\prod_{i=1}^n m(A_i)\\). Dermed også \\(m_n(\\prod_{i=1}^n (a_i,b_i])=\\prod_{i=1}^n m((a_i,b_i])\\). Definition 12.2. (Protter) Et sandsynlighedsmål \\(P\\) på \\((\\mathbb{R}^n,\\mathcal{B}^n)\\) har tæthed \\(f\\) hvis \\(f\\) er en ikke negativ Borelmålelig funktion på \\(\\mathbb{R}^n\\), der opfylder \\[ P(A)=\\int_A f(x)dm_n(x)=\\int f(x_1,x_2,...,x_n)1_A(x_1,x_2,...,x_n)dm_n(x_1,x_2,...,x_n),\\hspace{10pt} \\forall A\\in\\mathcal{B}^n. \\] Theorem 12.1. (Protter) En \\(f\\in\\mathcal{M}_{R^n}^+\\) er en tæthed for et sandsynlighedsmål \\(P\\) på \\((\\mathbb{R}^n,\\mathcal{B}^n)\\) hvis og kun hvis \\(\\int f(x)dm_n(x)=1\\). I det tilfælde karakterer \\(f\\) fuldkomment målet \\(P\\) og for alle \\(f&#39;\\in\\mathcal{M}_{R^n}^+\\) med \\(m_n(f\\ne f&#39;)=0\\) er denne også tæthed for målet \\(P\\). Omvendt bestemmer målet \\(P\\) også en tæthed \\(f\\) op til enhver ikke Lebesgue nulmængde. Theorem 12.2. (Protter) Antag \\(X=(Y,Z)\\) har tæthed \\(f=f_X=f_{Y,Z}\\) på \\(\\mathbb{R}^2\\). Så gælder Begge \\(Y\\) og \\(Z\\) har tætheder på \\((\\mathbb{R},\\mathcal{B})\\) givet ved \\[ f_Y(y)=\\int_{\\mathbb{R}}f(y,z)dm(z),\\hspace{15pt}f_Z(z)=\\int_{\\mathbb{R}}f(y,z)dm(y) \\] \\(Y\\) og \\(Z\\) er uafhængige hvis og kun hvis \\(f(y,z)=f_Y(y)f_Z(z)\\) \\(m_2\\)-n.s. dvs. \\(m_2(f(y,z)\\ne f_Y(y)f_Z(z)=0\\). Følgende bestemmer en fjerde tæthed på \\(\\mathbb{R}\\) for alle \\(y\\in\\mathbb{R}\\) sådan at \\(f_Y(y)\\ne 0\\): \\(f_{Y=y}(z)=\\frac{f(y,z)}{f_Y(y)}\\). Definition 12.3. (Protter) Lad \\(X,Y\\) være to reelle stokastiske variable begge med endelig varians. Covariansen af \\(X,Y\\) er defineret ved \\(\\text{Cov}(X,Y)=E[(X-E(X))(Y-E(Y))]=E[XY]-E[X]E[Y]\\). Varians er dermed også givet ved \\(\\text{Cov}(X,X)=Var(X)=\\sigma ^2(X)\\). Theorem 12.3. (Protter) Hvis \\(X\\) og \\(Y\\) er uafhængige så er \\(\\text{Cov}(X,Y)=0\\). Definition 12.4. (Protter) Lad \\(X\\) og \\(Y\\) være stokastiske variable begge med endelig varians. Korrelationskoefficienten af \\(X\\) og \\(Y\\) er tallet \\(\\rho_{X,Y}=\\text{Cov}(X,Y)/(\\sigma(X)\\sigma(Y))\\). Definition 12.5. (Protter) Lad \\(X=(X_1,...,X_n)\\) være en \\(\\mathbb{R}^n\\) stokastisk variabel. Covariansmatricen for \\(X\\) er en \\(n\\times n\\) matrice med indgange \\(c_{ij}=\\text{Cov}(X_i,X_j)\\). Theorem 12.4. (Protter) En covarians matrice er positiv semidefinit dvs. den er symmetrisk (\\(c_{ij}=c_{ji}\\)) og \\(\\sum a_ia_jc_{ij}&gt;0\\) for alle \\(\\mathbf{a}\\in\\mathbb{R}^n\\). Theorem 12.5. (Protter) Lad \\(X\\) være en \\(\\mathbb{R}^n\\) stokastisk variabel med covarians matrice \\(C\\). Lad \\(A\\) være en \\(m\\times n\\) matrice og set \\(Y=AX\\). Så er \\(Y\\) en \\(\\mathbb{R}^m\\) stokastisk variabel med covarians matrice \\(C&#39;=ACA^*\\), hvor \\(A^*\\) er den transponerede matrice til \\(A\\). Theorem 12.6. (Protter) (Jacobi’s transformations formel) Lad \\(G\\subseteq \\mathbb{R}^n\\) være åben og lad \\(g : G \\to \\mathbb{R}^n\\) være kontinuer og differentiabel. Antag \\(g\\) er injektiv på \\(G\\) og \\(J_g(x)\\ne0\\) for alle \\(x\\in G\\). For en funktion \\(f\\in\\mathcal{M}\\) med \\(f1_{g(G)}\\) positiv eller integrabel mht. Lebesguemålet gælder: \\(\\int_{g(G)}f(y)dm_n(y)=\\int_G f(g(x))\\vert \\det(J_g(x))\\vert dm_n(x)\\), hvor \\(g(G)\\) er mængden \\(\\{y\\in\\mathbb{R}^n : \\exists x\\in G, g(x)=y\\}\\). Theorem 12.7. (Protter) Lad \\(X=(X_1,...,X_n)\\) have simultan tæthed \\(f\\) og lad \\(g : \\mathbb{R}^n\\to\\mathbb{R}^n\\) være en kontinuer, diffenrentiabel, injektiv funktion med \\(J_g(x)\\ne 0\\). Så har \\(Y=g(X)\\) tæthed \\[ f_Y(y)=f_X(g^{-1}(y))\\vert \\det J_{g^{-1}}(y)\\vert 1_{g(\\mathbb{R}^n)}(y) \\] Corollary 12.1. (Protter) Lad \\(S\\in\\mathcal{B}^n\\) være inddelt af et endeligt indeks \\(I=\\{0,1,...,m\\}\\), så \\(\\cup_{i=0}^m S_i=S\\), \\(S_i\\) parvist disjunkte, og med \\(m_n(S_0)=0\\) og funktionen \\(g : S_i \\to\\mathbb{R}^n\\) være kontinuer, differentiabel, injektiv og \\(J_{g_i^{-1}}(x)\\ne 0\\) for alle \\(i\\in I\\). Lad \\(X\\) være giver som i teorem 12.7. Da har den stokastiske variabel \\(Y=g(X)\\) tæthed \\[ f_Y(y)=\\sum_{i=1}^m f_X(g_i^{-1}(y))\\vert\\det J_{g_i^{-1}}(y)\\vert1_{g_i(S_i)}(y). \\] "],["equivalent-probability-measures.html", "11.10 Equivalent Probability Measures", " 11.10 Equivalent Probability Measures 11.10.1 The Radon-Nikodym Theorem Definition A.50. (Bjork) Consider a measurable space \\((X,\\mathcal{F})\\) on which there are defined two seperate measures \\(\\mu\\) and \\(\\nu\\): If, for all \\(A\\in \\mathcal{F}\\), it holds that \\[ \\mu(A)=0\\ \\Rightarrow\\ \\nu(A)=0,\\tag{A.7} \\] t_hen \\(\\nu\\) is said to be absolutely continuous with respect to \\(\\mu\\) on \\(\\mathcal{F}\\) and we write this as \\(\\nu &lt; &lt; \\mu\\)._ If we have both \\(\\mu &lt;&lt; \\nu\\) and \\(\\nu &lt;&lt; \\mu\\), then \\(\\mu\\) and \\(\\nu\\) said to be equivalent and we write \\(\\mu\\sim \\nu\\). If there exists two events, \\(A\\) and \\(B\\) such that: \\(A\\cup B=X\\), \\(A\\cap B=\\emptyset\\), \\(\\mu(B)=0\\), and \\(\\nu(A)=0\\), then \\(\\nu\\) and \\(\\mu\\) are said to be mutually singular, and we write \\(\\mu\\ \\bot\\ \\nu\\). Theorem A.52. (Bjork) (The Radon-Nikodym Theorem) Consider the measure space \\((X,\\mathcal{F},\\mu)\\), where we assume that \\(\\mu\\) is finite, i.e. that \\(\\mu(X)&lt;\\infty\\). Let \\(\\nu\\) be a measure on \\((X,\\mathcal{F})\\) such that \\(\\nu &lt;&lt;\\mu\\) on \\(\\mathcal{F}\\). Then there exists a non-negative function \\(f : X\\to \\mathbb{R}\\) such that: \\[\\begin{align*} &amp;f\\ \\text{is}\\ \\mathcal{F}\\text{-measurable}\\tag{A.9}\\\\ &amp;\\int_X f(x)\\ d\\mu(x)&lt;\\infty,\\tag{A.10}\\\\ &amp;\\nu(A)=\\int_Af(x)\\ d\\mu(x),\\ \\text{for all}\\ A\\in \\mathcal{F}.\\tag{A.11} \\end{align*}\\] The function \\(f\\) is called the Radon-Nikodym derivative of \\(\\nu\\) w.r.t. \\(\\mu\\). It is uniquely determined \\(\\mu\\)-a.e. and we write \\[ f(x)=\\frac{d\\nu(x)}{d\\mu(x)},\\tag{A.12} \\] or alternatively \\[ d\\nu(x)=f(x)\\ d\\mu(x).\\tag{A.13} \\] 11.10.2 Equivalent Probability Measures Lemma B.38. (Bjork) For two probability measures \\(P\\) and \\(Q\\), the relation \\(P\\sim Q\\) on \\(\\mathcal{F}\\) holds if and only if \\(P(A)=1\\) if and only if \\(Q(A)=1\\) for all \\(A\\in\\mathcal{F}\\). Proposition B.39. (Bjork) Assume that \\(Q &lt;&lt; P\\) on \\(\\mathcal{F}\\) and that \\(\\mathcal{G}\\subseteq \\mathcal{F}\\). Then the Radon-Nikodym derivatives \\(L^{\\mathcal{F}}\\) and \\(L^{\\mathcal{G}}\\) are related by \\[ L^{\\mathcal{G}}=E^P[L^{\\mathcal{F}}\\ \\vert\\ \\mathcal{G}].\\tag{B.17} \\] Proposition B.41. (Bjork) (Bayes’ Theorem) Assume that \\(X\\) is a random variable on \\((\\Omega, \\mathcal{F},P)\\), and let \\(Q\\) be another probability measure on \\((\\Omega,\\mathcal{F})\\) the Radon-Nikodym derivative \\[ L=\\frac{d Q}{dP} \\] on \\(\\mathcal{F}\\). Assume that \\(X\\in L^1(\\Omega,\\mathcal{F},Q)\\) and \\(\\mathcal{G}\\) is a sigma-algebra with \\(\\mathcal{G}\\subseteq \\mathcal{F}\\). Then \\[ E^Q[X\\ \\vert\\ \\mathcal{G}]=\\frac{E^P[L\\cdot X\\ \\vert\\ \\mathcal{G}]}{E^P[L\\ \\vert\\ \\mathcal{G}]},\\ Q-a.s.\\tag{B.18} \\] 11.10.3 Likelihood processes Proposition C.12. (Bjork) Consider a filtered probability space \\((\\Omega, \\mathcal{F},P,\\mathcal{F}_t)\\) on a compact interval \\([0,T]\\). Suppose \\(L_T\\) is some non-negative integrable random variable in \\(\\mathcal{F}_T\\). We can then define a new measure \\(Q\\) on \\(\\mathcal{F}_T\\) by setting \\[ dQ=L_T\\ dP \\] on \\(\\mathcal{F}_T\\) and if \\(E^P[L_T]=1\\) the measure \\(Q\\) will also be a probability measure. The likelihood process \\(L\\), defined by \\[ L_t=\\frac{dQ}{dP},\\ on\\ \\mathcal{F}_t,\\tag{C.8} \\] is a \\((P,\\mathcal{F}_t)\\)-martingale. Proposition C.13. (Bjork) A process \\(M\\) is a \\(Q\\)-martingale if and only if the process \\(L\\cdot M\\) is a \\(P\\)-martingale. "],["random-variables-1.html", "Chapter 12 Random Variables ", " Chapter 12 Random Variables "],["introduction-1.html", "12.1 Introduction", " 12.1 Introduction Definition 1.1. (Hansen) A real-valued random variable \\(X\\) on a probability space \\((\\Omega, \\mathbb{F},P)\\) is a measurable map \\(X : (\\Omega,\\mathbb{F})\\to (\\mathbb{R},\\mathbb{B})\\). We never specify the background space \\((\\Omega, \\mathbb{F},P)\\) however we always assume \\(X\\) is \\(\\mathbb{F}-\\mathbb{B}\\) measurable. This assumption implies \\((X\\in A)\\in \\mathbb{F}\\) for every \\(A\\in \\mathbb{B}\\). We may want to show measurability for constructed variables and so it surfises to show measurability for generaters for \\(\\mathbb{B}\\) such as checking \\((X\\le a)\\in\\mathbb{F}\\) for every \\(a\\in\\mathbb{R}\\). Definition 1.2. (Hansen) The distribution of a real-valued random variable \\(X\\), defined on a probability space \\((\\Omega,\\mathbb{F},P)\\), is the collection of probability values \\[\\begin{align*} P(X\\in A)\\hspace{15pt}\\text{for}\\ A\\in \\mathbb{B}.\\tag{1.3} \\end{align*}\\] In other words: the distribution of \\(X\\) is the image measure \\(X(P)\\) on \\((\\mathbb{R},\\mathbb{B})\\). Lemma 1.3. (Hansen) Let \\(X\\) and \\(X&#39;\\) be two real-valued random variables on a probability space \\((\\Omega,\\mathbb{F},P)\\). If \\[\\begin{align*} P(X=X&#39;)=1 \\end{align*}\\] then \\(X\\) and \\(X&#39;\\) has the same distribution. An often used way of summarizing the distribution is through the distribution function \\(F(x)=P(X\\le x)\\) for some \\(x\\in\\mathbb{R}\\). Definition 1.4. (Hansen) A real-valued random variable \\(X\\) has a discrete distribution if there is a countable set \\(S\\subset\\mathbb{R}\\) such that \\(P(X\\in S)=1\\). Usually \\(S\\) is one of \\(\\mathbb{N},\\mathbb{Z},\\mathbb{Q}\\) or a subset of these. We may in the discrete case define the distribution by the point probabilities \\(P(X=x)=p(x)\\) for \\(x\\in S\\). Definition 1.5. (Hansen) A real-valued random variable \\(X\\) has a distribution with density \\(f : \\mathbb{R}\\to [0,\\infty)\\) if \\[\\begin{align*} P(X\\in A)=\\int_Af(x)dx\\hspace{15pt}\\text{for}\\ A\\in \\mathbb{B}.\\tag{1.5} \\end{align*}\\] If this is the case we will write \\(X(P)=f\\cdot m\\) or \\(X\\sim f\\cdot m\\). Definition 1.6. (Hansen) A real-valued random variable \\(X\\) defined on a probability space \\((\\Omega, \\mathbb{F},P)\\) is said to have \\(p\\)’th moment for som \\(p&gt;0\\) if \\[\\begin{align*} E\\vert X\\vert^p&lt;\\infty\\tag{1.12} \\end{align*}\\] The collection of all variables that satisfies (1.12) is denoted by \\(\\mathcal{L}^p(\\Omega,\\mathbb{F},P)\\). Recall the definition of the expectation of \\(X\\) by \\[\\begin{align*} E\\, X=\\int XdP \\in R\\cup \\{-\\infty,+\\infty\\}.\\tag{1.11} \\end{align*}\\] We recall that for any measurable function \\(f : \\mathbb{R}\\to \\mathbb{R}\\) that are continuous on a set \\(A\\in\\mathbb{B}\\) such that \\(P(X\\in A)=1\\) we may change variable simply by computing \\[\\begin{align*} E\\, f(X)=\\int f\\circ XdP=\\int f(x)dX(P)(x). \\end{align*}\\] Lemma 1.7. (Hansen) (Markov’s inequality) Let \\(X\\) be a non-negative random variable. For any \\(c&gt;0\\) it holds that \\[\\begin{align*} P(X\\ge c)\\le \\frac{E\\, X}{c}\\left(\\le \\frac{E\\ X^n}{c^n}\\text{ or }\\le \\frac{E\\left(\\varphi(X)\\right)}{\\varphi(c)}\\right).\\tag{1.14} \\end{align*}\\] for some \\(\\varphi\\) non-negative monotome increasing function. Some other versions of Markov’s inequality can be found in the form of Chebyshev’s inequality, Chebyshev-Cantelli’s inequality or Jensen’s inequality repectively: Let \\(X\\) be a real-valued random variable in \\(\\mathcal{L}^2(\\Omega,\\mathbb{F},P)\\) it holds for any \\(\\varepsilon&gt;0\\). \\[\\begin{align*} P\\left(\\vert X-E\\, X\\vert \\ge \\varepsilon\\right)&amp;\\le \\frac{V\\, X}{\\varepsilon^2}\\tag{1.15}\\\\ P\\left( X-E\\, X \\ge \\varepsilon\\right)&amp;\\le \\frac{V\\, X}{V\\, X+\\varepsilon^2}\\tag{prob: 1.13(c)}\\\\ \\varphi\\left(E\\ X\\right)&amp;\\le E\\left( \\varphi(X)\\right) \\end{align*}\\] for some convex function \\(\\varphi\\). Lemma 1.8. (Hansen) Let \\(X\\) be a non-negative random variable. It holds that \\[\\begin{align*} E\\, X=\\int_0^\\infty P(X&gt;t)dt.\\tag{1.16} \\end{align*}\\] where the integral on the right hand side is with respect to Lebesgue measure. Definition 1.9. (Hansen) The joint distribution of real-valued random variables \\(X_1,...,X_k\\), defined on a probability space \\((\\Omega, \\mathbb{F},P)\\), is the collection of probability values \\[\\begin{align*} P(\\mathbf{X}\\in A)\\hspace{15pt}\\text{for}\\ A\\in\\mathbb{B}_k.\\tag{1.21} \\end{align*}\\] In other words: the joint distribution of \\(X_1,...,X_k\\) (or simply: the distribution of \\(\\mathbf{X}\\)) is the image measure \\(\\mathbf{X}(P)\\) on \\(\\left(\\mathbb{R}^k,\\mathbb{B}_k\\right)\\). Definition 1.11. (Hansen) Real-valued random variables \\(X_1,...,X_k\\), defined on a probability space \\((\\Omega, \\mathbb{F},P)\\), are jointly independent if \\[\\begin{align*} P\\left(X_1\\in A_1,...,X_k\\in A_k\\right)=\\prod_{i=1}^kP(X_i\\in A_i)\\hspace{15pt}\\text{for}\\ A_1,...,A_k\\in\\mathbb{B}.\\tag{1.23} \\end{align*}\\] In other words: the variables are independent if the joint distribution \\(\\mathbf{X}(P)\\) equals the product measure \\(X_1(P)\\otimes ... \\otimes X_k(P)\\). Theorem 1.12. (Hansen) Let \\(X_1,...,X_k\\) be real-valued random variables defined on a probability space \\((\\Omega, \\mathbb{F},P)\\). If the variables are independent and if \\(E\\vert X_i\\vert&lt;\\infty\\) for \\(i=1,...,k\\), then the product \\(X_1\\cdot ...\\cdot X_k\\) has first moment and \\[\\begin{align*} E\\left(X_1\\cdot ... \\cdot X_k\\right)=\\prod_{i=1}^kE\\, X_i\\tag{1.24} \\end{align*}\\] The equality only holds for two independent variables. However the Cauchy-Schwarz inequality which closely resembles (1.24) holds wether or not \\(X\\) or \\(Y\\) are independent: \\[\\begin{align*} \\left(E\\vert XY\\vert\\right)^2\\le E\\, X^2\\, E\\, Y^2\\text{ or }E\\vert XY\\vert \\le \\sqrt{E\\ X^2}\\sqrt{E\\ Y^2}.\\tag{1.25} \\end{align*}\\] Furthermore the theorem give rise to a measure for dependence i.e. the covariance between two variables \\(X\\) and \\(Y\\) \\[\\begin{align*} \\text{Cov}(X,Y)=E\\left((X-E\\,X)(Y-E\\,Y)\\right)=E(XY)-(E\\, X)(E\\, Y)\\tag{1.26} \\end{align*}\\] with Cov\\((X,Y)\\ne 0\\) if and only if \\(X\\) and \\(Y\\) are dependent. With Cov\\((X,Y)=0\\) the test is inconlusive. However independence implies Cov\\((X,Y)=0\\). "],["conditional-expectation.html", "12.2 Conditional expectation", " 12.2 Conditional expectation The theory of conditional expectation is well-known from courses on the bachelor. Because of this we will only summarise the most important results. We consider a background space \\((\\Omega,\\mathcal{F},P)\\) and a sub-sigma algebra \\(\\mathcal{G}\\subseteq \\mathcal{F}\\). We assume that some stochastic variable is \\(\\mathcal{F}\\)-measurable, that is the mapping \\(X : (\\Omega,\\mathcal{F},P) \\to (\\mathbb{R},\\mathbb{B},m)\\) is \\(\\mathcal{F}-\\mathbb{B}\\)-measurable i.e. \\(\\forall B\\in\\mathbb{B} : \\{X\\in B\\}\\in\\mathcal{F}\\). For some random variable \\(Z\\) defined on the subspace \\((\\Omega,\\mathcal{G},P)\\), we say that \\(Z\\) is the conditional expectation of \\(X\\) given \\(\\mathcal{G}\\) if \\[ \\forall G\\in\\mathcal{G} : \\int_G Z(\\omega)\\ dP(\\omega)=\\int_G X(\\omega)\\ dP(\\omega). \\] This fact is summed up in the definition below. Definition B.27. (Bjork) (Conditional expectation) Let \\((\\Omega,\\mathcal{F},P)\\) be a probability space and \\(X\\) a random variable in \\(L^1(\\Omega,\\mathcal{F},P)\\) (\\(\\vert X\\vert\\) is integrable). Let furthermore \\(\\mathcal{G}\\) be a sigma-algebra such that \\(\\mathcal{G}\\subseteq \\mathcal{F}\\). If \\(Z\\) is a random variable with the properties that: \\(Z\\) is \\(\\mathcal{G}\\)-measurable. For every \\(G\\in\\mathcal{G}\\) it holds that \\[\\int_G Z(\\omega)\\ dP(\\omega)=\\int_G X(\\omega)\\ dP(\\omega).\\tag{B.5}\\] Then we say that \\(Z\\) is the conditional expectation of \\(X\\) given the sigma-algebra \\(\\mathcal{G}\\). In that case we denote \\(Z\\) by the symbol \\(E[X\\ \\vert\\ \\mathcal{G}]\\). We see that from the above it always holds that \\(X\\) satisfies (ii). It does not, however, always hold that \\(X\\) is \\(\\mathcal{G}\\)-measurable. Given this fact it is not trivial that a random variable \\(E[X\\ \\vert\\ \\mathcal{G}]\\) even exists. This nontriviality is fortunatly resolved by the Radon-Nikodym theorem. Theorem B.28. (Bjork) (Existance and uniqueness of Conditional expectation) Let \\((\\Omega,\\mathcal{F},P)\\), \\(X\\) and \\(\\mathcal{G}\\) be given as in the definition above. Then the following holds: There will always exist a random variable \\(Z\\) satisfying conditions (i)-(ii) above. The variable \\(Z\\) is unique, i.e. if both \\(Y\\) and \\(Z\\) satisfy (i)-(ii) then \\(Y=Z\\) \\(P\\)-a.s. This result ensures that we may condition on any sigma-algebra for instance \\(\\mathcal{G}=\\sigma(Y)\\) in that case we (pure notation) write \\[ E[X\\ \\vert\\ \\sigma(Y)]=E[X\\ \\vert\\ Y],\\hspace{20pt}\\sigma(Y)=\\sigma\\left(\\left\\{ Y\\in A,\\ A\\in\\mathbb{B}\\right\\}\\right). \\] In the above \\(\\sigma(Y)\\) is simply the smallest sigma-algebra containing all the pre-images of \\(Y\\), that is the smallest sigma-algebra making \\(Y\\) measurable! Giving this foundation there are a few properties conditional expectation have which is rather useful (for instance the tower property). Below we assume: Let \\((\\Omega,\\mathcal{F},P)\\) be a probability space and \\(X,Y\\) be random variables in \\(L^1(\\Omega,\\mathcal{F},P)\\). Proposition B.29. (Monotinicity/Linearity of Conditional expectation) The following holds: \\[X\\le Y\\ \\Rightarrow\\ E[X\\ \\vert\\ \\mathcal{G}]\\le E[Y\\ \\vert\\ \\mathcal{G}],\\hspace{20pt}P-\\text{a.s.}\\tag{B.6}\\] \\[E[\\alpha X + \\beta Y\\ \\vert\\ \\mathcal{G}]=\\alpha E[X\\ \\vert\\ \\mathcal{G}]+ \\beta E[Y\\ \\vert\\ \\mathcal{G}],\\hspace{20pt}\\forall \\alpha,\\beta\\in\\mathbb{R}.\\tag{B.7}\\] Proposition B.30. (Bjork) (Tower property) Assume that it holds that \\(\\mathcal{H}\\subseteq\\mathcal{G}\\subseteq\\mathcal{F}\\). Then the following hold: \\[E[E[X\\vert \\mathcal{G}]\\vert\\mathcal{H}]=E[X\\vert \\mathcal{H}],\\tag{B.8}\\] \\[E[X]=E[E[X\\vert \\mathcal{G}]].\\tag{B.9}\\] Proposition B.31. (Bjork) Assume \\(X\\) is \\(\\mathcal{G}\\) and that both \\(X,Y\\) and \\(XY\\) are in \\(L^1\\) (only assuming \\(Y\\) is \\(\\mathcal{F}\\)-measurable), then \\[E[X\\vert\\mathcal{G}]=X,\\hspace{20pt}P-\\text{a.s.}\\tag{B.11}\\] \\[E[XY\\vert\\mathcal{G}]=XE[Y\\vert\\mathcal{G}],\\hspace{20pt}P-\\text{a.s.}\\tag{B.12}\\] Proposition B.32. (Bjork) (Jensen inequality) Let \\(f:\\mathbb{R}\\to\\mathbb{R}\\) be a convex (measurable) function and assume \\(f(X)\\) is in \\(L^1\\). Then \\[f(E[X\\vert\\mathcal{G}])\\le E[f(X)\\vert\\mathcal{G}],\\hspace{20pt}P-\\text{a.s.}\\] Proposition B.37. (Bjork) Let \\((\\Omega,\\mathcal{F},P)\\) be a given probability space, let \\(\\mathcal{G}\\) be a sub-sigma-algebra of \\(\\mathcal{F}\\), and let \\(X\\) be a square integrable random variable. Consider the problem of minimizing \\[E\\left[(X-Z)^2\\right]\\] where \\(Z\\) is allowed to vary over the class of all square integrable \\(\\mathcal{G}\\) measurable random variables. The optimal solution \\(\\hat{Z}\\) is then given by. \\[\\hat{Z}=E[X\\vert\\mathcal{G}].\\] Proof. Let \\(X\\in L^2(\\Omega,\\mathcal{F},P)\\) be a random variable. Now consider an arbitrary \\(Z\\in L^2(\\Omega,\\mathcal{G},P)\\). Recall that \\(\\mathcal{G}\\subset \\mathcal{F}\\) and so \\(X\\) is also in \\(Z\\in L^2(\\Omega,\\mathcal{G},P)\\), as it is bothe square integrable and \\(\\mathcal{G}\\)-measurable. Then \\[E\\left[Z\\cdot(X-E[X\\vert\\mathcal{G}])\\right]=E\\left[Z\\cdot X\\right]-E\\left[Z\\cdot E[X\\vert\\mathcal{G}]\\right].\\] Then by using the law of total expectation and secondly that \\(Z\\) is \\(\\mathcal{G}\\)-measurable we have that \\[E\\left[Z\\cdot X\\right]=E\\left[E[Z\\cdot X\\vert\\mathcal{G}]\\right]=E\\left[Z\\cdot E[ X\\vert\\mathcal{G}]\\right].\\] Combining the two equations gives the desired result. Obviously, we have that \\[X-Z=X-Z+E[X\\vert\\mathcal{G}]-E[X\\vert\\mathcal{G}]=(X-E[X\\vert\\mathcal{G}])+(E[X\\vert\\mathcal{G}]-Z).\\] Then squaring the terms gives \\[(X-Z)^2=(X-E[X\\vert\\mathcal{G}])^2+(E[X\\vert\\mathcal{G}]-Z)^2+2(X-E[X\\vert\\mathcal{G}])(E[X\\vert\\mathcal{G}]-Z)\\] Taking expectation on each side and using linearity of the expectation we have that \\[E[(X-Z)^2]=E\\left[(X-E[X\\vert\\mathcal{G}])^2\\right]+E\\left[(E[X\\vert\\mathcal{G}]-Z)^2\\right]+2E\\left[(X-E[X\\vert\\mathcal{G}])(E[X\\vert\\mathcal{G}]-Z)\\right].\\] We can now use that \\(E[X\\vert\\mathcal{G}]-Z\\) is \\(\\mathcal{G}\\)-measurable with the above result on the last term. \\[E[(X-Z)^2]=E\\left[(X-E[X\\vert\\mathcal{G}])^2\\right]+E\\left[(E[X\\vert\\mathcal{G}]-Z)^2\\right].\\] Now since \\(X\\) is given the term \\(E\\left[(X-E[X\\vert\\mathcal{G}])^2\\right]\\) is simply a constant not depending on the choice og \\(Z\\). The optimal choice of \\(Z\\) is then \\(E[X\\vert\\mathcal{G}]\\) since this minimizes the second term. The statement is then proved. "],["independence.html", "12.3 Independence", " 12.3 Independence Definition 3.1. (Hansen) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space. Two events \\(A,B\\in\\mathbb{F}\\) are independent if \\[\\begin{align*} P(A\\cap B)=P(A)P(B)\\tag{3.1} \\end{align*}\\] Definition 3.4. (Hansen) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space and let \\(\\mathbb{G},\\mathbb{H}\\subset \\mathbb{F}\\) be two classes of measurable sets. We sat that \\(\\mathbb{G}\\) and \\(\\mathbb{H}\\) are independent, written \\(\\mathbb{G}\\perp \\!\\!\\! \\perp\\mathbb{H}\\), if \\[\\begin{align*} P(A\\cap B)=P(A)P(B)\\hspace{15pt}\\text{for all}\\ A\\in\\mathbb{G},B\\in\\mathbb{H}.\\tag{3.2} \\end{align*}\\] Lemma 3.5. (Hansen) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space and let \\(\\mathbb{G},\\mathbb{H}\\subset \\mathbb{F}\\) be two classes of measurable sets. Let \\(\\mathbb{G}_1\\subset \\mathbb{G}\\) and \\(\\mathbb{H}_1\\subset\\mathbb{H}\\) be two subclasses. If \\(\\mathbb{G}\\perp \\!\\!\\! \\perp \\mathbb{H}\\) then it holds that \\(\\mathbb{G}_1\\perp \\!\\!\\! \\perp \\mathbb{H}_1\\). Definition 3.6. (Hansen) A class \\(\\mathbb{H}\\) of subsets of \\(\\Omega\\) is a Dynkin class if \\(\\Omega \\in\\mathbb{H}\\), \\(A,B\\in\\mathbb{H},A\\subset B\\hspace{15pt}\\Rightarrow\\hspace{15pt}B\\setminus A\\in\\mathbb{H}\\), \\(A_1,A_2,...\\in\\mathbb{H},A_1\\subset A_2\\subset ...\\hspace{15pt}\\Rightarrow\\hspace{15pt}\\bigcup_{n=1}^\\infty A_n\\in\\mathbb{H}\\). Lemma 3.7. (Hansen) (Dynkin) Let \\(\\mathbb{D}\\subset \\mathbb{H}_0\\subset \\mathbb{H}\\) be three nested classes of subsets of \\(\\Omega\\). if \\(\\sigma(\\mathbb{D})=\\mathbb{H}\\), \\(A,B\\in\\mathbb{D}\\hspace{10pt}\\Rightarrow\\hspace{10pt}A\\cap B\\in\\mathbb{D}\\) \\(\\mathbb{H}_0\\) is a Dynkin class. then it holds that \\(\\mathbb{H}_0=\\mathbb{H}\\). Lemma 3.8. (Hansen) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space, and let \\(A\\in\\mathbb{F}\\) be a fixed event. The class \\[\\begin{align*} \\mathbb{H}=\\{B\\in \\mathbb{F}\\ \\vert\\ A\\perp \\!\\!\\! \\perp B\\} \\end{align*}\\] is a Dynkin class. Theorem 3.9. (Hansen) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space, and let \\(\\mathbb{G}_1,\\mathbb{G}_2\\subset \\mathbb{F}\\) be two sigma-algebras. Let \\(\\mathbb{D}_1\\) and \\(\\mathbb{D}_2\\) be two classes such that \\(\\sigma(\\mathbb{D}_i)=\\mathbb{G}_i\\) for \\(i=1,2\\). If both \\(\\mathbb{D}_1\\) and \\(\\mathbb{D}_2\\) are \\(\\cap\\)-stable then it holds that \\[\\begin{align*} \\mathbb{D}_1\\perp \\!\\!\\! \\perp\\mathbb{D}_2\\hspace{10pt}\\Rightarrow \\hspace{10pt} \\mathbb{G}_1\\perp \\!\\!\\! \\perp\\mathbb{G}_2. \\end{align*}\\] Definition 3.10. (Hansen) Two real-valued random variable \\(X\\) and \\(Y\\) on a background space \\((\\Omega,\\mathbb{F},P)\\) are , written \\(X\\perp \\!\\!\\! \\perp Y\\), if the corresponding sigma-algebras \\(\\sigma(X)\\) and \\(\\sigma(Y)\\) are independent. Definition 3.15. (Hansen) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space, and let \\(\\mathbb{G}_1,...,\\mathbb{G}_n\\subset \\mathbb{F}\\) be finitely many classes of measurable sets. We say that \\(\\mathbb{G}_1,...,\\mathbb{G}_n\\) are jointly independent, written \\(\\mathbb{G}_1\\perp \\!\\!\\! \\perp...\\perp \\!\\!\\! \\perp\\mathbb{G}_n\\), if \\[\\begin{align*} P(A_1\\cap ...\\cap A_n=\\prod_{i=1}^nP(A_i)\\hspace{15pt}\\text{for }A_1\\in\\mathbb{G}_1,...,A_n\\in\\mathbb{G}_n.\\tag{3.8} \\end{align*}\\] Lemma 3.16. (Hansen) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space, and let \\(\\mathbb{G}_1,...,\\mathbb{G}_n\\subset \\mathbb{F}\\) be finitely many classes of measurable sets. It holds that \\[\\begin{align*} \\mathbb{G}_1\\perp \\!\\!\\! \\perp...\\perp \\!\\!\\! \\perp\\mathbb{G}_n\\hspace{10pt}\\Rightarrow \\hspace{10pt} \\mathbb{G}_1\\perp \\!\\!\\! \\perp...\\perp \\!\\!\\! \\perp\\mathbb{G}_{n-1} \\end{align*}\\] provided that \\(\\Omega\\in\\mathbb{G}_n\\). Theorem 3.17. (Hansen) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space, and let \\(\\mathbb{G}_1,...,\\mathbb{G}_n\\subset \\mathbb{F}\\) be sigma-algebras. Let \\(\\mathbb{D}_1,...,\\mathbb{D}_n\\) be classes such that \\(\\sigma(\\mathbb{D}_i)=\\mathbb{G}_1\\) for \\(i=1,...,n\\). Suppose that for all lengths \\(k=2,...,n\\) an all choices of indices \\(\\le j_1&lt;...&lt;j_k\\le n\\) it holds that \\[\\begin{align*} \\mathbb{D}_{j_1}\\perp \\!\\!\\! \\perp ... \\perp \\!\\!\\! \\perp \\mathbb{D}_{j_k}\\tag{3.9} \\end{align*}\\] If all the generators \\(\\mathbb{D}_i\\) are \\(\\cap\\)-stable, then it holds that \\(\\mathbb{G}_1\\perp \\!\\!\\! \\perp ... \\perp \\!\\!\\! \\perp \\mathbb{G}_n\\). Lemma 3.18. (Hansen) (Grouping) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space, and let \\(\\mathbb{G}_1,...,\\mathbb{G}_n\\subset \\mathbb{F}\\) be sigma-algebras. It holds that \\[\\begin{align*} \\mathbb{G}_1\\perp \\!\\!\\! \\perp ... \\perp \\!\\!\\! \\perp \\mathbb{G}_n \\hspace{10pt}\\Rightarrow \\hspace{10pt} \\mathbb{G}_1\\perp \\!\\!\\! \\perp ... \\perp \\!\\!\\! \\perp \\mathbb{G}_{n-2}\\perp \\!\\!\\! \\perp\\sigma(\\mathbb{G}_{n-1},\\mathbb{G}_n). \\end{align*}\\] Definition 3.19. (Hansen) The real-valued random variables \\(X_1,...,X_n\\) on a background space \\((\\Omega,\\mathbb{F},P)\\) are jointly independent, written \\(X_1\\perp \\!\\!\\! \\perp ... \\perp \\!\\!\\! \\perp X_n\\), if the corresponding sigma-algebras \\(\\sigma(X_1),...,\\sigma(X_n)\\) are jointly independent. Definition 3.20. (Hansen) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space, and let \\((\\mathbb{G}_i)_{i\\in I}\\) be a family of classes of measurable sets. We say that the family \\((\\mathbb{G}_i)_{i\\in I}\\) is jointly independent if any finite subfamily is jointly independent. Theorem 3.21. (Hansen) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space, and let \\(\\mathbb{G}_1,\\mathbb{G}_2,...\\subset \\mathbb{F}\\) be sigma-algebras. Let \\(\\mathbb{D}_1,\\mathbb{D}_2,...\\) be classes such that \\(\\sigma(\\mathbb{D}_n)=\\mathbb{G}_n\\) for all \\(n\\in\\mathbb{N}\\). Suppose that for all lengths \\(k\\in\\mathbb{N}\\) and all choices of indices \\(1\\le j_1&lt; ... &lt; j_k\\) it holds that \\[\\begin{align*} \\mathbb{D}_{j_1}\\perp \\!\\!\\! \\perp ... \\perp \\!\\!\\! \\perp \\mathbb{D}_{j_k}.\\tag{3.14} \\end{align*}\\] If all the generators \\(\\mathbb{D}_n\\) are \\(\\cap\\)-stable, then it holds that \\(\\mathbb{G}_1\\perp \\!\\!\\! \\perp \\mathbb{G}_2 \\perp \\!\\!\\! \\perp ...\\). Lemma 3.22. (Hansen) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space, and let \\(\\mathbb{G}_1,\\mathbb{G}_2,...\\subset \\mathbb{F}\\) be sigma-algebras. It holds that \\[\\begin{align*} \\mathbb{G}_1\\perp \\!\\!\\! \\perp \\mathbb{G}_2 \\perp \\!\\!\\! \\perp ... \\hspace{10pt}\\Rightarrow \\hspace{10pt} \\mathbb{G}_1\\perp \\!\\!\\! \\perp ... \\perp \\!\\!\\! \\perp \\mathbb{G}_n \\perp \\!\\!\\! \\perp \\sigma(\\mathbb{G}_{n+1},\\mathbb{G}_{n+2}, ... ). \\end{align*}\\] Definition 3.23. (Hansen) The real-valued random variables \\((X_i)_{i\\in I}\\) on a background space \\((\\Omega,\\mathbb{F},P)\\) are jointly independent if the corresponding sigma-algebras \\((\\sigma(X_i))_{i\\in I}\\) are jointly independent. Definition 3.28. (Hansen) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space. A sigma-algebra \\(\\mathbb{G}\\subset \\mathbb{F}\\) satisfies a zero-one law if \\[\\begin{align*} P(A)\\in\\{0,1\\}\\hspace{15pt}\\text{for all}\\ A\\in\\mathbb{G}. \\end{align*}\\] Theorem 3.29. (Hansen) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space and let \\(\\mathbb{G}\\subset \\mathbb{F}\\) be a sigma-algebra. The following three conditions are equivalent: For any sigma-algebra \\(\\mathbb{H}\\subset \\mathbb{F}\\) it holds that \\(\\mathbb{G} \\perp \\!\\!\\! \\perp \\mathbb{H}\\), It holds that \\(\\mathbb{G}\\perp \\!\\!\\! \\perp\\mathbb{G}\\), \\(\\mathbb{G}\\) satisfies a 0-1 law. Definition 3.30. (Hansen) Let \\(X_1,X_2,...\\) be real-valued random variables on a background space \\((\\Omega,\\mathbb{F},P)\\). The tail sigma-algebra of the process is defined as \\[\\begin{align*} \\mathbb{J}(X_1,X_2,...)=\\bigcap_{n=1}^\\infty \\sigma(X_n,X_{n+1}, ... ). \\end{align*}\\] Theorem 3.32. (Hansen) (Kolmogorov’s zero-one law) Let \\(X_1,X_2,...\\) be real-valued random variables on a background space \\((\\Omega,\\mathbb{F},P)\\). If \\(X_1 \\perp \\!\\!\\! \\perp X_2 \\perp \\!\\!\\! \\perp ...\\) then the tail-algebra \\(\\mathbb{J}(X_1,X_2,...)\\) satisfies a 0-1 law. Lemma 3.35. (Hansen) (2nd half of Borel-Cantelli) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space, and let \\(A_1,A_2,...\\) be a sequence of \\(\\mathbb{F}\\)-measurable sets. If \\(A_1 \\perp \\!\\!\\! \\perp A_2 \\perp \\!\\!\\! \\perp ...\\) then it holds that \\[\\begin{align*} \\sum_{n=1}^\\infty P(A_n)&lt;\\infty \\iff P(A_n\\text{ i.o.})=0. \\end{align*}\\] "],["moment-generating-function.html", "12.4 Moment generating function", " 12.4 Moment generating function Let \\(X\\) be a random variable with distribution function \\(F(x)=P(X\\le x)\\) and \\(Y\\) be a random variable with distribution function \\(G(y)=P(Y\\le y)\\). Definition. (Ex. FinKont) The moment generating function or Laplace transform of \\(X\\) is \\[\\psi_X(\\lambda)=E\\left[e^{\\lambda X}\\right]=\\int_{-\\infty}^\\infty e^{\\lambda x}dF(x)\\] provided the expectation is finite for \\(\\vert\\lambda\\vert&lt;h\\) for some \\(h&gt;0\\). The MGF uniquely determine the distribution of a random variable, due to the following result. Theorem. (Ex. FinKont) (Uniqueness) If \\(\\psi_X(\\lambda)=\\psi_Y(\\lambda)\\) when \\(\\vert\\lambda\\vert&lt;h\\) for some \\(h&gt;0\\), then \\(X\\) and \\(Y\\) has the same distribution, that is, \\(F=G\\). There is also the following result of independence for Moment generating functions. Theorem. (Ex. FinKont) (Independence) If \\[E\\left[e^{\\lambda_1X+\\lambda_2Y}\\right]=\\psi_X(\\lambda_1)\\psi_Y(\\lambda_2)\\] for \\(\\vert\\lambda_i\\vert&lt;h\\) for \\(i=1,2\\) for some \\(h&gt;0\\), then \\(X\\) and \\(Y\\) are independent random variables. "],["standard-distributions.html", "12.5 Standard distributions", " 12.5 Standard distributions 12.5.1 Normal disribution The following gives a comprehensive table of the standard some properties. Description Symbol Normal distribution Definition \\(\\sim\\) \\(X\\sim\\mathcal{N}(\\mu,\\sigma^2)\\) Parameters \\(\\theta\\in \\Theta\\) \\(\\theta=(\\mu,\\sigma^2)\\in \\mathbb{R}\\times \\mathbb{R}_+\\) Support \\(\\text{Im}(X)\\) \\(x\\in \\mathbb{R}\\) Density \\(f\\) \\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\left(\\frac{x-\\mu}{\\sqrt{2\\sigma^2}}\\right)^2}\\) Distribution \\(F\\) \\(\\frac{1}{2}\\left(1+N\\left(\\frac{x-\\mu}{\\sqrt{2\\sigma^2}}\\right)\\right)\\) Mean value \\(E[X]\\) \\(\\mu\\) Variance \\(\\text{Var}(X)\\) \\(\\sigma^2\\) MGF* \\(\\psi_X=E[e^{\\lambda X}]\\) \\(e^{\\mu \\lambda+\\frac{1}{2}\\lambda^2\\sigma^2}\\) Characteristic function \\(\\varphi_X(t)=E[e^{itX}]\\) \\(e^{it\\mu-\\frac{1}{2}\\sigma^2 t^2}\\) In the table above we used the abbreviations: *MGF = Moment Generating function. We also used the shorthand: \\(N\\) being the distribution of a standard normal distributed variable \\(\\mathcal{N}(0,1)\\). "],["discrete-time-stochastic-processes.html", "Chapter 13 Discrete Time Stochastic Processes ", " Chapter 13 Discrete Time Stochastic Processes "],["convergence-concepts.html", "13.1 Convergence concepts", " 13.1 Convergence concepts We start this chapter by refering to a sequence \\(X_1,X_2,...\\) of real-valued random variables as a proces. Consider the event \\((X_n\\to X)=\\left\\{\\omega\\in\\Omega\\ \\vert\\ X_m(\\omega)\\to X(\\omega)\\ \\text{for}\\ n\\to \\infty\\right\\}\\). We want to study such convergence in detail. However first we check measurability. Consider a family \\((A_i)_{i\\in I}\\subset \\Omega\\) and observe that \\[\\begin{align*} \\Big\\{\\omega\\in \\Omega\\ \\vert\\ \\forall i\\in I : \\omega \\in A_i\\Big\\}=\\bigcap_{i\\in I} A_i,\\tag{2.1}\\\\ \\Big\\{\\omega\\in \\Omega\\ \\vert\\ \\exists i\\in I : \\omega \\in A_i\\Big\\}=\\bigcup_{i\\in I} A_i.\\tag{2.2} \\end{align*}\\] From the standard \\(N,\\varepsilon\\) definition of a convergent sequence \\((x_n)_{n\\in \\mathbb{N}}\\) we may formulate this convergens in the stochastic setting: \\[\\begin{align*} (X_n\\to X)&amp;=\\Big\\{\\omega\\in \\Omega\\ \\vert\\ \\forall\\varepsilon&gt;0 \\exists N\\in \\mathbb{N} \\forall n\\ge N : \\vert X_n(\\omega)-X(\\omega)\\vert &lt;\\varepsilon\\Big\\}\\\\ &amp;=\\Big(\\forall\\varepsilon&gt;0 \\exists N\\in \\mathbb{N} \\forall n\\ge N : \\vert X_n-X\\vert &lt;\\varepsilon\\Big)\\\\ &amp;=\\bigcap_{\\epsilon\\in \\mathbb{R}^+}\\Big(\\exists N\\in \\mathbb{N} \\forall n\\ge N : \\vert X_n-X\\vert&lt;\\varepsilon\\Big)\\\\ &amp;=\\bigcap_{\\epsilon\\in \\mathbb{R}^+}\\bigcup_{N=1}^\\infty\\Big( \\forall n\\ge N : \\vert X_n-X\\vert&lt;\\varepsilon\\Big)\\\\ &amp;=\\bigcap_{\\epsilon\\in \\mathbb{R}^+}\\bigcup_{N=1}^\\infty\\bigcap_{n=N}^\\infty\\Big( \\vert X_n-X\\vert&lt;\\varepsilon\\Big)\\in \\mathbb{F}\\hspace{15pt}\\text{for all}\\ \\varepsilon&gt;0. \\end{align*}\\] Hence \\((X_n\\to X)\\) lies in \\(\\mathbb{F}\\) since \\((\\vert X_n-X\\vert &lt;\\varepsilon)\\) lies in \\(\\mathbb{F}\\) since \\(X_n-X\\) is measurable. Lemma 2.1. (Hansen) Let \\(X,X_1,X_2,...\\) be real-valued random variables on \\((\\Omega, \\mathbb{F},P)\\). It holds that \\[\\begin{align*} (X_n\\to X)\\in \\mathbb{F}. \\end{align*}\\] Definition 2.2. (Hansen) Let \\(X,X_1,X_2,...\\) be real-valued random variables on \\((\\Omega,\\mathbb{F},P)\\). We say that \\(X_n\\) converges to \\(X\\) almost surely, written \\(X_n\\stackrel{\\text{a.s.}}{\\to}X\\), if \\[\\begin{align*} P(X_n\\to X)=1.\\tag{2.6} \\end{align*}\\] Lemma 2.3. (Hansen) Let \\(X,X&#39;,X_1,X_2,...\\) be real-valued random variables on \\((\\Omega,\\mathbb{F},P)\\). If \\(X_n\\stackrel{\\text{a.s.}}{\\to}X\\) and \\(X_n\\stackrel{\\text{a.s.}}{\\to}X&#39;\\) then \\(X=X&#39;\\) almost surely. Lemma 2.7. (Hansen) Let \\(X_1,X_2,...\\) be real-valued random variables on \\((\\Omega,\\mathbb{F},P)\\). Then \\[\\begin{align*} \\Big((X_n)\\text{ is Cauchy}\\Big)\\in \\mathbb{F}. \\end{align*}\\] Lemma 2.8. (Hansen) Let \\(X_1,X_2,...\\) be real-valued random variables on \\((\\Omega,\\mathbb{F},P)\\). If \\(P\\Big((X_n)\\text{ is Cauchy}\\Big)=1\\) then there exists and \\(\\mathbb{F}\\)-measurable real-valued random variable \\(X\\) such that \\(X_n\\stackrel{\\text{a.s.}}{\\to}X\\). Theorem 2.10. (Hansen) Let \\(X_1,X_2,...\\) and \\(Y_1,Y_2,...\\) be real-valued random variables on \\((\\Omega,\\mathbb{F},P)\\). Assume that the \\(X\\)-process and the \\(Y\\)-process have the same distribution in the sense that \\((X_1,...,X_n)\\) has the same distribution ad \\((Y_1,...,Y_n)\\) for all \\(n\\in\\mathbb{N}\\). If \\(X_n\\stackrel{\\text{a.s.}}{\\to}X\\) for som limit variable \\(X\\), there is a limit variable \\(Y\\) such that \\(Y_n\\stackrel{\\text{a.s.}}{\\to}Y\\). Definition 2.11. (Hansen) Let \\(\\mathbf{X},\\mathbf{X}_1,\\mathbf{X}_2,...\\) be \\(\\mathbb{R}^k\\) valued random variables on \\((\\Omega, \\mathbb{F},P)\\). We say that \\(\\mathbf{X}_n\\) converges to \\(\\mathbf{X}\\) almost surely, written \\(\\mathbf{X}_n\\stackrel{\\text{a.s.}}{\\to}\\mathbf{X}\\), if \\[\\begin{align*} \\vert\\mathbf{X}_n-\\mathbf{X}\\vert \\stackrel{\\text{a.s.}}{\\to} 0.\\tag{2.15} \\end{align*}\\] Lemma 2.12. (Hansen) Let \\(\\mathbf{X},\\mathbf{X}_1,\\mathbf{X}_2,...\\) be \\(\\mathbb{R}^k\\) valued random variables on \\((\\Omega, \\mathbb{F},P)\\) such that \\(\\mathbf{X}_n\\stackrel{\\text{a.s.}}{\\to}\\mathbf{X}\\). Let \\(f : \\mathbb{R}^k\\to\\mathbb{R}^m\\) be a measurable map. Assume that there is a set \\(A\\in \\mathbb{B}_k\\) such that \\(f\\) is continuous on \\(A\\) and such that \\(P(\\mathbf{X}\\in A)=1\\). Then it holds that \\(f(\\mathbf{X}_n)\\stackrel{\\text{a.s.}}{\\to} f(\\mathbf{X})\\). Definition 2.13. (Hansen) Let \\(X,X_1,X_2,...\\) be real-valued random variables on \\((\\Omega,\\mathbb{F},P)\\). We say that \\(X_n\\) converges to \\(X\\) in probability, written \\(X_n\\stackrel{\\text{P}}{\\to} X\\), if \\[\\begin{align*} \\forall \\varepsilon&gt;0 :\\hspace{10pt} P\\big(\\vert X_n-X\\vert\\ge \\varepsilon\\big)\\to 0\\hspace{10pt}\\text{for}\\ n\\to \\infty.\\tag{2.17} \\end{align*}\\] Lemma 2.14. (Hansen) Let \\(X,X&#39;,X_1,X_2,...\\) be real-valued random variables on \\((\\Omega,\\mathbb{F},P)\\). If \\(X_n\\stackrel{\\text{P}}{\\to} X\\) and \\(X_n\\stackrel{\\text{P}}{\\to} X&#39;\\) then \\(X=X&#39;\\) almost surely. Lemma 2.14. (Hansen) Let \\(X,X&#39;,X_1,X_2,...\\) be real-valued random variables on \\((\\Omega,\\mathbb{F},P)\\). If \\(X_n\\stackrel{\\text{a.s.}}{\\to} X\\), then \\(X_n\\stackrel{\\text{P}}{\\to} X\\). Definition 2.17. (Hansen) Let \\(\\mathbf{X},\\mathbf{X}_1,\\mathbf{X}_2,...\\) be \\(\\mathbb{R}^k\\) valued random variables on \\((\\Omega, \\mathbb{F},P)\\). We say that \\(\\mathbf{X}_n\\) converges to \\(\\mathbf{X}\\) in probability, written \\(\\mathbf{X}_n\\stackrel{\\text{P}}{\\to} \\mathbb{X}\\), if \\[\\begin{align*} \\vert \\mathbf{X}_n-\\mathbf{X}\\vert \\stackrel{\\text{P}}{\\to} 0.\\tag{2.23} \\end{align*}\\] Lemma 2.18. (Hansen) Let \\(X,Y,X_1,Y_1,X_2,Y_2,...\\) be real-valued random variables on \\((\\Omega,\\mathbb{F},P)\\). It holds that \\[\\begin{align*} \\begin{pmatrix} X_n\\\\Y_n \\end{pmatrix}\\stackrel{\\text{P}}{\\to} \\begin{pmatrix} X\\\\Y \\end{pmatrix}\\hspace{15pt}\\iff \\hspace{15pt} X_n\\stackrel{\\text{P}}{\\to} X\\text{ and }Y_n\\stackrel{\\text{P}}{\\to} Y.\\tag{2.24} \\end{align*}\\] Definition 2.19. (Hansen) Let \\(X,X_1,X_2,...\\) be real-valued random variables in \\(\\mathcal{L}^p(\\Omega,\\mathbb{F},P)\\) for some \\(p\\ge 1\\). We say that \\(X_n\\) converges to \\(X\\) \\(\\mathcal{L}^p\\), written \\(X_n\\stackrel{\\mathcal{L}^p}{\\to} X\\), if \\[\\begin{align*} \\Vert X_n - X\\Vert_p\\to 0.\\tag{2.27} \\end{align*}\\] Where the \\(p\\)’th norm is defined as the mapping \\(\\Vert \\cdot \\Vert_p : \\Omega\\to [0,\\infty)\\) given by \\(X\\mapsto \\left(\\int_\\Omega \\vert X\\vert ^p\\ dP\\right)^{1/p}\\). One might also define convergence in \\(\\mathcal{L}^p\\) by simply saying if \\(X_n\\stackrel{\\mathcal{L}^p}{\\to} X\\) then \\(E\\,\\Vert X_n-X\\Vert_p\\to 0\\). Lemma 2.20. (Hansen) (Extended Cauchy-Schwarz inequality) Let \\(X,Y\\in\\mathcal{L}^p(\\Omega,\\mathbb{F},P)\\) for some \\(p\\ge 1\\). For any \\(a\\in[0,p]\\) it holds that \\[\\begin{align*} E\\, \\vert X\\vert^a\\vert Y\\vert^{p-a}\\le \\Big(E\\, \\vert X\\vert^p\\Big)^{\\frac{a}{p}}\\Big(E\\, \\vert Y\\vert^p\\Big)^{\\frac{p-a}{p}}.\\tag{2.29} \\end{align*}\\] Theorem 2.21. (Hansen) Let \\(X,X_1,X_2,...\\) be real-valued random variables in \\(\\mathcal{L}^p(\\Omega,\\mathbb{F},P)\\) for some \\(p\\in\\mathbb{N}\\). If \\(X_n\\stackrel{\\mathcal{L}^p}{\\to} X\\), then it holds that \\(E\\, X_n^p\\to E\\, X^p\\). Lemma 2.22. (Hansen) Let \\(X,X_1,X_2,...\\) be real-valued random variables in \\(\\mathcal{L}^p(\\Omega,\\mathbb{F},P)\\) for some \\(p\\ge 1\\). If \\(X_n\\stackrel{\\mathcal{L}^p}{\\to} X\\), then \\(X_n\\stackrel{\\text{P}}{\\to} X\\). Lemma 2.25. (Hansen) (Borel-Cantelli) Let \\((\\Omega,\\mathbb{F},P)\\) be a probability space, and let \\(A_1,A_2,...\\) be a sequence of \\(\\mathbb{F}\\)-measurable sets. It holds that \\[\\begin{align*} \\sum_{n=1}^\\infty P(A_n)&lt;\\infty\\hspace{10pt}\\Rightarrow\\hspace{10pt}P(A_n\\ \\text{i.o.})=0. \\end{align*}\\] Let \\(A_1,A_2,...\\) be a sequence of subsets of \\(\\Omega\\). We define \\[\\begin{align*} (A_n\\ \\text{i.o.})=\\bigcap_{n=1}^\\infty\\bigcup_{m=n}^\\infty A_m,\\hspace{10pt}(A_n\\ \\text{evt.})=\\bigcup_{n=1}^\\infty\\bigcap_{m=n}^\\infty A_m. \\end{align*}\\] One might also define \\(Y=\\sum_{n=1}^\\infty 1_{A_n}\\) and realise that \\((A_n\\ \\text{i.o.})=(Y=\\infty)\\) and \\((A_n\\ \\text{evt.})=(Y&lt;\\infty)\\). Also by de Morgan’s law it follows that \\((A_n\\ \\text{evt.})^c=(A_n^c\\ \\text{i.o.})\\). Theorem 2.26. (Hansen) Let \\(X,X_1,X_2,...\\) be real-valued random variables on \\((\\Omega,\\mathbb{F},P)\\). If \\[\\begin{align*} \\forall \\varepsilon&gt;0:\\hspace{10pt}\\sum_{n=1}^\\infty P(\\vert X_n-X\\vert \\ge \\varepsilon)&lt;\\infty,\\tag{2.32} \\end{align*}\\] then it holds that \\(X_n\\stackrel{\\text{a.s.}}{\\to} X\\). Theorem 2.27. (Hansen) Let \\(X,X_1,X_2,...\\) be real-valued random variables on \\((\\Omega,\\mathbb{F},P)\\). If \\(X_n\\stackrel{\\text{P}}{\\to} X\\), then there is a subsequence \\(X_{n_1},X_{n_2},...\\) such that \\(X_{n_k}\\stackrel{\\text{a.s.}}{\\to} X\\) for \\(k\\to \\infty\\). Lemma 2.28. (Hansen) Let \\(\\mathbf{X},\\mathbf{X}_1,\\mathbf{X}_2,...\\) be \\(\\mathbb{R}^k\\)-valued random variables on \\((\\Omega,\\mathbb{F},P)\\) such that \\(\\mathbf{X}_n\\stackrel{\\text{P}}{\\to} \\mathbf{X}\\). Let \\(f : \\mathbb{R}^k\\to \\mathbb{R}^m\\) be a measurable map. Assume that there is a set \\(A\\in\\mathbb{B}_k\\) such that \\(f\\) is continuous on \\(A\\) and such that \\(P(\\mathbf{X}\\in A)=1\\). Then it holds that \\(f(\\mathbf{X}_n)\\stackrel{\\text{P}}{\\to} f(\\mathbf{X})\\). Lemma. (Fatou’s Lemma) Let \\((\\Omega,\\mathbb{F},P)\\) be a measure space (here probability space). Let \\(f_n : \\mathcal{X} \\to [0,\\infty]\\), with \\(\\mathcal{X}\\in\\mathbb{F}\\), be a sequence of non-negative measurable functions. Assume \\(f_n\\) converge pointwise to \\(f : \\mathcal{X}\\to [0,\\infty)\\). Then \\[\\begin{align*} \\int_{\\mathcal{X}} \\liminf_{n\\to\\infty} f_n\\ dP\\le \\liminf_{n\\to\\infty} \\int_{\\mathcal{X}} f_n\\ dP. \\end{align*}\\] Lemma. (Holder’s Inequality) Let \\((\\Omega,\\mathbb{F},P)\\) be a measure space (here probability space). Let \\(f\\) and \\(g\\) be real-valued (or complex-valued) functions defined on \\(\\Omega\\). Assume \\(f\\) and \\(g\\) are measurable. For any \\(p,q\\ge 1\\) such that \\(\\frac{1}{p}+\\frac{1}{q}=1\\) it holds that \\[\\begin{align*} \\left(\\int_\\Omega \\vert fg\\vert^1\\ dP\\right)^1\\le \\left(\\int_\\Omega \\vert f\\vert^p\\ dP\\right)^{1/p}\\left(\\int_\\Omega \\vert g\\vert^q\\ dP\\right)^{1/q} \\end{align*}\\] 13.1.1 Sums and average processes Lemma 4.1. (Hansen) Let \\(X_1,...,X_n\\) be independent real-valued random variables with \\(E\\, X_i^4&lt;\\infty\\) for all \\(i\\). If \\(E\\, X_i=0\\) for all \\(i\\) then it holds that \\[\\begin{align*} E\\left(\\sum_{i=1}^n X_i\\right)^4=\\sum_{i=1}^n E\\, X_i^4+6\\sum_{i=1}^{n-1}\\sum_{j=i+1}^n E\\, X_i^2\\,E\\,X_j^2. \\end{align*}\\] Theorem 4.2. (Hansen) (SLLN, weak form) Let \\(X_1,X_2,...\\) be a sequence of independent and identically distributed real-valued random variables. If \\(E\\, X_1^4&lt;\\infty\\) it holds that \\[\\begin{align*} \\frac{1}{n}\\sum_{i=1}^n X_i \\hspace{10pt}\\stackrel{\\text{a.s.}}{\\to} \\hspace{10pt}E\\, X_1.\\tag{4.3} \\end{align*}\\] Theorem 4.10. (Hansen) (Etemadi’s maximal inequality) Let \\(X_1,...,X_n\\) be independent real-valued random variables. Consider the cumulative sums \\[\\begin{align*} S_k=\\sum_{i=1}^kX_i\\hspace{10pt}\\text{for}\\ k=1,..., n. \\end{align*}\\] For any \\(\\alpha &gt;0\\) it holds that \\[\\begin{align*} P\\left(\\max_{j=1,...,n}\\ \\vert S_j\\vert\\ge 3\\alpha\\right)\\le 3\\ \\max_{j=1,...,n}\\ P(\\vert S_j\\vert \\ge \\alpha).\\tag{4.11} \\end{align*}\\] Theorem 4.11. (Hansen) (Levy’s maximal inequality) Let \\(X_1,...,X_n\\) be independent real-valued random variables, each with a symmetric distribution. Consider the cumulative sums \\[\\begin{align*} S_k=\\sum_{i=1}^kX_i\\hspace{10pt}\\text{for}\\ k=1,..., n. \\end{align*}\\] For any \\(\\alpha&gt;0\\) it holds that \\[\\begin{align*} P\\left(\\max_{j=1,...,n}\\ S_j\\ge \\alpha\\right)\\le 2 P(S_j\\ge \\alpha).\\tag{4.13} \\end{align*}\\] Corollary 4.12. (Hansen) Let \\(X_1,...,X_n\\) be independent real-valued random variables, each with a symmetric distribution. Consider the cumulative sums \\[\\begin{align*} S_k=\\sum_{i=1}^kX_i\\hspace{10pt}\\text{for}\\ k=1,..., n. \\end{align*}\\] For any \\(\\alpha&gt;0\\) it holds that \\[\\begin{align*} P\\left(\\max_{j=1,...,n}\\ \\vert S_j\\vert\\ge \\alpha\\right)\\le 2 P(\\vert S_j\\vert\\ge \\alpha).\\tag{4.14} \\end{align*}\\] Theorem 4.13. (Hansen) (Skorokhod) Let \\(X_1,X_2,...\\) be a sequence of independent real-valued random variables, and consider the cumulative sums \\(S_k=\\sum_{i=1}^kX_i\\). Let \\(S\\) be a potential limit variable. It holds that \\[\\begin{align*} S_n \\stackrel{\\text{P}}{\\to} S\\hspace{10pt}\\Rightarrow\\hspace{10pt} S_n\\stackrel{\\text{a.s.}}{\\to} S. \\end{align*}\\] Corollary 4.14. (Hansen) (Khintchine-Kolmogorov) Let \\(X_1,X_2,...\\) be a sequence of independent real-valued random variables. Assume \\(E\\, X_n^2&lt;\\infty\\) and that \\(E\\, X_n=0\\) for every \\(n\\in\\mathbb{N}\\). Consider the cumulative sums \\(S_k=\\sum_{i=1}^kX_i\\). If \\[\\begin{align*} \\sum_{n=1}^\\infty E\\, X_n^2&lt;\\infty\\tag{4.18} \\end{align*}\\] then there exist a limit variable \\(S\\) such that \\(S_n\\to S\\) almost surely and in \\(\\mathcal{L}^2\\). The limit variable satisfies that \\[\\begin{align*} E\\, S=0\\hspace{10pt}\\text{and}\\hspace{10pt}V\\, S=\\sum_{n=1}^\\infty V\\, X_n. \\end{align*}\\] Theorem 4.17. (Hansen) Let \\(X_1,X_2,...\\) be a sequence of independent real-valued random variables, and consider the cumulative sums \\(S_k=\\sum_{i=1}^kX_i\\). Let \\(S\\) be a potential limit variable. Assume that there is a constant \\(c&gt;0\\) such that \\(P(\\vert X_n\\vert \\le c)=1\\) for all \\(n\\), and assume that \\(E\\, X_n=0\\) for all \\(n\\). The the three statements 1. \\(S_n\\stackrel{\\text{P}}{\\to} S\\), 2. \\(S_n\\stackrel{\\text{a.s.}}{\\to} S\\) 3. \\(S_n\\stackrel{\\mathcal{L}^2}{\\to} S\\) are equivalent. Lemma 4.18. (Hansen) Let \\(X_1,X_2,...\\) be a sequence of independent real-valued random variables. Assume that there is a constant \\(c&gt;0\\) such that \\(P(\\vert X_n\\vert \\le c)=1\\) for all \\(n\\). If the associated random walk \\(S_n=\\sum_{i=1}^n X_i\\) satisfies that \\(S_n\\to S\\) almost surely for some limit variable then it holds that \\[\\begin{align} \\text{1)}\\hspace{10pt}&amp; \\sum_{n=1}^NE\\, X_n\\hspace{5pt}\\text{converges in }\\mathbb{R}\\hspace{5pt}\\text{for}\\ N\\to \\infty,\\\\ \\text{2)}\\hspace{10pt}&amp;\\sum_{n=1}^\\infty V(X_n)&lt;\\infty. \\end{align}\\] Theorem 4.19. (Hansen) (Kolmogorov’s 3-series theorem) Let \\(X_1,X_2,...\\) be a sequence of independent real-valued random variables. Consider the assoiciated random walk \\(S_n=\\sum_{i=1}^n X_i\\). If there is a cut-off value \\(c&gt;0\\) such that the capped variables \\(\\tilde{X}_n=1_{\\vert X_n\\vert \\le c}X_n\\) satisfies that \\[\\begin{align*} \\text{1)}\\hspace{10pt}&amp; \\sum_{n=1}^\\infty P(X_n\\ne \\tilde{X}_n)&lt;\\infty,\\\\ \\text{2)}\\hspace{10pt}&amp; \\sum_{n=1}^N E\\, \\tilde{X}_n\\ \\text{converges in }\\mathbb{R}\\ \\text{for}\\ N\\to \\infty,\\\\ \\text{3)}\\hspace{10pt}&amp; \\sum_{n=1}^\\infty V(\\tilde{X}_n)&lt;\\infty, \\end{align*}\\] then there is a real-valued limit variable \\(S\\) such that \\(S_n\\to S\\) almost surely. Conversely, if \\((S_n)_{n\\in\\mathbb{N}}\\) is almost surely convergent, then the three series above converge for any cut-off value \\(c&gt;0\\). Lemma 4.20. (Hansen) Let \\((x_n)_{n\\in\\mathbb{N}}\\) be a real-valued sequence, and let \\(c\\) be a real number. It holds that \\[\\begin{align*} x_n\\to c\\ \\text{for}\\ n\\to \\infty \\hspace{10pt}\\Rightarrow\\hspace{10pt} \\frac{1}{n}\\sum_{i=1}^nx_i\\to x\\ \\text{for}\\ n\\to\\infty. \\end{align*}\\] Lemma 4.21. (Hansen) (Kronecker) Let \\((x_n)_{n\\in\\mathbb{N}}\\) be real-valued sequence, and let \\(c\\) be a real number. It holds that \\[\\begin{align*} \\sum_{i=1}^n\\frac{x_i}{i}\\to c \\hspace{10pt}\\Rightarrow\\hspace{10pt} \\frac{1}{n}\\sum_{i=1}^nx_i\\to 0 \\end{align*}\\] for \\(n\\to \\infty\\). Lemma 4.23. (Hansen) Let \\(X_1,X_2,...\\) be a sequence of identically distributed real-valued random variables, and let \\(\\tilde{X}_n=1_{(\\vert X_n\\vert \\le n}X_n\\). If \\(E\\vert X_1\\vert&lt;\\infty\\) it holds that \\[\\begin{align*} \\sum_{n=1}^\\infty\\frac{E\\ \\tilde{X}_n^2}{n^2}&lt;\\infty. \\end{align*}\\] Theorem 4.24. (Hansen) (SLLN, strong version) Let \\(X_1,X_2,...\\) be sequence of independent and identically distributed real-valued random variables. If \\(E\\vert X_1\\vert &lt;\\infty\\) it holds that \\[\\begin{align*} \\frac{1}{n}\\sum_{n=1}^\\infty X_i\\stackrel{\\text{a.s.}}{\\to} E\\ X_1.\\tag{4.24} \\end{align*}\\] Theorem 4.25. (Hansen) (SLLN, \\(\\mathcal{L}^p\\)-version) Let \\(X_1,X_2,...\\) be sequence of independent and identically distributed real-valued random variables. If \\(E\\vert X_1\\vert^p &lt;\\infty\\) for some \\(p\\ge 1\\), then it holds that \\[\\begin{align*} \\frac{1}{n}\\sum_{n=1}^\\infty \\stackrel{\\mathcal{L}^p}{\\to} E\\ X_1.\\tag{4.26} \\end{align*}\\] Lemma 4.26. (Hansen) Let \\(X_1,X_2,...\\) be a sequence of pairwise independent, identically distributed real-valued random variables with \\(E\\vert X_1\\vert &lt;\\infty\\). Let \\(n_1&lt;n_2&lt;...\\) be a sequence of natural numbers. If there are constants \\(C_1,C_2&gt;0\\) and \\(\\alpha&gt;1\\) such that \\[\\begin{align*} C_1\\alpha^k\\le n_k\\le C_2\\alpha^k\\hspace{15pt}\\text{for}\\ k\\to\\infty\\tag{4.27} \\end{align*}\\] then it holds that \\[\\begin{align*} \\frac{1}{n_k}\\sum_{i=1}^{n_k}X_i\\stackrel{\\text{a.s.}}{\\to} E\\ X_1\\hspace{15pt}\\text{for}\\ k\\to \\infty. \\end{align*}\\] Theorem 4.27. (Hansen) (Etemahdi’s version) Let \\(X_1,X_2,...\\) be a sequence of pairwise independent, identically distributed real-valued random variables. If \\(E\\vert X_1\\vert&lt;\\infty\\) it holds that \\[\\begin{align*} \\frac{1}{n}\\sum_{i=1}^nX_i\\stackrel{\\text{a.s.}}{\\to} E\\ X_1.\\tag{4.30} \\end{align*}\\] 13.1.2 Ergodic Theory Definition 5.3. (Hansen) Let \\((\\mathcal{X},\\mathbb{E})\\) be a measurable space and let \\(T : \\mathcal{X}\\to \\mathcal{X}\\) be measurable. A probability measure \\(\\mu\\) on \\((\\mathcal{X},\\mathbb{E})\\) is invariant under \\(T\\) if \\[\\begin{align*} \\mu\\big(T^{-1}(A)\\big)=\\mu(A)\\hspace{10pt}\\text{for all}\\ A\\in\\mathbb{E}\\tag{5.5} \\end{align*}\\] In this case we call the quadruple \\((\\mathcal{X},\\mathbb{E},\\mu,T)\\) a measure-preserving dynamical system. We say that a set \\(A\\in\\mathbb{E}\\) is an invariant set if \\(T^{-1}(A)=A\\) i.e. the orbit of all \\(x\\in A\\) stays in \\(A\\). Definition 5.5. (Hansen) A measure-preserving dynamical system \\((\\mathcal{X},\\mathbb{E},\\mu,T)\\) is ergodic if \\[\\begin{align*} T^{-1}(A)=A,\\ A\\in\\mathbb{E} \\hspace{10pt}\\Rightarrow\\hspace{10pt} \\mu(A)\\in\\{0,1\\}.\\tag{5.7} \\end{align*}\\] Definition 5.6. (Hansen) A measure-preserving dynamical system \\((\\mathcal{X},\\mathbb{E},\\mu,T)\\) is mixing if \\[\\begin{align*} \\mu(A\\cap T^{-n}(B))\\to \\mu(A)\\mu(B)\\hspace{10pt}\\text{for all}\\ A,B\\in\\mathbb{E}.\\tag{5.8} \\end{align*}\\] Lemma 5.7. (Hansen) If a measure-preserving dynamical system \\((\\mathcal{X},\\mathbb{E},\\mu,T)\\) is mixing then it is also ergodic. Lemma 5.8. (Hansen) Let \\((\\mathcal{X},\\mathbb{E},\\mu,T)\\) be a measure-preserving dynamical system. Let \\(\\mathbb{D}\\) be an \\(\\cap\\)-stable generator for \\(\\mathbb{E}\\). If \\[\\begin{align*} \\mu(A\\cap T^{-n}(B))\\to \\mu(A)\\mu(B)\\hspace{10pt}\\text{for all}\\ A,B\\in\\mathbb{D}.\\tag{5.10} \\end{align*}\\] then the system is mixing. (and ergodic) Lemma 5.9. (Hansen) Let \\((\\mathcal{X},\\mathbb{E},\\mu)\\) be a probability space, and let \\(T : \\mathcal{X}\\to \\mathcal{X}\\) be a measure-preserving map. Let \\((\\mathcal{Y},\\mathbb{G})\\) be another measurable space, and let \\(S : \\mathcal{Y} \\to \\mathcal{Y}\\) be a measurable map. Suppose there is a measurable map \\(\\gamma : \\mathcal{X}\\to \\mathcal{Y}\\) such that the following diagram commutes: Then \\((\\mathcal{Y},\\mathbb{G},\\gamma(\\mu),S)\\) is a measure-preserving dynamical system. Lemma 5.10. (Hansen) Let \\((\\mathcal{X},\\mathbb{E},\\mu,T)\\) and \\((\\mathcal{Y},\\mathbb{G},\\nu,S)\\) be two measure-preserving dynamical systems. Suppose there is a measurable map \\(\\gamma : \\mathcal{X}\\to\\mathcal{Y}\\) such that \\(\\nu =\\gamma(\\mu)\\) and such that the diagram in lemma 5.9 commutes. If \\((\\mathcal{X},\\mathbb{E},\\mu,T)\\) is ergodic then \\((\\mathcal{Y},\\mathbb{G},\\nu,S)\\) is also ergodic. Lemma 5.11. (Hansen) (Maximal Ergodic Lemma) Let \\((\\mathcal{X},\\mathbb{E},\\mu,T)\\) be a measure-preserving dynamical system, and let \\(f : \\mathcal{X}\\to \\mathbb{R}\\) be Borel measurable. If \\(f\\in \\mathcal{L}^1(\\mu)\\) then it holds that \\[\\begin{align*} \\int_{(M_n&gt;0)}f\\ d\\mu\\ge 0\\tag{5.14} \\end{align*}\\] where \\(M_n=\\max\\{0,S_1,S_2,...,S_n\\}\\) from the sequence \\[\\begin{align*} \\Big(f(x), f\\circ T(x),f\\circ T^2(x),f\\circ T^3(x),...\\Big)\\hspace{10pt}\\text{with}\\hspace{10pt}S_n=\\sum_{i=0}^{n-1}f\\circ T^i. \\end{align*}\\] Theorem 5.12. (Hansen) (Birkhoff’s ergodic theorem) Let \\((\\mathcal{X},\\mathbb{E},\\mu,T)\\) be an ergodic system. For \\(f\\in \\mathcal{L}^1(\\mu)\\) it holds that \\[\\begin{align*} \\frac{1}{n}\\sum_{i=0}^{n-1}f\\circ T^i\\stackrel{\\text{a.s.}}{\\to} \\int f\\ d\\mu.\\tag{5.16} \\end{align*}\\] Theorem 5.13. (Hansen) (Ergodic theorem, \\(\\mathcal{L}^p\\)-version) Let \\((\\mathcal{X},\\mathbb{E},\\mu,T)\\) be an ergodic system. If \\(f\\in \\mathcal{L}^p(\\mu)\\) for some \\(p\\ge 1\\) then it holds that \\[\\begin{align*} \\frac{1}{n}\\sum_{i=0}^{n-1}f\\circ T^i\\stackrel{\\mathcal{L}^p}{\\to}\\int f\\ d\\mu.\\tag{5.21} \\end{align*}\\] Lemma 5.14. (Hansen) Let \\((\\mathcal{X},\\mathbb{E})\\) be a measurable space. The measurable finite dimensional product sets in \\(\\mathcal{X}^{\\mathbb{N}}\\) form an \\(\\cap\\)-stable generator for \\({\\mathbb{E}}^{\\otimes\\mathbb{N}}\\). An element of the space \\(\\mathcal{X}^{\\mathbb{N}}\\) is a countable set of coordinates \\(x_n\\) for \\(n\\in\\mathcal{N}\\) with \\(x_n\\in\\mathcal{X}\\). A finite dimensional product set in \\(\\mathcal{X}^{\\mathbb{N}}\\) is set on the form \\[\\begin{align*} A_1\\times ... \\times A_k\\times \\mathcal{X}\\times \\mathcal{X}\\times ... \\end{align*}\\] where \\(A_1,...,A_k\\subset \\mathcal{X}\\). We also define the projection sigma-algebra \\(\\mathbb{E}^{\\otimes \\mathbb{N}}\\) as \\(\\sigma\\left(\\big(\\hat{X}_n(\\mathcal{X}^n)\\big)_{n\\in\\mathbb{N}}\\right)\\) where \\(\\hat{X}_n(x_1,...,x_n)=x_n\\). Definition 5.15. (Hansen) Let \\(X_1,X_2,...\\) be a sequence of \\((\\mathcal{X},\\mathbb{E})\\)-valued random variable, defined on a background space \\((\\Omega,\\mathbb{F},P)\\), and let \\(\\mathbb{X}=(X_1,X_2,...)\\) be their bundling. The distribution of the process is the image measure \\(\\mathbb{X}(P)\\) on \\((\\mathcal{X}^{\\mathbb{N}},{\\mathbb{E}}^{\\otimes \\mathbb{N}})\\). Lemma 5.16. (Hansen) Let \\(\\mathbb{X}=(X_1,X_2,...)\\) and \\(\\mathbb{Y}=(Y_1,Y_2,...)\\) be two \\((\\mathcal{X},\\mathbb{E})\\)-valued stochastic process, defined on a common background space . The two processes \\(\\mathbb{X}\\) and \\(\\mathbb{Y}\\) have the same distribution if and only if the have the same fidis. This can be checked by showing that \\[\\begin{align*} P(X_1\\in A_i,...,X_k\\in A_k)=P(Y_1\\in A_1,...,Y_k\\in A_k)\\tag{5.25} \\end{align*}\\] for any \\(k\\in\\mathbb{N}\\) and any choice of \\(A_1,...,A_k\\in\\mathbb{E}\\). Definition 5.18. (Hansen) Let \\(X_1,X_2,...\\) be a sequence of \\((\\mathcal{X},\\mathbb{E})\\)-valued random variable, defined on a background space \\((\\Omega,\\mathbb{F},P)\\), and let \\(\\mathbb{X}=(X_1,X_2,...)\\) be their bundling. Then we define: 1. The proces \\(\\mathbb{X}\\) is stationary if the distrbution \\(\\mathbb{X}(P)\\) is an \\(S\\)-invariant probability on \\(\\Big(\\mathbb{R}^{\\mathbb{N}},\\mathbb{B}^{\\otimes \\mathbb{N}}\\Big)\\), 2. The proces \\(\\mathbb{X}\\) is ergodic if it is stationary and if the dynamical system \\(\\Big(\\mathbb{R}^{\\mathbb{N}},\\mathbb{B}^{\\otimes \\mathbb{N}},\\mathbb{X}(P),S\\Big)\\) is ergodic. 3. The proces \\(\\mathbb{X}\\) is mixing if it is stationary and if the dynamical system \\(\\Big(\\mathbb{R}^{\\mathbb{N}},\\mathbb{B}^{\\otimes \\mathbb{N}},\\mathbb{X}(P),S\\Big)\\) is mixing. with \\(S\\) being the shift map defined as \\(S(x_1,x_2,...)=(x_2,x_3,...)\\). Theorem 5.20. (Hansen) (Khintchine’s ergodic theorem) Let \\(X_1,X_2,...\\) be a stationary and ergodic sequence of real-valued random variables. If \\(E\\vert X_1\\vert &lt;\\infty\\) it holds that \\[\\begin{align*} \\frac{1}{n}\\sum_{i=1}^nX_i\\stackrel{\\text{a.s.}}{\\to} E\\ X_1.\\tag{5.27} \\end{align*}\\] If \\(E\\vert X_1\\vert ^p&lt;\\infty\\) for some \\(p\\ge 1\\), the convergence is also in \\(\\mathcal{L}^p\\). Theorem 5.21. (Hansen) (Ergodic transformation theorem) Let \\(X_1,X_2,...\\) be a sequence of real-valued random variables. For a measurable function \\(\\phi : \\big(\\mathbb{R}^{\\mathbb{N}},\\mathbb{B}^{\\otimes\\mathbb{N}}\\big)\\to (\\mathbb{R},\\mathbb{B})\\) we define new real-valued random variables \\[\\begin{align*} Y_n=\\phi(X_n,X_{n+1},...)=\\phi\\circ S^{n-1}(\\mathbb{X})\\hspace{15pt}\\text{for}\\ n\\in\\mathbb{N}. \\end{align*}\\] If \\(X_1,X_2,...\\) is stationary and ergodic then \\(Y_1,Y_2,...\\) is also stationary and ergodic. Definition 5.23. (Hansen) Let \\((X_n)_{n\\in\\mathbb{Z}}\\) be a two-sided sequence of real-valued random variables, defined on a background space \\((\\Omega,\\mathbb{F},P)\\), and let \\(\\mathbb{X}\\) be their bundling. 1. The proces \\(\\mathbb{X}\\) is stationary if the distrbution \\(\\mathbb{X}(P)\\) is an \\(S\\)-invariant probability on \\(\\Big(\\mathbb{R}^{\\mathbb{Z}},\\mathbb{B}^{\\otimes \\mathbb{Z}}\\Big)\\), 2. The proces \\(\\mathbb{X}\\) is ergodic if it is stationary and if the dynamical system \\(\\Big(\\mathbb{R}^{\\mathbb{Z}},\\mathbb{B}^{\\otimes \\mathbb{Z}},\\mathbb{X}(P),S\\Big)\\) is ergodic. 3. The proces \\(\\mathbb{X}\\) is mixing if it is stationary and if the dynamical system \\(\\Big(\\mathbb{R}^{\\mathbb{Z}},\\mathbb{B}^{\\otimes \\mathbb{Z}},\\mathbb{X}(P),S\\Big)\\) is mixing. Theorem 5.25. (Hansen) (Khintchine’s ergodic theorem, two-sided version) Let \\((X_n)_{n\\in\\mathbb{Z}}\\) be a two-sided sequence of real-valued random variables. If the sequence is stationary and ergodic and if \\(E\\vert X_1\\vert &lt;\\infty\\) then it holds that \\[\\begin{align*} \\frac{1}{n}\\sum_{i=1}^nX_i\\stackrel{\\text{a.s.}}{\\to} E\\ X_1.\\tag{5.30} \\end{align*}\\] If \\(E\\vert X_1\\vert^p&lt;\\infty\\) for some \\(p\\ge 1\\), the convergence is also in \\(\\mathcal{L}^p\\). Theorem 5.26. (Hansen) (Ergodic transformation theorem) Let \\((X_n)_{n\\in\\mathbb{Z}}\\) be a two-sided sequence of real-valued random variables. For a measurable function \\(\\phi : \\big(\\mathbb{R}^{\\mathbb{Z}},\\mathbb{B}^{\\otimes\\mathbb{Z}}\\big)\\to (\\mathbb{R},\\mathbb{B})\\) we define new real-valued random variables \\[\\begin{align*} Y_n=\\phi\\circ S^{n}(\\mathbb{X})\\hspace{15pt}\\text{for}\\ n\\in\\mathbb{Z}. \\end{align*}\\] If \\((X_n)_{n\\in\\mathbb{Z}}\\) is stationary and ergodic then \\((Y_n)_{n\\in\\mathbb{Z}}\\) is also stationary and ergodic. 13.1.3 Weak Convergence Definition 6.1. (Hansen) A sequence of probability measures \\(\\nu_1,\\nu_2,...\\) on \\((\\mathbb{R},\\mathbb{B})\\) is said to converge weakly to a limit probability measure \\(\\nu\\) if \\[\\begin{align*} \\int f\\ d\\nu_n\\to \\int f\\ d\\nu\\hspace{15pt}\\text{for every}\\ f\\in C_b(\\mathbb{R})\\tag{6.2} \\end{align*}\\] We write \\(\\nu_n\\stackrel{\\text{wk}}{\\to} \\nu\\) to denote weak convergence. Theorem 6.4. (Hansen) (Scheffe’s) Let \\(\\nu_1,\\nu_2,...\\) and \\(\\nu\\) be probability measures on \\((\\mathbb{R},\\mathbb{B})\\). Assume that for some choice of basic measure \\(\\mu\\) it holds that \\(\\nu_n=f_n\\cdot \\mu\\) for every \\(n\\) and \\(\\nu = f\\cdot \\mu\\) for suitable density functions \\(f_n\\) and \\(f\\). If \\[\\begin{align*} f_n(x)\\to f(x)\\hspace{15pt}\\mu\\text{-almost surely} \\end{align*}\\] then it holds that \\(\\nu_n\\stackrel{\\text{wk}}{\\to} \\nu\\). Lemma 6.8. (Hansen) Let \\(\\mu\\) and \\(\\nu\\) be two probability measures on \\((\\mathbb{R},\\mathbb{B})\\). If \\[\\begin{align*} \\int f\\ d\\mu=\\int f\\ d\\nu\\hspace{15pt}\\text{for all}\\ f\\in C_b(\\mathbb{R})\\tag{6.7} \\end{align*}\\] then it holds that \\(\\mu=\\nu\\). Theorem 6.9. (Hansen) Let \\(\\nu_1,\\nu_2,...\\) be a sequence of probability measures on \\((\\mathbb{R},\\mathbb{B})\\) and let \\(\\mu\\) and \\(\\nu\\) be two extra probability measures. If \\[\\begin{align*} \\nu_n\\stackrel{\\text{wk}}{\\to} \\mu\\hspace{15pt}\\text{and}\\hspace{15pt}\\nu_n\\stackrel{\\text{wk}}{\\to} \\nu \\end{align*}\\] then \\(\\mu=\\nu\\). Definition 6.10. (Hansen) A sequence of real-valued variables \\(X_1,X_2,...\\), defined on a common background space \\((\\Omega,\\mathbb{F},P)\\) is said to converge in distrbution to a limit variable \\(X\\) if \\[\\begin{align*} \\int f(X_n)\\ dP\\to \\int f(X)\\ dP\\hspace{15pt}\\text{for every}\\ f\\in C_b(\\mathbb{R}).\\tag{6.9} \\end{align*}\\] We will write \\(X_n\\stackrel{\\mathcal{D}}{\\to} X\\) to denote convergence in distribution. Lemma 6.11. (Hansen) Let \\(X_1,X_2,...\\) and \\(X\\) be real-valued random variables. It holds that \\[\\begin{align*} X_n\\stackrel{\\text{P}}{\\to} X\\hspace{10pt}\\Rightarrow\\hspace{10pt} X_n\\stackrel{\\mathcal{D}}{\\to} X. \\end{align*}\\] Lemma 6.12. (Hansen) Let \\(X_1,X_2,...\\) be real-valued random variable and let \\(x_0\\in\\mathbb{R}\\). It holds that \\[\\begin{align*} X_n\\stackrel{\\mathcal{D}}{\\to} x_0\\hspace{10pt}\\Rightarrow\\hspace{10pt} X_n\\stackrel{\\text{P}}{\\to} x_0. \\end{align*}\\] Theorem 6.13. (Hansen) Let \\(\\nu,\\nu_1,\\nu_2,...\\) be probability measures on \\((\\mathbb{R},\\mathbb{B})\\). Let \\(\\mathcal{H}\\) be a class of bounded, non-negative and measurable functions with the following approximation property: For \\(f\\in C_b(\\mathbb{R})\\) with \\(f\\ge 0\\) there is a sequence \\(h_1,h_2,...\\) of \\(\\mathcal{H}\\)-functions such that \\(h_n\\nearrow f\\). If \\[\\begin{align*} \\int h\\ d\\nu_n\\to \\int h\\ d\\nu\\hspace{15pt}\\text{for all}\\ h\\in\\mathcal{H}.\\tag{6.11} \\end{align*}\\] then it holds that \\(\\nu_n\\stackrel{\\text{wk}}{\\to} \\nu\\). Theorem 6.14. (Hansen) Let \\(\\nu,\\nu_1,\\nu_2,...\\) be probability measures on \\((\\mathbb{R},\\mathbb{B})\\). If \\[\\begin{align*} \\int f\\ d\\nu_n\\to \\int f\\ d\\nu\\hspace{15pt}\\text{for all}\\ f\\in C_c(\\mathbb{R})\\tag{6.13} \\end{align*}\\] then it holds that \\(\\nu_n\\stackrel{\\text{wk}}{\\to} \\nu\\). The class \\(C_c(\\mathbb{R})\\) is denoted as the set of all continuous real-valued functions with compact support i.e. there exist a \\(M&gt;0\\) such that \\(f(x)=0\\) for all \\(\\vert x\\vert&gt;M\\). Lemma 6.15. (Hansen) Let \\(\\nu,\\nu_1,\\nu_2,...\\) be probability measures on \\((\\mathbb{R},\\mathbb{B})\\), and let \\(F,F_1,F_2,...\\) be the corresponding distribution functions. If \\(\\nu_n\\stackrel{\\text{wk}}{\\to}\\nu\\) then it holds that \\[\\begin{align*} F_n(x_0)\\to F(x_0), \\end{align*}\\] whenever \\(x_0\\) is a point of continuity for \\(F\\). Theorem 6.17. (Hansen) (Helly-Bray) Let \\(\\nu,\\nu_1,\\nu_2,...\\) be probability measures on \\((\\mathbb{R},\\mathbb{B})\\), and let \\(F,F_1,F_2,...\\) be the corresponding distribution functions. It holds that \\(\\nu_n\\stackrel{\\text{wk}}{\\to}\\nu\\) if and only if there is a dense subset \\(A\\subset\\mathbb{R}\\) such that \\[\\begin{align*} F_n(x)\\to F(x)\\hspace{15pt}\\text{for every}\\ x\\in A.\\tag{6.16} \\end{align*}\\] Theorem 6.18. (Hansen) Let \\(\\nu,\\nu_1,\\nu_2,...\\) be probability measures on \\((\\mathbb{R},\\mathbb{B})\\). Let \\(F,F_1,F_2,...\\) be the corresponding distribution functions, and let \\(q,q_1,q_2,...\\) be the corresponding quantile functions. If \\(\\nu_n\\stackrel{\\text{wk}}{\\to}\\nu\\) then it holds that \\[\\begin{align*} q_n(p)\\to q(p) \\end{align*}\\] for any \\(p\\in(0,1)\\) such that the equation \\(F(x)=p\\) hast at most one solution. Theorem 6.19. (Hansen) (Skorokhod’s representation theorem) Let \\(\\nu,\\nu_1,\\nu_2,...\\) be probability measures on \\((\\mathbb{R},\\mathbb{B})\\). If \\(\\nu_n\\stackrel{\\text{wk}}{\\to} \\nu\\) then it is possible to find random variables \\(X,X_1,X_2,...\\) on a background space \\((\\Omega,\\mathbb{F},P)\\) such that \\[\\begin{align*} X(P)=\\nu,\\ X_1(P)=\\nu_1,\\ X_2(P)=\\nu_2, ... \\end{align*}\\] and such that \\(X_n\\stackrel{\\text{a.s.}}{\\to} X\\). Corollary 6.20. (Hansen) Let \\(\\nu,\\nu_1,\\nu_2,...\\) be probability measures on \\((\\mathbb{R},\\mathbb{B})\\) such that \\(\\nu_n\\stackrel{\\text{wk}}{\\to}\\nu\\). Let \\(h : \\mathbb{R}\\to\\mathbb{R}\\) be a bounded and measurable function. If there is a Boral-measurable set \\(C\\subset \\mathbb{R}\\) such that \\(h\\) is continuous in every point of \\(C\\) and such that \\(\\nu(C)=1\\), then it holds that \\[\\begin{align*} \\int h\\ d\\nu_n\\to \\int h\\ d\\nu.\\tag{6.20} \\end{align*}\\] Corollary 6.21. (Hansen) (Portmanteau’s lemma) Let \\(\\nu,\\nu_1,\\nu_2,...\\) be probability measures on \\((\\mathbb{R},\\mathbb{B})\\) such that \\(\\nu_n\\stackrel{\\text{wk}}{\\to}\\nu\\). For any open set \\(G\\subset \\mathbb{R}\\) it holds that \\[\\begin{align*} \\liminf{\\nu_n(G)}\\ge \\nu(G)\\tag{6.21} \\end{align*}\\] Definition 6.22. (Hansen) The characteristic function for a probability measure \\(\\nu\\) on \\((\\mathbb{R},\\mathbb{B})\\) is the function \\(\\phi : \\mathbb{R}\\to \\mathbb{C}\\) given by \\[\\begin{align*} \\phi(\\theta)=\\int e^{ix\\theta}\\ d\\nu(x).\\hspace{15pt}\\text{for}\\ \\theta\\in\\mathbb{R}.\\tag{6.23} \\end{align*}\\] Some useful observations include \\(\\vert e^{ix\\theta}\\vert = 1\\) hence \\(\\phi(\\theta)\\le 1\\) for all \\(\\theta\\in\\mathbb{R}\\). We may also split the \\(\\phi\\) into an imaginary part and a real part with Euler’s cartesian form \\[\\begin{align*} \\phi(\\theta)=\\int \\cos (x\\theta)\\ d\\nu(x)+i\\int \\sin (x\\theta)\\ d\\nu(x)\\tag{6.24} \\end{align*}\\] And lastly we have the implication \\[\\begin{align*} \\nu_n\\stackrel{\\text{wk}}{\\to} \\nu \\hspace{10pt}\\Rightarrow\\hspace{10pt} \\phi_n(\\theta)\\to \\phi(\\theta)\\hspace{10pt}\\text{for all}\\ \\theta\\in\\mathbb{R}. \\end{align*}\\] Furthermore, if \\(Y=\\xi+\\sigma X\\) and \\(X\\sim \\mathcal{N}(0,1)\\) we have \\[\\begin{align*} \\phi_Y(\\theta)=e^{i\\xi\\theta}e^{-\\sigma^2\\theta^2/2}. \\end{align*}\\] Theorem 6.28. (Hansen) The characteristic function for any probability measure \\(\\nu\\) on \\((\\mathbb{R},\\mathbb{B})\\) is uniformly continuous. Theorem 6.29. (Hansen) Let \\(\\nu\\) be a probability measure on \\((\\mathbb{R},\\mathbb{B})\\). If \\[\\begin{align*} \\int \\vert x\\vert^k\\ d\\nu(x)&lt;\\infty \\end{align*}\\] for some \\(k\\in\\mathbb{N}\\), then the characteristic function \\(\\phi\\) is \\(C^k\\) and it holds that \\[\\begin{align*} \\phi^{(k)}(\\theta)=i^k\\int x^ke^{i\\theta x}\\ d\\nu(x)\\hspace{15pt}\\text{for}\\ \\theta\\in\\mathbb{R}.\\tag{6.31} \\end{align*}\\] Definition 6.30. (Hansen) The convolution of two probability measures \\(\\nu\\) and \\(\\xi\\) on \\((\\mathbb{R},\\mathbb{B})\\) is the image measure \\[\\begin{align*} \\nu * \\xi=\\kappa (\\nu\\otimes\\xi)\\tag{6.33} \\end{align*}\\] where \\(\\kappa : \\mathbb{R}^2\\to\\mathbb{R}\\) is the addition map \\(\\kappa(x,y)=x+y\\). Theorem 6.31. (Hansen) Let \\(\\nu\\) and \\(\\xi\\) be two probability measures on \\(\\mathbb{R}\\). If \\(\\xi=f\\cdot m\\), then the convolution \\(\\nu*\\xi\\) will have a density with respect to \\(m\\). The density is given as \\[\\begin{align*} g(x)=\\int f(x-y)\\ d\\nu(y)\\hspace{15pt}\\text{for}\\ x\\in\\mathbb{R}.\\tag{6.35} \\end{align*}\\] Lemma 6.31. (Hansen) Let \\(\\nu_1\\) and \\(\\nu_2\\) be two probability measures on \\((\\mathbb{R},\\mathbb{B})\\) with characteristic functions \\(\\phi_1\\) and \\(\\phi_2\\). The convolution \\(\\nu_1*\\nu_2\\) has characteristic function \\(\\gamma\\) given by \\[\\begin{align*} \\gamma(\\theta)=\\phi_1(\\theta)\\phi_2(\\theta)\\hspace{15pt}\\text{for}\\ \\theta\\in\\mathbb{R}.\\tag{6.37} \\end{align*}\\] Theorem 6.34. (Hansen) Let \\(\\xi,\\nu,\\nu_1,\\nu_2,...\\) be probability measures on \\((\\mathbb{R},\\mathbb{B})\\). It holds that \\[\\begin{align*} \\nu_n\\stackrel{\\text{wk}}{\\to} \\nu\\hspace{10pt}\\Rightarrow\\hspace{10pt} \\nu_n*\\xi\\stackrel{\\text{wk}}{\\to} \\nu *\\xi. \\end{align*}\\] Definition 6.35. (Hansen) A probability measure \\(\\nu=f\\cdot m\\) on \\((\\mathbb{R},\\mathbb{B})\\) with density \\(f\\) with respect to Lebesgue measure is of Polya class if \\(f\\in C_b(\\mathbb{R})\\) and if the Fourier transform \\(\\hat{f}\\) is \\(m\\)-integrable. Lemma 6.39. (Hansen) Let \\(\\nu\\) and \\(\\xi\\) be two probability measures on \\(\\mathbb{R}\\). If \\(\\xi\\) is of Polya class then the convolution \\(\\nu *\\xi\\) is also of Polya class. Theorem 6.40. (Hansen) (Inversion theorem) Let \\(\\nu=f\\cdot m\\) be a probability measure on \\((\\mathbb{R},\\mathbb{B})\\) of Polya class. It holds that \\[\\begin{align*} f(x)=\\frac{1}{2\\pi}\\int_{-\\infty}^\\infty e^{-i\\theta x}\\hat{f}(\\theta)\\ d\\theta,\\ x\\in\\mathbb{R}.\\tag{6.39} \\end{align*}\\] Theorem 6.41. (Hansen) Let \\(\\nu_1\\) and \\(\\nu_2\\) be two probability measures on \\((\\mathbb{R},\\mathbb{B})\\) with characteristic functions \\(\\phi_1\\) and \\(\\phi_2\\). If \\[\\begin{align*} \\phi_1(\\theta)=\\phi_2(\\theta),\\ \\forall \\theta \\in\\mathbb{R} \\end{align*}\\] then \\(\\nu_1=\\nu_2\\). Lemma 6.42. (Hansen) Let \\(f : \\mathbb{R}\\to\\mathbb{R}\\) be bounded and uniformly continuous. For every \\(\\varepsilon&gt;0\\) there is a probability measure \\(\\xi\\) of Polya class with the property that \\[\\begin{align*} \\Big\\vert f(x)-\\int f(x+y)\\ d\\xi(y)\\Big\\vert&lt;\\varepsilon,\\ \\forall x\\in\\mathbb{R}.\\tag{6.43} \\end{align*}\\] Theorem 6.43. (Hansen) (Continuity theorem) Let \\(\\nu,\\nu_1,\\nu_2,...\\) be probability measures on \\((\\mathbb{R},\\mathbb{B})\\) with characteristic functions \\(\\phi,\\phi_1,\\phi_2,...\\). If \\[\\begin{align*} \\phi_n(\\theta)\\to \\phi(\\theta),\\ \\theta\\in\\mathbb{R},\\tag{6.45} \\end{align*}\\] then it holds that \\(\\nu_n\\stackrel{\\text{wk}}{\\to}\\nu\\). Definition 6.44. (Hansen) A sequence of probability measures \\(\\nu_1,\\nu_2,...\\) on \\(\\big(\\mathbb{R}^k,\\mathbb{B}_k\\big)\\) is said to converge weakly to a limit probability measure \\(\\nu\\) if \\[\\begin{align*} \\int f(x)\\ d\\nu_n(x)\\to\\int f(x)\\ d\\nu(x), \\forall f\\in C_b(\\mathbb{R}^k).\\tag{6.46} \\end{align*}\\] We will write \\(\\nu_n\\stackrel{\\text{wk}}{\\to}\\nu\\) to denote weak convergence. Theorem 6.45. (Hansen) (Continuity theorem) Let \\(\\nu,\\nu_1,\\nu_2,...\\) be probability measures on \\(\\big(\\mathbb{R}^k,\\mathbb{B}_k\\big)\\) with characteristic functions \\(\\phi,\\phi_1,\\phi_2,...\\). If \\[\\begin{align*} \\phi_n(\\theta)\\to \\phi(\\theta),\\ \\theta\\in\\mathbb{R}^k,\\tag{6.47} \\end{align*}\\] then it holds that \\(\\nu_n\\stackrel{\\text{wk}}{\\to}\\nu\\). Lemma 6.46. (Hansen) Let \\(\\mathbf{X}\\) be an \\(\\mathbb{R}^k\\)-valued random variable with characteristic function \\(\\phi_\\mathbf{X}\\), and let \\(\\mathbf{Y}\\) be an \\(\\mathbb{R}^m\\)-valued random variable with characteristic function \\(\\phi_\\mathbf{Y}\\). If \\(\\mathbf{X} \\perp \\!\\!\\! \\perp \\mathbf{Y}\\) then the bundle \\((\\mathbf{X},\\mathbf{Y})\\) is an \\(\\mathbb{R}^{k+m}\\)-valued random variable with \\[\\begin{align*} \\phi_{(\\mathbf{X},\\mathbf{Y})}(\\theta_1,\\theta_2)=\\phi_\\mathbf{X}(\\theta_1)\\phi_\\mathbf{Y}(\\theta_2),\\ \\theta_1\\in\\mathbb{R}^k,\\theta_2\\in\\mathbb{R}^m.\\tag{6.49} \\end{align*}\\] Theorem 6.47. (Hansen) (Continuous mapping theorem) Let \\(\\mathbf{X}_1,\\mathbf{X}_2,...\\) and \\(\\mathbf{X}\\) be random variables with values in \\(\\mathbb{R}^k\\), and let \\(h : \\mathbb{R}^k\\to\\mathbb{R}^m\\) be continuous. If \\(\\mathbf{X}_n\\stackrel{\\mathcal{D}}{\\to} \\mathbf{X}\\), then it holds that \\(h(\\mathbf{X}_n)\\stackrel{\\mathcal{D}}{\\to} h(\\mathbf{X})\\). Theorem 6.48. (Hansen) (Cramer-Wold’s device) Let \\(\\mathbf{X}_1,\\mathbf{X}_2,...\\) and \\(\\mathbf{X}\\) be random variables with values in \\(\\mathbb{R}^k\\). If \\[\\begin{align*} \\mathbf{v}^\\top\\mathbf{X}_n\\stackrel{\\mathcal{D}}{\\to} \\mathbf{v}^\\top\\mathbf{X},\\tag{6.51} \\end{align*}\\] for any fixed vector \\(\\mathbf{v}\\in\\mathbb{R}^k\\), then it holds that \\(\\mathbf{X}_n\\stackrel{\\mathcal{D}}{\\to} \\mathbf{X}\\). Lemma 6.49. (Hansen) Let \\(\\mathbf{X},\\mathbf{X}_1,\\mathbf{X}_2,...\\) be random variables with values in \\(\\mathbb{R}^k\\), let \\(\\mathbf{Y}_1,\\mathbf{Y}_2,...\\) be random variables in \\(\\mathbb{R}^m\\), and let \\(\\mathbf{y}\\) be a vector in \\(\\mathbb{R}^m\\). If it holds that \\[\\begin{align*} \\mathbf{X}_n\\stackrel{\\mathcal{D}}{\\to} \\mathbf{X},\\hspace{15pt}\\mathbf{Y}_n\\stackrel{\\text{P}}{\\to} \\mathbf{y} \\end{align*}\\] then the bundle \\((\\mathbf{X}_n,\\mathbf{Y}_n)\\) in \\(\\mathbb{R}^{k+m}\\) will satisfy that \\[\\begin{align*} (\\mathbf{X}_n,\\mathbf{Y}_n)\\stackrel{\\mathcal{D}}{\\to} (\\mathbf{X},\\mathbf{y}). \\end{align*}\\] Corollary 6.50. (Hansen) (Slutsky’s lemma) Let \\(\\mathbf{X},\\mathbf{X}_1,\\mathbf{X}_2,...\\) and \\(\\mathbf{Y}_1,\\mathbf{Y}_2,...\\) be random variables with values in \\(\\mathbb{R}^k\\). It holds that \\[\\begin{align*} \\mathbf{X}_n\\stackrel{\\mathcal{D}}{\\to} \\mathbf{X},\\hspace{10pt}\\mathbf{Y}_n\\stackrel{\\text{P}}{\\to} \\mathbf{0}\\hspace{10pt}\\Rightarrow\\hspace{10pt} \\mathbf{X}_n+\\mathbf{Y}_n\\stackrel{\\mathcal{D}}{\\to}\\mathbf{X}. \\end{align*}\\] Corollary 6.51. (Hansen) Let \\(\\mathbf{X}_1,\\mathbf{X}_2,...\\) and \\(\\mathbf{X}\\) be random variables with values in \\(\\mathbb{R}^k\\) and let \\(Y_1,Y_2,...\\) be real-valued random variables. It holds that \\[\\begin{align*} \\mathbf{X}_n\\stackrel{\\mathcal{D}}{\\to} \\mathbf{X},\\hspace{10pt}Y_n\\stackrel{\\text{P}}{\\to} 1\\hspace{10pt}\\Rightarrow\\hspace{10pt} Y_n\\mathbf{X}_n\\stackrel{\\mathcal{D}}{\\to}\\mathbf{X}. \\end{align*}\\] Corollary 6.52. (Hansen) Let \\(\\mathbf{X}_1,\\mathbf{X}_2,...\\) and \\(\\mathbf{X}\\) be random variables with values in \\(\\mathbb{R}^k\\) and let \\(Y_1,Y_2,...\\) be real-valued random variables. If \\[\\begin{align*} \\mathbf{X}_n\\stackrel{\\mathcal{D}}{\\to} \\mathbf{X},\\hspace{10pt}Y_n\\stackrel{\\text{P}}{\\to} 0\\hspace{10pt}\\Rightarrow\\hspace{10pt} Y_n\\mathbf{X}_n\\stackrel{\\text{P}}{\\to} \\mathbf{0}. \\end{align*}\\] Definition 6.53. (Hansen) The \\(\\mathbb{R}^k\\)-valued random variable \\(\\mathbf{Z}=(Z_1,...,Z_k)\\) has a multivariate Gaussian distribution if and only if the real-valued random variable \\(\\sum_{j=1}^kc_jZ_j\\) has a one-dimensional Gaussian distribution for every choice of \\(c_1,...,c_k\\in\\mathbb{R}\\). Theorem 6.54. (Hansen) Let \\(\\mathbf{Z}=(Z_1,...,Z_k)\\) have a multivariate Gaussian distribution with \\(E\\ \\mathbf{Z}=\\xi\\) and \\(V\\ \\mathbf{Z}=\\Sigma\\). Then the characteristic function is \\[\\begin{align*} \\phi_\\mathbf{Z}(\\theta)=e^{i\\theta^\\top\\xi}\\exp\\left(-\\frac{1}{2}\\theta^\\top\\Sigma\\theta\\right),\\ \\theta\\in\\mathbb{R}^k.\\tag{6.53} \\end{align*}\\] Conversly, if \\(\\mathbf{Z}\\) has characteristic function given by (6.53) for some \\(\\xi\\in\\mathbb{R}^k\\) and some symmetric, positive semi-definite \\(k\\times k\\) matrix \\(\\Sigma\\) then \\(\\mathbf{Z}\\) has a multivariate Gaussian distribution with \\(E\\ \\mathbf{Z}=\\xi\\) and \\(V\\ \\mathbf{Z}=\\Sigma\\). Corollary 6.55. (Hansen) Let \\(\\mathbf{Z}\\) be an \\(\\mathbb{R}^k\\)-valued random variable, let \\(\\mathbf{a}\\in\\mathbb{R}^m\\) and let \\(B\\) be an \\(m\\times k\\) matrix. It holds that \\[\\begin{align*} \\mathbf{Z}\\sim \\mathcal{N}(\\xi,\\Sigma)\\hspace{10pt}\\Rightarrow\\hspace{10pt} \\mathbf{a}+B\\mathbf{Z}\\sim \\mathcal{N}\\left(\\mathbf{a}+B\\xi,B\\Sigma B^\\top\\right). \\end{align*}\\] Lemma 6.56. (Hansen) Let \\(\\mathbf{X}=(X_1,...,X_k)\\) and \\(\\mathbf{Y}=(Y_1,...,Y_m)\\) be random variables with values in \\(\\mathbb{R}^k\\) respectively \\(\\mathbb{R}^m\\). If both \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) have multivariate Gaussian distributions and if \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are independent, then the \\(\\mathbb{R}^{k+m}\\)-valued bundle \\((\\mathbf{X},\\mathbf{Y})\\) has a multivariate Gaussian distribution. Lemma 6.58. (Hansen) Let \\(\\mathbf{X}=(X_1,...,X_k)\\) and \\(\\mathbf{Y}=(Y_1,...,Y_m)\\) be random variables with values in \\(\\mathbb{R}^k\\) respectively \\(\\mathbb{R}^m\\). If the \\(\\mathbb{R}^{k+m}\\)-valued bundle \\((\\mathbf{X},\\mathbf{Y})\\) has a multivariate Gaussian distribution, and if \\[\\begin{align*} \\text{Cov}(X_j,Y_l)=0,\\ \\forall j\\text{ and }l \\end{align*}\\] then \\(\\mathbf{X}\\perp \\!\\!\\! \\perp\\mathbf{Y}\\). Definition 6.59. (Hansen) Let \\(\\mathbf{X}_1,\\mathbf{X}_2,...\\) be \\(\\mathbb{R}^k\\)-valued random variables. Let \\(\\xi\\in\\mathbb{R}^k\\) be a vector and let \\(\\Sigma\\) be a symmetric, positive semi-definite \\(k\\times k\\) matrix. We sat that \\(\\mathbf{X}_n\\) has an asymptotic normal distribution with parameters \\(\\big(\\xi,\\frac{1}{n}\\Sigma\\big)\\), written \\[\\begin{align*} \\mathbf{X}_n\\stackrel{\\text{a.s.}}{\\sim}\\mathcal{N}\\left(\\xi,\\frac{1}{n}\\Sigma\\right), \\end{align*}\\] if it holds that \\[\\begin{align*} \\sqrt{n}(\\mathbf{X}_n-\\xi)\\stackrel{\\mathcal{D}}{\\to} \\mathcal{N}(0,\\Sigma). \\end{align*}\\] Lemma 6.60. (Hansen) Let \\(\\mathbf{X}_1,\\mathbf{X}_2,...\\) and \\(\\mathbf{Y}\\) be random variables with values in \\(\\mathbb{R}^k\\). If it holds that \\(\\mathbf{X}_n\\stackrel{\\text{a.s.}}{\\sim} \\mathcal{N}(\\xi,\\frac{1}{n}\\Sigma)\\) then it follows that \\(\\mathbf{X}_n\\stackrel{\\text{P}}{\\to}\\xi\\). Lemma 6.61. (Hansen) Let \\(\\mathbf{X}_1,\\mathbf{X}_2,...\\) and \\(\\mathbf{Y}\\) be \\(\\mathbb{R}^k\\)-valued random variables, and assume that \\(\\sqrt{n}\\mathbf{X}_n\\stackrel{\\mathcal{D}}{\\to} \\mathbf{Y}\\). Let \\(g : \\mathbb{R}^k\\to \\mathbb{R}^m\\) be a measurable map. Assume that \\(g(\\mathbf{0})=\\mathbf{0}\\) and that \\(g\\) is differentiable in \\(\\mathbf{0}\\) with deriviate \\(Dg(\\mathbf{0})=A\\). Then it holds that \\[\\begin{align*} \\sqrt{n}g(\\mathbf{X}_n)\\stackrel{\\mathcal{D}}{\\to} A\\ \\mathbf{Y}. \\end{align*}\\] Lemma 6.62. (Hansen) (Delta method) Let \\(\\mathbf{X}_1,\\mathbf{X}_2,...\\) be \\(\\mathbb{R}^k\\)-valued random variables, and let \\(f : \\mathbb{R}^k\\to \\mathbb{R}^m\\) be measurable. If \\(f\\) is differentiable in \\(\\xi\\), then it holds that \\[\\begin{align*} \\mathbf{X}_n\\stackrel{\\text{a.s.}}{\\sim} \\mathcal{N}\\left(\\xi,\\frac{1}{n}\\Sigma\\right)\\hspace{10pt}\\Rightarrow\\hspace{10pt} f(\\mathbf{X}_n)\\stackrel{\\text{a.s.}}{\\sim} \\mathcal{N}\\left(f(\\xi),\\frac{1}{n}Df(\\xi)\\Sigma Df(\\xi)^\\top\\right). \\end{align*}\\] 13.1.4 Central Limit Theorems Lemma 7.1. (Hansen) Let \\(z_1,...,z_n\\) and \\(w_1,...,w_n\\) be complex numbers. If \\(\\vert z_i\\vert \\le 1\\) and \\(\\vert w_i\\vert\\le 1\\) for all \\(i=1,...,n\\) then it holds that \\[\\begin{align*} \\left\\vert\\prod_{i=1}^n z_i-\\prod_{i=1}^n w_i\\right\\vert\\le \\sum_{i=1}^n \\vert z_i-w_i\\vert.\\tag{7.1 } \\end{align*}\\] Theorem 7.2. (Hansen) (Basic CLT) Let \\(X_1,X_2,...\\) be independent and identically distributed real-valued random variables. Assume that \\(E\\ X_1=0\\) and \\(E\\ X_1^2=1\\). Then it holds that \\[\\begin{align*} \\frac{1}{\\sqrt{n}}\\sum_{i=1}^nX_i\\stackrel{\\mathcal{D}}{\\to} \\mathcal{N}(0,1)\\tag{7.3} \\end{align*}\\] Theorem 7.3. (Hansen) (Laplace’s CLT) Let \\(X_1,X_2,...\\) be independent and identically distributed real-valued random variables with \\(E\\ X_1^2&lt;\\infty\\). It holds that \\[\\begin{align*} \\frac{1}{n}\\sum_{i=1}^nX_i\\stackrel{\\mathcal{D}}{\\to} \\mathcal{N}\\left(E\\ X_1,\\frac{V\\ X_1}{n}\\right)\\tag{7.4} \\end{align*}\\] Theorem 7.7. (Hansen) (Laplace’s CLT, multivariate version) Let \\(\\mathbf{X}_1,\\mathbf{X}_2,...\\) be independent and identically distributed random variables with values in \\(\\mathbb{R}^k\\). Assume that \\(E\\vert \\mathbf{X}_1\\vert^2&lt;\\infty\\). It holds that \\[\\begin{align*} \\frac{1}{n}\\sum_{i=1}^n\\mathbf{X}_i\\stackrel{\\mathcal{D}}{\\to} \\mathcal{N}\\left(E\\ \\mathbf{X}_1,\\frac{1}{n}V\\ \\mathbf{X}_1\\right)\\tag{7.7} \\end{align*}\\] Definition 7.10. (Hansen) Let \\((X_{nm})\\) be a centralized array of real-valued random variables. a. The array satisfies the vanishing variance condition if \\[\\begin{align*} \\max_{m=1,...,n}E\\ X_{nm}^2\\to 0.\\tag{7.8} \\end{align*}\\] b. The array satisfies Lindeberg’s condition if \\[\\begin{align*} \\forall c&gt;0:\\hspace{15pt}\\sum_{m=1}^n\\int_{(\\vert X_{nm}\\vert&gt;c)}X_{nm}^2\\ dP\\to 0.\\tag{7.9} \\end{align*}\\] c. The array satisfies Lyapounov’s condition of order \\(\\alpha&gt;2\\) if \\[\\begin{align*} \\sum_{m=1}^nE\\ \\vert X_{nm}\\vert ^\\alpha\\to 0.\\tag{7.10} \\end{align*}\\] Lemma 7.11. (Hansen) Lyapounov’s condition of order \\(\\alpha&gt;2\\) implies Lindeberg’s condition. Lemma 7.12. (Hansen) Lindeberg’s condition implies the vanishing variance condition. Theorem 7.14. (Hansen) (Lindeberg’s CLT) Let \\((X_{nm})\\) be a centralized array of real-valued random variables with \\(E\\ X_{nm}^2&lt;\\infty\\). Assume that the array satisfies that \\[\\begin{align*} E\\ X_{nm}=0,\\ \\forall n,m, \\end{align*}\\] and that \\[\\begin{align*} \\sum_{m=1}^n E\\ X^2_{nm}=1.\\tag{7.13} \\end{align*}\\] Assume that the array has independence within rows i.e. \\(X_{i1}\\perp \\!\\!\\! \\perp ... \\perp \\!\\!\\! \\perp X_{ii}\\) for all \\(i=1,...,n\\) and satisfies Lindeberg’s condition. Then it holds that \\[\\begin{align*} \\sum_{m=1}^nX_{nm}\\stackrel{\\mathcal{D}}{\\to} \\mathcal{N}(0,1). \\end{align*}\\] Theorem 7.19. (Hansen) (Lindeberg’s CLT, multivariate version) Let \\((\\mathbf{X}_{nm})\\) be a triangular array of random variables with values in \\(\\mathbb{R}^k\\) with \\(E\\ \\vert \\mathbf{X}_{nm}\\vert^2&lt;\\infty\\) for all \\(n\\), \\(m\\). Assume that \\[\\begin{align*} E\\ \\mathbf{X}_{nm}=\\mathbf{0},\\ \\forall n,m \\end{align*}\\] and \\[\\begin{align*} \\sum_{m=1}^n V\\ \\mathbf{X}_{nm}\\to \\Sigma \\end{align*}\\] for a fixed \\(k\\times k\\) matrix \\(\\Sigma\\). Assume that the array has independence within rows, and assume that the associated real-valued array \\((\\vert\\mathbf{X}_{nm}\\vert )\\) satisfies Lindeberg’s condition. Then it holds that \\[\\begin{align*} \\sum_{m=1}^n\\mathbf{X}_{nm}\\stackrel{\\mathcal{D}}{\\to} \\mathcal{N}(\\mathbf{0},\\Sigma) \\end{align*}\\] "],["markov-chains.html", "Chapter 14 Markov Chains ", " Chapter 14 Markov Chains "],["definition-of-a-markov-chain.html", "14.1 Definition of a Markov Chain", " 14.1 Definition of a Markov Chain Transitionsdiagram. Et diagram, hvor der fremgår ssh. for et system overgår til et nyt stadie. Definition. (Stokastisk process i diskret tid) Lad \\(\\mathcal{X}=(\\mathcal{X}_n)_{n\\in\\mathbb{N}_0}\\) være en familie af stokastiske variable og \\(\\mathcal{S}\\) et udfaldsrum . Da er \\(\\mathcal{X}\\) en funktion der opfylder \\(\\mathcal{X} : \\mathbb{N}_0\\times \\Omega \\to \\mathcal{S}\\) med \\((n,w) \\mapsto \\mathcal{X}_n(w)\\). Bemærkning. \\(\\mathbb{N}_0\\) er tidsvariablen senere \\([0,\\infty)\\) \\(\\mathcal{S}\\) er højst tællelig Definition. (Markov kæder i diskret tid) En stokastisk proces i diskret tid \\((X_n)_{n\\in\\mathbb{N}_0}\\) har hvis for alle stadier \\(i_0,...,i_n\\in\\mathcal{S}\\) opfylder \\[ \\mathbb{P}(\\mathcal{X}_n=i_n\\vert \\mathcal{X}_0,...,\\mathcal{X}_{n-1}=i_{n-1})=\\mathbb{P}(\\mathcal{X}_n=i_n \\vert \\mathcal{X}_{n-1}=i_{n-1}). \\] Da kaldes denne stokastiske proces en Markov kæde. Dvs. For hvert \\(n\\) er det næste udfald kun afhængig af dette forrige udfald. Definition. (Initial fordeling) For alle \\(i\\in\\mathcal{S}\\) tilhører en initial sandsynlighed for at starte i stadiet \\(i\\) givet ved \\[ \\phi(i):=\\mathbb{P}(\\mathcal{X}_0=i), \\] hvor familien \\(\\overline{\\phi}=(\\phi(i))_{i\\in\\mathcal{S}}\\) er initial fordelingen i.e. \\(\\phi\\) er en vektor over alle mulige startsudfald og deres tilhørende ssh. Hvis \\(\\mathcal{S}\\) er endelig er \\(\\overline{\\phi}\\) blot en rækkevektor. Definition. (Transitionsmatricen) For to stadier \\(i,j\\in\\mathcal{S}\\) og et tidspunkt \\(n\\) er transitionssandsynligheden for at flytte fra \\(i\\) til \\(j\\) er \\[ P_{ij}(n)=\\mathbb{P}(\\mathcal{X}_{n+1}=j \\vert \\mathcal{X}_n=i). \\] Hvis \\(P_{ij}(n)=P_{ij}(m)\\) for alle \\(n,m\\in\\mathbb{N}_0\\) så kaldes \\((\\mathcal{X}_n)_{n\\in\\mathbb{N}_0}\\) tidshomogent (definition 1). Den tilhørende matrice \\(P=(P_{ij})_{i,j\\in\\mathcal{S}}\\) er kaldet transitionsmatricen. Bemærkning. En transitionsmatrice karrakteriserer et transitions diagram og omvendt. \\[ P=\\begin{pmatrix} 2/3 &amp; 1/3\\\\ 1/2 &amp; 1/2 \\end{pmatrix}. \\] Theorem 2. (Fordeling af \\(\\mathcal{X}_n\\)) For en Markov kæde på \\(\\mathcal{S}\\) med initial fordeling \\(\\phi\\) og transitionsmatrix \\(P\\) er \\[ (\\mathbb{P}(X_n=i))_{i\\in\\mathcal{S}}=\\overline{\\phi} P^n \\] "],["classification-of-states.html", "14.2 Classification of states", " 14.2 Classification of states Definition 4. (Communication classes) Lad \\(i,j\\in\\mathcal{S}\\) være to stadier. Vi definere \\(i\\longrightarrow j\\) dvs. \\(j\\) er accesible fra \\(i\\), hvis der eksisterer et \\(n\\in\\mathbb{N}\\) så \\(P_{ij}^n&gt;0\\). Samt \\(i\\longleftrightarrow j\\) dvs. \\(i\\) og \\(j\\) kommunikerer, hvis både \\(i\\longrightarrow j\\) og \\(j\\longrightarrow i\\). Bemærkning. “\\(i\\longleftrightarrow j\\)” er en ækvivalens relation. Derfor sepererer denne relation \\(\\mathcal{S}\\) ind i to disjunkte ækvivalens klasser, hvad vi definerer som communication classes. Definition 6. (Lukket kommunikations klasser) En kommunikations klasse \\(e\\subseteq \\mathcal{S}\\) kaldes lukket, hvis for alle \\(i\\in e\\) gælder \\(\\sum_{i\\in e}P_{ij}=1\\) (den \\(i\\)’te søjlesummen sum er lig 1 eller ssh. for at forblive i klassen er 1). Definition. (Ikke reducibel Markov kæde) En Markov kæde kaldes irreducible, hvis der eksisterer kun en kommunikations klasse. Ellers er den reducible. Definition. (Hitting time) Tiden for at ramme \\(i\\in \\mathcal{S}\\) er givet ved \\(T_i:=\\inf\\{n&gt;0 \\vert X_n=i\\}\\). Hvis \\(X_0=i\\) kaldes denne tid return/recurrence tiden. Definition 9. (Recurrance/transients) For en Markov kæde \\((X_n)_{n\\in\\mathbb{N}}\\) på \\(\\mathcal{S}\\) er et stadie \\(i\\in\\mathcal{S}\\) kaldet recurrent hvis \\(\\mathbb{P}(T_i&lt;\\infty \\vert X_0=i)=1\\). Ellers kaldes \\(i\\) transient. Bemærkning. Det kan vises at \\(\\mathbb{P}(T_i&lt;\\infty \\vert X_0=i)=1\\) er ensbetydende med at \\(\\mathbb{P}(N_i=+\\infty \\vert X_0=i)=1\\). (se Thm 14 i noterne) Theorem 11. (Recurrence criterium 1) For en Markov kæde på \\(\\mathcal{S}\\) med transistionsmatrice \\(P\\), da er følgende ækvivalent. Det holder at \\(\\sum_{n=1}^\\infty (P^n)_{ii}=+\\infty\\) og \\(i\\) er recurrent. Theorem 12. (Recurrence som klasseegenskab) Alle stadier i en kommunikations klasse er enten recurrent eller transient dvs. hvis blot en af stadierne \\(i\\in e\\) er recurrent er alle recurrent og ligeledes med transient. Theorem 13. (Antal af besøg) Antal af besøg er givet ved den stokastiske variabel \\(N_i:=\\sum_{n=1}^\\infty 1(X_n=i)\\). Hvis \\(i\\) er recurrent er \\(\\mathbb{P}(N_i=+\\infty)=1\\), hvis \\(i\\) er transient er \\(N_i\\sim Geo(q)\\) på \\(N_0\\) dvs. \\(\\mathbb{P}(N_i=k)=(1-q)^kq\\) for \\(k\\in\\mathbb{N}_0\\) og \\(q=P(T_i=+\\infty\\vert X(0)=i)\\). Theorem 14. For en endelig kommunikationsklasse \\(C\\subseteq S\\) gælder: (\\(C\\) er recurrent) \\(\\Leftrightarrow\\) (\\(C\\) er lukket.) Theorem 16. (Recurrence criterium 2) For en irreducibel Markov Kæde på \\(S\\) gælder (\\(i\\in S\\) er recurrent) \\(\\Leftrightarrow\\) (For ligningssystemmet \\(\\alpha(j)=\\sum_{k\\ne i}P_{j,k}\\alpha(k)\\) gælder \\(\\alpha(j)=0\\) for alle \\(j\\ne i\\) er den eneste endelige løsning). Generaliseret matrix multiplikation. For \\(\\underline{\\phi}=(\\phi_i)_{i\\in\\mathcal{S}},\\underline{\\psi}=(\\psi_i)_{i\\in\\mathcal{S}}\\) (vektorer) og \\(P=(P_{ij})_{i,j\\in\\mathcal{S}},Q=(Q_{ij})_{i,j\\in\\mathcal{S}}\\) (matricer) kan følgende produkter udregnes ved \\((\\underline{\\phi}\\cdot \\underline{\\psi}^T):=\\sum_{i\\in\\mathcal{S}}\\phi_i\\psi_i\\) \\((\\underline{\\phi}\\cdot P)_j:=\\left(\\sum_{i\\in\\mathcal{S}}\\phi_i P_{ij}\\right)_j\\) \\((P\\cdot Q)_{ij}:=\\sum_{k,l\\in\\mathcal{S}}P_{i,k}Q_{l,j}\\) "],["limit-results-and-invariant-probabilities.html", "14.3 Limit results and invariant probabilities", " 14.3 Limit results and invariant probabilities Sidenote. For en vektor \\(\\overline{\\phi}=(\\phi_i)_{i\\in \\mathcal{S}}\\) kaldes en fordeling, hvis for alle \\(i\\in\\mathcal{S}\\) er \\(\\phi_i\\ge0\\) og \\(\\sum_{i\\in\\mathcal{S}}\\phi_i=1\\) og et mål hvis kun \\(\\phi_i\\ge 0\\). Definition 18. (Løkker og perioder) For en Markov kæde i diskret tid er en mulig løkke af længde \\(n\\) er en følge af stadier \\(i_0,i_1,i_2,...,i_n\\in\\mathcal{S}\\) med \\(i_0=i_n\\) hvor \\[ P_{i_0,i_1}\\cdots P_{i_{n-1},i_n}&gt;0 \\] Perioden af et stadie \\(i\\in\\mathcal{S}\\) er den største fælles divisor af \\[ D_i=\\{n\\in\\mathbb{N}\\ \\vert\\ \\exists \\text{a possible loop of length $n$ med $i_0=i_n=i$}\\}, \\] dvs. \\(\\text{per}(i)=GCD(D_i)\\). Hvis perioden er 1 kaldes stadiet aperiodisk. Theorem 19. Alle stadier i en kommunikations klasse har samme periode. Theorem 21. For en irreducibel, recurrent og ikkeperiodisk Markov kæde på \\(\\mathcal{S}\\) gælder for alle \\(i\\in\\mathcal{S}\\) at \\[ \\lim_{n\\to\\infty}P(X_n=i)=\\frac{1}{E[T_i \\vert X_0=i]} \\] Hvis \\(E[T_i \\vert X_0=i]=\\infty\\) defineres \\(\\lim_{n\\to\\infty}P(X_n=i):=0\\). Bemærkning. 1) Kun hvis Markov kæden er recurrent og dermed at \\(P(T_i=\\infty \\vert X_i=0)=0\\) er \\(E[T_i \\vert X_0=i]=\\sum_{n=1}^\\infty nP(T_i=n\\vert X_0=i)\\) eller er \\(E[T_i \\vert X_0=i]=+\\infty\\). 2) Hvis \\(E[T_i \\vert X_0=i]=\\infty\\) så er grænsen ikke en fordeling. Definition 22. Et recurrent stadie \\(i\\in\\mathcal{S}\\) kaldes positivt recurrent hvis og kun hvis \\(E[T_i \\vert X_0=i]&lt;\\infty\\). Ellers kaldes stadiet nul-recurrent. Definition. (Invariant fordeling og mål) En ikke negativ vektor \\(\\overline{\\pi}=(\\pi_j)_{j\\in S}\\) kaldes et invariant mål, hvis \\(\\overline{\\pi}=\\overline{\\pi}P\\) og en invariant fordeling, hvis \\(\\vert\\overline{\\pi}\\vert\\). Theorem 23. For en irreducibel og recurrent Markov kæde på \\(\\mathcal{S}\\) findes et invariant mål \\(\\nu\\) der opfylder \\(\\nu =\\nu P\\). Specielt gælder for alle fixed \\(i\\in\\mathcal{S}\\) holder det at \\[ \\nu_j=E\\left[\\sum_{n=0}^{T_i-1}1(X_n=j \\vert X_0=i)\\right] \\text{for alle } j\\in\\mathcal{S} \\] er et invariant mål. Hvis og kun hvis Markov kæden er positiv recurrent kan ovenstående nomaliseres til en invariant fordeling. Bemærkning. 1) \\(i\\in\\mathcal{S}\\) er abitrær, 2) \\(\\nu(i)=1\\) og 3) for irreducible MC gælder (Positiv recurrent) \\(\\Leftrightarrow\\) (Eksisterer unik invariant fordeling) Theorem 24. or en irreducibel, aperiodisk og positiv recurrent Markov kæde gælder \\(\\lim_{n\\to\\infty}P(X_n=j)=\\pi_j=1/E[T_j\\vert X_0=j]\\). Hvor \\(\\overline{\\pi}=(\\pi_j)_{j\\in S}\\) er en invariant fordeling. Theorem 25 og 26. (Grænseresultat for nul-recurrance og tranience) For et nul-recurrent (THM 25) eller transient (THM 26) stadie \\(j\\in S\\) gælder \\(\\lim_{n\\to\\infty}P(X_n=j)=0\\) for alle valg af initialfordeling \\(\\overline{\\phi}\\). Theorem 27. (Grænseresultat for periodiske stadier) For en irreducibel Markov kæde med periode \\(d&gt;1\\) findes grænsen \\(\\lim_{n\\to \\infty}P(X_n=i)\\) ikke. Men gennemsnittet for en periode gør dvs \\[ \\nu_j=\\lim_{n\\to\\infty}\\frac{P(X_n=i)+P(X_{n+1}=i)+...+P(X_{n+d-1}=i)}{d} \\] \\(\\nu=(\\nu_j)_{j\\in S}\\) er et invariant mål og en invariant fordeling hvis \\(\\vert\\nu\\vert=1\\). "],["absorbing-probabilities.html", "14.4 Absorbing probabilities", " 14.4 Absorbing probabilities Theorem 29. (Absorberende sandsynlighed - endelige udfaldsrum) Lad en endelig Markov kæde være givet ved transistionsmatricen \\(P\\). Antaget at transtionsmatricen kan inddeles efter kummunikations klasser så \\[ P= \\left[ \\begin{array}{c|c} \\tilde{P} &amp; 0 \\\\ \\hline S &amp; Q \\end{array} \\right], \\] hvor \\(\\tilde{P}\\) er transistionsmatricen indeholdende kun recurrent stadier. Dertil er \\(Q\\) og \\(S\\) sub-matricer af \\(P\\), hvor \\(Q\\) indeholder de transient stadier og \\(S\\) beskriver sandsynlighederne for overgangen fra et transient til et recurrent stadie. Matricen 0 repræsenterer sandsynligheden for at gå fra en recurrent til transient stadie, hvad er umuligt. Definer dertil matricerne \\(M=(I-Q)^{-1}\\) og \\(A=(I-Q)^{-1}S\\). Tallet \\(M_{i,j}\\) repræsenterer \\(E[N_j\\vert X_0=i]\\) for transient stadier \\(i,j\\). Tallet \\(A_{i,j}\\) repræsenterer sandsynligheden for at \\(j\\) er det første recurrent stadie, der besøger hvis \\(X_0=i\\) for et transient stadie \\(i\\). Theorem 31. (Absorberende sandsynlighed - tællelige udfaldsrum) For en Markov kæde på \\(S\\) lad \\(C\\subseteq S\\) være en recurrent klasse og \\(C&#39;\\in S\\setminus C\\) være en transient klasse. Absorberingssandsynligheden \\((\\alpha_j)_{j\\in C&#39;}\\) givet ved \\(\\alpha_j=P(X_n\\in C \\vert X_0=j)\\) løser ligningssystemet \\[ \\alpha_i=\\sum_{l\\in S\\setminus C}P_{i,l}\\alpha_l+\\sum_{l\\in C}P_{i,l} \\] Der findes en endelig løsning hvis og kun hvis \\(\\lim_{n\\to\\infty}P(X_n\\in C&#39;\\vert X_o\\in C&#39;)=0\\). "],["markov-chains-in-continuous-times.html", "14.5 Markov Chains in Continuous Times", " 14.5 Markov Chains in Continuous Times Definition. (Stokastisk process i kontinuer tid) En stokastisk proces i kontinuer tid er en familie \\(X=(X_t)_{t\\in[0,\\infty)}\\) af stokastiske variable. Eksempel 33. (Poisson processen) Lad \\((W_i)_{i\\in\\mathbb{N}}\\) være en følge af uafhængige stokastiske variable, hvor \\(W_i\\sim Exp(\\lambda)\\) dvs. \\(W_i\\) har tæthed \\(f(x)=\\lambda \\exp(-\\lambda x)\\). Betragt \\(W_i\\) som ventetiden indtil en begivenhed af interesse indtræffer. Lad \\(\\tau_n:=W_1+...+W_n\\) være tiden indtil den \\(n\\)’te begivenhed indtræffer. Den stokastiske variabel \\(N_t=\\sum_{n=1}^\\infty 1(\\tau_n\\le t)\\) der tæller antal begivenheder indtil tidspunkt \\(t\\) definerer en stokastisk proces i kontinuer tid dvs. processen \\((N_t)_{t\\ge 0}\\). Vi kalder denne proces Poisson processen. Definition 34. (Homogen Markov kæde i kontinuer tid) En kontinuer Markov kæde på en højst tællelig mængde \\(S\\) er en familie af stokastiske variable \\((X_t)_{t\\ge0}\\) på sandsynlighedsrummet \\((\\Omega, \\mathcal{F},P)\\) der opfylder \\[\\begin{align*} &amp;P(X(t_{n+1})=j\\vert X(t_n)=i, X(t_{n-1})=i_{n-1},...,X(t_0)=i_0)\\\\ =\\ &amp;P(X(t_{n+1})=j\\vert X(t_n)=i)=P_{i,j}(t_{n+1}-t_n) \\end{align*}\\] for \\(j,i,i_{n-1},...,i_0\\in S\\) og \\(t_{n+1}&gt;t_n&gt;...&gt;t_0\\ge0\\). Fordelingen af Markov kæden er givet ved initialfordelingen \\(\\overline{\\phi}=(P(X_0=i))_{i\\in S}\\) og transistionssandsynlighederne \\(P_{i,j}(t)=P(X(t+s)=j\\vert X(s)=i)\\) og identiteten \\[\\begin{align*} &amp;P(X(t_{n+1})=j, X(t_n)=i, X(t_{n-1})=i_{n-1},...,X(t_0)=i_0)\\\\ =\\ &amp;P_{i,j}(t_{n+1}-t_n)\\cdot ...\\cdot P_{i_0,i_1}(t_1-t_0)\\phi(i_0) \\end{align*}\\] Definition 35. (Minimal konstruktion) Lad \\(\\overline{\\phi}=(\\phi_i)_{i\\in S}\\) være en sandsynlighedsvektor og lad \\(Q=(q_{i,j})_{i,j\\in S}\\) være en intensitetsmatrice dvs. \\(q_{i,j}\\) er reelle tal med egenskaberne 1) alle ikke diagonal indgange er ikke negative dvs. \\(q_{i,j}\\ge 0\\) for alle \\(i,j\\in S\\) og \\(i\\ne j\\) og 2) diagonal er negativ lig rækkesummen dvs. \\(q_{i,i}=-\\sum_{j\\ne i}q_{i,j}\\). Givet \\(\\overline{\\phi}\\) og \\(Q\\) kan en tids-homogen Markov kæde konstrueres ved følgende fem trin. Matricen \\(Q\\) kaldes transistionsintensiteten for den stokastiske proces \\((X_t)_{t\\ge 0}\\). Vælg \\(\\gamma(0)\\) ud fra initialfordelingen, så \\(P(\\gamma(0)=i)=\\phi(i)\\). givet \\(\\gamma(0)\\) sæt \\(\\tau_1:=W_1\\sim Exp(q_{\\gamma(0)})\\) og definer \\(X(t)=\\gamma(0),t\\in[0,W_1)\\) givet \\(\\gamma(0)\\) og \\(W_1\\) vælg \\(\\gamma(1)\\) sådan at \\(P(\\gamma(1)\\vert \\gamma (0))=\\frac{q_{\\gamma(0),i}}{q_{\\gamma(0)}},i\\ne \\gamma(0)\\) recursivt givet \\(\\gamma(0),...,\\gamma(n),W_1,...,W_n\\) vælg \\(W_{n+1}\\sim Exp(q_{\\gamma(n)}\\). Lad \\(\\tau_{n+1}=\\tau_n+W_{n+1}\\) og definer \\(X(t)=\\gamma(n),t\\in[\\tau_n,\\tau_{n+1})\\) vælg \\(\\gamma(n+1)\\) sådan at \\(P(\\gamma(n+1)\\vert \\gamma(0),...,\\gamma(n),W_1,...,W_n)=\\frac{q_{\\gamma(n),i}}{q_{\\gamma(n)}},i\\ne Y(n)\\). Definition 37. (Absorbering) Hvis Markov kæden givet ved minimal konstruktion på tidspunkt \\(\\tau_n\\) hopper til stadie \\(\\gamma(n)=i\\) med \\(q_i=0\\) så lader vi \\(X(t)=\\gamma(n)\\) for \\(t\\ge \\tau_n\\) og vi siger at Markov kæden er absorberet på stadie \\(i\\). Definition 38. (Eksplosion) Hvis Markov kæden hopper uendeligt mange gange på et endeligt interval dvs. \\(\\tau_\\infty :=\\lim_{n\\to\\infty}\\tau_n&lt;+\\infty\\) og dermed \\(P(\\tau_\\infty&lt;+\\infty)&gt;0\\). Lader vi \\(X(t)=\\Delta\\) for \\(t\\ge \\tau_\\infty\\). Vi kalder tidspunktet \\(\\tau_\\infty\\) for eksplosionstidspunktet. Theorem 39. (Embedded Markov Chain of jumps) For en markov kæde i kontinuert tid med transitions intensiteter (intensitetsmatrice) \\(Q = (q_{i,j})_{i,j\\in s}\\) givet ved den minimale konstruktion fra definition 35, er følgen \\((\\Gamma(n))_{n \\in \\mathbb{N}_0}\\) af besøgte stadier en markov kæde i diskret tid med transitionsmatricen \\(P\\) givet ved \\[\\begin{align*} \\left\\{\\begin{array}{cc} -\\frac{q_{i,j}}{q_{i,i}} = \\frac{q_{i,j}}{q_i} &amp; i \\in S \\setminus A, j \\notin \\{i,\\Delta\\} \\\\ 0 &amp; i \\in S \\setminus A, j \\in \\{i, \\Delta\\} \\\\ 0 &amp; i \\in A, j \\ne i \\\\ 1 &amp; i \\in A, j = i \\end{array}\\right. \\end{align*}\\] Hvor \\(A = \\{i \\in S \\vert q_i = 0\\}\\) er delmængden af absorberende stadier. Bemærkning: Det er klart, at de enkelte indgange \\((P_{i,j}(t))_{i,j \\in S}\\) (\\(t\\ge0\\)) i transistionsmatricen må være ikke negative. Endvidere er rækkesummerne, for en markov kæde hvor eksplosion ikke er mulig, lig 1. Specielt er transitionssandsynlighederne for et valgt \\(t \\ge 0\\) i overenstemmelse med dem som vi har set i kapitel 2. For skift i tid, kræves næste teorem. "],["properties-of-transitionsprobabilities.html", "14.6 Properties of transitionsprobabilities", " 14.6 Properties of transitionsprobabilities Theorem 42. (Chapman-Kolmogorov ligninger) Transitionssandsynlighederne for en homogen markov kæde i kontinuert tid opfylder Chapman-Kolmogorov ligningerne \\[ \\forall s,t \\ge 0, \\forall i,j \\in S : P_{i,j}(t+s) = \\sum_{l\\in S}P_{i,l}(t) \\cdot P_{l,j}(s) \\] Hvis state space er endeligt, så kan \\(P(t) = (P_{i,j}(t))_{i,j\\in S}\\) ses som en matrice for hvilket som helst valgt \\(T \\ge 0\\) og så kan Chapman-Kolmogorov ligningerne skrives som matrice ligningen \\[ P(t+s) = P(t)P(s) \\] Theorem 43. (Infinitesimal generatoren af en markov kæde) For en markov kæde i kontinuert tid, kan transitionsintensiteterne udledes fra transitionssandsynlighederne som grænserne \\[\\begin{align*} \\lim_{t\\to0+}\\frac{P_{i,i}-1}{t} = -q_i \\\\ \\lim_{t\\to0+}\\frac{P_{i,j}}{t} = q_{i,j}, \\hspace{5pt} i \\ne j \\end{align*}\\] Theorem 44. (Backward differential equations) For en markov kæde i kontinuert tid holder det altid, at \\[ DP_{i,j}(t) = P_{i,j}&#39;(t) = -q_iP_{i,j}(t) + \\sum_{k\\ne i}q_{i,k}P_{k,j}(t) \\] Theorem 45. (Forward differential and integral equations) For en markov kæde i kontinuert tid, holder det, at \\[ P_{i,j}(t) = \\delta_{i,j}\\exp(-q_jt)+\\int^t_0\\sum_{l\\ne j}P_{i,l}(u)q_{l,j}\\exp(-q_j(t-u))du \\] og at \\[ DP_{i,j}(t) = -P_{i,j}(t)q_j+\\sum_{l \\ne j}P_{i,l}(t)q_{l,j}. \\] Theorem 47. (Transitionssandssynlighederne for et endeligt state space) For en markov kæde i kontinuert tid med endeligt state space, kan den “backward differential equation” udtrykkes i matrice form som \\[ DP(t) = P&#39;(t) = QP(t) \\] Hvor \\(P(t) = (P_{i,j}(t))_{i,j \\in S}\\) Ved at bruge startbetingelsen \\(P(0) = I\\), kan transistionssandsynlighederne udtrykkes på formen af eksponentiel matricen \\[ P(t) = \\exp(Qt), \\hspace{5pt} t \\ge 0. \\] "],["invariant-probabilies-and-absorption.html", "14.7 Invariant probabilies and absorption", " 14.7 Invariant probabilies and absorption Definition 49. (Kommunikationsklasser og “irreducibility”) To stadier \\(i,j \\in S\\) siges at kommunikere, hvis der findes \\(s,t &gt; 0\\) således at \\[ P_{i,j}(s) &gt; 0 \\text{ og } P_{j,i}(t) &gt; 0. \\] Denne definition inddeler \\(S\\) i disjunkte kommunikationsklasser, ligesom vi har set i kapitel 2. En markov kæde i kontinuert tid siges at være “irreducible” hvis der kun findes én kommunikationsklasse. Bemærkning: Man kan anvende, at to stadier \\(i,j \\in S\\), hvor \\(i \\ne j\\) kommunikerer, hvis og kun hvis, der eksisterer en følge af stadier \\(i_1, i_2, ..., i_n \\in S\\) (som indeholder stadie \\(j\\)) således, at \\(q_{i,i_1}\\cdot q_{i_1,i_2}\\cdot...\\cdot q_{i_{n-1},i_n}\\cdot q_{i_n,i} &gt; 0\\). (Er der en mulig sti frem og tilbage mellem \\(i\\) og \\(j\\)?) Definition 50. (Recurrence og transience) En irreducible markov kæde i kontinuert tid er recurrent, hvis og kun hvis, den embedded markov kæde (se teorem. 39) er recurrent. Den er transient, hvis og kun hvis, den embedded markov kæde er transient. Stadiet \\(i\\) er transient, hvis og kun hvis, den totale tid tilbragt i stadiet \\(i\\) \\[ V_i = \\int^\\infty_01_{X(t) = i}dt \\] er endelig med sandsynlighed 1, altså \\(P(V_i &lt; +\\infty \\vert X(0) = i) = 1\\). Tilsvarende er stadiet \\(i\\) recurrent hvis \\(P(V_i = +\\infty \\vert X(0) = i) = 1\\). Bemærkning: Denne definition kan anvendes på de enkelte kommunikationsklasser. Specielt er et absorberende stadie altid recurrent. Definition 51. (Invariant fordeling) En sandsynlighedsvektor \\(\\overline{\\pi} = (\\pi(i))_{i \\in S}\\) er en invariant (eller stationær) fordeling for en markov kæde i kontinuert tid, hvis for alle \\(t \\ge 0\\) og \\(j \\in S\\) \\[ \\pi(j) = \\sum_{i \\in S}\\pi(i)P_{i,j}(t). \\] Theorem 52. (Entydigheden af den invariante fordeling) Overvej en markov kæde i kontinuert tid. En invariant fordeling er unik, hvis den eksisterer. Hvis der for et \\(t_0 &gt; 0\\) findes en sandsynlighed \\(\\overline{\\pi}=(\\pi(i))_{i\\in S}\\) sådan, at \\[ \\forall j \\in S : \\pi(j) = \\sum_{i \\in S}\\pi(i)P_{i,j}(t_0) \\] kan vi konkludere, at \\(\\forall i \\in S : \\pi(i) &gt; 0\\) \\(P(t_0)\\) er en overgangssandsynlighed, dvs. \\[ \\forall i \\in S : \\sum_{j \\in S}\\pi(i)P_{i,j}(t_0) = 1 \\] \\(\\overline{\\pi}\\) er en invariant fordeling for markov kæden, dvs. \\[ \\forall t \\ge 0, \\forall j \\in S : \\pi(j) = \\sum_{j \\in S}P_{i,j}(t_0) = 1 \\] Bemærkning: Det følger af (3), at hvis vi kan finde en invariant fordeling for et bestemt \\(t_0 &gt; 0\\), så gælder denne for alle \\(t &gt; 0\\). Det følger også, som resultat af dette, at alle rækkesummerne \\(\\sum_{j \\in S}P_{i,j}(h) = 1\\) for alle \\(h \\ge 0\\), hvor en invariant fordeling kan findes. Theorem 53. (Grænseresultater for overganssandsynligheder) For en irreducible markov kæde i kontinuert tid, med en invariant fordeling \\(\\overline{\\pi}\\), gælder det for alle \\(i,j \\in S\\), at \\[ \\lim_{t \\to \\infty}P_{i,j}(t) = \\pi(j). \\] Endvidere gælder det for enhver initial fordeling \\(\\overline{\\phi}\\), at \\[ \\lim_{t \\to \\infty}P(X(t) = j) = \\pi(j). \\] Hvis der ikke findes en invariant fordeling, så gælder der \\[ \\lim_{t \\to \\infty}P_{i,j}(t) = 0. \\] Theorem 55. (Nødvendig betingelse for invariant fordeling) For en markov kæde i kontinuert tid, er det nødvendigt, at den invariante fordeling \\(\\overline{\\pi}\\) passer med ligningsystemet \\[ \\forall j \\in s : \\sum_{i \\in S}\\pi(i)q_{i,j} = 0 \\] Eller tilsvarende \\[ \\forall j \\in S : \\sum_{i\\ne j}\\pi(i)q_{i,j} = \\pi(j)(-q_{j,j}) = \\pi(j)q_j \\] Tænker man på \\(\\overline{\\pi}\\) som en række vektor og \\(Q\\) som en matrice, har ligningssystemet en mere kompakt formulering \\[ \\overline{\\pi}Q = 0. \\] Theorem 56. (Tilstrækkelig betingelse for en invariant fordeling) Hvis \\(\\overline{\\pi}\\) overholder betingelsen i teorem 55 og endvidere overholder \\[ \\sum_{j \\in S}\\pi(j)q_j &lt; \\infty \\] Så er \\(\\overline{\\pi}\\) en unik invariant fordeling for en irreducible markov kæde. Theorem 58. En irreducible markov kæde i kontinuert tid har en invariant fordeling, hvis og kun hvis, den indlejrede (embedded) markov kæde er recurrent og der ekstisterer en sandsynlighedsvektor \\(\\overline{\\pi}\\) således, at teorem 55 er overholdt. (\\(\\overline{\\pi}Q = 0\\)) Theorem 60. (Tids-invariant vs. hændelses-invariant fordeling) Antag, at der for en irreducible markov kæde i kontinuert tid findes en invariant fordeling \\(\\overline{\\nu}\\). Hvis vi også har bekræftet eksistensen af en invariant fordeling for den indlejrede markov kæde, så gælder følgende forhold mellem dem. \\[ \\pi(i) = \\frac{\\nu(i)q_i}{\\sum_{j \\in S}\\nu(j)q_j}, \\hspace{5pt} i \\in S. \\] Theorem 61. (Eksistens af invariant fordeling og positive recurrence) For en irreducible og recurrent markov kæde i kontinuert tid definerer vi “escape time” fra stadiet \\(i\\) som \\[ W_i=\\inf\\{t &gt; 0 \\vert X(t) \\ne i\\} \\] Og “return time” til stadiet \\(i\\) som \\[ R_i = \\inf\\{t &gt; W_i \\vert X(t) = i\\} \\] Og der gælder, at \\[ \\pi(i) = \\frac{E[W_i \\vert X(0) = i]}{E[R_i \\vert X(0) = i]} = \\frac{1}{q_iE[R_i \\vert X(0) = i]} \\] Vi siger så, at en kommunikationsklasse er “positive recurrent” hvis \\[ E[R_i \\vert X(0) = i] &lt; +\\infty \\] Bemærkning: Positive recurrence er en klasseegenskab. Theorem 62. (Tid tilbragt i stadiet j før absorbering) For en markov kæde i kontinuert tid findes det gennemsnitlige antal af besøg til stadiet \\(j\\) før det absorberende stadie \\(i\\) rammes, ved at betragte overgangssandsynlighederne af den indlejrede markov kæde. For et endeligt state space, kan man bruge teorem 29, mens man kan bruge teorem 31 for tælleligt uendelige state spaces. Hvis \\(N_j\\) er middelværdien af antal besøg til stadiet \\(j\\) før absorbering i stadiet \\(i\\), så er middelværdien af tid tilbragt i stadiet j lig \\(\\frac{N_j}{q_j}\\). Side note: Eksempel 63 har været meget brugbar til at forstå kapitel 3.3. "],["birth-death-processes.html", "14.8 Birth-death processes", " 14.8 Birth-death processes Definition. (Birth-and-death process) En birth-and-death proces er en markov kæde på \\(S = \\mathbb{N}_0\\) som kun tillader hop (op eller ned) af størrelset ét. Dvs, at for overgangsintesiterne, gælder \\(q_{i,j} = 0, i,j \\in \\mathbb{N}_0, |i-j| &gt; 1\\), mens de eneste ikke-nul indgange (udover diagonalen) er lig \\[\\begin{align*} q_{i,i+1} = \\beta_i, i \\in \\mathbb{N}_0 \\rightarrow \\text{ birth intensitet} \\\\ q_{i,i+1} = \\delta_i, i \\in \\mathbb{N} \\rightarrow \\text{ death intensitet} \\end{align*}\\] Adfærden af sådan en proces er meget simpelt at beskrive. Hvis man befinder sig i stadiet \\(i\\), så er ventetiden til det næste hop eksponentiel fordelt med parametren \\(\\beta_i+\\delta_i\\) og med gennemsnitstiden \\(\\frac{1}{\\beta_i+\\delta_i}\\). Når kæden hopper, kan den hoppe op med sandsynlighed \\(\\frac{\\beta_i}{\\beta_i + \\delta_i}\\) eller hoppe ned med sandsynlighed \\(\\frac{\\delta_i}{\\beta_i+\\delta_i}\\). Side note: Meget af kapitel 3.4 er eksempler på birth-and-death processer. Theorem 66. (Birth-and-death processer: recurrence) En birth-and-death proces er recurrent, hvis og kun hvis, \\[ \\sum_{i=1}^\\infty\\frac{\\delta_i\\cdot...\\cdot \\delta_1}{\\beta_i\\cdot...\\cdot \\beta_1} = \\infty. \\] Ækvivalent er en birth-and-death proces transient, hvis og kun hvis, \\[ \\sum_{i=1}^\\infty\\frac{\\delta_i\\cdot...\\cdot \\delta_1}{\\beta_i\\cdot...\\cdot \\beta_1} &lt; \\infty. \\] Theorem 67. (Birth-and-death processer: positive recurrence) En birth-and-death proces er positive recurrent, hvis og kun hvis, \\[ \\sum_{i=1}^\\infty\\frac{\\beta_{i-1}\\cdot...\\cdot \\beta_0}{\\delta_i\\cdot...\\cdot \\delta_1} &lt; \\infty \\hspace{10pt} \\text{og} \\hspace{10pt} \\sum_{i=1}^\\infty\\frac{\\delta_i\\cdot...\\cdot \\delta_1}{\\beta_i\\cdot...\\cdot \\beta_1} = \\infty \\] Theorem 68. (Eksplosion for en birth-and-death proces) For en birth-and-death proces med intensiteterne \\[ q_{i,i+1} = \\beta_i, \\hspace{10pt} q_{i+1,i}=\\delta_{i+1}, \\hspace{10pt} q_{i+1,i+1} = -(\\delta_{i+1}+\\beta_{i+1}) \\] og med \\(q_{i,j} = 0\\) ellers, \\(i,j \\in \\mathbb{N}_0\\), så er eksplosion muligt, hvis og kun hvis, \\[ \\sum_{i=1}^\\infty\\left(\\frac{1}{\\beta_i}+\\frac{\\delta_i}{\\beta_i\\beta_{i-1}}+...+\\frac{\\delta_i\\cdot...\\cdot \\delta_1}{\\beta_i\\cdot...\\cdot \\beta_0}\\right)&lt;\\infty \\] "],["continuous-time-stochastic-processes.html", "Chapter 15 Continuous Time Stochastic Processes ", " Chapter 15 Continuous Time Stochastic Processes "],["brownian-motion.html", "15.1 Brownian Motion", " 15.1 Brownian Motion Definition 4.1. (Bjork) A stochastic process \\(W\\) is called a Brownian motion or Wiener process if the following conditions hold \\(W_0=0\\). The process \\(W\\) has independent increments, i.e. if \\(r&lt;s\\le t&lt; u\\) then \\(W_u-W_t\\) and \\(W_s-W_r\\) are independent random variables. For \\(s&lt;t\\) the random variable \\(W_t-W_s\\) has the Gaussian distribution \\(\\mathcal{N}(0,t-s)\\). \\(W\\) has continuous trajectories i.e. \\(s\\mapsto W(s;\\omega)\\) i continuous for all \\(\\omega \\in\\Omega\\). #Example of trajectory for BM set.seed(1) t &lt;- 0:1000 N &lt;- rnorm( n = length(t)-1, #initial value = 0 mean = 0, #incements mean = 0 sd = sqrt(t[2:length(t)] - t[1:(length(t)-1)]) #increment sd = sqrt(t-s) ) W &lt;- c(0,cumsum(N)) As one can see from the simulated sample path on the right, the Brownian motion is rather irratic. In fact, the process varies infinitely on any interval with length greater than 0. This gives some of the characteristics of the process including that: \\(W\\) is continuous and \\(W\\) is non-differential everywhere. This irratic behaviour is summed up in the theorem. Theorem 4.2. (Bjork) A Brownian motions trajectory \\(t\\mapsto W_t\\) is with probability one nowhere differential, and it has locally infinite total variation. "],["filtration.html", "15.2 Filtration", " 15.2 Filtration Filtrations is widely used in stochastic processes, as they allow for the concept of knowledge/information. This is useful when considering mean-values of future states but in an increasing information setting. For this we introduce the term adapted processes. Definition B.17. (Bjork) (Adapted process) Let \\((\\mathcal{F}_t)_{t\\ge 0}\\) be a filtration on the probability space \\((\\mathcal{F}_t)_{t\\ge 0}\\). Furthermore, let \\((X_t)_{t\\ge 0}\\) be a stochastic process on the same space. We say that \\(X_t\\) is adapted to the filtration \\(\\mathbf{F}\\) if \\[X_t\\ \\text{ is }\\ \\mathcal{F}_t-\\text{measurable},\\hspace{20pt}\\forall t\\ge 0.\\] Obviously, we may introduce the natural filtration \\(\\mathcal{F}^X_t\\) given by the tragetory of the process \\(X_t\\): \\[\\mathcal{F}^X_t=\\sigma(\\{X_s,\\ s\\le t\\}).\\] Indeed, \\(X_t\\) is adapted to this filtration. "],["martingale.html", "15.3 Martingale", " 15.3 Martingale Definition. Let \\(M_t\\) be a stochastic process defined on a background space \\((\\Omega,\\mathcal{F},P)\\). Let \\((\\mathcal{F}_t)_{t\\ge 0}\\) be a filtration. If \\(M_t\\) is adapted to the filtration \\(\\mathcal{F}_t\\), \\(E\\vert M_t\\vert &lt;\\infty\\) and \\[E[M_t\\vert \\mathcal{F}_s]=M_s,\\hspace{20pt}P-\\text{a.s.}\\] holds for any \\(t&gt;s\\) we say that \\(M_t\\) is a martingale (\\(\\mathbf{F}\\)-martingale). If the above has \\(\\le\\) or \\(\\ge\\) we say that \\(M_t\\) is either a submartingale or supermartingale respectively. Naturally, this defintions may easily be extended to discrete models and we have the trivial equality: \\[E[M_t-M_s\\ \\vert\\ \\mathcal{F}_s]=0.\\] Martingales is useful, when proofing probalistic statements as the posses tractable properties. A useful technique often include the construction of the martingale \\[M_t=E[X\\ \\vert\\ \\mathcal{F}_t].\\] "],["stochastic-calculus.html", "Chapter 16 Stochastic calculus ", " Chapter 16 Stochastic calculus "],["stochastic-integrals.html", "16.1 Stochastic Integrals", " 16.1 Stochastic Integrals We want to formulate financial markets in continuous time and the most elegant theory is obtained from processes that can be defined in terms of stochastic differential equations or in other words by their dynamics. We may call them diffusion processes, as they may be approximated by a stochastic difference equation: \\[ X_{t+\\Delta t}-X_t=\\mu(t,X_t)\\Delta t+\\sigma(t,X_t)Z_t.\\tag{4.1} \\] Above \\(Z_t\\) is a normally distributed random variable (a disturbance). In this formulation we say that \\(S_t\\) is driven by two forces: on one hand a locally deterministic velocity or drift \\(\\mu(t,X_t)\\) and on the other hand a Gaussian term amplified by the deterministic factor \\(\\sigma(t,X_t)\\). 16.1.1 Information We consider a primary process \\(X_t\\) and we introduce the notion of information generated by \\(X_t\\) in terms of the natural filtration. The idea can be summed up in the following definition. Definition 4.3. (Bjork) The symbol \\(\\mathcal{F}^X_t\\subseteq\\mathcal{F}\\) denotes “the information generated by \\(X_t\\) on the interval \\([0,t]\\)”, or alternatively “what has happened to \\(X_t\\) over ther interval \\([0,t]\\)”. If, based upon observations of the trajectory \\(\\{X_s;\\ 0\\le s\\le t\\}\\), it is possible to decide whether a given event \\(A\\) has occurred or not, then we write this as \\[ A\\in\\mathcal{F}^X_t \\] or say that “\\(A\\) is \\(\\mathcal{F}^X_t\\)-measurable”. If the value of a given random variable \\(Z\\) can be completely determined given observations of the tragectory \\(\\{X_s;\\ 0\\le s\\le t\\}\\), then we also write \\[ Z\\in\\mathcal{F}^X_t.\\ \\text{(}Z\\text{ is }\\mathcal{F}^X_t\\text{-measurable)} \\] 3._ If \\(Y\\) is a stochastic process such that we have_ \\[ Y_t\\in\\mathcal{F}^X_t \\] for all \\(t\\ge0\\) then we say that \\(Y_t\\) is adapted to the filtration \\(\\{\\mathcal{F}^X_t\\}_{t\\ge 0}\\). For brevity of notation, we will sometimes write the filtration as \\(\\{\\mathcal{F}^X_t\\}_{t\\ge 0}=\\mathbf{F}\\). 16.1.2 Stochastic Integrals We will now formulate the theory of stochastic integrals, that is, processes written in terms of stochastic processes with stochastic integrator and/or stochastic integrant. We will consider some given standard Brownian motion \\(W_t\\) and another stochastic process \\(X_t\\). We need som integrability condition on \\(X_t\\) in order to do the calculations. We therefore determine a selection of suitable stochastic processes \\(X\\) must be contained in. Definition 4.4. (Bjork) Let \\(X_t\\) be a stochastic process, then We say that \\(X_t\\) belongs to the class \\(\\mathcal{L}^2[a,b]\\) if \\(X_t\\) is adapted to the filtration \\(\\mathcal{F}^X_t\\) and the following holds \\[\\int_a^bE[X_s^2]\\ ds&lt;\\infty\\] We say that \\(X_t\\) belongs to the class \\(\\mathcal{L}^2\\) if \\(X_t\\in\\mathcal{L}^2[0,t]\\) for all \\(t&gt;0\\). We now want to define what we mean by \\[ \\int_a^bX_t\\ dW_s \\] for some process \\(X_t\\in\\mathcal{L}^2\\). We see that a way to go about this problem is to start by defining the concept for a simple stochastic process \\(X_t\\). By simple we mean a process \\(X_t\\) that is constant on between some deterministic points in time \\(a=t_0&lt;t_1&lt;\\cdots&lt;t_n=b\\). In that case we may define the integral as \\[ \\int_a^bX_s\\ dW_s = \\sum_{k=0}^{n-1}X_{t_k}[W_{t_{k+1}}-W_{t_k}].\\tag{4.8} \\] In the more general setting we may follow the following approach: Approximate \\(X\\) with a sequence \\(\\{X^n\\}_{n\\in\\mathbb{N}}\\) of simple processes such that the following convergence criteria hold \\[ \\int_a^bE[(X_s^n-X_s)^2]\\ ds\\to 0,\\ n\\to\\infty \\] For each \\(n\\) the integral \\(\\int_a^b X_s^n\\ dW_s:=Z^n\\) is well defined and it is possible to prove, using DCT, that a variable \\(Z\\) exists such that \\(Z^n\\to Z\\) that is in \\(L^2\\). We now define the stochastic integral by the limit \\[ \\int_a^b X_s\\ dW_s=\\lim_{n\\to \\infty}\\int_a^b X_s^n\\ dW_s.\\tag{4.9} \\] Obviously the hardest step is finding the processes \\(X^n\\). This stochastic har some properties we will use. Proposition 4.5. (Bjork) Let \\(X_t\\in\\mathcal{L}^2\\), then \\[\\begin{align*} &amp;E\\left[\\int_a^b X_s\\ dW_s\\right]=0.\\tag{4.12}\\\\ &amp;E\\left[\\left(\\int_a^b X_s\\ dW_s\\right)^2\\right]=\\int_a^b E[ X_s^2]\\ dW_s.\\tag{4.13}\\\\ &amp;\\int_a^b X_s\\ dW_s\\ \\text{ is }\\mathcal{F}_b^W\\text{-measurable.}\\tag{4.14} \\end{align*}\\] 16.1.3 Martingales Definition 4.7. (Bjork) Let \\(M_t\\) be a stochastic process defined on a background space \\((\\Omega,\\mathcal{F},P)\\). Let \\((\\mathcal{F}_t)_{t\\ge 0}\\) be a filtration. If \\(M_t\\) is adapted to the filtration \\(\\mathcal{F}_t\\), \\(E\\vert M_t\\vert &lt;\\infty\\) and \\[E[M_t\\vert \\mathcal{F}_s]=M_s,\\hspace{20pt}P-\\text{a.s.}\\] holds for any \\(t&gt;s\\) we say that \\(M_t\\) is a martingale (\\(\\mathbf{F}\\)-martingale). If the above has \\(\\ge\\) or \\(\\le\\) we say that \\(M_t\\) is either a submartingale or supermartingale respectively. Proposition 4.8. (Bjork) For any process \\(X_t\\in\\mathcal{L}^2[s,t]\\) the following hold: \\[ E\\left[\\left.\\int_s^t X_s\\ dW_s\\right\\vert\\mathcal{F}_s^W\\right]=0 \\] Corollary 4.9. (Bjork) For any process \\(X_t\\in\\mathcal{L}^2\\) the process \\[ M_t=\\int_s^t X_s\\ dW_s, \\] is an \\((\\mathcal{F}_t^W)\\)-martingale. In other words, modulo an integrability condition, every stochastic integral is a martingale. Lemma 4.10. (Bjork) Let \\(M_t\\) be a stochastic process with stochastic differential, then \\(M_t\\) is a martingale if and only if the stochastic differential has the form \\(dM_t=X_t\\ dW_t\\) i.e. \\(M_t\\) as no \\(dt\\)-term. 16.1.4 Stochastic Calculus and the Ito Formula Given this breif introduction to stochastic integrals we may formulate som simple calculus revolving around Ito’s formula. We consider the stochastic process \\(X_t\\) and we suppose that there exist a real number \\(X_0\\) and adapted processes \\(\\mu\\) and \\(\\sigma\\) wrt. \\(\\mathcal{F}_t^W\\) such that for all \\(t\\ge0\\) we have \\[ X_t=X_0+\\int_0^t\\mu_s\\ ds+\\int_0^t\\sigma_s\\ dW_s,\\tag{4.16} \\] where \\(W_t\\) is a standard Brownian motion. We know from earlier courses that the above may be written in terms of the dynamics (pure notation): \\[ \\left\\{\\begin{matrix}dX_t=\\mu_t\\ dt+\\sigma_t\\ dW_t,\\tag{4.17/18}\\\\ X_0=X_0.\\end{matrix}\\right. \\] Here we intepret the above as \\(X_t\\) has boundary condition \\(X_0\\) and evolves with a drift \\(\\mu_t\\ dt\\) amplified and distorted by the drift \\(\\sigma_t\\ dW_t\\). We say that \\(X_t\\) has stochastic differential \\(dX_t\\) and initial condition \\(X_0\\). We want to understand how transformation of such an integral behaves and therefore we introduce some calculus which will tell how for instance \\(f(t,X_t)\\) (for some \\(C^{1,2}\\)-function) behaves. This insight is given by the important Ito’s formula. Thoerem 4.11. (Bjork) (Ito’s formula, one-dimensional) Assume that the process \\(X\\) has a stochastic differential form given by \\[ dX_t=\\mu_t\\ dt + \\sigma_t\\ dW_t,\\tag{4.28} \\] where \\(\\mu\\) and \\(\\sigma\\) are adapted processes, and let \\(f:\\mathbb{R}_+\\times\\mathbb{R}\\to\\mathbb{R}\\) be a \\(C^{1,2}\\)-function. Define the process \\(Z\\) by \\(Z_t=f(t,X_t)\\). Then \\(Z\\) has stochastic differential given by \\[ df(t,X_t)=\\left(\\frac{\\partial f}{\\partial t}(t,X_t) + \\mu_t\\frac{\\partial f}{\\partial x}(t,X_t) + \\frac{1}{2}\\sigma^2_t\\frac{\\partial^2 f}{\\partial x^2}(t,X_t)\\right)\\ dt+\\sigma_t\\frac{\\partial f}{\\partial x}(t,X_t)\\ dW_t.\\tag{4.29} \\] Proposition 4.12. (Bjork) (Ito’s formula, one-dimensional) With assumptions as in theorem 4.11, \\(df\\) is given by \\[ df=\\frac{\\partial f}{\\partial t}\\ dt + \\frac{\\partial f}{\\partial x}\\ dX_t + \\frac{1}{2}\\frac{\\partial^2 f}{\\partial x^2}\\ (dX_t)^2,\\tag{4.31} \\] where we use the following table \\[ \\left\\{\\begin{matrix}(dt)^2=0,\\\\ dt\\cdot dW_t=0,\\\\ (dW_t)^2=dt.\\end{matrix}\\right. \\] Lemma 4.18. (Bjork) Let \\(\\sigma(t)\\) be deterministic function of time and define the process \\(X\\) by \\[ X_t=\\int_0^t \\sigma(s)\\ dW_s.\\tag{4.37} \\] Then \\[ X_t\\sim\\mathcal{N}\\left(0,\\int_0^t\\sigma^2(s)\\ ds\\right). \\] 16.1.5 The multidimensional Ito Formula Consider a vector process \\(X=(X^1,...,X^n)^\\top\\) where each component \\(X^i\\) has stochastic differential \\[ d X_t^i=\\mu_t^i\\ dt+\\sum_{j=1}^d\\sigma^{ij}_t\\ dW_t^j \\] where \\(W^1,...,W^d\\) is independent Brownian motions. Then we have respectively the drift vector process \\(\\mu_t\\) in \\(n\\) dimensions, the vector Brownian motion in \\(d\\) dimensions and a \\(n\\times d\\)-dimensional diffusion matrix \\(\\sigma_t\\) given as below \\[ \\mu_t=\\begin{bmatrix}\\mu^1_t\\\\ \\vdots\\\\ \\mu^n_t\\end{bmatrix},\\hspace{10pt}W_t=\\begin{bmatrix}W^1_t\\\\ \\vdots\\\\ W^d_t\\end{bmatrix},\\hspace{10pt}\\sigma_t=\\begin{bmatrix}\\sigma^{11}_t &amp; \\cdots &amp; \\sigma^{1d}_t \\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\sigma^{n1}_t &amp;\\cdots&amp; \\sigma^{nd}_t\\end{bmatrix}. \\] Given this we may write the dynamics of \\(X\\) as \\[ d X_t=\\mu_t\\ dt+\\sigma_t\\ dW_t\\in\\mathbb{R}^n. \\] Consider now a function \\(f:\\mathbb{R}_+\\times \\mathbb{R}^n\\to\\mathbb{R}\\) which is a \\(C^{1,2}\\)-mapping. We want to study the dynamics of the process \\[ Z_t=f(t,X_t). \\] The dynamics is given in the multidimensional version of Ito’s formula. Thoerem 4.19. (Bjork) (Ito’s formula, multi-dimensional) Let \\(X\\) be given as above. Then the following holds: The process \\(f(t,X_t)\\) has the stochastisc differential given by \\[ df(t,X_t)=\\left(\\frac{\\partial f}{\\partial t}(t,X_t) + \\sum_{i=1}^n\\mu^i_t\\frac{\\partial f}{\\partial x^i}(t,X_t) + \\frac{1}{2}\\sum_{i,j=1}^nC_t^{ij}\\frac{\\partial^2 f}{\\partial x^i\\partial x^j}(t,X_t)\\right)\\ dt+\\sum_{i=1}^n\\sigma^i_t\\frac{\\partial f}{\\partial x^i}(t,X_t)\\ dW_t. \\] Here the row vector \\(\\sigma^i_t\\) is the \\(i\\)’th row of the matrix \\(\\sigma_t\\) and the matrix \\(C\\) is defined by \\(C=\\sigma\\sigma^\\top\\). Alternatively, the differential is given by the formula \\[ df(t,X_t)=\\frac{\\partial f}{\\partial t}(t,X_t)\\ dt + \\sum_{i=1}^n\\frac{\\partial f}{\\partial x^i}(t,X_t)\\ dX^i_t + \\frac{1}{2}\\sum_{i,j=1}^n\\frac{\\partial^2 f}{\\partial x^i\\partial x^j}(t,X_t)\\ dX^i_tdX^j_t, \\] with the formal multiplication table \\[ \\left\\{\\begin{matrix}(dt)^2=0,\\\\ dt\\cdot dW_t^i=0, &amp; i = 1,...,d,\\\\ (dW_t^i)^2=dt, &amp; i=1,...,d, \\\\ dW_t^i\\cdot dW_t^i =0, &amp; i\\ne j.\\end{matrix}\\right. \\] Obviously, one can write the differential in Ito’s formula in many other ways including a matrix-wise version using the Hessian matrix \\(H_{ij}=\\frac{\\partial^2 f}{\\partial x^i\\partial x^j}\\). 16.1.6 The multidimensional Ito Formula with states Definition. (\\(n\\)-dimensional Markov-jump process) Let \\(Z(t)\\) be a Markov-jump process on a at most countable state space \\(E\\). Let \\(\\mathbf X(t)=(X_1(t),X_2(t),...,X_n(t))\\in\\mathcal X\\) be a \\(n\\)-dimensional stochastic process. Let \\(f : \\mathcal X\\to \\mathbb R\\) be a twice differential function. We call the process \\[ f^{Z(t)}(\\mathbf X(t))=f^{Z(t)}(X_1(t),...,X_n(t)) \\] a \\(n\\)-dimensional Markov-jump process on \\(E\\). One could for instance have the 2 dimensional process \\(f^{Z(t)}(t,X(t))\\) where \\(X\\) has dynamics \\[ dX(t)=rX(t)\\ dt + \\sigma X(t)\\ dW^\\mathbb Q(t) \\] i.e. a geometric brownian motion. For such af process \\(f\\) we have the change of variables formula Theorem. (Ito’s formula for \\(n\\)-dimensional Markov-jump process) Let \\(f^{Z(t)}(\\mathbf X(t))\\) be a \\(n\\)-dimensional Markov-jump process on \\(E\\). The dynamics of \\(f\\) is given by \\[\\begin{align*} d \\Big(f^{Z(t)}(\\mathbf X(t))\\Big)&amp;=\\sum_{i=1}^n f_{x_i}^{Z(t)}(\\mathbf X(t))\\ dX_i^c(t)\\\\ &amp;+\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n f_{x_ix_j}^{Z(t)}(\\mathbf X(t))\\ d(X_iX_j)^c(t)\\\\ &amp;+\\Big(f^{Z(t)}(\\mathbf X(t))-f^{Z(t-)}(\\mathbf X(t-))\\Big)dN^Z(t)\\\\ &amp;+\\Big(f^{Z(t)}(\\mathbf X(t))-f^{Z(t)}(\\mathbf X(t-))\\Big)dN^\\mathbf X(t) \\end{align*}\\] where \\(f^{Z(t)}_{x_i}\\) denotes the derivative \\(\\frac{\\partial f^{Z(t)}}{\\partial x_i}\\) and \\(f^{Z(t)}_{x_ix_j}\\) denotes the second derivative \\(\\frac{\\partial^2 f^{Z(t)}}{\\partial x_i\\partial x_j}\\). In the above \\(dX^c(t)\\) refers to the continuous part of the differential form of \\(X(t)\\). Furthermore, the quadratic variation is given on differential form \\[ d(XY)(t)=X(t-)dY(t)+Y(t-)dX(t)+dX(t)dY(t) \\] where \\[ d(XY)^c(t)=dX^c(t)dY^c(t). \\] The processes \\(N^Z\\) and \\(N^\\mathbf X\\) is defined as \\[\\begin{align*} N^Z(t)&amp;=\\#\\left\\{ 0\\le s\\le t: Z(s)\\ne Z(s-) \\right\\},\\\\ N^\\mathbf X(t)&amp;=\\#\\left\\{ 0\\le s\\le t: \\mathbf X(s)\\ne \\mathbf X(s-) \\right\\}. \\end{align*}\\] Take the example \\(X_1(t)=t\\) and \\(X_2=X\\) is a geometric brownian motions with jumps according to \\(Z\\) and deterministic jumps given \\(Z\\) i.e. \\[\\begin{align*} dX(t)&amp;=rX(t)\\ dt+\\sigma X(t)\\ dW^\\mathbb Q(t)+\\Delta X^{Z(t)}(t)\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}\\Big(\\chi^k(t,X(t-))-X(t-)\\Big)dN^{k}(t) \\end{align*}\\] Then we know that \\[\\begin{align*} dX^c(t)&amp;=rX(t)\\ dt+\\sigma X(t)\\ dW^\\mathbb Q(t)\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}\\Big(\\chi^k(t,X(t-))-X(t-)\\Big)\\lambda_{Z(t-)k}(t)\\ dt \\end{align*}\\] Using that \\(dN^k(t)=dM^k(t)+\\lambda_k(t)\\ dt\\) for some martingale process \\(M^k\\) and the predictable compensator. Furthermore, we see that the only non-zero quadratic term is \\(dW^\\mathbb Q(t)dW^\\mathbb Q(t)=dt\\) hence \\[ \\frac{1}{2}\\sum_{i,j=1}^2d(X_iX_j)(t)=\\frac{1}{2}\\sigma^2X^2(t)\\ dt. \\] Combining this with \\[ \\Big(f^{Z(t)}(\\mathbf X(t))-f^{Z(t)}(\\mathbf X(t-))\\Big)dN^\\mathbf X(t)=\\Delta X^{Z(t)}(t) \\] we obtain the differential form \\[\\begin{align*} d \\Big(f^{Z(t)}(t,X(t))\\Big)&amp;=f_t^{Z(t)}(t,X(t))\\ dt+f_x^{Z(t)}(t,X(t))\\ dX^c(t)\\\\ &amp;+\\frac{1}{2}f^{Z(t)}_{xx}(t,X(t))\\sigma^2X^2(t)\\ dt\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}\\Big(f^{Z(t)}(t,X(t))-f^{Z(t-)}(t,X(t-))\\Big)\\ dN^k(t)\\\\ &amp;+\\Delta X^{Z(t)}(t)\\\\ &amp;=f_t^{Z(t)}(t,X(t))\\ dt+\\frac{1}{2}f^{Z(t)}_{xx}(t,X(t))\\sigma^2X^2(t)\\ dt\\\\ &amp;+f_x^{Z(t)}(t,X(t))rX(t)\\ dt+f_x^{Z(t)}(t,X(t))\\sigma X(t)\\ dW^\\mathbb Q(t)\\\\ &amp;+f_x^{Z(t)}(t,X(t))\\sum_{k:k\\ne Z(t-)}\\Big(\\chi^k(t,X(t-))-X(t-)\\Big)\\lambda_{Z(t-)k}(t)\\ dt\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}\\Big(f^{Z(t)}(t,\\chi^k(t,X(t-)))-f^{Z(t-)}(t,X(t-))\\Big)\\ dN^k(t)\\\\ &amp;+\\Delta X^{Z(t)}(t) \\end{align*}\\] Notice we use that on the event \\(Z(t)=k\\) and \\(Z(t-)\\ne k\\) the variable \\(X\\) jumps to the value \\(\\chi^k(t,X(t-))\\). If we choose \\(f^j(t,x)=x\\) for all \\(j\\in E\\) we see that the above simplifies to \\[\\begin{align*} d \\Big(f^{Z(t)}(t,X(t))\\Big)&amp;=rX(t)\\ dt+\\sigma X(t)\\ dW^\\mathbb Q(t)\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}\\Big(\\chi^k(t,X(t-))-X(t-)\\Big)\\lambda_{Z(t-)k}(t)\\ dt\\\\ &amp;+\\sum_{k:k\\ne Z(t-)}\\Big(\\chi^k(t,X(t-))-X(t-))\\ dN^k(t)\\\\ &amp;+\\Delta X^{Z(t)}(t)\\ne dX(t) \\end{align*}\\] And so we are left with an extra term \\(\\sum_{k:k\\ne Z(t-)}\\Big(\\chi^k(t,X(t-))-X(t-)\\Big)\\lambda_{Z(t-)k}(t)\\ dt\\), where we should simply have had \\(dX(t)\\) that is without the term \\(\\sum_{k:k\\ne Z(t-)}\\Big(\\chi^k(t,X(t-))-X(t-)\\Big)\\lambda_{Z(t-)k}(t)\\ dt\\). 16.1.7 Correlated Brownian motions In the previous section the \\(d\\)-dimensional Brownian was assumed to have independent Brownian motions. However we may instead consider a variation where we have some dependence between the Brownian motions. This section has not been finished. "],["discrete-stochastic-integrals.html", "16.2 Discrete Stochastic Integrals", " 16.2 Discrete Stochastic Integrals This section has not been finished. "],["stochastic-differential-equations.html", "16.3 Stochastic Differential Equations", " 16.3 Stochastic Differential Equations We start the chapter by formalising some used objects. We consider the following objects. \\(M(n,d)\\) denotes the class of \\(n\\times d\\)-matrices. \\(W\\) is a \\(d\\)-dimensional Brownian motion \\(\\mu\\) is a \\(\\mathbb{R}^n\\)-valued function with arguments \\((t,X_t)\\) with \\(X_t\\) being a \\(n\\)-dimensional stochastic process. \\(\\sigma\\) a \\(M(n,d)\\)-valued function with arguments as in \\(\\mu\\). \\(x_0\\) a \\(\\mathbb{R}^n\\)-valued vector. We want then to understand when the following has a solution \\[ dX_t=\\mu(t,X_t)\\ dt + \\sigma(t,X_t)\\ dW_t,\\ \\ X_0=x_0.\\tag{5.1/2} \\] We call such an equation the stochastic differential equation or simply SDE. We know that the above is loosely notation for the integral form as \\[ X_t=x_0+\\int_0^t\\mu(s,X_s)\\ ds +\\int_0^t\\sigma(s,X_s)\\ dW_s,\\tag{5.3} \\] for all \\(t\\ge 0\\). The following proposition tells us when an solution exist to the problem above. In the below \\(\\Vert \\cdot \\Vert\\) is usual euclidian norm \\[ \\Vert x\\Vert=\\sqrt{\\sum_{i=1}^nx_i^2}. \\] Proposition 5.1. (Bjork) Suppose that there existis a constant \\(K\\) such that the following conditions are satisfied for all \\(x,y\\) and \\(t\\). \\[\\begin{align*} \\Vert \\mu(t,x) - \\mu(t,y) \\Vert &amp;\\le K\\Vert x-y\\Vert,\\tag{5.6}\\\\ \\Vert \\sigma(t,x) - \\sigma(t,y) \\Vert &amp;\\le K\\Vert x-y\\Vert,\\tag{5.7}\\\\ \\Vert \\mu(t,x) \\Vert +\\Vert \\sigma(t,x) \\Vert&amp;\\le K(1+\\Vert x\\Vert).\\tag{5.8} \\end{align*}\\] Then there exists a unique solution to the SDE above. Furthermore, the solution has the properties \\(X\\) is \\(\\mathcal{F}_t^W\\)-adapted. \\(X\\) has continuous trajectories. \\(X\\) is a Markov process. There exists a constant \\(C\\) such that \\[ E[\\Vert X_t\\Vert^2]\\le Ce^{Ct}(1+\\Vert x_0\\Vert^2).\\tag{5.9} \\] In genereal the solution to an SDE is so complicated, that it in practical terms is unsolvable and may only be approximated on a finely subdividet grid as jumps. There does however exist som nontrivial cases where we may infer a analytical solution. One is the rather important Geometric Brownian motion. Proposition 5.2. (Bjork) Consider the SDE \\[ dX_t=\\alpha X_t\\ dt+\\sigma X_t\\ dW_t,\\tag{5.13} \\] with \\(X_0=x_0\\). Then the solution is given as \\[ X_t=x_0\\cdot \\exp\\left\\{\\left(\\alpha- \\frac{\\sigma^2}{2}\\right)t+\\sigma W_t\\right\\}.\\tag{5.15} \\] The expected value of \\(X\\) is given as \\(E[X_t]=x_0e^{\\alpha t}\\) (eq. 5.16). One other generalisation that is analytically solvable is the Linear SDE. Proposition 5.3. (Bjork) Consider the SDE \\[ dX_t=(A X_t + b_t)\\ dt+ \\sigma_t\\ dW_t,\\tag{5.19} \\] with \\(X_0=x_0\\) and \\(A\\in M(n,n)\\) and \\(b_t\\) being a real-valued function. Then the solution is given as \\[ X_t=e^{At}x_0+\\int_0^te^{A(t-s)}b_s\\ ds+\\int_0^te^{A(t-s)}\\sigma_s\\ dW_s.\\tag{5.20} \\] Where we define the exponential of a matrix as below \\[ e^{At}=\\sum_{k=0}^\\infty A^k\\frac{1}{k!}t^k. \\] In general with the SDE we have a partial differential operator \\(\\mathcal{A}\\) called the infinitesimal operator of \\(X\\) which has some interesting analytical properties regarding \\(X\\). Definition 5.4. (Bjork) Consider the SDE \\[ dX_t=\\mu(t,X_t)\\ dt+\\sigma(t,X_t)\\ dW_t.\\tag{5.21} \\] The partial differential operator \\(\\mathcal{A}\\) is defined, for any function \\(h\\in C^2(\\mathbb{R}^n)\\), by \\[ \\mathcal{A}h(t,x)=\\sum_{i=1}^n\\mu_i(t,x)\\frac{\\partial h}{\\partial x_i}(x) + \\frac{1}{2}\\sum_{i,j=1}^n (\\sigma(t,x)\\sigma(t,x)^\\top)_{ij}\\frac{\\partial^2h}{\\partial x_i\\partial x_j}(x). \\] We see that in terms of Ito’s formula the operator is included as such \\[ df(t,X_t)=\\left\\{\\frac{\\partial f}{\\partial t}(t,X_t)+\\mathcal{A}f(t,x)\\right\\}\\ dt+[\\nabla_xf](t,X_t)\\sigma(t,X_t)\\ dW_t, \\] where \\(\\nabla_x\\) is the gradient for function \\(h\\in C^1(\\mathbb{R}^n)\\) as \\[ \\nabla_xh(x)=\\left[\\frac{\\partial h}{\\partial x_1}(x),...,\\frac{\\partial h}{\\partial x_n}(x)\\right]. \\] "],["partial-differential-equations.html", "16.4 Partial differential equations", " 16.4 Partial differential equations Proposition 5.5. (Bjork) (Feynmann-Kac) Assume that \\(F\\) is a solution to the boundary value problem \\[ \\frac{\\partial F}{\\partial t}(t,x)+\\mu(t,x)\\frac{\\partial F}{\\partial x}(x,t)+\\frac{1}{2}\\sigma^2(t,x)\\frac{\\partial^2 F}{\\partial x^2}(t,x)=0, \\] with boundary condition \\(F(T,x)=\\Phi(x)\\). Assume furthermore that the process \\[ \\sigma(s,X_s)\\frac{\\partial F}{\\partial x}(s,X_s) \\in \\mathcal{L}^2 \\] as per definition 4.4, where \\(X\\) is defined below. Then \\(F\\) has the representation \\[ F(t,x)=E_{t,x}[\\Phi(X_T)]=E[\\Phi(X_T)\\ \\vert\\ X_t=x],\\tag{5.29} \\] where \\(X\\) satisfies the SDE \\[ dX_s=\\mu(s,X_s)\\ ds+\\sigma(s,X_s)\\ dW_s,\\tag{5.30} \\] with boundary condition \\(X_t=x\\). Proposition 5.6. (Bjork) (Feynmann-Kac) Assume that \\(F\\) is a solution to the boundary value problem \\[ \\frac{\\partial F}{\\partial t}(t,x)+\\mu(t,x)\\frac{\\partial F}{\\partial x}(x,t)+\\frac{1}{2}\\sigma^2(t,x)\\frac{\\partial^2 F}{\\partial x^2}(t,x)-rF(t,x)=0,\\tag{5.34} \\] with boundary condition \\(F(T,x)=\\Phi(x)\\). Assume furthermore that the process \\[ e^{-rs}\\sigma(s,X_s)\\frac{\\partial F}{\\partial x}(s,X_s) \\in \\mathcal{L}^2 \\] as per definition 4.4, where \\(X\\) is defined below. Then \\(F\\) has the representation \\[ F(t,x)=e^{-r(T-t)}E_{t,x}[\\Phi(X_T)]=e^{-r(T-t)}E[\\Phi(X_T)\\ \\vert\\ X_t=x],\\tag{5.36} \\] where \\(X\\) satisfies the SDE \\[ dX_s=\\mu(s,X_s)\\ ds+\\sigma(s,X_s)\\ dW_s,\\tag{5.37} \\] with boundary condition \\(X_t=x\\). Proposition 5.8. (Bjork) (Feynmann-Kac) Assume that \\(F\\) is a solution to the boundary value problem \\[ \\frac{\\partial F}{\\partial t}(t,x)+\\sum_{i=1}^n\\mu_i(t,x)\\frac{\\partial F}{\\partial x}(x,t)+\\frac{1}{2}\\sum_{i,j=1}^n C_{ij}(t,x)\\frac{\\partial^2 F}{\\partial x^2}(t,x)-rF(t,x)=0, \\] with boundary condition \\(F(T,x)=\\Phi(x)\\) and \\(C_{ij}=\\sigma \\sigma^\\top\\). Assume furthermore that the process \\[ e^{-rs}\\sum_{i=1}^n\\sigma_i(s,X_s)\\frac{\\partial F}{\\partial x}(s,X_s) \\in \\mathcal{L}^2 \\] as per definition 4.4, where \\(X\\) is defined below. Then \\(F\\) has the representation \\[ F(t,x)=e^{-r(T-t)}E_{t,x}[\\Phi(X_T)],\\tag{5.39} \\] where \\(X\\) satisfies the SDE \\[ dX_s=\\mu(s,X_s)\\ ds+\\sigma(s,X_s)\\ dW_s,\\tag{5.40} \\] with boundary condition \\(X_t=x\\). Proposition 5.9. (Bjork) Consider as given a vector process \\(X\\) with generator \\(\\mathcal{A}\\), and a function \\(F(t,x)\\). Then, modulo some integrability condition, the following hold: The process \\(F(t,X_t)\\) is a martingale relative to the filtration \\(\\mathcal{F}^X\\) if and only if \\(F\\) satisfies the PDE \\[ \\frac{\\partial F}{\\partial t}+\\mathcal{A}F=0. \\] The process \\(F(t,X_t)\\) is a martingale relative to the filtration \\(\\mathcal{F}^X\\) if and only if, for every \\((t,x)\\) and \\(T\\ge t\\), we have \\[ F(t,x)=E_{t,x}[F(T,X_T)]. \\] "],["the-product-integral.html", "16.5 The Product Integral", " 16.5 The Product Integral Consider the (stochastic) function \\(Y : \\mathbb{R} \\to \\mathbb{R}^{n\\times n}\\), where \\(\\mathbb{R}^{n\\times n}\\) is the set of all \\(n\\times n\\) real-valued matrices, that is \\(Y\\) has the matrix-representation \\[ Y(t)=\\begin{bmatrix} Y_{11}(t) &amp; \\cdots &amp; Y_{1n}(t)\\\\ Y_{21}(t) &amp; \\cdots &amp; Y_{2n}(t)\\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ Y_{n1}(t) &amp; \\cdots &amp; Y_{nn}(t) \\end{bmatrix}. \\] Assume furthermore that for each coordinate function \\(Y_{ij}(t)\\) the equation \\[ \\frac{dY_{ij}}{dt}(t)=Y_{i1}(t)A_{1j}(t)+\\cdots+Y_{in}(t)A_{nj}(t),\\hspace{15pt}Y_{ij}(s)=C_{ij}, \\] is satisfied. That is on matrix form the linear system of differential equation \\[ \\frac{dY}{dt}(t)=Y(t)A(t),\\hspace{15pt}Y(s)=C, \\] for some function \\(A : \\mathbb{R}\\to\\mathbb{R}^{n\\times n}\\) and initial condition \\(C\\in \\mathbb{R}^{n\\times n}\\). If \\(A\\) is continuous we know that \\(Y\\) is well-defined and absolutely continuous. We may then state the following theorem regarding uniqueness and existence of such a function above. Theorem 1.1. (Bladt) (Uniqueness) Consider the homogeneous system of linear differential equations \\[ \\mathbf{Y}&#39;(t)=\\mathbf{Y}(t)\\mathbf{A}(t),\\hspace{15pt} \\mathbf{Y}(s)=\\mathbf{C},\\tag{1} \\] where \\(\\mathbf{Y}(t)\\), \\(\\mathbf{A}(t)\\) and \\(\\mathbf{C}\\) are \\(n\\times n\\)-matrices and \\(\\mathbf{A}(t)\\) is continuous on \\([s,t]\\). Then (1) has at most one solution. Theorem 1.2. (Bladt) (Existence) The matrix function \\[ \\mathbf{Y}(t)=\\sum_{k=0}^\\infty \\mathbf{Y}_k(t),\\tag{3} \\] converges uniformly and absolutely on finite intervals, and solves the differential equation \\[ \\mathbf{Y}&#39;(t)=\\mathbf{Y}(t)\\mathbf{A}(t),\\hspace{15pt} \\mathbf{Y}(s)=\\mathbf{C}. \\] Now, we know that for any system as in (1) with a continuous function \\(\\mathbf{A}(t)\\) we can always construct the solution as some converging series as per theorem 1.2 then theorem 1.1 gives that the solution is unique. We can then with a piece of mind define a symbol for such a solution, without care for the exact solution. Definition 1.3. (Bladt) (The Product Integral) For any continuous matrix function \\(\\mathbf{A}(t)\\) we define the product integral as \\[ \\prod_{s}^t(\\mathbf{I}+\\mathbf{A}(x)\\ dx) \\] as the unique solution \\(\\mathbf{Y}(t)\\) to \\[ \\mathbf{Y}&#39;(t)=\\mathbf{Y}(t)\\mathbf{A}(t),\\hspace{15pt} \\mathbf{Y}(s)=\\mathbf{I}. \\] From a simple integral argument we may construct a converging series not containing \\(\\mathbf{Y}\\) itself. We see that \\[\\begin{align*} \\prod_{s}^t(\\mathbf{I}+\\mathbf{A}(x)\\ dx)&amp;=\\mathbf{I}+\\int_s^t\\mathbf{Y}&#39;(x)\\ dx=\\mathbf{I}+\\int_s^t\\mathbf{Y}(x)\\mathbf{A}(x)\\ dx\\\\ &amp;=\\mathbf{I}+\\int_s^t\\left[ \\mathbf{I}+\\int_s^{x_1}\\mathbf{Y}&#39;(x_2)\\ dx_2\\right]\\mathbf{A}(x_1)\\ dx_1\\\\ &amp;=\\mathbf{I}+\\int_s^t\\left[ \\mathbf{I}+\\int_s^{x_1}\\mathbf{Y}(x_2)\\mathbf{A}(x_2)\\ dx_2\\right]\\mathbf{A}(x_1)\\ dx_1\\\\ &amp;=\\mathbf{I}+\\int_s^t\\mathbf{A}(x_1)\\ dx_1+\\int_s^t\\int_s^{x_1}\\mathbf{Y}(x_2)\\mathbf{A}(x_2)\\mathbf{A}(x_1)\\ dx_2\\ dx_1. \\end{align*}\\] One can continue indefinitely and see that we have the following representation. Corollary 1.4. (Bladt) (Peano Representation) The prduct integral has series representation given by \\[\\begin{align*} \\prod_{s}^t(\\mathbf{I}+\\mathbf{A}(x)\\ dx)&amp;=\\mathbf{I}+\\sum_{i=1}^\\infty\\int_s^t\\int_s^{x_1}\\cdots\\int_s^{x_n}A(x_n)\\cdots\\mathbf{A}(x_2)\\mathbf{A}(x_1)\\ dx_n\\cdots dx_2\\ dx_1\\\\ &amp;=\\mathbf{I}+\\int_s^t\\mathbf{A}(x_1)\\ dx_1+\\sum_{i=2}^\\infty\\int_s^t\\int_s^{x_1}\\cdots\\int_s^{x_n}A(x_n)\\cdots\\mathbf{A}(x_2)\\mathbf{A}(x_1)\\ dx_n\\cdots dx_2\\ dx_1. \\end{align*}\\] 16.5.1 Properties of the Product Integral The product integral is the fundamental solution in the sense that if \\[ \\mathbf{Y}&#39;(t)=\\mathbf{Y}(t)\\mathbf{A}(t),\\hspace{15pt}\\mathbf{Y}(s)=\\mathbf{C}, \\] then \\[ \\mathbf{Y}(t)=\\mathbf{C}\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big). \\] We furthermore have for any \\(s,t,u\\) have \\[ \\mathbf{Y}(t)=\\mathbf{Y}(s)\\prod_{s}^u\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\prod_{u}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big). \\] From uniqueness we then get the following theorem. Theorem 1.5. (Bladt) For any \\(s,t,u\\) we have that \\[ \\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)=\\prod_{s}^u\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\prod_{u}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big). \\] By choosing \\(u=t\\) in the above we get the following corollary. Corollary 1.6. (Bladt) The inverse of the product integral is \\[ \\left[\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\right]^{-1}=\\prod_{t}^s\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big). \\] Theorem 1.7. (Bladt) If \\(\\mathbf{A}(x)\\) commutes withall \\(\\mathbf{B}(x=\\) i.e. \\(\\mathbf{A}(x)\\mathbf{B}(x)=\\mathbf{B}(x)\\mathbf{A}(x)\\), then \\[ \\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{B}(x)\\ dx\\big)=\\prod_{s}^t\\big(\\mathbf{I}+\\left(\\mathbf{A}(x)+\\mathbf{B}(x)\\right)\\ dx\\big). \\] Corollary 1.8. (Bladt) Let \\(r\\) be a real-valued function, then \\[ \\exp\\left(-\\int_s^t r(x)\\ dx\\right)\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)=\\prod_{s}^t\\big(\\mathbf{I}+\\left(\\mathbf{A}(x)-r(x)\\mathbf{I}\\right)\\ dx\\big). \\] Theorem 1.9. (Bladt) If \\(\\mathbf{A}(x)\\) commutes for all \\(x\\) i.e. \\(\\mathbf{A}(y)\\mathbf{A}(x)=\\mathbf{A}(x)\\mathbf{A}(y)\\), then \\[ \\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)=e^{\\int_s^t\\mathbf{A}(x)\\ dx}. \\] In particular, if \\(\\mathbf{A}(x)=\\mathbf{A}\\lambda(x)\\) for some real-valued function \\(\\lambda\\) and a \\(n\\times n\\) matrix \\(\\mathbf{A}\\) then \\[ \\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)=e^{\\mathbf{A}\\int_s^t\\lambda(x)\\ dx}. \\] Taking the function \\(\\lambda=1\\) we have that \\(\\int_s^t\\lambda(x)\\ dx=(t-s)\\) and so we get the result. Theorem 1.10. (Bladt) If \\(\\mathbf{A}(x)=\\mathbf{A}\\) is constant then \\[ \\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)=e^{\\mathbf{A}(t-s)}. \\] Theorem 1.11. (Bladt) The product integral satisfies \\[ \\frac{\\partial}{\\partial s}\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)=-\\mathbf{A}(s)\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big). \\] On the other hand, the solution to the system of differential equations \\[ \\frac{\\partial}{\\partial s}\\mathbf{X}(s)=-\\mathbf{A}(s)\\mathbf{X}(s) \\] with initial condition \\(\\mathbf{X}(t)=\\mathbf{I}\\) is the function \\[ s\\mapsto\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big) \\] We have a very useful result for insurance mathematics and markov chains given by the Van Loan’s theorem giving the product integral of a block matrix with zero lower left block. Theorem 1.12. (Bladt) Let \\(\\mathbf{A}(x)\\), \\(\\mathbf{B}(x)\\) and \\(\\mathbf{C}(x)\\) be continuous matrix functions such that \\[ \\mathbf{D}(x)=\\begin{bmatrix} \\mathbf{A}(x) &amp; \\mathbf{B}(x)\\\\ \\mathbf{0} &amp; \\mathbf{C}(x) \\end{bmatrix}, \\] is a square matrix. Then the product integral of \\(\\mathbf{D}\\) is \\[ \\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{D}(x)\\ dx\\big)= \\begin{bmatrix} \\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big) &amp; \\int_s^t\\prod_{s}^u\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\mathbf{B}(u)\\prod_{u}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\ du\\\\ \\mathbf{0} &amp; \\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big) \\end{bmatrix}. \\] Proof. Start by defining \\(\\mathbf{X}(t)\\) as below \\[\\begin{align*} \\mathbf{X}(t)&amp;= \\begin{bmatrix} \\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big) &amp; \\int_s^t\\prod_{s}^u\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\mathbf{B}(u)\\prod_{u}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\ du\\\\ \\mathbf{0} &amp; \\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big) \\end{bmatrix}\\\\ &amp;:=\\begin{bmatrix} \\mathbf{X}_{11}(t) &amp; \\mathbf{X}_{12}(t)\\\\ \\mathbf{X}_{21}(t) &amp; \\mathbf{X}_{22}(t) \\end{bmatrix}. \\end{align*}\\] The derivative is then \\[\\begin{align*} \\frac{d}{dt}\\mathbf{X}(t)&amp;= \\begin{bmatrix} \\frac{d}{dt}\\mathbf{X}_{11}(t) &amp; \\frac{d}{dt}\\mathbf{X}_{12}(t)\\\\ \\frac{d}{dt}\\mathbf{X}_{21}(t) &amp; \\frac{d}{dt}\\mathbf{X}_{22}(t) \\end{bmatrix}. \\end{align*}\\] with \\[\\begin{align*} \\frac{d}{dt}\\mathbf{X}_{11}(t)&amp;=\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\mathbf{A}(t),\\\\ \\frac{d}{dt}\\mathbf{X}_{12}(t)&amp;= \\frac{d}{dt}\\int_s^t\\prod_{s}^u\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\mathbf{B}(u)\\prod_{u}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\ du,\\\\ \\frac{d}{dt}\\mathbf{X}_{21}(t)&amp;=\\mathbf{0},\\\\ \\frac{d}{dt}\\mathbf{X}_{22}(t)&amp;=\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\mathbf{C}(t). \\end{align*}\\] Consider that \\[\\begin{align*} \\prod_{u}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)&amp;=\\prod_{u}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\underbrace{\\prod_{t}^s\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)}_{=\\mathbf{I}}\\\\ &amp;=\\prod_{u}^s\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big), \\end{align*}\\] hence using af Riemann approximation of the integral \\(\\mathbf{X}_{12}(t)\\) we may write \\[\\begin{align*} \\mathbf{X}_{12}(t)&amp;=\\int_s^t\\prod_{s}^u\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\mathbf{B}(u)\\prod_{u}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\ du\\\\ &amp;=\\int_s^t\\prod_{s}^u\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\mathbf{B}(u)\\prod_{u}^s\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\ du\\\\ &amp;=\\lim_{n\\to \\infty}\\frac{t-s}{n}\\sum_{i=0}^n\\prod_{s}^{s+(t-s)i/n}\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\mathbf{B}(s+(t-s)i/n)\\prod_{s+(t-s)i/n}^s\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\\\ &amp;\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\\\ &amp;=\\left\\{\\lim_{n\\to \\infty}\\frac{t-s}{n}\\sum_{i=0}^n\\prod_{s}^{s+(t-s)i/n}\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\mathbf{B}(s+(t-s)i/n)\\prod_{s+(t-s)i/n}^s\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\right\\}\\\\ &amp;\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\\\ &amp;=\\left\\{\\int_s^t\\prod_{s}^u\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\mathbf{B}(u)\\prod_{u}^s\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\ du\\right\\}\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big). \\end{align*}\\] We then get that \\[\\begin{align*} \\frac{d}{dt}\\mathbf{X}_{12}(t)&amp;=\\frac{d}{dt}\\left\\{\\int_s^t\\prod_{s}^u\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\mathbf{B}(u)\\prod_{u}^s\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\ du\\right\\}\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\\\ &amp;=\\left\\{\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\mathbf{B}(t)\\prod_{t}^s\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\right\\}\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\\\ &amp;+\\left\\{\\int_s^t\\prod_{s}^u\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\mathbf{B}(u)\\prod_{u}^s\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\ du\\right\\}\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{C}(x)\\ dx\\big)\\mathbf{C}(t)\\\\ &amp;=\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{A}(x)\\ dx\\big)\\mathbf{B}(t)+\\mathbf{X}_{12}(t)\\mathbf{C}(t). \\end{align*}\\] We hope to show that \\(\\frac{d}{dt}\\mathbf{X}(t)=\\mathbf{X}(t)\\mathbf{D}(t)\\). Calculating the right side we have \\[\\begin{align*} \\mathbf{X}(t)\\mathbf{D}(t)&amp;= \\begin{bmatrix} \\mathbf{X}_{11}(t) &amp; \\mathbf{X}_{12}(t)\\\\ \\mathbf{0} &amp; \\mathbf{X}_{22}(t) \\end{bmatrix} \\begin{bmatrix} \\mathbf{A}(x) &amp; \\mathbf{B}(x)\\\\ \\mathbf{0} &amp; \\mathbf{C}(x) \\end{bmatrix}\\\\ &amp;=\\begin{bmatrix} \\mathbf{X}_{11}(t)\\mathbf{A}(x) &amp; \\mathbf{X}_{11}(t)\\mathbf{B}(x)+\\mathbf{X}_{12}(t)\\mathbf{C}(x)\\\\ \\mathbf{0} &amp; \\mathbf{X}_{22}(t)\\mathbf{C}(x) \\end{bmatrix}, \\end{align*}\\] given that \\(\\mathbf{X}\\) satisfies the desired differential equation and so it follows that \\(\\mathbf{X}\\) is the product integral \\(\\prod_{s}^t\\big(\\mathbf{I}+\\mathbf{D}(x)\\ dx\\big)\\) as desired.\\(\\blacksquare\\) "],["linear-algebra.html", "Chapter 17 Linear Algebra ", " Chapter 17 Linear Algebra "],["invertible-matrices.html", "17.1 Invertible matrices", " 17.1 Invertible matrices This sections study some fundamental properties of the invertible matrix. We start by defining what an invertible matrix is. Definition. Let \\(A\\) be an \\(n\\times m\\) matrix and \\(B\\) be an \\(m\\times n\\) matrix. We say that \\(A\\) is invertible if \\[ AB=I_{\\min(m,n)}. \\] In general, we only consider square matrices. If the above holds we say that \\(B\\) is \\(A\\)’s inverse and we write \\(B=A^{-1}\\). We can consider some equivalence statements regarding invertible matrices. Theorem. (The Invertible Matrix Theorem) Let \\(A\\) be a \\(n\\times n\\) matrix over a field \\(K\\) (\\(\\mathbb{R}^n\\)), then the following statements are equivalent There exists an \\(n\\times n\\) matrix \\(B\\) such that \\(AB=I_n=BA\\). There exist either a left inverse \\(B\\) or a right invers \\(C\\) i.e. \\(BA=I_n=AC\\). In this case, \\(B=C\\). \\(A\\) has an inverse and is nonsingular and is nondegenerate. \\(A\\) is row-equivalent to \\(I_n\\). \\(A\\) is column-equivalent to \\(I_n\\). \\(A\\) has \\(n\\) pivot positions. \\(A\\) has full rank i.e. \\(\\text{rank}(A)=n\\) (spans \\(K\\)). The equation \\(Ax=0\\) (\\(x\\in K\\)) has only the trivial solution \\(x=0\\). The equation \\(Ax=b\\) has only one solution \\(x\\). The kernal of \\(A\\) is trivial i.e. \\(\\text{ker}(A)=\\{0\\}\\). The columns of \\(A\\) are linearly independent. The columns of \\(A\\) span \\(K\\). \\(\\text{span}(A)=K\\). The columns of \\(A\\) form a basis of \\(K\\). The linear transformation \\(Ax\\) is a bijection from \\(K\\) to \\(K\\). \\(A\\) has non-zero determinant i.e. \\(\\text{det}(A)\\ne 0\\). \\(A\\) has not 0 as an eigenvalue. The transpose of \\(A\\) is invertible. \\(A\\) can be expressed as a finite product of elemtary matrices. We futhermore have some properties. Proposition. (Properties) Let \\(A\\) be an \\(n\\times n\\) invertible matrix. Then \\((A^{-1})^{-1}=A\\) \\((kA)^{-1}=k^{-1}A^{-1}\\) with \\(k\\ne 0\\). \\((Ax)^+=x^+A^{-1}\\) if \\(A\\) has orthonormal columns. \\((\\cdot)^+\\) denotes the Moore-Penrose inverse and \\(x\\) is a vector. If \\(B\\) is an \\(n\\times n\\) invertible matrix then \\((AB)^{-1}=B^{-1}A^{-1}\\). \\(\\text{det}(A^{-1})=(\\text{det}(A))^{-1}\\) The property 2 is especially useful in some settings. Consider for instance \\[ A= \\begin{bmatrix} \\sigma &amp; 0\\\\ \\sigma &amp; \\sigma \\end{bmatrix}=\\sigma\\begin{bmatrix} 1 &amp; 0\\\\ 1 &amp; 1 \\end{bmatrix}=\\sigma \\tilde{A}. \\] Then we simply find the inverse of \\(\\tilde{A}\\) and multiply by \\(\\sigma^{-1}\\). That is, \\[ A^{-1}=\\frac{1}{\\sigma}\\tilde{A}^{-1}=\\frac{1}{\\sigma} \\begin{bmatrix} 1 &amp; 0\\\\ -1 &amp; 1 \\end{bmatrix}= \\begin{bmatrix} 1/\\sigma &amp; 0\\\\ -1/\\sigma &amp; 1/\\sigma \\end{bmatrix}. \\] We have an easy propositions regarding diagonal matrices. Proposition. If \\(A\\) is an diagonal matrix, then \\(A\\) is invertible. In particular, \\[ A^{-1}=\\text{diag}(A_{11}^{-1},...,A_{nn}^{-1}). \\] "],["coding.html", "Chapter 18 Coding ", " Chapter 18 Coding "],["r-packages.html", "18.1 R-Packages", " 18.1 R-Packages 18.1.1 mlr3 This short introduction to the mlr3 package is based on the official documentation and introduction book and exercises done in the course Machine Learning in Non-Life Insurance. We start by installing the package mlr3verse containing all of the importatant packages. install.packages(&quot;mlr3verse&quot;) library(mlr3) task_mtcars = tsk(&quot;mtcars&quot;) task_mtcars ## &lt;TaskRegr:mtcars&gt; (32 x 11): Motor Trends ## * Target: mpg ## * Properties: - ## * Features (10): ## - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
