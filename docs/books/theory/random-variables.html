<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Random Variables | Complete Theory</title>
  <meta name="description" content="Chapter 8 Random Variables | Complete Theory" />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Random Variables | Complete Theory" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Random Variables | Complete Theory" />
  
  
  

<meta name="author" content="Joakim Bilyk" />


<meta name="date" content="2023-02-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="measure-theory.html"/>
<link rel="next" href="discrete-time-stochastic-processes.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="https://code.jquery.com/jquery-1.9.1.js"></script>

<script>
    $(function() {
        var element = document.body;
        if (localStorage.chkbx && localStorage.chkbx != '') {
            $('#remember_me').attr('checked', 'checked');
            element.classList.toggle("dark-mode");
            document.querySelector('.book-header.fixed').click();
        } else {
            $('#remember_me').removeAttr('checked');
            document.querySelector('.book-header.fixed').click();
        }

        $('#remember_me').click(function() {

            if ($('#remember_me').is(':checked')) {
                // save username and password
                localStorage.chkbx = $('#remember_me').val();
                element.classList.toggle("dark-mode");
                document.querySelector('.book-header.fixed').click();
            } else {
                localStorage.chkbx = '';
                element.classList.toggle("dark-mode");
                document.querySelector('.book-header.fixed').click();
            }
        });
    });

</script>

<div class = "sticky-darkmode-toggle">
  <label class="switch">
  <input type="checkbox" value="remember-me" id="remember_me">
  <span class="slider round">
  </span>
  </label>
</div>

<script>
$(function() {
  $('body').after($('.sticky-darkmode-toggle'));
})
</script>

<style>

.sticky-darkmode-toggle {
  position: fixed;
  right: 20px;
  bottom: 20px;
}
</style>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Comlete theory</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Abbreviations</a></li>
<li class="chapter" data-level="2" data-path="life-insurance-mathematics.html"><a href="life-insurance-mathematics.html"><i class="fa fa-check"></i><b>2</b> Life Insurance Mathematics</a></li>
<li class="chapter" data-level="3" data-path="finance.html"><a href="finance.html"><i class="fa fa-check"></i><b>3</b> Finance</a>
<ul>
<li class="chapter" data-level="3.1" data-path="finance.html"><a href="finance.html#discrete-time-models"><i class="fa fa-check"></i><b>3.1</b> Discrete time models</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="finance.html"><a href="finance.html#one-period-time-models"><i class="fa fa-check"></i><b>3.1.1</b> One-period time models</a></li>
<li class="chapter" data-level="3.1.2" data-path="finance.html"><a href="finance.html#multi-period-model"><i class="fa fa-check"></i><b>3.1.2</b> Multi-period model</a></li>
<li class="chapter" data-level="3.1.3" data-path="finance.html"><a href="finance.html#generelised-one-period-model"><i class="fa fa-check"></i><b>3.1.3</b> Generelised one-period model</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="finance.html"><a href="finance.html#self-financing-portfolios"><i class="fa fa-check"></i><b>3.2</b> Self-financing portfolios</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="finance.html"><a href="finance.html#discrete-time-sf-portfolio"><i class="fa fa-check"></i><b>3.2.1</b> Discrete time SF portfolio</a></li>
<li class="chapter" data-level="3.2.2" data-path="finance.html"><a href="finance.html#continuous-time-sf-portfolio"><i class="fa fa-check"></i><b>3.2.2</b> Continuous time SF portfolio</a></li>
<li class="chapter" data-level="3.2.3" data-path="finance.html"><a href="finance.html#portfolio-weights"><i class="fa fa-check"></i><b>3.2.3</b> Portfolio weights</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="finance.html"><a href="finance.html#black-scholes-pde"><i class="fa fa-check"></i><b>3.3</b> Black-Scholes PDE</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="finance.html"><a href="finance.html#contingent-claims-and-arbitrage"><i class="fa fa-check"></i><b>3.3.1</b> Contingent Claims and Arbitrage</a></li>
<li class="chapter" data-level="3.3.2" data-path="finance.html"><a href="finance.html#risk-neutral-valuation-1"><i class="fa fa-check"></i><b>3.3.2</b> Risk Neutral Valuation</a></li>
<li class="chapter" data-level="3.3.3" data-path="finance.html"><a href="finance.html#black-scholes-formula"><i class="fa fa-check"></i><b>3.3.3</b> Black-Scholes formula</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="finance.html"><a href="finance.html#completeness-and-hedging"><i class="fa fa-check"></i><b>3.4</b> Completeness and Hedging</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="finance.html"><a href="finance.html#completeness-in-black-scholes"><i class="fa fa-check"></i><b>3.4.1</b> Completeness in Black-Scholes</a></li>
<li class="chapter" data-level="3.4.2" data-path="finance.html"><a href="finance.html#absence-of-arbitrage-1"><i class="fa fa-check"></i><b>3.4.2</b> Absence of Arbitrage</a></li>
<li class="chapter" data-level="3.4.3" data-path="finance.html"><a href="finance.html#incomplete-markets"><i class="fa fa-check"></i><b>3.4.3</b> Incomplete Markets</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="finance.html"><a href="finance.html#parity-relations"><i class="fa fa-check"></i><b>3.5</b> Parity relations</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="finance.html"><a href="finance.html#put-call-parity"><i class="fa fa-check"></i><b>3.5.1</b> Put-call Parity</a></li>
<li class="chapter" data-level="3.5.2" data-path="finance.html"><a href="finance.html#the-greeks"><i class="fa fa-check"></i><b>3.5.2</b> The Greeks</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="finance.html"><a href="finance.html#fundamental-pricing-theorem-i-and-ii"><i class="fa fa-check"></i><b>3.6</b> Fundamental pricing theorem I and II</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="finance.html"><a href="finance.html#completeness-1"><i class="fa fa-check"></i><b>3.6.1</b> Completeness</a></li>
<li class="chapter" data-level="3.6.2" data-path="finance.html"><a href="finance.html#risk-neutral-valuation-formula"><i class="fa fa-check"></i><b>3.6.2</b> Risk Neutral Valuation Formula</a></li>
<li class="chapter" data-level="3.6.3" data-path="finance.html"><a href="finance.html#stochastic-discount-factors-1"><i class="fa fa-check"></i><b>3.6.3</b> Stochastic Discount Factors</a></li>
<li class="chapter" data-level="3.6.4" data-path="finance.html"><a href="finance.html#summary"><i class="fa fa-check"></i><b>3.6.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="finance.html"><a href="finance.html#mathematics-of-the-martingale-approach"><i class="fa fa-check"></i><b>3.7</b> Mathematics of the martingale approach</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="finance.html"><a href="finance.html#martingale-representation-theorem"><i class="fa fa-check"></i><b>3.7.1</b> Martingale representation theorem</a></li>
<li class="chapter" data-level="3.7.2" data-path="finance.html"><a href="finance.html#girsanov-theorem"><i class="fa fa-check"></i><b>3.7.2</b> Girsanov theorem</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="finance.html"><a href="finance.html#black-scholes-model---martingale-approach"><i class="fa fa-check"></i><b>3.8</b> Black-Scholes model - martingale approach</a></li>
<li class="chapter" data-level="3.9" data-path="finance.html"><a href="finance.html#multidimensional-models"><i class="fa fa-check"></i><b>3.9</b> Multidimensional models</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="non-life-insurance-mathematics.html"><a href="non-life-insurance-mathematics.html"><i class="fa fa-check"></i><b>4</b> Non-Life Insurance Mathematics</a></li>
<li class="chapter" data-level="5" data-path="probabilistic-machine-learning.html"><a href="probabilistic-machine-learning.html"><i class="fa fa-check"></i><b>5</b> Probabilistic Machine Learning</a></li>
<li class="chapter" data-level="6" data-path="quantative-risk-management.html"><a href="quantative-risk-management.html"><i class="fa fa-check"></i><b>6</b> Quantative Risk Management</a>
<ul>
<li class="chapter" data-level="6.1" data-path="quantative-risk-management.html"><a href="quantative-risk-management.html#the-loss-variable"><i class="fa fa-check"></i><b>6.1</b> The Loss Variable</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="quantative-risk-management.html"><a href="quantative-risk-management.html#risk-measures"><i class="fa fa-check"></i><b>6.1.1</b> Risk measures</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="measure-theory.html"><a href="measure-theory.html"><i class="fa fa-check"></i><b>7</b> Measure theory</a>
<ul>
<li class="chapter" data-level="7.1" data-path="measure-theory.html"><a href="measure-theory.html#equivalent-probability-measures"><i class="fa fa-check"></i><b>7.1</b> Equivalent Probability Measures</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="measure-theory.html"><a href="measure-theory.html#the-radon-nikodym-theorem"><i class="fa fa-check"></i><b>7.1.1</b> The Radon-Nikodym Theorem</a></li>
<li class="chapter" data-level="7.1.2" data-path="measure-theory.html"><a href="measure-theory.html#equivalent-probability-measures-1"><i class="fa fa-check"></i><b>7.1.2</b> Equivalent Probability Measures</a></li>
<li class="chapter" data-level="7.1.3" data-path="measure-theory.html"><a href="measure-theory.html#likelihood-processes"><i class="fa fa-check"></i><b>7.1.3</b> Likelihood processes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>8</b> Random Variables</a>
<ul>
<li class="chapter" data-level="8.1" data-path="random-variables.html"><a href="random-variables.html#introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="random-variables.html"><a href="random-variables.html#conditional-expectation"><i class="fa fa-check"></i><b>8.2</b> Conditional expectation</a></li>
<li class="chapter" data-level="8.3" data-path="random-variables.html"><a href="random-variables.html#independence"><i class="fa fa-check"></i><b>8.3</b> Independence</a></li>
<li class="chapter" data-level="8.4" data-path="random-variables.html"><a href="random-variables.html#moment-generating-function"><i class="fa fa-check"></i><b>8.4</b> Moment generating function</a></li>
<li class="chapter" data-level="8.5" data-path="random-variables.html"><a href="random-variables.html#standard-distributions"><i class="fa fa-check"></i><b>8.5</b> Standard distributions</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="random-variables.html"><a href="random-variables.html#normal-disribution"><i class="fa fa-check"></i><b>8.5.1</b> Normal disribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="discrete-time-stochastic-processes.html"><a href="discrete-time-stochastic-processes.html"><i class="fa fa-check"></i><b>9</b> Discrete Time Stochastic Processes</a>
<ul>
<li class="chapter" data-level="9.1" data-path="discrete-time-stochastic-processes.html"><a href="discrete-time-stochastic-processes.html#convergence-concepts"><i class="fa fa-check"></i><b>9.1</b> Convergence concepts</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="discrete-time-stochastic-processes.html"><a href="discrete-time-stochastic-processes.html#sums-and-average-processes"><i class="fa fa-check"></i><b>9.1.1</b> Sums and average processes</a></li>
<li class="chapter" data-level="9.1.2" data-path="discrete-time-stochastic-processes.html"><a href="discrete-time-stochastic-processes.html#ergodic-theory"><i class="fa fa-check"></i><b>9.1.2</b> Ergodic Theory</a></li>
<li class="chapter" data-level="9.1.3" data-path="discrete-time-stochastic-processes.html"><a href="discrete-time-stochastic-processes.html#weak-convergence"><i class="fa fa-check"></i><b>9.1.3</b> Weak Convergence</a></li>
<li class="chapter" data-level="9.1.4" data-path="discrete-time-stochastic-processes.html"><a href="discrete-time-stochastic-processes.html#central-limit-theorems"><i class="fa fa-check"></i><b>9.1.4</b> Central Limit Theorems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="continuous-time-stochastic-processes.html"><a href="continuous-time-stochastic-processes.html"><i class="fa fa-check"></i><b>10</b> Continuous Time Stochastic Processes</a>
<ul>
<li class="chapter" data-level="10.1" data-path="continuous-time-stochastic-processes.html"><a href="continuous-time-stochastic-processes.html#brownian-motion"><i class="fa fa-check"></i><b>10.1</b> Brownian Motion</a></li>
<li class="chapter" data-level="10.2" data-path="continuous-time-stochastic-processes.html"><a href="continuous-time-stochastic-processes.html#filtration"><i class="fa fa-check"></i><b>10.2</b> Filtration</a></li>
<li class="chapter" data-level="10.3" data-path="continuous-time-stochastic-processes.html"><a href="continuous-time-stochastic-processes.html#martingale"><i class="fa fa-check"></i><b>10.3</b> Martingale</a></li>
<li class="chapter" data-level="10.4" data-path="continuous-time-stochastic-processes.html"><a href="continuous-time-stochastic-processes.html#stochastic-calculus"><i class="fa fa-check"></i><b>10.4</b> Stochastic calculus</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="continuous-time-stochastic-processes.html"><a href="continuous-time-stochastic-processes.html#stochastic-integrals"><i class="fa fa-check"></i><b>10.4.1</b> Stochastic Integrals</a></li>
<li class="chapter" data-level="10.4.2" data-path="continuous-time-stochastic-processes.html"><a href="continuous-time-stochastic-processes.html#discrete-stochastic-integrals"><i class="fa fa-check"></i><b>10.4.2</b> Discrete Stochastic Integrals</a></li>
<li class="chapter" data-level="10.4.3" data-path="continuous-time-stochastic-processes.html"><a href="continuous-time-stochastic-processes.html#stochastic-differential-equations"><i class="fa fa-check"></i><b>10.4.3</b> Stochastic Differential Equations</a></li>
<li class="chapter" data-level="10.4.4" data-path="continuous-time-stochastic-processes.html"><a href="continuous-time-stochastic-processes.html#partial-differential-equations"><i class="fa fa-check"></i><b>10.4.4</b> Partial differential equations</a></li>
<li class="chapter" data-level="10.4.5" data-path="continuous-time-stochastic-processes.html"><a href="continuous-time-stochastic-processes.html#the-product-integral"><i class="fa fa-check"></i><b>10.4.5</b> The Product Integral</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>11</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="11.1" data-path="linear-algebra.html"><a href="linear-algebra.html#invertible-matrices"><i class="fa fa-check"></i><b>11.1</b> Invertible matrices</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://joakim-bilyk.github.io/index.html">Back to main site</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Complete Theory</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="random-variables" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Random Variables<a href="random-variables.html#random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Introduction<a href="random-variables.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<blockquote class="def">
<strong>Definition 1.1. (Hansen)</strong> <em>A <strong>real-valued random variable</strong> <span class="math inline">\(X\)</span> on a probability space <span class="math inline">\((\Omega, \mathbb{F},P)\)</span> is a measurable map <span class="math inline">\(X : (\Omega,\mathbb{F})\to (\mathbb{R},\mathbb{B})\)</span>.</em>
</blockquote>
<p>We never specify the background space <span class="math inline">\((\Omega, \mathbb{F},P)\)</span> however we always assume <span class="math inline">\(X\)</span> is <span class="math inline">\(\mathbb{F}-\mathbb{B}\)</span> measurable. This assumption implies <span class="math inline">\((X\in A)\in \mathbb{F}\)</span> for every <span class="math inline">\(A\in \mathbb{B}\)</span>. We may want to show measurability for constructed variables and so it surfises to show measurability for generaters for <span class="math inline">\(\mathbb{B}\)</span> such as checking <span class="math inline">\((X\le a)\in\mathbb{F}\)</span> for every <span class="math inline">\(a\in\mathbb{R}\)</span>.</p>
<blockquote class="def">
<strong>Definition 1.2. (Hansen)</strong> <em>The <strong>distribution</strong> of a real-valued random variable <span class="math inline">\(X\)</span>, defined on a probability space <span class="math inline">\((\Omega,\mathbb{F},P)\)</span>, is the collection of probability values</em>
<span class="math display">\[\begin{align*}
    P(X\in A)\hspace{15pt}\text{for}\ A\in \mathbb{B}.\tag{1.3}
\end{align*}\]</span>
<em>In other words: the distribution of <span class="math inline">\(X\)</span> is the image measure <span class="math inline">\(X(P)\)</span> on <span class="math inline">\((\mathbb{R},\mathbb{B})\)</span>.</em>
</blockquote>
<blockquote class="lem">
<strong>Lemma 1.3. (Hansen)</strong> <em>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(X&#39;\)</span> be two real-valued random variables on a probability space <span class="math inline">\((\Omega,\mathbb{F},P)\)</span>. If</em>
<span class="math display">\[\begin{align*}
    P(X=X&#39;)=1
\end{align*}\]</span>
<em>then <span class="math inline">\(X\)</span> and <span class="math inline">\(X&#39;\)</span> has the same distribution.</em>
</blockquote>
<p>An often used way of summarizing the distribution is through the <strong>distribution function</strong> <span class="math inline">\(F(x)=P(X\le x)\)</span> for some <span class="math inline">\(x\in\mathbb{R}\)</span>.</p>
<blockquote class="def">
<strong>Definition 1.4. (Hansen)</strong> <em>A real-valued random variable <span class="math inline">\(X\)</span> has a <strong>discrete</strong> distribution if there is a countable set <span class="math inline">\(S\subset\mathbb{R}\)</span> such that <span class="math inline">\(P(X\in S)=1\)</span>.</em>
</blockquote>
<p>Usually <span class="math inline">\(S\)</span> is one of <span class="math inline">\(\mathbb{N},\mathbb{Z},\mathbb{Q}\)</span> or a subset of these. We may in the discrete case define the distribution by the point probabilities <span class="math inline">\(P(X=x)=p(x)\)</span> for <span class="math inline">\(x\in S\)</span>.</p>
<blockquote class="def">
<strong>Definition 1.5. (Hansen)</strong> <em>A real-valued random variable <span class="math inline">\(X\)</span> has a distribution with <strong>density</strong> <span class="math inline">\(f : \mathbb{R}\to [0,\infty)\)</span> if</em>
<span class="math display">\[\begin{align*}
    P(X\in A)=\int_Af(x)dx\hspace{15pt}\text{for}\ A\in \mathbb{B}.\tag{1.5}
\end{align*}\]</span>
<em>If this is the case we will write <span class="math inline">\(X(P)=f\cdot m\)</span> or <span class="math inline">\(X\sim f\cdot m\)</span>.</em>
</blockquote>
<blockquote class="def">
<strong>Definition 1.6. (Hansen)</strong> <em>A real-valued random variable <span class="math inline">\(X\)</span> defined on a probability space <span class="math inline">\((\Omega, \mathbb{F},P)\)</span> is said to have <span class="math inline">\(p\)</span>’th moment for som <span class="math inline">\(p&gt;0\)</span> if</em>
<span class="math display">\[\begin{align*}
    E\vert X\vert^p&lt;\infty\tag{1.12}
\end{align*}\]</span>
<em>The collection of all variables that satisfies (1.12) is denoted by <span class="math inline">\(\mathcal{L}^p(\Omega,\mathbb{F},P)\)</span>.</em>
</blockquote>
<p>Recall the definition of the <strong>expectation</strong> of <span class="math inline">\(X\)</span> by
<span class="math display">\[\begin{align*}
    E\, X=\int XdP \in R\cup \{-\infty,+\infty\}.\tag{1.11}
\end{align*}\]</span>
We recall that for any measurable function <span class="math inline">\(f : \mathbb{R}\to \mathbb{R}\)</span> that are continuous on a set <span class="math inline">\(A\in\mathbb{B}\)</span> such that <span class="math inline">\(P(X\in A)=1\)</span> we may change variable simply by computing
<span class="math display">\[\begin{align*}
    E\, f(X)=\int f\circ XdP=\int f(x)dX(P)(x).
\end{align*}\]</span></p>
<blockquote class="lem">
<strong>Lemma 1.7. (Hansen)</strong> <em>(Markov’s inequality) Let <span class="math inline">\(X\)</span> be a non-negative random variable. For any <span class="math inline">\(c&gt;0\)</span> it holds that</em>
<span class="math display">\[\begin{align*}
    P(X\ge c)\le \frac{E\, X}{c}\left(\le \frac{E\ X^n}{c^n}\text{ or }\le \frac{E\left(\varphi(X)\right)}{\varphi(c)}\right).\tag{1.14}
\end{align*}\]</span>
<em>for some <span class="math inline">\(\varphi\)</span> non-negative monotome increasing function.</em>
</blockquote>
<p>Some other versions of Markov’s inequality can be found in the form of <strong>Chebyshev’s inequality</strong>, <strong>Chebyshev-Cantelli’s inequality</strong> or <strong>Jensen’s inequality</strong> repectively: Let <span class="math inline">\(X\)</span> be a real-valued random variable in <span class="math inline">\(\mathcal{L}^2(\Omega,\mathbb{F},P)\)</span> it holds for any <span class="math inline">\(\varepsilon&gt;0\)</span>.
<span class="math display">\[\begin{align*}
    P\left(\vert X-E\, X\vert \ge \varepsilon\right)&amp;\le \frac{V\, X}{\varepsilon^2}\tag{1.15}\\
    P\left( X-E\, X \ge \varepsilon\right)&amp;\le \frac{V\, X}{V\, X+\varepsilon^2}\tag{prob: 1.13(c)}\\
    \varphi\left(E\ X\right)&amp;\le E\left( \varphi(X)\right)
\end{align*}\]</span>
for some convex function <span class="math inline">\(\varphi\)</span>.</p>
<blockquote class="lem">
<strong>Lemma 1.8. (Hansen)</strong> <em>Let <span class="math inline">\(X\)</span> be a non-negative random variable. It holds that</em>
<span class="math display">\[\begin{align*}
    E\, X=\int_0^\infty P(X&gt;t)dt.\tag{1.16}
\end{align*}\]</span>
<em>where the integral on the right hand side is with respect to Lebesgue measure.</em>
</blockquote>
<blockquote class="def">
<strong>Definition 1.9. (Hansen)</strong> <em>The <strong>joint distribution</strong> of real-valued random variables <span class="math inline">\(X_1,...,X_k\)</span>, defined on a probability space <span class="math inline">\((\Omega, \mathbb{F},P)\)</span>, is the collection of probability values</em>
<span class="math display">\[\begin{align*}
    P(\mathbf{X}\in A)\hspace{15pt}\text{for}\ A\in\mathbb{B}_k.\tag{1.21}
\end{align*}\]</span>
<em>In other words: the joint distribution of <span class="math inline">\(X_1,...,X_k\)</span> (or simply: the distribution of <span class="math inline">\(\mathbf{X}\)</span>) is the image measure <span class="math inline">\(\mathbf{X}(P)\)</span> on <span class="math inline">\(\left(\mathbb{R}^k,\mathbb{B}_k\right)\)</span>.</em>
</blockquote>
<blockquote class="def">
<strong>Definition 1.11. (Hansen)</strong> <em>Real-valued random variables <span class="math inline">\(X_1,...,X_k\)</span>, defined on a probability space <span class="math inline">\((\Omega, \mathbb{F},P)\)</span>, are <strong>jointly independent</strong> if</em>
<span class="math display">\[\begin{align*}
    P\left(X_1\in A_1,...,X_k\in A_k\right)=\prod_{i=1}^kP(X_i\in A_i)\hspace{15pt}\text{for}\ A_1,...,A_k\in\mathbb{B}.\tag{1.23}
\end{align*}\]</span>
<em>In other words: the variables are independent if the joint distribution <span class="math inline">\(\mathbf{X}(P)\)</span> equals the product measure <span class="math inline">\(X_1(P)\otimes ... \otimes X_k(P)\)</span>.</em>
</blockquote>
<blockquote class="thm">
<strong>Theorem 1.12. (Hansen)</strong> <em>Let <span class="math inline">\(X_1,...,X_k\)</span> be real-valued random variables defined on a probability space <span class="math inline">\((\Omega, \mathbb{F},P)\)</span>. If the variables are independent and if <span class="math inline">\(E\vert X_i\vert&lt;\infty\)</span> for <span class="math inline">\(i=1,...,k\)</span>, then the product <span class="math inline">\(X_1\cdot ...\cdot X_k\)</span> has first moment and</em>
<span class="math display">\[\begin{align*}
    E\left(X_1\cdot ... \cdot X_k\right)=\prod_{i=1}^kE\, X_i\tag{1.24}
\end{align*}\]</span>
</blockquote>
<p>The equality only holds for two independent variables. However the <strong>Cauchy-Schwarz inequality</strong> which closely resembles (1.24) holds wether or not <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> are independent:
<span class="math display">\[\begin{align*}
    \left(E\vert XY\vert\right)^2\le E\, X^2\, E\, Y^2\text{ or }E\vert XY\vert \le \sqrt{E\ X^2}\sqrt{E\ Y^2}.\tag{1.25}
\end{align*}\]</span>
Furthermore the theorem give rise to a measure for dependence i.e. the <strong>covariance</strong> between two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>
<span class="math display">\[\begin{align*}
    \text{Cov}(X,Y)=E\left((X-E\,X)(Y-E\,Y)\right)=E(XY)-(E\, X)(E\, Y)\tag{1.26}
\end{align*}\]</span>
with Cov<span class="math inline">\((X,Y)\ne 0\)</span> if and only if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are dependent. With Cov<span class="math inline">\((X,Y)=0\)</span> the test is inconlusive. However independence implies Cov<span class="math inline">\((X,Y)=0\)</span>.</p>
<div style="page-break-after: always;"></div>
</div>
<div id="conditional-expectation" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Conditional expectation<a href="random-variables.html#conditional-expectation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The theory of conditional expectation is well-known from courses on the bachelor. Because of this we will only summarise the most important results.</p>
<p>We consider a background space <span class="math inline">\((\Omega,\mathcal{F},P)\)</span> and a sub-sigma algebra <span class="math inline">\(\mathcal{G}\subseteq \mathcal{F}\)</span>. We assume that some stochastic variable is <span class="math inline">\(\mathcal{F}\)</span>-measurable, that is the mapping <span class="math inline">\(X : (\Omega,\mathcal{F},P) \to (\mathbb{R},\mathbb{B},m)\)</span> is <span class="math inline">\(\mathcal{F}-\mathbb{B}\)</span>-measurable i.e. <span class="math inline">\(\forall B\in\mathbb{B} : \{X\in B\}\in\mathcal{F}\)</span>. For some random variable <span class="math inline">\(Z\)</span> defined on the subspace <span class="math inline">\((\Omega,\mathcal{G},P)\)</span>, we say that <span class="math inline">\(Z\)</span> is the conditional expectation of <span class="math inline">\(X\)</span> given <span class="math inline">\(\mathcal{G}\)</span> if</p>
<p><span class="math display">\[
\forall G\in\mathcal{G} : \int_G Z(\omega)\ dP(\omega)=\int_G X(\omega)\ dP(\omega).
\]</span></p>
<p>This fact is summed up in the definition below.</p>
<blockquote class="def">
<p><strong>Definition B.27. (Bjork)</strong> <strong>(Conditional expectation)</strong> <em>Let <span class="math inline">\((\Omega,\mathcal{F},P)\)</span> be a probability space and <span class="math inline">\(X\)</span> a random variable in <span class="math inline">\(L^1(\Omega,\mathcal{F},P)\)</span> (<span class="math inline">\(\vert X\vert\)</span> is integrable). Let furthermore <span class="math inline">\(\mathcal{G}\)</span> be a sigma-algebra such that <span class="math inline">\(\mathcal{G}\subseteq \mathcal{F}\)</span>. If <span class="math inline">\(Z\)</span> is a random variable with the properties that:</em></p>
<ol style="list-style-type: lower-roman">
<li><em><span class="math inline">\(Z\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable.</em></li>
<li><em>For every <span class="math inline">\(G\in\mathcal{G}\)</span> it holds that</em>
<span class="math display">\[\int_G Z(\omega)\ dP(\omega)=\int_G X(\omega)\ dP(\omega).\tag{B.5}\]</span></li>
</ol>
<p><em>Then we say that <span class="math inline">\(Z\)</span> is the <strong>conditional expectation of <span class="math inline">\(X\)</span> given the sigma-algebra <span class="math inline">\(\mathcal{G}\)</span></strong>. In that case we denote <span class="math inline">\(Z\)</span> by the symbol <span class="math inline">\(E[X\ \vert\ \mathcal{G}]\)</span>.</em></p>
</blockquote>
<p>We see that from the above it always holds that <span class="math inline">\(X\)</span> satisfies (ii). It does not, however, always hold that <span class="math inline">\(X\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable. Given this fact it is not trivial that a random variable <span class="math inline">\(E[X\ \vert\ \mathcal{G}]\)</span> even exists. This nontriviality is fortunatly resolved by the Radon-Nikodym theorem.</p>
<blockquote class="thm">
<p><strong>Theorem B.28. (Bjork)</strong> <strong>(Existance and uniqueness of Conditional expectation)</strong> <em>Let <span class="math inline">\((\Omega,\mathcal{F},P)\)</span>, <span class="math inline">\(X\)</span> and <span class="math inline">\(\mathcal{G}\)</span> be given as in the definition above. Then the following holds:</em></p>
<ul>
<li><em>There will always exist a random variable <span class="math inline">\(Z\)</span> satisfying conditions (i)-(ii) above.</em></li>
<li><em>The variable <span class="math inline">\(Z\)</span> is unique, i.e. if both <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> satisfy (i)-(ii) then <span class="math inline">\(Y=Z\)</span> <span class="math inline">\(P\)</span>-a.s.</em></li>
</ul>
</blockquote>
<p>This result ensures that we may condition on any sigma-algebra for instance <span class="math inline">\(\mathcal{G}=\sigma(Y)\)</span> in that case we (pure notation) write</p>
<p><span class="math display">\[
E[X\ \vert\ \sigma(Y)]=E[X\ \vert\ Y],\hspace{20pt}\sigma(Y)=\sigma\left(\left\{ Y\in A,\ A\in\mathbb{B}\right\}\right).
\]</span></p>
<p>In the above <span class="math inline">\(\sigma(Y)\)</span> is simply the smallest sigma-algebra containing all the pre-images of <span class="math inline">\(Y\)</span>, that is the smallest sigma-algebra making <span class="math inline">\(Y\)</span> measurable! Giving this foundation there are a few properties conditional expectation have which is rather useful (for instance the tower property).</p>
<p>Below we assume: Let <span class="math inline">\((\Omega,\mathcal{F},P)\)</span> be a probability space and <span class="math inline">\(X,Y\)</span> be random variables in <span class="math inline">\(L^1(\Omega,\mathcal{F},P)\)</span>.</p>
<blockquote class="prop">
<p><strong>Proposition B.29.</strong> <strong>(Monotinicity/Linearity of Conditional expectation)</strong> <em>The following holds:</em></p>
<p><span class="math display">\[X\le Y\ \Rightarrow\ E[X\ \vert\ \mathcal{G}]\le E[Y\ \vert\ \mathcal{G}],\hspace{20pt}P-\text{a.s.}\tag{B.6}\]</span>
<span class="math display">\[E[\alpha X + \beta Y\ \vert\ \mathcal{G}]=\alpha E[X\ \vert\ \mathcal{G}]+ \beta E[Y\ \vert\ \mathcal{G}],\hspace{20pt}\forall \alpha,\beta\in\mathbb{R}.\tag{B.7}\]</span></p>
</blockquote>
<blockquote class="prop">
<p><strong>Proposition B.30. (Bjork)</strong> <strong>(Tower property)</strong> <em>Assume that it holds that <span class="math inline">\(\mathcal{H}\subseteq\mathcal{G}\subseteq\mathcal{F}\)</span>. Then the following hold:</em></p>
<p><span class="math display">\[E[E[X\vert \mathcal{G}]\vert\mathcal{H}]=E[X\vert \mathcal{H}],\tag{B.8}\]</span>
<span class="math display">\[E[X]=E[E[X\vert \mathcal{G}]].\tag{B.9}\]</span></p>
</blockquote>
<blockquote class="prop">
<p><strong>Proposition B.31. (Bjork)</strong> <em>Assume <span class="math inline">\(X\)</span> is <span class="math inline">\(\mathcal{G}\)</span> and that both <span class="math inline">\(X,Y\)</span> and <span class="math inline">\(XY\)</span> are in <span class="math inline">\(L^1\)</span> (only assuming <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mathcal{F}\)</span>-measurable), then</em></p>
<p><span class="math display">\[E[X\vert\mathcal{G}]=X,\hspace{20pt}P-\text{a.s.}\tag{B.11}\]</span>
<span class="math display">\[E[XY\vert\mathcal{G}]=XE[Y\vert\mathcal{G}],\hspace{20pt}P-\text{a.s.}\tag{B.12}\]</span></p>
</blockquote>
<blockquote class="prop">
<p><strong>Proposition B.32. (Bjork)</strong> <strong>(Jensen inequality)</strong> <em>Let <span class="math inline">\(f:\mathbb{R}\to\mathbb{R}\)</span> be a convex (measurable) function and assume <span class="math inline">\(f(X)\)</span> is in <span class="math inline">\(L^1\)</span>. Then</em></p>
<p><span class="math display">\[f(E[X\vert\mathcal{G}])\le E[f(X)\vert\mathcal{G}],\hspace{20pt}P-\text{a.s.}\]</span></p>
</blockquote>
<blockquote class="prop">
<p><strong>Proposition B.37. (Bjork)</strong> <em>Let <span class="math inline">\((\Omega,\mathcal{F},P)\)</span> be a given probability space, let <span class="math inline">\(\mathcal{G}\)</span> be a sub-sigma-algebra of <span class="math inline">\(\mathcal{F}\)</span>, and let <span class="math inline">\(X\)</span> be a square integrable random variable.
Consider the problem of minimizing</em></p>
<p><span class="math display">\[E\left[(X-Z)^2\right]\]</span></p>
<p><em>where <span class="math inline">\(Z\)</span> is allowed to vary over the class of all square integrable <span class="math inline">\(\mathcal{G}\)</span> measurable random variables. The optimal solution <span class="math inline">\(\hat{Z}\)</span> is then given by.</em></p>
<p><span class="math display">\[\hat{Z}=E[X\vert\mathcal{G}].\]</span></p>
</blockquote>

<details>
<summary>
<strong>Proof.</strong>
</summary>
<p>Let <span class="math inline">\(X\in L^2(\Omega,\mathcal{F},P)\)</span> be a random variable. Now consider an arbitrary <span class="math inline">\(Z\in L^2(\Omega,\mathcal{G},P)\)</span>. Recall that <span class="math inline">\(\mathcal{G}\subset \mathcal{F}\)</span> and so <span class="math inline">\(X\)</span> is also in <span class="math inline">\(Z\in L^2(\Omega,\mathcal{G},P)\)</span>, as it is bothe square integrable and <span class="math inline">\(\mathcal{G}\)</span>-measurable. Then</p>
<p><span class="math display">\[E\left[Z\cdot(X-E[X\vert\mathcal{G}])\right]=E\left[Z\cdot X\right]-E\left[Z\cdot E[X\vert\mathcal{G}]\right].\]</span></p>
<p>Then by using the law of total expectation and secondly that <span class="math inline">\(Z\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable we have that</p>
<p><span class="math display">\[E\left[Z\cdot X\right]=E\left[E[Z\cdot X\vert\mathcal{G}]\right]=E\left[Z\cdot E[ X\vert\mathcal{G}]\right].\]</span></p>
<p>Combining the two equations gives the desired result. Obviously, we have that</p>
<p><span class="math display">\[X-Z=X-Z+E[X\vert\mathcal{G}]-E[X\vert\mathcal{G}]=(X-E[X\vert\mathcal{G}])+(E[X\vert\mathcal{G}]-Z).\]</span></p>
<p>Then squaring the terms gives</p>
<p><span class="math display">\[(X-Z)^2=(X-E[X\vert\mathcal{G}])^2+(E[X\vert\mathcal{G}]-Z)^2+2(X-E[X\vert\mathcal{G}])(E[X\vert\mathcal{G}]-Z)\]</span></p>
<p>Taking expectation on each side and using linearity of the expectation we have that</p>
<p><span class="math display">\[E[(X-Z)^2]=E\left[(X-E[X\vert\mathcal{G}])^2\right]+E\left[(E[X\vert\mathcal{G}]-Z)^2\right]+2E\left[(X-E[X\vert\mathcal{G}])(E[X\vert\mathcal{G}]-Z)\right].\]</span></p>
<p>We can now use that <span class="math inline">\(E[X\vert\mathcal{G}]-Z\)</span> is <span class="math inline">\(\mathcal{G}\)</span>-measurable with the above result on the last term.</p>
<p><span class="math display">\[E[(X-Z)^2]=E\left[(X-E[X\vert\mathcal{G}])^2\right]+E\left[(E[X\vert\mathcal{G}]-Z)^2\right].\]</span></p>
<p>Now since <span class="math inline">\(X\)</span> is given the term <span class="math inline">\(E\left[(X-E[X\vert\mathcal{G}])^2\right]\)</span> is simply a constant not depending on the choice og <span class="math inline">\(Z\)</span>. The optimal choice of <span class="math inline">\(Z\)</span> is then <span class="math inline">\(E[X\vert\mathcal{G}]\)</span> since this minimizes the second term. The statement is then proved.</p>
</details>
<div style="page-break-after: always;"></div>
</div>
<div id="independence" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Independence<a href="random-variables.html#independence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<blockquote class="def">
<strong>Definition 3.1. (Hansen)</strong> <em>Let <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> be a probability space. Two events <span class="math inline">\(A,B\in\mathbb{F}\)</span> are <strong>independent</strong> if</em>
<span class="math display">\[\begin{align*}
    P(A\cap B)=P(A)P(B)\tag{3.1}
\end{align*}\]</span>
</blockquote>
<blockquote class="def">
<strong>Definition 3.4. (Hansen)</strong> <em>Let <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> be a probability space and let <span class="math inline">\(\mathbb{G},\mathbb{H}\subset \mathbb{F}\)</span> be two classes of measurable sets. We sat that <span class="math inline">\(\mathbb{G}\)</span> and <span class="math inline">\(\mathbb{H}\)</span> are independent, written <span class="math inline">\(\mathbb{G}\perp \!\!\! \perp\mathbb{H}\)</span>, if</em>
<span class="math display">\[\begin{align*}
    P(A\cap B)=P(A)P(B)\hspace{15pt}\text{for all}\ A\in\mathbb{G},B\in\mathbb{H}.\tag{3.2}
\end{align*}\]</span>
</blockquote>
<blockquote class="lem">
<strong>Lemma 3.5. (Hansen)</strong> <em>Let <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> be a probability space and let <span class="math inline">\(\mathbb{G},\mathbb{H}\subset \mathbb{F}\)</span> be two classes of measurable sets. Let <span class="math inline">\(\mathbb{G}_1\subset \mathbb{G}\)</span> and <span class="math inline">\(\mathbb{H}_1\subset\mathbb{H}\)</span> be two subclasses. If <span class="math inline">\(\mathbb{G}\perp \!\!\! \perp \mathbb{H}\)</span> then it holds that <span class="math inline">\(\mathbb{G}_1\perp \!\!\! \perp \mathbb{H}_1\)</span>.</em>
</blockquote>
<blockquote class="def">
<p><strong>Definition 3.6. (Hansen)</strong> <em>A class <span class="math inline">\(\mathbb{H}\)</span> of subsets of <span class="math inline">\(\Omega\)</span> is a <strong>Dynkin class</strong> if</em></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\Omega \in\mathbb{H}\)</span>,</li>
<li><span class="math inline">\(A,B\in\mathbb{H},A\subset B\hspace{15pt}\Rightarrow\hspace{15pt}B\setminus A\in\mathbb{H}\)</span>,</li>
<li><span class="math inline">\(A_1,A_2,...\in\mathbb{H},A_1\subset A_2\subset ...\hspace{15pt}\Rightarrow\hspace{15pt}\bigcup_{n=1}^\infty A_n\in\mathbb{H}\)</span>.</li>
</ol>
</blockquote>
<blockquote class="lem">
<p><strong>Lemma 3.7. (Hansen)</strong> <em>(Dynkin) Let <span class="math inline">\(\mathbb{D}\subset \mathbb{H}_0\subset \mathbb{H}\)</span> be three nested classes of subsets of <span class="math inline">\(\Omega\)</span>. if</em></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\sigma(\mathbb{D})=\mathbb{H}\)</span>,</li>
<li><span class="math inline">\(A,B\in\mathbb{D}\hspace{10pt}\Rightarrow\hspace{10pt}A\cap B\in\mathbb{D}\)</span></li>
<li><span class="math inline">\(\mathbb{H}_0\)</span> <em>is a Dynkin class.</em></li>
</ol>
<em>then it holds that <span class="math inline">\(\mathbb{H}_0=\mathbb{H}\)</span>.</em>
</blockquote>
<blockquote class="lem">
<strong>Lemma 3.8. (Hansen)</strong> <em>Let <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> be a probability space, and let <span class="math inline">\(A\in\mathbb{F}\)</span> be a fixed event. The class</em>
<span class="math display">\[\begin{align*}
    \mathbb{H}=\{B\in \mathbb{F}\ \vert\ A\perp \!\!\! \perp B\}
\end{align*}\]</span>
<em>is a Dynkin class.</em>
</blockquote>
<blockquote class="thm">
<strong>Theorem 3.9. (Hansen)</strong> <em>Let <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> be a probability space, and let <span class="math inline">\(\mathbb{G}_1,\mathbb{G}_2\subset \mathbb{F}\)</span> be two sigma-algebras. Let <span class="math inline">\(\mathbb{D}_1\)</span> and <span class="math inline">\(\mathbb{D}_2\)</span> be two classes such that <span class="math inline">\(\sigma(\mathbb{D}_i)=\mathbb{G}_i\)</span> for <span class="math inline">\(i=1,2\)</span>.</em>
<em>If both <span class="math inline">\(\mathbb{D}_1\)</span> and <span class="math inline">\(\mathbb{D}_2\)</span> are <span class="math inline">\(\cap\)</span>-stable then it holds that</em>
<span class="math display">\[\begin{align*}
    \mathbb{D}_1\perp \!\!\! \perp\mathbb{D}_2\hspace{10pt}\Rightarrow \hspace{10pt} \mathbb{G}_1\perp \!\!\! \perp\mathbb{G}_2.
\end{align*}\]</span>
</blockquote>
<blockquote class="def">
<strong>Definition 3.10. (Hansen)</strong> <em>Two real-valued random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> on a background space <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> are , written <span class="math inline">\(X\perp \!\!\! \perp Y\)</span>, if the corresponding sigma-algebras <span class="math inline">\(\sigma(X)\)</span> and <span class="math inline">\(\sigma(Y)\)</span> are independent.</em>
</blockquote>
<blockquote class="def">
<strong>Definition 3.15. (Hansen)</strong> <em>Let <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> be a probability space, and let <span class="math inline">\(\mathbb{G}_1,...,\mathbb{G}_n\subset \mathbb{F}\)</span> be finitely many classes of measurable sets. We say that <span class="math inline">\(\mathbb{G}_1,...,\mathbb{G}_n\)</span> are <strong>jointly independent</strong>, written <span class="math inline">\(\mathbb{G}_1\perp \!\!\! \perp...\perp \!\!\! \perp\mathbb{G}_n\)</span>, if</em>
<span class="math display">\[\begin{align*}
    P(A_1\cap ...\cap A_n=\prod_{i=1}^nP(A_i)\hspace{15pt}\text{for }A_1\in\mathbb{G}_1,...,A_n\in\mathbb{G}_n.\tag{3.8}
\end{align*}\]</span>
</blockquote>
<blockquote class="lem">
<strong>Lemma 3.16. (Hansen)</strong> <em>Let <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> be a probability space, and let <span class="math inline">\(\mathbb{G}_1,...,\mathbb{G}_n\subset \mathbb{F}\)</span> be finitely many classes of measurable sets. It holds that</em>
<span class="math display">\[\begin{align*}
    \mathbb{G}_1\perp \!\!\! \perp...\perp \!\!\! \perp\mathbb{G}_n\hspace{10pt}\Rightarrow \hspace{10pt} \mathbb{G}_1\perp \!\!\! \perp...\perp \!\!\! \perp\mathbb{G}_{n-1}
\end{align*}\]</span>
<em>provided that <span class="math inline">\(\Omega\in\mathbb{G}_n\)</span>.</em>
</blockquote>
<blockquote class="thm">
<strong>Theorem 3.17. (Hansen)</strong> <em>Let <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> be a probability space, and let <span class="math inline">\(\mathbb{G}_1,...,\mathbb{G}_n\subset \mathbb{F}\)</span> be sigma-algebras. Let <span class="math inline">\(\mathbb{D}_1,...,\mathbb{D}_n\)</span> be classes such that <span class="math inline">\(\sigma(\mathbb{D}_i)=\mathbb{G}_1\)</span> for <span class="math inline">\(i=1,...,n\)</span>. Suppose that for all lengths <span class="math inline">\(k=2,...,n\)</span> an all choices of indices <span class="math inline">\(\le j_1&lt;...&lt;j_k\le n\)</span> it holds that</em>
<span class="math display">\[\begin{align*}
    \mathbb{D}_{j_1}\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{D}_{j_k}\tag{3.9}
\end{align*}\]</span>
<em>If all the generators <span class="math inline">\(\mathbb{D}_i\)</span> are <span class="math inline">\(\cap\)</span>-stable, then it holds that <span class="math inline">\(\mathbb{G}_1\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{G}_n\)</span>.</em>
</blockquote>
<blockquote class="lem">
<strong>Lemma 3.18. (Hansen)</strong> <em>(Grouping) Let <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> be a probability space, and let <span class="math inline">\(\mathbb{G}_1,...,\mathbb{G}_n\subset \mathbb{F}\)</span> be sigma-algebras. It holds that</em>
<span class="math display">\[\begin{align*}
    \mathbb{G}_1\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{G}_n \hspace{10pt}\Rightarrow \hspace{10pt} \mathbb{G}_1\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{G}_{n-2}\perp \!\!\! \perp\sigma(\mathbb{G}_{n-1},\mathbb{G}_n).
\end{align*}\]</span>
</blockquote>
<blockquote class="def">
<strong>Definition 3.19. (Hansen)</strong> <em>The real-valued random variables <span class="math inline">\(X_1,...,X_n\)</span> on a background space <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> are <strong>jointly independent</strong>, written <span class="math inline">\(X_1\perp \!\!\! \perp ... \perp \!\!\! \perp X_n\)</span>, if the corresponding sigma-algebras <span class="math inline">\(\sigma(X_1),...,\sigma(X_n)\)</span> are jointly independent.</em>
</blockquote>
<blockquote class="def">
<strong>Definition 3.20. (Hansen)</strong> <em>Let <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> be a probability space, and let <span class="math inline">\((\mathbb{G}_i)_{i\in I}\)</span> be a family of classes of measurable sets. We say that the family <span class="math inline">\((\mathbb{G}_i)_{i\in I}\)</span> is <strong>jointly independent</strong> if any finite subfamily is jointly independent.</em>
</blockquote>
<blockquote class="thm">
<strong>Theorem 3.21. (Hansen)</strong> <em>Let <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> be a probability space, and let <span class="math inline">\(\mathbb{G}_1,\mathbb{G}_2,...\subset \mathbb{F}\)</span> be sigma-algebras. Let <span class="math inline">\(\mathbb{D}_1,\mathbb{D}_2,...\)</span> be classes such that <span class="math inline">\(\sigma(\mathbb{D}_n)=\mathbb{G}_n\)</span> for all <span class="math inline">\(n\in\mathbb{N}\)</span>. Suppose that for all lengths <span class="math inline">\(k\in\mathbb{N}\)</span> and all choices of indices <span class="math inline">\(1\le j_1&lt; ... &lt; j_k\)</span> it holds that</em>
<span class="math display">\[\begin{align*}
    \mathbb{D}_{j_1}\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{D}_{j_k}.\tag{3.14}
\end{align*}\]</span>
<em>If all the generators <span class="math inline">\(\mathbb{D}_n\)</span> are <span class="math inline">\(\cap\)</span>-stable, then it holds that <span class="math inline">\(\mathbb{G}_1\perp \!\!\! \perp \mathbb{G}_2 \perp \!\!\! \perp ...\)</span>.</em>
</blockquote>
<blockquote class="lem">
<strong>Lemma 3.22. (Hansen)</strong> <em>Let <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> be a probability space, and let <span class="math inline">\(\mathbb{G}_1,\mathbb{G}_2,...\subset \mathbb{F}\)</span> be sigma-algebras. It holds that</em>
<span class="math display">\[\begin{align*}
    \mathbb{G}_1\perp \!\!\! \perp \mathbb{G}_2 \perp \!\!\! \perp ... \hspace{10pt}\Rightarrow \hspace{10pt} \mathbb{G}_1\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{G}_n \perp \!\!\! \perp \sigma(\mathbb{G}_{n+1},\mathbb{G}_{n+2}, ... ).
\end{align*}\]</span>
</blockquote>
<blockquote class="def">
<strong>Definition 3.23. (Hansen)</strong> <em>The real-valued random variables <span class="math inline">\((X_i)_{i\in I}\)</span> on a background space <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> are <strong>jointly independent</strong> if the corresponding sigma-algebras <span class="math inline">\((\sigma(X_i))_{i\in I}\)</span> are jointly independent.</em>
</blockquote>
<blockquote class="def">
<strong>Definition 3.28. (Hansen)</strong> <em>Let <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> be a probability space. A sigma-algebra <span class="math inline">\(\mathbb{G}\subset \mathbb{F}\)</span> satisfies a <strong>zero-one law</strong> if</em>
<span class="math display">\[\begin{align*}
    P(A)\in\{0,1\}\hspace{15pt}\text{for all}\ A\in\mathbb{G}.
\end{align*}\]</span>
</blockquote>
<blockquote class="thm">
<p><strong>Theorem 3.29. (Hansen)</strong> <em>Let <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> be a probability space and let <span class="math inline">\(\mathbb{G}\subset \mathbb{F}\)</span> be a sigma-algebra. The following three conditions are equivalent:</em></p>
<ol style="list-style-type: decimal">
<li><em>For any sigma-algebra <span class="math inline">\(\mathbb{H}\subset \mathbb{F}\)</span> it holds that <span class="math inline">\(\mathbb{G} \perp \!\!\! \perp \mathbb{H}\)</span>,</em></li>
<li><em>It holds that <span class="math inline">\(\mathbb{G}\perp \!\!\! \perp\mathbb{G}\)</span>,</em></li>
<li><em><span class="math inline">\(\mathbb{G}\)</span> satisfies a 0-1 law.</em></li>
</ol>
</blockquote>
<blockquote class="def">
<strong>Definition 3.30. (Hansen)</strong> <em>Let <span class="math inline">\(X_1,X_2,...\)</span> be real-valued random variables on a background space <span class="math inline">\((\Omega,\mathbb{F},P)\)</span>. The <strong>tail sigma-algebra</strong> of the process is defined as</em>
<span class="math display">\[\begin{align*}
    \mathbb{J}(X_1,X_2,...)=\bigcap_{n=1}^\infty \sigma(X_n,X_{n+1}, ... ).
\end{align*}\]</span>
</blockquote>
<blockquote class="thm">
<strong>Theorem 3.32. (Hansen)</strong> <em>(Kolmogorov’s zero-one law) Let <span class="math inline">\(X_1,X_2,...\)</span> be real-valued random variables on a background space <span class="math inline">\((\Omega,\mathbb{F},P)\)</span>. If <span class="math inline">\(X_1 \perp \!\!\! \perp X_2 \perp \!\!\! \perp ...\)</span> then the tail-algebra <span class="math inline">\(\mathbb{J}(X_1,X_2,...)\)</span> satisfies a 0-1 law.</em>
</blockquote>
<blockquote class="lem">
<strong>Lemma 3.35. (Hansen)</strong> <em>(2nd half of Borel-Cantelli) Let <span class="math inline">\((\Omega,\mathbb{F},P)\)</span> be a probability space, and let <span class="math inline">\(A_1,A_2,...\)</span> be a sequence of <span class="math inline">\(\mathbb{F}\)</span>-measurable sets. If <span class="math inline">\(A_1 \perp \!\!\! \perp A_2 \perp \!\!\! \perp ...\)</span> then it holds that</em>
<span class="math display">\[\begin{align*}
    \sum_{n=1}^\infty P(A_n)&lt;\infty \iff P(A_n\text{ i.o.})=0.
\end{align*}\]</span>
</blockquote>
<div style="page-break-after: always;"></div>
</div>
<div id="moment-generating-function" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Moment generating function<a href="random-variables.html#moment-generating-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(X\)</span> be a random variable with distribution function <span class="math inline">\(F(x)=P(X\le x)\)</span> and <span class="math inline">\(Y\)</span> be a random variable with distribution function <span class="math inline">\(G(y)=P(Y\le y)\)</span>.</p>
<blockquote class="def">
<p><strong>Definition. (Ex. FinKont)</strong> <em>The moment generating function or Laplace transform of <span class="math inline">\(X\)</span> is</em></p>
<p><span class="math display">\[\psi_X(\lambda)=E\left[e^{\lambda X}\right]=\int_{-\infty}^\infty e^{\lambda x}dF(x)\]</span></p>
<p><em>provided the expectation is finite for <span class="math inline">\(\vert\lambda\vert&lt;h\)</span> for some <span class="math inline">\(h&gt;0\)</span>.</em></p>
</blockquote>
<p>The MGF uniquely determine the distribution of a random variable, due to the following result.</p>
<blockquote class="thm">
<p><strong>Theorem. (Ex. FinKont)</strong> <strong>(Uniqueness)</strong> <em>If <span class="math inline">\(\psi_X(\lambda)=\psi_Y(\lambda)\)</span> when <span class="math inline">\(\vert\lambda\vert&lt;h\)</span> for some <span class="math inline">\(h&gt;0\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> has the same distribution, that is, <span class="math inline">\(F=G\)</span>.</em></p>
</blockquote>
<p>There is also the following result of independence for Moment generating functions.</p>
<blockquote class="thm">
<p><strong>Theorem. (Ex. FinKont)</strong> <strong>(Independence)</strong> <em>If</em></p>
<p><span class="math display">\[E\left[e^{\lambda_1X+\lambda_2Y}\right]=\psi_X(\lambda_1)\psi_Y(\lambda_2)\]</span></p>
<p><em>for <span class="math inline">\(\vert\lambda_i\vert&lt;h\)</span> for <span class="math inline">\(i=1,2\)</span> for some <span class="math inline">\(h&gt;0\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables.</em></p>
</blockquote>
<div style="page-break-after: always;"></div>
</div>
<div id="standard-distributions" class="section level2 hasAnchor" number="8.5">
<h2><span class="header-section-number">8.5</span> Standard distributions<a href="random-variables.html#standard-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="normal-disribution" class="section level3 hasAnchor" number="8.5.1">
<h3><span class="header-section-number">8.5.1</span> Normal disribution<a href="random-variables.html#normal-disribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The following gives a comprehensive table of the standard some properties.</p>
<table>
<colgroup>
<col width="30%" />
<col width="16%" />
<col width="52%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Description</th>
<th align="center">Symbol</th>
<th align="center">Normal distribution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Definition</td>
<td align="center"><span class="math inline">\(\sim\)</span></td>
<td align="center"><span class="math inline">\(X\sim\mathcal{N}(\mu,\sigma^2)\)</span></td>
</tr>
<tr class="even">
<td align="left">Parameters</td>
<td align="center"><span class="math inline">\(\theta\in \Theta\)</span></td>
<td align="center"><span class="math inline">\(\theta=(\mu,\sigma^2)\in \mathbb{R}\times \mathbb{R}_+\)</span></td>
</tr>
<tr class="odd">
<td align="left">Support</td>
<td align="center"><span class="math inline">\(\text{Im}(X)\)</span></td>
<td align="center"><span class="math inline">\(x\in \mathbb{R}\)</span></td>
</tr>
<tr class="even">
<td align="left">Density</td>
<td align="center"><span class="math inline">\(f\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\left(\frac{x-\mu}{\sqrt{2\sigma^2}}\right)^2}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Distribution</td>
<td align="center"><span class="math inline">\(F\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{2}\left(1+N\left(\frac{x-\mu}{\sqrt{2\sigma^2}}\right)\right)\)</span></td>
</tr>
<tr class="even">
<td align="left">Mean value</td>
<td align="center"><span class="math inline">\(E[X]\)</span></td>
<td align="center"><span class="math inline">\(\mu\)</span></td>
</tr>
<tr class="odd">
<td align="left">Variance</td>
<td align="center"><span class="math inline">\(\text{Var}(X)\)</span></td>
<td align="center"><span class="math inline">\(\sigma^2\)</span></td>
</tr>
<tr class="even">
<td align="left">MGF*</td>
<td align="center"><span class="math inline">\(\psi_X=E[e^{\lambda X}]\)</span></td>
<td align="center"><span class="math inline">\(e^{\mu \lambda+\frac{1}{2}\lambda^2\sigma^2}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Characteristic function</td>
<td align="center"><span class="math inline">\(\varphi_X(t)=E[e^{itX}]\)</span></td>
<td align="center"><span class="math inline">\(e^{it\mu-\frac{1}{2}\sigma^2 t^2}\)</span></td>
</tr>
</tbody>
</table>
<p>In the table above we used the abbreviations: *MGF = Moment Generating function.</p>
<p>We also used the shorthand: <span class="math inline">\(N\)</span> being the distribution of a standard normal distributed variable <span class="math inline">\(\mathcal{N}(0,1)\)</span>.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="measure-theory.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="discrete-time-stochastic-processes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
