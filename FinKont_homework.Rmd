---
title: "Homework (FinKont)"
author: "Joakim Bilyk"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{wrapfig}
   - \usepackage{graphics}
output:
  html_document:
    toc: no
    code_folding: hide
  pdf_document:
    toc: yes
---

```{r, setup,include=FALSE}
if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(
    fig.show = 'hide',
    wrapf = TRUE
  )
} else {
  knitr::opts_chunk$set(
    out.extra = 'style="float:right; padding:10px"',
    out.width= "40%"
  )
}
library(knitr)
knit_hooks$set(wrapf = function(before, options, envir) {
  if (!before) {
    output <- vector(mode = "character", length = options$fig.num + 1)

    for (i in 1:options$fig.num) {
      output[i] <- sprintf("\\includegraphics{%s}", fig_path(number = i))
    }

    output[i+1] <- "\\end{wrapfigure}"
    output <- c("\\begin{wrapfigure}{R}{0.5\\textwidth}",output)
    if (length(output)>= 3) {
      return(paste(output, collapse = ""))
    }
  }
})
```

```{r, include=FALSE, results = 'hide'}
library(ggplot2)
library(dplyr)
#rmarkdown::render(input = "FinKont_homework.Rmd", output_format = "pdf_document")
```


# Introduction

# Weeks {.tabset}

## Week 1

### Material

  * Brownian motion (Chapter 4.1)
  * Conditional expectation (Appendix B.5) 
  * Filtration (Appendix B.3 and Chapter 4.2)
  * Martingales (Appendix C.1 and Chapter 4.4)
  * Introduction (Chapter 1)
  * Discrete time models (Chapter 2 and 3)

### Theory

This week revolves around the theory of the Brownian motion and martingale processes. Other main topics are the binomial model and an introduction to financial derivatives. Financial derivatives is contingent on the outcome of a stochastic process at some future time $t=T$ and often is a function $\Phi$ of some assets price $S_t$. As such the derivative will give a stochastic payout, at time $t=T$ of the size $X_T=\Phi(S_T)$. Naturally we want to say something about the *fair* price of the derivative in the form of

$$\Pi_t(X_T)=\mathbb{E}\left[\Phi(S_T)\ \vert\ \mathcal{F}_t\right],$$

where $\mathcal{F}_t\subset\mathcal{F}$ is the available information at time $t$. We will by defualt intepret the times $t=0$ as *today* and $t=T$ as *tomorrow*. This indeed require some fundamental understanding of the behaviour of the asset price $S_t$. This lead us over to discussing the process in center of the *Black-Scholes* model: the Brownian motion.

&nbsp;

#### The Brownian motion

<blockquote class = "def">
**Definition 4.1.** *(Brownian motion)* A stochastic process $W$ is called a **Brownian motion** or **Wiener process** if the following conditions hold

 1. $W_0=0$.
 2. The process $W$ has independent increments, i.e. if $r<s\le t< u$ then $W_u-W_t$ and $W_s-W_r$ are independent random variables.
 3. For $s<t$ the random variable $W_t-W_s$ has the Gaussian distribution $\mathcal{N}(0,t-s)$.
 4. $W$ has continuous trajectories i.e. $s\mapsto W(s;\omega)$ i continuous for all $\omega \in\Omega$.
</blockquote>

```{r}
#Example of trajectory for BM
set.seed(1)
t <- 0:1000
N <- rnorm(
  n = length(t)-1, #initial value = 0
  mean = 0, #incements mean = 0
  sd = sqrt(t[2:length(t)] - t[1:(length(t)-1)]) #increment sd = sqrt(t-s)
)
W <- c(0,cumsum(N))
```
```{r,echo=FALSE,include=TRUE}
data.frame(t = t, W = W) %>%
  ggplot() +
  geom_line(aes(x=t,y=W)) +
  labs(title = "Realisation of a Brownian motion") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
        title = element_text(size = 18))
```

As one can see from the simulated sample path on the right, the Brownian motion is rather irratic. In fact, the process varies infinitely on any interval with length greater than 0. This gives some of the characteristics of the process including that: $W$ is continuous and $W$ is non-differential everywhere. This irratic behaviour is summed up in the theorem.


<blockquote class = "thm">

**Theorem 4.2.** A Brownian motions trajectory $t\mapsto W_t$ is with probability one nowhere differential, and it has locally infinite total variation.

</blockquote>

This may seem not that horrifying since we can observe the process at any time and conclude an increment $W_{t+\Delta t}-W_t$ for any $\Delta t>0$ but any integral constructed with $W_t$ as integrator becomes nonsensible. We will be studying processes on the form

$$S_{t+\Delta t}-S_t=\mu(t,S_t)\Delta t+\sigma(t,S_t)\Delta W_t,\hspace{20pt}\Delta W_t=W_{t+\Delta t}-W_t.$$

where $W_t$ is a standard Brownian motion and $\mu(t,S_t)$ is locally deterministic (velocity), that is $\mu(t,S_t)$ is deterministic on a small time interval. One could consider the dynamics of the process $S_t$ by studying the equation below as $\Delta t\to 0_+$

$$\frac{S_{t+\Delta t}-S_t}{\Delta t}=\mu(t,S_t)+\sigma(t,S_t)\frac{W_{t+\Delta t}-W_t}{\Delta t}.$$

The limit however is impossible to determine as $W_t$ is non-differentiable and as such $dS_t$ is not well-defined. From LivStok we know that the dynamics of $S_t$ is given by letting $\Delta t$ tend to 0 without dividing by it, that is

$$
dS_t=\mu(t,S_t)\ dt+\sigma(t,S_t)\ dW_t.
$$

Giving that $S_0$ is observable we could intepret the dynamics on the integral form

$$
S_t=S_0+\int_0^t\mu(s,S_s)\ ds+\int_0^t\sigma(s,S_s)\ dW_s,
$$

where the above integrals is Riemann-Stieltjes integral. This is however still at dead-end, since from theorem 4.2 we know that $W_t$ has unbounded variation on any interval. So **we cannot define $S_t$ for each $W$-trajectory seperately** we will despite this define another integral (the Ito integral) that in some other sense give a global solution to this integral. To this we will be considering a $L^2$-definition.

&nbsp;

#### Conditional expectation

The theory of conditional expectation is well-known from courses on the bachelor. Because of this we will only summarise the most important results.

We consider a background space $(\Omega,\mathcal{F},P)$ and a sub-sigma algebra $\mathcal{G}\subseteq \mathcal{F}$. We assume that some stochastic variable is $\mathcal{F}$-measurable, that is the mapping $X : (\Omega,\mathcal{F},P) \to (\mathbb{R},\mathbb{B},m)$ is $\mathcal{F}-\mathbb{B}$-measurable i.e. $\forall B\in\mathbb{B} : \{X\in B\}\in\mathcal{F}$. For some random variable $Z$ defined on the subspace $(\Omega,\mathcal{G},P)$, we say that $Z$ is the conditional expectation of $X$ given $\mathcal{G}$ if

$$
\forall G\in\mathcal{G} : \int_G Z(\omega)\ dP(\omega)=\int_G X(\omega)\ dP(\omega).
$$

This fact is summed up in the definition below.

<blockquote class = "def">

**Definition B.27.** *(Conditional expectation)* Let $(\Omega,\mathcal{F},P)$ be a probability space and $X$ a random variable in $L^1(\Omega,\mathcal{F},P)$ ($\vert X\vert$ is integrable). Let furthermore $\mathcal{G}$ be a sigma-algebra such that $\mathcal{G}\subseteq \mathcal{F}$. If $Z$ is a random variable with the properties that:

  i. $Z$ is $\mathcal{G}$-measurable.
  ii. For every $G\in\mathcal{G}$ it holds that
  $$\int_G Z(\omega)\ dP(\omega)=\int_G X(\omega)\ dP(\omega).$$

Then we say that $Z$ is the *conditional expectation of $X$ given the sigma-algebra $\mathcal{G}$*. In that case we denote $Z$ by the symbol $E[X\ \vert\ \mathcal{G}]$.

</blockquote>

We see that from the above it always holds that $X$ satisfies (ii). It does not, however, always hold that $X$ is $\mathcal{G}$-measurable. Given this fact it is not trivial that a random variable $E[X\ \vert\ \mathcal{G}]$ even exists. This nontriviality is fortunatly resolved by the Radon-Nikodym theorem.

<blockquote class = "thm">

**Theorem B.28.** *(Existance and uniqueness of Conditional expectation)* Let $(\Omega,\mathcal{F},P)$, $X$ and $\mathcal{G}$ be given as in the definition above. Then the following holds:

  * There will always exist a random variable $Z$ satisfying conditions (i)-(ii) above.
  * The variable $Z$ is unique, i.e. if both $Y$ and $Z$ satisfy (i)-(ii) then $Y=Z$ $P$-a.s.

</blockquote>

This result ensures that we may condition on any sigma-algebra for instance $\mathcal{G}=\sigma(Y)$ in that case we (pure notation) write

$$
E[X\ \vert\ \sigma(Y)]=E[X\ \vert\ Y],\hspace{20pt}\sigma(Y)=\sigma\left(\left\{ Y\in A,\ A\in\mathbb{B}\right\}\right).
$$

In the above $\sigma(Y)$ is simply the smallest sigma-algebra containing all the pre-images of $Y$, that is the smallest sigma-algebra making $Y$ measurable! Giving this foundation there are a few properties conditional expectation have which is rather useful (for instance the tower property).

Below we assume: Let $(\Omega,\mathcal{F},P)$ be a probability space and $X,Y$ be random variables in $L^1(\Omega,\mathcal{F},P)$.

<blockquote class = "prop">

**Proposition B.29.** *(Monotinicity/Linearity of Conditional expectation)* The following holds:

$$X\le Y\ \Rightarrow\ E[X\ \vert\ \mathcal{G}]\le E[Y\ \vert\ \mathcal{G}],\hspace{20pt}P-\text{a.s.}$$
$$E[\alpha X + \beta Y\ \vert\ \mathcal{G}]=\alpha E[X\ \vert\ \mathcal{G}]+ \beta E[Y\ \vert\ \mathcal{G}],\hspace{20pt}\forall \alpha,\beta\in\mathbb{R}.$$

</blockquote>

<blockquote class = "prop">

**Proposition B.30.** *(Tower property)* Assume that it holds that $\mathcal{H}\subseteq\mathcal{G}\subseteq\mathcal{F}$. Then the following hold:

$$E[E[X\vert \mathcal{G}]\vert\mathcal{H}]=E[X\vert \mathcal{H}],$$
$$E[X]=E[E[X\vert \mathcal{G}]].$$

</blockquote>

<blockquote class = "prop">

**Proposition B.31.** Assume $X$ is $\mathcal{G}$ and that both $X,Y$ and $XY$ are in $L^1$ (only assuming $Y$ is $\mathcal{F}$-measurable), then

$$E[X\vert\mathcal{G}]=X,\hspace{20pt}P-\text{a.s.}$$
$$E[XY\vert\mathcal{G}]=XE[Y\vert\mathcal{G}],\hspace{20pt}P-\text{a.s.}$$

</blockquote>

<blockquote class = "prop">

**Proposition B.32.** *(Jensen inequality)* Let $f:\mathbb{R}\to\mathbb{R}$ be a convex (measurable) function and assume $f(X)$ is in $L^1$. Then

$$f(E[X\vert\mathcal{G}])\le E[f(X)\vert\mathcal{G}],\hspace{20pt}P-\text{a.s.}$$

</blockquote>

&nbsp;

#### Filtrations

Let $(\Omega,\mathcal{F},P)$ be a probability space. We define a filtration as an increasing famility of sub-sigma-algebras in the following definition.

<blockquote class = "def">

**Definition B.16.** *(Filtration)* Let $\mathbf{F}=(\mathcal{F}_t)_{t\ge 0}$ be an time indexed family of sub-sigma-algebras such that $F_s\subseteq F_t$ for $s\le t$ and $\mathcal{F}_t\subseteq \mathcal{F}$ for all $t\ge 0$. We may given this filtration define $\mathcal{F}_\infty$ as $\sigma\left(\bigcup_{t\ge 0}\mathcal{F}_t\right)$.

</blockquote>

Filtrations is widely used in stochastic processes, as they allow for the concept of knowledge/information. This is useful when considering mean-values of future states but in an increasing information setting. For this we introduce the term adapted processes.

<blockquote class = "def">

**Definition B.17.** *(Adapted process)* Let $(\mathcal{F}_t)_{t\ge 0}$ be a filtration on the probability space $(\mathcal{F}_t)_{t\ge 0}$. Furthermore, let $(X_t)_{t\ge 0}$ be a stochastic process on the same space. We say that $X_t$ is adapted to the filtration $\mathbf{F}$ if

$$X_t\ \text{ is }\ \mathcal{F}_t-\text{measurable},\hspace{20pt}\forall t\ge 0.$$

</blockquote>

Obviously, we may introduce the **natural filtration** $\mathcal{F}^X_t$ given by the tragetory of the process $X_t$:

$$\mathcal{F}^X_t=\sigma(\{X_s,\ s\le t\}).$$

Indeed, $X_t$ is adapted to this filtration.

&nbsp;

#### Martingales

<blockquote class = "def">

**Definition C.1.** Let $M_t$ be a stochastic process defined on a background space $(\Omega,\mathcal{F},P)$. Let $(\mathcal{F}_t)_{t\ge 0}$ be a filtration. If $M_t$ is adapted to the filtration $\mathcal{F}_t$, $E\vert M_t\vert <\infty$ and

$$E[M_t\vert \mathcal{F}_s]=M_s,\hspace{20pt}P-\text{a.s.}$$

holds for any $t>s$ we say that $M_t$ is a martingale ($\mathbf{F}$-martingale). If the above has $\le$ or $\ge$ we say that $M_t$ is either a **submartingale** or **supermartingale** respectively.

</blockquote>

Naturally, this defintions may easily be extended to discrete models and we have the trivial equality:

$$E[M_t-M_s\ \vert\ \mathcal{F}_s]=0.$$

Martingales is useful, when proofing probalistic statements as the posses tractable properties. A useful technique often include the construction of the martingale

$$M_t=E[X\ \vert\ \mathcal{F}_t].$$

&nbsp;

#### Discrete time models

The study of this course is the **European call** option (and *put* option). This financial derivative is an agreement between two parties where the holder of the option has the right to *"exercise"* the derivative, at a future time $t=T$. Exercising means buying an asset at a certain agreed opon price-strike $K$. In the case of the put-option: the holder has the right (but not obligation) to sell the asset at the strike price $K$. As such the derivative has the payoff

$$\text{Call}\ \text{option:}\hspace{10pt}\Phi(S_T)=(S_T-K)^+,\hspace{20pt}\text{Put}\ \text{option:}\hspace{10pt}\Phi(S_T)=(K-S_T)^+.$$

Our objective is to understand when an arbitrage exist and to find the fair price of these derivative. The strategy in pricing is finding a replicating portfolio with the same payoff as the option (with probability one) and then price the derivative accordingly.

##### Model description

In the one-period model we consider the simplest possible market. We have two distinct times $t=0$ (today) and $t=1$ (tomorrow) and we may buy any portfolio as a mixture of bonds and one stock. We denote the bonds price by $B_t$ and the stocks price by $S_t$ and we assume the following:

$$
B_0=1,\ B_1=1+R,\hspace{20pt}S_0=s,\ S_1=\left\{\begin{matrix}s\cdot u, & with\ probability\ p_u.\\s\cdot d, & with\ probability\ p_d.\end{matrix}\right.
$$

We may introduce $Z$ as the random variable

$$
Z=u\cdot (I)+d\cdot (1-I),
$$

for an bernoulli variable $I$ with succes probability $p_u$. Naturally, we assume $d\le (1+R)\le u$ (this is imperative to ensure no arbitrage as we will see).

##### Portfolios and arbirtage



&nbsp;

### Exercises

**Probability exercises**

Let $(W(t))_{t\ge}$ be a Brownian motion (Bjork, Definition 4.1).

**Exercise 1.** Show that the following processes also are Brownian motions.

 i. $(-W(t))_{t\ge 0}$ (symmetry)
 ii. For any $s\ge 0$, $(W(t+s)-W(s))_{t\ge 0}$ (time-homogeneity).
 iii. For every $c>0$, $(cW(t/c^2))_{t\ge 0}$ (scaling).

<details>
<summary>**Solution (i).**</summary>

By assumption $W$ is a Brownian motion and so it follows that

$$-W_0=-1\cdot0=0$$

Furthermore, for $r<s\le t< u$ it holds that $W_u-W_t$ and $W_s-W_r$ is independent. By seperate transformations the independence property is preserved and $-(W_u-W_t)$ and $-(W_s-W_r)$ is independent. Next, for a normal distributed random variable $N\sim\mathcal{N}(\mu,\sigma^2)$ it holds, that for a scaler $c\in\mathbb{R}$ we have $c N\sim\mathcal{N}(c\mu,c^2\sigma ^2)$. Then obviously;

$$-(W_t)=(-1)W_t\stackrel{d}{=}\mathcal{N}((-1)\cdot0,(-1)^2(t-s))\stackrel{d}{=}\mathcal{N}( 0,t-s).$$

Lastly, let $\omega \in \Omega$ and consider the sample path $s\mapsto (-W_s)(\omega)$. Clearly for two continuous functions $f$ and $g$ it holds that $(g\circ f)$ is continuous. Then with $g(f)=-f$ and $f(t)=W_t(\omega)"/>$ it follows that $(-W_t)=(g\circ W)(t)$ is also continuous.

</details>
<details>
<summary>**Solution (ii).**</summary>

Much like the previous exercise we define a new process and show the properties hold. Let $s\ge 0$ be chosen arbitrary. Now define $X_t=W(t+s)-W(s)$.

First, we let $t=0$ and see

$$X_0=W(0+s)-W(s)=W(s)-W(s)=0.$$

Secondly, we have that for $r<u$:

$$X_u-X_r=W(u+s)-W(s)-(W(r+s)-W(s))=W(u+s)-W(r+s)\sim \mathcal{N}(0,u+s-(r+s))=\mathcal{N}(0,u-r).$$

and since for $r<u\le k<l$ the translation $r+s<u+s\le k+s<l+s$ still holds and $X_l-X_k=W(l+s)-W(k+s)$ and $X_u-X_r=W(u+s)-W(k+s)$ are independent. Finally since $W_t(\omega)$ is continuous in $t$ hence the translation $W_{t+s}$ is continouos. Adding a constant yields a function that is also continuous, hence $X_t$ is continuous.

</details>
<details>
<summary>**Solution (iii).**</summary>

Let $c>0$ be given. We show that

$$X_t=cW\left(\frac{t}{c^2}\right)$$

is a Brownian motion. We simply show the four properties. Let $t=0$ and notice

$$X_0=cW\left(\frac{0}{c^2}\right)=cW(0)=0.$$

The second property follows from seperate transformation and that for $r<u\le s<t$ we consider

$$X_u-X_r=c\left(W\left(\frac{u}{c^2}\right)-W\left(\frac{r}{c^2}\right)\right)\hspace{20pt}\text{and}\hspace{20pt}X_t-X_s=c\left(W\left(\frac{t}{c^2}\right)-W\left(\frac{s}{c^2}\right)\right)$$

and since $c,r,u,t,s>0$ we have the same order for the scaled version of $r,u,t,s$ and hence we have two independent RV scaled by $c$. Then by seperate transformations the variables is still independent. Next for the third property:

$$X_t-X_s=c\left(W\left(\frac{t}{c^2}\right)-W\left(\frac{s}{c^2}\right)\right)\sim\mathcal{N}\left(c\cdot 0,c^2\left(\frac{t}{c^2}-\frac{s}{c^2}\right)\right)=\mathcal{N}(0,t-s).$$

Where we use the properties of scaling a normal distributed random variable i.e. for $c>0$ and $N\sim\mathcal{N}(\mu,\sigma ^2)$ it follows that $c N\sim\mathcal{N}(c\mu,c^2\sigma ^2)$. Finally, the forth property follows since $g(f)=cf$ is continuous and $h(t)=t/c^2$ is continuous, then for any continuous function $f(s)$ it follows that $(g \circ f\circ h)=g(f(h(t)))$ is continuous.

</details>

&nbsp;

<blockquote class = "prop">
**Proposition B.37.** Let $(\Omega,\mathcal{F},P)$ be a given probability space, let $\mathcal{G}$ be a sub-sigma-algebra of $\mathcal{F}$, and let $X$ be a square integrable random variable.
Consider the problem of minimizing
$$E\left[(X-Z)^2\right]$$
where $Z$ is allowed to vary over the class of all square integrable $\mathcal{G}$ measurable random variables. The optimal solution $\hat{Z}$ is then given by.
$$\hat{Z}=E[X\vert\mathcal{G}].$$
</blockquote>

**Exercise 2.** *(Bjork, exercise B.11.)* Prove proposition B.37 by going along the following lines.

  a. Prove that the "estimation error" $X-E[X\vert\mathcal{G}]$ is orthogonal to $L^2(\Omega,\mathcal{G},P)$ in the sence that for any $Z\in L^2(\Omega,\mathcal{G},P)$ we have
  $$E[Z\cdot(X-E[X\vert\mathcal{G}])]=0$$
  b. Now prove the proposition by writing
  $$X-Z=(X-E[X\vert\mathcal{G}])+(E[X\vert\mathcal{G}]-Z)$$
  and use the result just proved.

<details>
<summary>**Solution (a).**</summary>

Let $X\in L^2(\Omega,\mathcal{F},P)$ be a random variable. Now consider an arbitrary $Z\in L^2(\Omega,\mathcal{G},P)$. Recall that $\mathcal{G}\subset \mathcal{F}$ and so $X$ is also in $Z\in L^2(\Omega,\mathcal{G},P)$, as it is bothe square integrable and $\mathcal{G}$-measurable. Then

$$E\left[Z\cdot(X-E[X\vert\mathcal{G}])\right]=E\left[Z\cdot X\right]-E\left[Z\cdot E[X\vert\mathcal{G}]\right].$$

Then by using the law of total expectation and secondly that $Z$ is $\mathcal{G}$-measurable we have that

$$E\left[Z\cdot X\right]=E\left[E[Z\cdot X\vert\mathcal{G}]\right]=E\left[Z\cdot E[ X\vert\mathcal{G}]\right].$$

Combining the two equations gives the desired result.

</details>
<details>
<summary>**Solution (b).**</summary>

Obviously, we have that

$$X-Z=X-Z+E[X\vert\mathcal{G}]-E[X\vert\mathcal{G}]=(X-E[X\vert\mathcal{G}])+(E[X\vert\mathcal{G}]-Z).$$

Then squaring the terms gives

$$(X-Z)^2=(X-E[X\vert\mathcal{G}])^2+(E[X\vert\mathcal{G}]-Z)^2+2(X-E[X\vert\mathcal{G}])(E[X\vert\mathcal{G}]-Z)$$

Taking expectation on each side and using linearity of the expectation we have that

$$E[(X-Z)^2]=E\left[(X-E[X\vert\mathcal{G}])^2\right]+E\left[(E[X\vert\mathcal{G}]-Z)^2\right]+2E\left[(X-E[X\vert\mathcal{G}])(E[X\vert\mathcal{G}]-Z)\right].$$

We can now use that $E[X\vert\mathcal{G}]-Z$ is $\mathcal{G}$-measurable with the above result on the last term.

$$E[(X-Z)^2]=E\left[(X-E[X\vert\mathcal{G}])^2\right]+E\left[(E[X\vert\mathcal{G}]-Z)^2\right].$$

Now since $X$ is given the term $E\left[(X-E[X\vert\mathcal{G}])^2\right]$ is simply a constant not depending on the choice og $Z$. The optimal choice of $Z$ is then $E[X\vert\mathcal{G}]$ since this minimizes the second term. The statement is then proved.

</details>

&nbsp;

**Exercise 3.** Discuss the following theory/results of Moment generating functions (Laplace transform).

Let $X$ be a random variable with distribution function $F(x)=P(X\le x)$ and $Y$ be a random variable with distribution function $G(y)=P(Y\le y)$.

<blockquote class = "def">
**Definition.** The moment generating function or Laplace transform of $X$ is

$$\psi_X(\lambda)=E\left[e^{\lambda X}\right]=\int_{-\infty}^\infty e^{\lambda x}dF(x)$$

provided the expectation is finite for $\vert\lambda\vert<h$ for some $h>0$.
</blockquote>

The MGF uniquely determine the distribution of a random variable, due to the following result.

<blockquote class = "thm">
**Theorem 1.** *(Uniqueness)* If $\psi_X(\lambda)=\psi_Y(\lambda)$ when $\vert\lambda\vert<h$ for some $h>0$, then $X$ and $Y$ has the same distribution, that is, $F=G$.
</blockquote>

There is also the following result of independence for Moment generating functions.

<blockquote class = "thm">
**Theorem 1.** *(Independence)* If 

$$E\left[e^{\lambda_1X+\lambda_2Y}\right]=\psi_X(\lambda_1)\psi_Y(\lambda_2)$$

for $\vert\lambda_i\vert<h$ for $i=1,2$ for some $h>0$, then $X$ and $Y$ are independent random variables.
</blockquote>

**Example.** Recall that the Moment generating function of a normal (Gaussian) distribution is given by

$$\psi_X(\lambda)=E\left[e^{\lambda X}\right]=\exp\left(\lambda \mu + \frac{\lambda^2}{2}\sigma^2\right)$$

where $X$ is normally distributed with mean $\mu$ and variance $\sigma^2$ and $\lambda\in\mathbb{R}$ is a constant. Since a Brownian motion $W(t)$ is normally distributed with zero mean and variance $t$, we have that

$$E[\exp(\lambda W(t))]=\exp\left(\frac{\lambda^2}{2}t\right).$$

<details>
<summary>**Discussion.**</summary>



</details>

&nbsp;

**Exercise 4.** *(Bjork, exercise C.8.(a-c))* Let $W$ be a Brownian motion. Notice that for the natural filtration $\mathcal{F}_s=\sigma(W_t\vert t\le s)$ $W_t-W_s$ is independent of $\mathcal{F}_s$

  a. Show that $W_t$ is a martingale.
  b. Show that $W^2_t-t$ is a martingale.
  c. Show that $\exp(\lambda W_t-\frac{\lambda^2}{2}t)$ is a martingale.

<details>
<summary>**Solution (a).**</summary>

We show that for the natural filtration that $W_t$ is a martingale. This include showing integrability and the martingale property. For the first we note that for a normal distributed random variable with mean 0 we have

$$E[\vert N\vert]=\int_{-\infty}^\infty \vert x\vert dF_N(x)=2\int_{0}^\infty xdF_N(x)$$

since the distribution is symmetric. Substituting the distribution function $\Phi(x)=P(N\le x)$ in we see that

$$E[\vert N\vert]=2\int_{0}^\infty xd\Phi(x)=2\int_{0}^\infty x\frac{1}{\sqrt{2\pi\sigma^2}}e^{-x^2/(2\sigma^2)}dx=(*)$$

by substituting $u=x^2/(2\sigma^2)$ ($x=\sqrt{2\sigma^2u}$) we have that

$$\frac{dx}{du}=\frac{1}{2}\sqrt{2\sigma^2u}2\sigma^2=(\sigma^2)^{3/2}\sqrt{2}u\iff dx=(\sigma^2)^{3/2}\sqrt{2}u\ du$$

hence

$$(*)=\frac{2}{\sqrt{2\pi\sigma^2}}\int_0^{\infty}\sqrt{2\sigma^2u}e^{-u}(\sigma^2)^{3/2}\sqrt{2}u\ du=\frac{2\sqrt{2\sigma^2}(\sigma^2)^{3/2}\sqrt{2}}{\sqrt{2\pi\sigma^2}}\int_0^{\infty}\sqrt{u}e^{-u}u\ du.$$

This then simplify to

$$(*)=\frac{(2\sigma^2)^{3/2}}{\sqrt{\pi}}\int_0^{\infty}u^{3/2}e^{-u}\ du=(2\sigma^2)^{1/2}\sqrt{\frac{2\sigma^2}{\pi}}\int_0^{\infty}u^{3/2}e^{-u}\ du=\sqrt{\frac{2\sigma^2}{\pi}}<\infty.$$

(Obviously the above is not derived correctly, but the end expression is valid, source: [link](https://arxiv.org/pdf/1402.3559.pdf)) However since

$$W_t=W_t-0=W_t-W_0\sim\mathcal{N}(0,t)$$

we have that $E\vert W_t\vert<\infty$ as desired.

Next, we have that

$$E[W_t\vert \mathcal{F}_s]=E[W_t-W_s\vert\mathcal{F}_s]+W_s=0+W_s=W_s.$$

In the above we used that $W_t-W_s$ is $\mathcal{F}_s$-measurable with mean 0. Then it follows that $W_t$ is a martingale.

</details>
<details>
<summary>**Solution (b).**</summary>

Let $M_t=W_t^2-t$. First, we observe that two measurable functions composed is still a measurable function. Hence we know that $M_t$ is measurable wrt. the filtration since $W_t$ is measurable and $w\mapsto w^2+t$ is measurable. Secondly, we have that

$$E[\vert W_t^2-t\vert]\le E\vert W_t^2\vert +E\vert t\vert=t+t=2t<\infty$$

where we use the triangle inequality. Thirdly, for the martingale property we have that for $t>s$:

$$E[M_t\vert \mathcal{F}_s]=E[W_t^2-t\vert \mathcal{F}_s]=E[W_t^2+W_s^2-2W_tW_s-W_s^2+2W_tW_s-t\vert \mathcal{F}_s]$$

which by linearity and independence of increments to the filtration gives

$$E[M_t\vert \mathcal{F}_s]=E[(W_t-W_s)^2-W_s^2+2W_tW_s-t\vert \mathcal{F}_s]=t-s-t+E[2W_tW_s-W_s^2\vert \mathcal{F}_s]$$

However since $W_s$ is measurable wrt. the filtration at time $s$ the above is

$$E[M_t\vert \mathcal{F}_s]=2W_sE[W_t\vert \mathcal{F}_s]-W_s^2-s=2W_s^2-W_s^2-s=W_s^2-s=M_s.$$

Since from (a) we know that $W_t$ is a martingale. Then we arrive at the desired result.

</details>
<details>
<summary>**Solution (c).**</summary>

Let $M_t=\exp\left(\lambda W_t-\frac{\lambda^2}{2}t\right)$. First, by composition of measurable functions $M_t$ is $\mathcal{F}_t$-measurable. Secondly, we have using the MGF for a normal distributed random variable:

$$E\vert M_t=E\left(\exp\left(\lambda W_t-\frac{\lambda^2}{2}t\right)\right)\le E\left(\exp\left(\lambda W_t\right)\right)=\exp\left(\frac{\lambda^2}{2}t\right)<\infty.$$

Thirdly, we consider

$$E[M_t\vert\mathcal{F}_s]=E\left.\left[\left(\exp\left(\lambda W_t-\frac{\lambda^2}{2}t\right)\right)\right\vert\mathcal{F}_s\right]=\exp\left(-\frac{\lambda^2}{2}t\right)E\left.\left[\left(\exp\left(\lambda W_t\right)\right)\right\vert\mathcal{F}_s\right].$$

By adding and subtracting $W_s$ in the exponent we get

\begin{align*}
E[M_t\vert\mathcal{F}_s]&=\exp\left(-\frac{\lambda^2}{2}t\right)E\left.\left[\left(\exp\left(\lambda (W_t-W_s)+\lambda W_s\right)\right)\right\vert\mathcal{F}_s\right]\\
&=\exp\left(-\frac{\lambda^2}{2}t\right)\exp\left(\frac{\lambda^2}{2}(t-s)\right)E\left.\left[\left(\exp\left(\lambda W_s\right)\right)\right\vert\mathcal{F}_s\right].
\end{align*}

Using that $E\left.\left[\left(\exp\left(\lambda W_s\right)\right)\right\vert\mathcal{F}_s\right]=\exp\left(\lambda W_s\right)$ and combining the exponents gives the desired:

$$E[M_t\vert\mathcal{F}_s]=\exp\left(\lambda W_s-\frac{\lambda^2}{2}s\right)=M_s.$$

</details>

## Week 2 

### Material

  * Stochastic integrals and Ito formula (Chapter 4 and Appendix C.2)
  * Stochastic differential equations (Chapter 5.1-4)

### Theory

### Exercises

## Week 3

### Material

  * Partial differential equations (Chapter 5.5)
  * Self-financing portfolios (Chapter 6)
  * Black-Scholes PDE (classic approach) and risk neutral valuation (Chapter 7.1-5)

### Theory

### Exercises

## Week 4

### Material

  * Black-Scholes formula (Chapter 7.6, see also Remark to Black-Scholes formula)
  * Completeness and hedging (Chapter 8)
  * Put-call parity (Chapter 10.1)
  * The Greeks (Chapter 10.2)
  * Risk neutral valuation formula (Chapter 11.6) 
  * Equivalent probability measures (Appendix A.11, B.6 and C.3) 

### Theory

### Exercises

## Week 5

### Material

  * Girsanov theorem (Chapter 12, see also Levy characterization of Brownian motion and proof of Girsanov)
  * Martingale representation theorem (Chapter 12)

### Theory

### Exercises

## Week 6

### Material

  * Black-Scholes model, martingale approach (Chapter 13)
  * Multidimensional models (Chapter 14)

### Theory

### Exercises

## Week 7

### Material

  * Pricing and proof of fundamental pricing theorem I and II (Chapter 11)
  * Incomplete Markets (Chapter 9)

### Theory

### Exercises
