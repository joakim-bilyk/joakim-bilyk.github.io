---
title: "Homework (FinKont)"
output:
  html_document:
    toc: no
    toc_float: yes
    code_folding: hide
---

```{r, include=FALSE, results = 'hide'}
library(ggplot2)
library(dplyr)
#rmarkdown::render(input = "FinKont_homework.Rmd", output_format = "pdf_document")
```


# Introduction

# Weeks {.tabset}

## Week 1

### Material

  * Brownian motion (Chapter 4.1)
  * Conditional expectation (Appendix B.5) 
  * Filtration (Appendix B.3 and Chapter 4.2)
  * Martingales (Appendix C.1 and Chapter 4.4)
  * Introduction (Chapter 1)
  * Discrete time models (Chapter 2 and 3)

### Theory

<blockquote class = "def">
**Definition 4.1.** *(Brownian motion)* A stochastic process $W$ is called a **Brownian motion** or **Wiener process** if the following conditions hold

 1. $W_0=0$.
 2. The process $W$ has independent increments, i.e. if $r<s\le t< u$ then $W_u-W_t$ and $W_s-W_r$ are independent random variables.
 3. For $s<t$ the random variable $W_t-W_s$ has the Gaussian distribution $\mathcal{N}(0,t-s)$.
 4. $W$ has continuous trajectories i.e. $s\mapsto W(s;\omega)$ i continuous for all $\omega \in\Omega$.
</blockquote>

```{r}
#Example of trajectory for BM
set.seed(1)
t <- 0:1000
N <- rnorm(
  n = length(t)-1, #initial value = 0
  mean = 0, #incements mean = 0
  sd = sqrt(t[2:length(t)] - t[1:(length(t)-1)]) #increment sd = sqrt(t-s)
)
W <- c(0,cumsum(N))
```
```{r,echo=FALSE,out.width= "50%", out.extra='style="float:right; padding:10px"',include=TRUE}
data.frame(t = t, W = W) %>%
  ggplot() +
  geom_line(aes(x=t,y=W)) +
  labs(title = "Realisation of a Brownian motion") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
        title = element_text(size = 18))
```

<blockquote class = "def">

**Definition C.1.** Let $M_t$ be a stochastic process defined on a background space $(\Omega,\mathcal{F},P)$. Let $(\mathcal{F}_t)_{t\ge 0}$ be a filtration. If $M_t$ is adapted to the filtration $\mathcal{F}_t$, $E\vert M_t\vert <\infty$ and

$$E[M_t\vert \mathcal{F}_s]=M_s$$

holds for any $t>s$ we say that $M_t$ is a martingale.

</blockquote>

### Exercises

**Probability exercises**

Let $(W(t))_{t\ge}$ be a Brownian motion (Bjork, Definition 4.1).

**Exercise 1.** Show that the following processes also are Brownian motions.

 i. $(-W(t))_{t\ge 0}$ (symmetry)
 ii. For any $s\ge 0$, $(W(t+s)-W(s))_{t\ge 0}$ (time-homogeneity).
 iii. For every $c>0$, $(cW(t/c^2))_{t\ge 0}$ (scaling).

<details>
<summary>**Solution (i).**</summary>

By assumption $W$ is a Brownian motion and so it follows that

$$-W_0=-1\cdot0=0$$

Furthermore, for $r<s\le t< u$ it holds that $W_u-W_t$ and $W_s-W_r$ is independent. By seperate transformations the independence property is preserved and $-(W_u-W_t)$ and $-(W_s-W_r)$ is independent. Next, for a normal distributed random variable $N\sim\mathcal{N}(\mu,\sigma^2)$ it holds, that for a scaler $c\in\mathbb{R}$ we have $c N\sim\mathcal{N}(c\mu,c^2\sigma ^2)$. Then obviously;

$$-(W_t)=(-1)W_t\stackrel{d}{=}\mathcal{N}((-1)\cdot0,(-1)^2(t-s))\stackrel{d}{=}\mathcal{N}( 0,t-s).$$

Lastly, let $\omega \in \Omega$ and consider the sample path $s\mapsto (-W_s)(\omega)$. Clearly for two continuous functions $f$ and $g$ it holds that $(g\circ f)$ is continuous. Then with $g(f)=-f$ and $f(t)=W_t(\omega)"/>$ it follows that $(-W_t)=(g\circ W)(t)$ is also continuous.

</details>
<details>
<summary>**Solution (ii).**</summary>

Much like the previous exercise we define a new process and show the properties hold. Let $s\ge 0$ be chosen arbitrary. Now define $X_t=W(t+s)-W(s)$.

First, we let $t=0$ and see

$$X_0=W(0+s)-W(s)=W(s)-W(s)=0.$$

Secondly, we have that for $r<u$:

$$X_u-X_r=W(u+s)-W(s)-(W(r+s)-W(s))=W(u+s)-W(r+s)\sim \mathcal{N}(0,u+s-(r+s))=\mathcal{N}(0,u-r).$$

and since for $r<u\le k<l$ the translation $r+s<u+s\le k+s<l+s$ still holds and $X_l-X_k=W(l+s)-W(k+s)$ and $X_u-X_r=W(u+s)-W(k+s)$ are independent. Finally since $W_t(\omega)$ is continuous in $t$ hence the translation $W_{t+s}$ is continouos. Adding a constant yields a function that is also continuous, hence $X_t$ is continuous.

</details>
<details>
<summary>**Solution (iii).**</summary>

Let $c>0$ be given. We show that

$$X_t=cW\left(\frac{t}{c^2}\right)$$

is a Brownian motion. We simply show the four properties. Let $t=0$ and notice

$$X_0=cW\left(\frac{0}{c^2}\right)=cW(0)=0.$$

The second property follows from seperate transformation and that for $r<u\le s<t$ we consider

$$X_u-X_r=c\left(W\left(\frac{u}{c^2}\right)-W\left(\frac{r}{c^2}\right)\right)\hspace{20pt}\text{and}\hspace{20pt}X_t-X_s=c\left(W\left(\frac{t}{c^2}\right)-W\left(\frac{s}{c^2}\right)\right)$$

and since $c,r,u,t,s>0$ we have the same order for the scaled version of $r,u,t,s$ and hence we have two independent RV scaled by $c$. Then by seperate transformations the variables is still independent. Next for the third property:

$$X_t-X_s=c\left(W\left(\frac{t}{c^2}\right)-W\left(\frac{s}{c^2}\right)\right)\sim\mathcal{N}\left(c\cdot 0,c^2\left(\frac{t}{c^2}-\frac{s}{c^2}\right)\right)=\mathcal{N}(0,t-s).$$

Where we use the properties of scaling a normal distributed random variable i.e. for $c>0$ and $ N\sim\mathcal{N}(\mu,\sigma ^2)$ it follows that $c N\sim\mathcal{N}(c\mu,c^2\sigma ^2)$. Finally, the forth property follows since $g(f)=cf$ is continuous and $h(t)=t/c^2$ is continuous, then for any continuous function $f(s)$ it follows that $(g \circ f\circ h)=g(f(h(t)))$ is continuous.

</details>

&nbsp;

<blockquote class = "prop">
**Proposition B.37.** Let $(\Omega,\mathcal{F},P)$ be a given probability space, let $\mathcal{G}$ be a sub-sigma-algebra of $\mathcal{F}$, and let $X$ be a square integrable random variable.
Consider the problem of minimizing
$$E\left[(X-Z)^2\right]$$
where $Z$ is allowed to vary over the class of all square integrable $\mathcal{G}$ measurable random variables. The optimal solution $\hat{Z}$ is then given by.
$$\hat{Z}=E[X\vert\mathcal{G}].$$
</blockquote>

**Exercise 2.** *(Bjork, exercise B.11.)* Prove proposition B.37 by going along the following lines.

  a. Prove that the "estimation error" $X-E[X\vert\mathcal{G}]$ is orthogonal to $L^2(\Omega,\mathcal{G},P)$ in the sence that for any $Z\in L^2(\Omega,\mathcal{G},P)$ we have
  $$E[Z\cdot(X-E[X\vert\mathcal{G}])]=0$$
  b. Now prove the proposition by writing
  $$X-Z=(X-E[X\vert\mathcal{G}])+(E[X\vert\mathcal{G}]-Z)$$
  and use the result just proved.

<details>
<summary>**Solution (a).**</summary>

Let $X\in L^2(\Omega,\mathcal{F},P)$ be a random variable. Now consider an arbitrary $Z\in L^2(\Omega,\mathcal{G},P)$. Recall that $ \mathcal{G}\subset \mathcal{F}$ and so $X$ is also in $Z\in L^2(\Omega,\mathcal{G},P)$, as it is bothe square integrable and $\mathcal{G}$-measurable. Then

$$E\left[Z\cdot(X-E[X\vert\mathcal{G}])\right]=E\left[Z\cdot X\right]-E\left[Z\cdot E[X\vert\mathcal{G}]\right].$$

Then by using the law of total expectation and secondly that $Z$ is $\mathcal{G}$-measurable we have that

$$E\left[Z\cdot X\right]=E\left[E[Z\cdot X\vert\mathcal{G}]\right]=E\left[Z\cdot E[ X\vert\mathcal{G}]\right].$$

Combining the two equations gives the desired result.

</details>
<details>
<summary>**Solution (b).**</summary>

Obviously, we have that

$$X-Z=X-Z+E[X\vert\mathcal{G}]-E[X\vert\mathcal{G}]=(X-E[X\vert\mathcal{G}])+(E[X\vert\mathcal{G}]-Z).$$

Then squaring the terms gives

$$(X-Z)^2=(X-E[X\vert\mathcal{G}])^2+(E[X\vert\mathcal{G}]-Z)^2+2(X-E[X\vert\mathcal{G}])(E[X\vert\mathcal{G}]-Z)$$

Taking expectation on each side and using linearity of the expectation we have that

$$E[(X-Z)^2]=E\left[(X-E[X\vert\mathcal{G}])^2\right]+E\left[(E[X\vert\mathcal{G}]-Z)^2\right]+2E\left[(X-E[X\vert\mathcal{G}])(E[X\vert\mathcal{G}]-Z)\right].$$

We can now use that $E[X\vert\mathcal{G}]-Z$ is $\mathcal{G}$-measurable with the above result on the last term.

$$E[(X-Z)^2]=E\left[(X-E[X\vert\mathcal{G}])^2\right]+E\left[(E[X\vert\mathcal{G}]-Z)^2\right].$$

Now since $X$ is given the term $E\left[(X-E[X\vert\mathcal{G}])^2\right]$ is simply a constant not depending on the choice og $Z$. The optimal choice of $Z$ is then $E[X\vert\mathcal{G}]$ since this minimizes the second term. The statement is then proved.

</details>

&nbsp;

**Exercise 3.** Discuss the following theory/results of Moment generating functions (Laplace transform).

Let $X$ be a random variable with distribution function $F(x)=P(X\le x)$ and $Y$ be a random variable with distribution function $G(y)=P(Y\le y)$.

<blockquote class = "def">
**Definition.** The moment generating function or Laplace transform of $X$ is

$$\psi_X(\lambda)=E\left[e^{\lambda X}\right]=\int_{-\infty}^\infty e^{\lambda x}dF(x)$$

provided the expectation is finite for $\vert\lambda\vert<h$ for some $h>0$.
</blockquote>

The MGF uniquely determine the distribution of a random variable, due to the following result.

<blockquote class = "thm">
**Theorem 1.** *(Uniqueness)* If $\psi_X(\lambda)=\psi_Y(\lambda)$ when $\vert\lambda\vert<h$ for some $h>0$, then $X$ and $Y$ has the same distribution, that is, $F=G$.
</blockquote>

There is also the following result of independence for Moment generating functions.

<blockquote class = "thm">
**Theorem 1.** *(Independence)* If 

$$E\left[e^{\lambda_1X+\lambda_2Y}\right]=\psi_X(\lambda_1)\psi_Y(\lambda_2)$$

for $\vert\lambda_i\vert<h$ for $i=1,2$ for some $h>0$, then $X$ and $Y$ are independent random variables.
</blockquote>

**Example.** Recall that the Moment generating function of a normal (Gaussian) distribution is given by

$$\psi_X(\lambda)=E\left[e^{\lambda X}\right]=\exp\left(\lambda \mu + \frac{\lambda^2}{2}\sigma^2\right)$$

where $X$ is normally distributed with mean $\mu$ and variance $\sigma^2$ and $\lambda\in\mathbb{R}$ is a constant. Since a Brownian motion $W(t)$ is normally distributed with zero mean and variance $t$, we have that

$$E[\exp(\lambda W(t))]=\exp\left(\frac{\lambda^2}{2}t\right).$$

<details>
<summary>**Discussion.**</summary>



</details>

&nbsp;

**Exercise 4.** *(Bjork, exercise C.8.(a-c))* Let $W$ be a Brownian motion. Notice that for the natural filtration $\mathcal{F}_s=\sigma(W_t\vert t\le s)$ $W_t-W_s$ is independent of $\mathcal{F}_s$

  a. Show that $W_t$ is a martingale.
  b. Show that $W^2_t-t$ is a martingale.
  c. Show that $\exp(\lambda W_t-\frac{\lambda^2}{2}t)$ is a martingale.

<details>
<summary>**Solution (a).**</summary>

We show that for the natural filtration that $W_t$ is a martingale. This include showing integrability and the martingale property. For the first we note that for a normal distributed random variable with mean 0 we have

$$E[\vert N\vert]=\int_{-\infty}^\infty \vert x\vert dF_N(x)=2\int_{0}^\infty xdF_N(x)$$

since the distribution is symmetric. Substituting the distribution function $\Phi(x)=P(N\le x)$ in we see that

$$E[\vert N\vert]=2\int_{0}^\infty xd\Phi(x)=2\int_{0}^\infty x\frac{1}{\sqrt{2\pi\sigma^2}}e^{-x^2/(2\sigma^2)}dx=(*)$$

by substituting $u=x^2/(2\sigma^2)$ ($x=\sqrt{2\sigma^2u}$) we have that

$$\frac{dx}{du}=\frac{1}{2}\sqrt{2\sigma^2u}2\sigma^2=(\sigma^2)^{3/2}\sqrt{2}u\iff dx=(\sigma^2)^{3/2}\sqrt{2}u\ du$$

hence

$$(*)=\frac{2}{\sqrt{2\pi\sigma^2}}\int_0^{\infty}\sqrt{2\sigma^2u}e^{-u}(\sigma^2)^{3/2}\sqrt{2}u\ du=\frac{2\sqrt{2\sigma^2}(\sigma^2)^{3/2}\sqrt{2}}{\sqrt{2\pi\sigma^2}}\int_0^{\infty}\sqrt{u}e^{-u}u\ du.$$

This then simplify to

$$(*)=\frac{(2\sigma^2)^{3/2}}{\sqrt{\pi}}\int_0^{\infty}u^{3/2}e^{-u}\ du=(2\sigma^2)^{1/2}\sqrt{\frac{2\sigma^2}{\pi}}\int_0^{\infty}u^{3/2}e^{-u}\ du=\sqrt{\frac{2\sigma^2}{\pi}}<\infty.$$

(Obviously the above is not derived correctly, but the end expression is valid, source: [link](https://arxiv.org/pdf/1402.3559.pdf)) However since

$$W_t=W_t-0=W_t-W_0\sim\mathcal{N}(0,t)$$

we have that $E\vert W_t\vert<\infty$ as desired.

Next, we have that

$$E[W_t\vert \mathcal{F}_s]=E[W_t-W_s\vert\mathcal{F}_s]+W_s=0+W_s=W_s.$$

In the above we used that $W_t-W_s$ is $\mathcal{F}_s$-measurable with mean 0. Then it follows that $W_t$ is a martingale.

</details>
<details>
<summary>**Solution (b).**</summary>

Let $M_t=W_t^2-t$. First, we observe that two measurable functions composed is still a measurable function. Hence we know that $M_t$ is measurable wrt. the filtration since $W_t$ is measurable and $w\mapsto w^2+t$ is measurable. Secondly, we have that

$$E[\vert W_t^2-t\vert]\le E\vert W_t^2\vert +E\vert t\vert=t+t=2t<\infty$$

where we use the triangle inequality. Thirdly, for the martingale property we have that for $t>s$:

$$E[M_t\vert \mathcal{F}_s]=E[W_t^2-t\vert \mathcal{F}_s]=E[W_t^2+W_s^2-2W_tW_s-W_s^2+2W_tW_s-t\vert \mathcal{F}_s]$$

which by linearity and independence of increments to the filtration gives

$$E[M_t\vert \mathcal{F}_s]=E[(W_t-W_s)^2-W_s^2+2W_tW_s-t\vert \mathcal{F}_s]=t-s-t+E[2W_tW_s-W_s^2\vert \mathcal{F}_s]$$

However since $W_s$ is measurable wrt. the filtration at time $s$ the above is

$$E[M_t\vert \mathcal{F}_s]=2W_sE[W_t\vert \mathcal{F}_s]-W_s^2-s=2W_s^2-W_s^2-s=W_s^2-s=M_s.$$

Since from (a) we know that $W_t$ is a martingale. Then we arrive at the desired result.

</details>
<details>
<summary>**Solution (c).**</summary>

Let $M_t=\exp\left(\lambda W_t-\frac{\lambda^2}{2}t\right)$. First, by composition of measurable functions $M_t$ is $\mathcal{F}_t$-measurable. Secondly, we have using the MGF for a normal distributed random variable:

$$E\vert M_t=E\left(\exp\left(\lambda W_t-\frac{\lambda^2}{2}t\right)\right)\le E\left(\exp\left(\lambda W_t\right)\right)=\exp\left(\frac{\lambda^2}{2}t\right)<\infty.$$

Thirdly, we consider

$$E[M_t\vert\mathcal{F}_s]=E\left.\left[\left(\exp\left(\lambda W_t-\frac{\lambda^2}{2}t\right)\right)\right\vert\mathcal{F}_s\right]=\exp\left(-\frac{\lambda^2}{2}t\right)E\left.\left[\left(\exp\left(\lambda W_t\right)\right)\right\vert\mathcal{F}_s\right].$$

By adding and subtracting $W_s$ in the exponent we get

\begin{align*}
E[M_t\vert\mathcal{F}_s]&=\exp\left(-\frac{\lambda^2}{2}t\right)E\left.\left[\left(\exp\left(\lambda (W_t-W_s)+\lambda W_s\right)\right)\right\vert\mathcal{F}_s\right]\\
&=\exp\left(-\frac{\lambda^2}{2}t\right)\exp\left(\frac{\lambda^2}{2}(t-s)\right)E\left.\left[\left(\exp\left(\lambda W_s\right)\right)\right\vert\mathcal{F}_s\right].
\end{align*}

Using that $E\left.\left[\left(\exp\left(\lambda W_s\right)\right)\right\vert\mathcal{F}_s\right]=\exp\left(\lambda W_s\right)$ and combining the exponents gives the desired:

$$E[M_t\vert\mathcal{F}_s]=\exp\left(\lambda W_s-\frac{\lambda^2}{2}s\right)=M_s.$$

</details>

## Week 2 

### Material

MFE refers to the book by McNeil, Frey, and Embrechts, while HL refers to the notes by Hult and Lindskog posted on Absalon.

 * Var-Cov method, simulation, importance sampling and bootstrap. For the Var-Cov method, see MFE Sec. 9.2. For importance sampling and bootstrap, see the supplementary reading in Absalon.
 * Extreme value theory: MFE Ch. 5 or alternative reading in HL (suggested for this part).

### Theory

### Exercises

## Week 3

### Material

MFE refers to the book by McNeil, Frey, and Embrechts, while HL refers to the notes by Hult and Lindskog posted on Absalon.

 * Spherical and elliptical distributions: MFE Sec. 6.3.
 * Spherical and elliptical distributions, cont.; introduction to copulas.

### Theory

### Exercises

## Week 4

### Material

MFE refers to the book by McNeil, Frey, and Embrechts, while HL refers to the notes by Hult and Lindskog posted on Absalon.

 * Copulas: MFE Sec. 7.1-7.5 (Sklar's theorem, Frechet bounds).
 * Copulas cont. (Transformations; examples; Archimedean copulas).

### Theory

### Exercises

## Week 5

### Material

MFE refers to the book by McNeil, Frey, and Embrechts, while HL refers to the notes by Hult and Lindskog posted on Absalon.

 * Copulas cont. (simulating Archimedean copulas, statistical methods, measures of dependence).
 * Credit risk: the Merton model. MFE Ch. 10, particularly Sec. 10.3. [This lecture will only last two hours.]

### Theory

### Exercises

## Week 6

### Material

MFE refers to the book by McNeil, Frey, and Embrechts, while HL refers to the notes by Hult and Lindskog posted on Absalon.

 * Portfolio credit risk: MFE Ch. 11, specifically Sections 11.1-11.3.
 * Portfolio credit risk, cont. [This lecture will only last two hours.]

### Theory

### Exercises

## Week 7

### Material

MFE refers to the book by McNeil, Frey, and Embrechts, while HL refers to the notes by Hult and Lindskog posted on Absalon.

 * Intro. to operational risk. Stochastic processes in risk management: stochastic models for operational risk; financial time series models. Connections to non-life insurance models and estimates (last lecture).

### Theory

### Exercises
