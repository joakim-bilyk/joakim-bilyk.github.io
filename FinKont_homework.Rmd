---
title: "Homework (FinKont)"
author: "Joakim Bilyk"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{wrapfig}
   - \usepackage{graphics}
output:
  html_document:
    toc: no
    code_folding: hide
  pdf_document:
    toc: yes
---

```{r, setup,include=FALSE}
library(knitr)
if (knitr::is_latex_output()) {
  knit_hooks$set(wrapf = function(before, options, envir) {
  if (!before) {
    output <- vector(mode = "character", length = options$fig.num + 1)

    for (i in 1:options$fig.num) {
      output[i] <- sprintf("\\includegraphics{%s}", fig_path(number = i))
    }

    output[i+1] <- "\\end{wrapfigure}"
    output <- c("\\begin{wrapfigure}{R}{0.5\\textwidth}",output)
    if (length(output)>= 3) {
      return(paste(output, collapse = ""))
    }
  }
})
  knitr::opts_chunk$set(
    fig.show = 'hide',
    echo = FALSE
  )
} else {
  knitr::knit_hooks$set(wrapf = function(before, options, envir) {})
  knitr::opts_chunk$set(
    out.extra = 'style="float:right; padding:10px"',
    out.width= "40%"
  )
}
```

```{r, include=FALSE, results = 'hide'}
library(ggplot2)
library(dplyr)
#rmarkdown::render(input = "FinKont_homework.Rmd", output_format = "pdf_document")
```


# Introduction

# Weeks {.tabset}

## Week 1

### Material

  * Brownian motion (Chapter 4.1)
  * Conditional expectation (Appendix B.5) 
  * Filtration (Appendix B.3 and Chapter 4.2)
  * Martingales (Appendix C.1 and Chapter 4.4)
  * Introduction (Chapter 1)
  * Discrete time models (Chapter 2 and 3)

### Theory

This week revolves around the theory of the Brownian motion and martingale processes. Other main topics are the binomial model and an introduction to financial derivatives. Financial derivatives is contingent on the outcome of a stochastic process at some future time $t=T$ and often is a function $\Phi$ of some assets price $S_t$. As such the derivative will give a stochastic payout, at time $t=T$ of the size $X_T=\Phi(S_T)$. Naturally we want to say something about the *fair* price of the derivative in the form of

$$\Pi_t(X_T)=\mathbb{E}\left[\Phi(S_T)\ \vert\ \mathcal{F}_t\right],$$

where $\mathcal{F}_t\subset\mathcal{F}$ is the available information at time $t$. We will by defualt intepret the times $t=0$ as *today* and $t=T$ as *tomorrow*. This indeed require some fundamental understanding of the behaviour of the asset price $S_t$. This lead us over to discussing the process in center of the *Black-Scholes* model: the Brownian motion.

&nbsp;

#### The Brownian motion

<blockquote class = "def">
**Definition 4.1.** *(Brownian motion)* A stochastic process $W$ is called a **Brownian motion** or **Wiener process** if the following conditions hold

 1. $W_0=0$.
 2. The process $W$ has independent increments, i.e. if $r<s\le t< u$ then $W_u-W_t$ and $W_s-W_r$ are independent random variables.
 3. For $s<t$ the random variable $W_t-W_s$ has the Gaussian distribution $\mathcal{N}(0,t-s)$.
 4. $W$ has continuous trajectories i.e. $s\mapsto W(s;\omega)$ i continuous for all $\omega \in\Omega$.
</blockquote>

```{r}
#Example of trajectory for BM
set.seed(1)
t <- 0:1000
N <- rnorm(
  n = length(t)-1, #initial value = 0
  mean = 0, #incements mean = 0
  sd = sqrt(t[2:length(t)] - t[1:(length(t)-1)]) #increment sd = sqrt(t-s)
)
W <- c(0,cumsum(N))
```
```{r,echo=FALSE,include=TRUE,wrapf = TRUE}
data.frame(t = t, W = W) %>%
  ggplot() +
  geom_line(aes(x=t,y=W)) +
  labs(title = "Realisation of a Brownian motion") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
        title = element_text(size = 18))
```

As one can see from the simulated sample path on the right, the Brownian motion is rather irratic. In fact, the process varies infinitely on any interval with length greater than 0. This gives some of the characteristics of the process including that: $W$ is continuous and $W$ is non-differential everywhere. This irratic behaviour is summed up in the theorem.


<blockquote class = "thm">

**Theorem 4.2.** A Brownian motions trajectory $t\mapsto W_t$ is with probability one nowhere differential, and it has locally infinite total variation.

</blockquote>

This may seem not that horrifying since we can observe the process at any time and conclude an increment $W_{t+\Delta t}-W_t$ for any $\Delta t>0$ but any integral constructed with $W_t$ as integrator becomes nonsensible. We will be studying processes on the form

$$S_{t+\Delta t}-S_t=\mu(t,S_t)\Delta t+\sigma(t,S_t)\Delta W_t,\hspace{20pt}\Delta W_t=W_{t+\Delta t}-W_t.$$

where $W_t$ is a standard Brownian motion and $\mu(t,S_t)$ is locally deterministic (velocity), that is $\mu(t,S_t)$ is deterministic on a small time interval. One could consider the dynamics of the process $S_t$ by studying the equation below as $\Delta t\to 0_+$

$$\frac{S_{t+\Delta t}-S_t}{\Delta t}=\mu(t,S_t)+\sigma(t,S_t)\frac{W_{t+\Delta t}-W_t}{\Delta t}.$$

The limit however is impossible to determine as $W_t$ is non-differentiable and as such $dS_t$ is not well-defined. From LivStok we know that the dynamics of $S_t$ is given by letting $\Delta t$ tend to 0 without dividing by it, that is

$$
dS_t=\mu(t,S_t)\ dt+\sigma(t,S_t)\ dW_t.
$$

Giving that $S_0$ is observable we could intepret the dynamics on the integral form

$$
S_t=S_0+\int_0^t\mu(s,S_s)\ ds+\int_0^t\sigma(s,S_s)\ dW_s,
$$

where the above integrals is Riemann-Stieltjes integral. This is however still at dead-end, since from theorem 4.2 we know that $W_t$ has unbounded variation on any interval. So **we cannot define $S_t$ for each $W$-trajectory seperately** we will despite this define another integral (the Ito integral) that in some other sense give a global solution to this integral. To this we will be considering a $L^2$-definition.

&nbsp;

#### Conditional expectation

The theory of conditional expectation is well-known from courses on the bachelor. Because of this we will only summarise the most important results.

We consider a background space $(\Omega,\mathcal{F},P)$ and a sub-sigma algebra $\mathcal{G}\subseteq \mathcal{F}$. We assume that some stochastic variable is $\mathcal{F}$-measurable, that is the mapping $X : (\Omega,\mathcal{F},P) \to (\mathbb{R},\mathbb{B},m)$ is $\mathcal{F}-\mathbb{B}$-measurable i.e. $\forall B\in\mathbb{B} : \{X\in B\}\in\mathcal{F}$. For some random variable $Z$ defined on the subspace $(\Omega,\mathcal{G},P)$, we say that $Z$ is the conditional expectation of $X$ given $\mathcal{G}$ if

$$
\forall G\in\mathcal{G} : \int_G Z(\omega)\ dP(\omega)=\int_G X(\omega)\ dP(\omega).
$$

This fact is summed up in the definition below.

<blockquote class = "def">

**Definition B.27.** *(Conditional expectation)* Let $(\Omega,\mathcal{F},P)$ be a probability space and $X$ a random variable in $L^1(\Omega,\mathcal{F},P)$ ($\vert X\vert$ is integrable). Let furthermore $\mathcal{G}$ be a sigma-algebra such that $\mathcal{G}\subseteq \mathcal{F}$. If $Z$ is a random variable with the properties that:

  i. $Z$ is $\mathcal{G}$-measurable.
  ii. For every $G\in\mathcal{G}$ it holds that
  $$\int_G Z(\omega)\ dP(\omega)=\int_G X(\omega)\ dP(\omega).$$

Then we say that $Z$ is the *conditional expectation of $X$ given the sigma-algebra $\mathcal{G}$*. In that case we denote $Z$ by the symbol $E[X\ \vert\ \mathcal{G}]$.

</blockquote>

We see that from the above it always holds that $X$ satisfies (ii). It does not, however, always hold that $X$ is $\mathcal{G}$-measurable. Given this fact it is not trivial that a random variable $E[X\ \vert\ \mathcal{G}]$ even exists. This nontriviality is fortunatly resolved by the Radon-Nikodym theorem.

<blockquote class = "thm">

**Theorem B.28.** *(Existance and uniqueness of Conditional expectation)* Let $(\Omega,\mathcal{F},P)$, $X$ and $\mathcal{G}$ be given as in the definition above. Then the following holds:

  * There will always exist a random variable $Z$ satisfying conditions (i)-(ii) above.
  * The variable $Z$ is unique, i.e. if both $Y$ and $Z$ satisfy (i)-(ii) then $Y=Z$ $P$-a.s.

</blockquote>

This result ensures that we may condition on any sigma-algebra for instance $\mathcal{G}=\sigma(Y)$ in that case we (pure notation) write

$$
E[X\ \vert\ \sigma(Y)]=E[X\ \vert\ Y],\hspace{20pt}\sigma(Y)=\sigma\left(\left\{ Y\in A,\ A\in\mathbb{B}\right\}\right).
$$

In the above $\sigma(Y)$ is simply the smallest sigma-algebra containing all the pre-images of $Y$, that is the smallest sigma-algebra making $Y$ measurable! Giving this foundation there are a few properties conditional expectation have which is rather useful (for instance the tower property).

Below we assume: Let $(\Omega,\mathcal{F},P)$ be a probability space and $X,Y$ be random variables in $L^1(\Omega,\mathcal{F},P)$.

<blockquote class = "prop">

**Proposition B.29.** *(Monotinicity/Linearity of Conditional expectation)* The following holds:

$$X\le Y\ \Rightarrow\ E[X\ \vert\ \mathcal{G}]\le E[Y\ \vert\ \mathcal{G}],\hspace{20pt}P-\text{a.s.}$$
$$E[\alpha X + \beta Y\ \vert\ \mathcal{G}]=\alpha E[X\ \vert\ \mathcal{G}]+ \beta E[Y\ \vert\ \mathcal{G}],\hspace{20pt}\forall \alpha,\beta\in\mathbb{R}.$$

</blockquote>

<blockquote class = "prop">

**Proposition B.30.** *(Tower property)* Assume that it holds that $\mathcal{H}\subseteq\mathcal{G}\subseteq\mathcal{F}$. Then the following hold:

$$E[E[X\vert \mathcal{G}]\vert\mathcal{H}]=E[X\vert \mathcal{H}],$$
$$E[X]=E[E[X\vert \mathcal{G}]].$$

</blockquote>

<blockquote class = "prop">

**Proposition B.31.** Assume $X$ is $\mathcal{G}$ and that both $X,Y$ and $XY$ are in $L^1$ (only assuming $Y$ is $\mathcal{F}$-measurable), then

$$E[X\vert\mathcal{G}]=X,\hspace{20pt}P-\text{a.s.}$$
$$E[XY\vert\mathcal{G}]=XE[Y\vert\mathcal{G}],\hspace{20pt}P-\text{a.s.}$$

</blockquote>

<blockquote class = "prop">

**Proposition B.32.** *(Jensen inequality)* Let $f:\mathbb{R}\to\mathbb{R}$ be a convex (measurable) function and assume $f(X)$ is in $L^1$. Then

$$f(E[X\vert\mathcal{G}])\le E[f(X)\vert\mathcal{G}],\hspace{20pt}P-\text{a.s.}$$

</blockquote>

&nbsp;

#### Filtrations

Let $(\Omega,\mathcal{F},P)$ be a probability space. We define a filtration as an increasing famility of sub-sigma-algebras in the following definition.

<blockquote class = "def">

**Definition B.16.** *(Filtration)* Let $\mathbf{F}=(\mathcal{F}_t)_{t\ge 0}$ be an time indexed family of sub-sigma-algebras such that $F_s\subseteq F_t$ for $s\le t$ and $\mathcal{F}_t\subseteq \mathcal{F}$ for all $t\ge 0$. We may given this filtration define $\mathcal{F}_\infty$ as $\sigma\left(\bigcup_{t\ge 0}\mathcal{F}_t\right)$.

</blockquote>

Filtrations is widely used in stochastic processes, as they allow for the concept of knowledge/information. This is useful when considering mean-values of future states but in an increasing information setting. For this we introduce the term adapted processes.

<blockquote class = "def">

**Definition B.17.** *(Adapted process)* Let $(\mathcal{F}_t)_{t\ge 0}$ be a filtration on the probability space $(\mathcal{F}_t)_{t\ge 0}$. Furthermore, let $(X_t)_{t\ge 0}$ be a stochastic process on the same space. We say that $X_t$ is adapted to the filtration $\mathbf{F}$ if

$$X_t\ \text{ is }\ \mathcal{F}_t-\text{measurable},\hspace{20pt}\forall t\ge 0.$$

</blockquote>

Obviously, we may introduce the **natural filtration** $\mathcal{F}^X_t$ given by the tragetory of the process $X_t$:

$$\mathcal{F}^X_t=\sigma(\{X_s,\ s\le t\}).$$

Indeed, $X_t$ is adapted to this filtration.

&nbsp;

#### Martingales

<blockquote class = "def">

**Definition C.1.** Let $M_t$ be a stochastic process defined on a background space $(\Omega,\mathcal{F},P)$. Let $(\mathcal{F}_t)_{t\ge 0}$ be a filtration. If $M_t$ is adapted to the filtration $\mathcal{F}_t$, $E\vert M_t\vert <\infty$ and

$$E[M_t\vert \mathcal{F}_s]=M_s,\hspace{20pt}P-\text{a.s.}$$

holds for any $t>s$ we say that $M_t$ is a martingale ($\mathbf{F}$-martingale). If the above has $\ge$ or $\le$ we say that $M_t$ is either a **submartingale** or **supermartingale** respectively.

</blockquote>

Naturally, this defintions may easily be extended to discrete models and we have the trivial equality:

$$E[M_t-M_s\ \vert\ \mathcal{F}_s]=0.$$

Martingales is useful, when proofing probalistic statements as the posses tractable properties. A useful technique often include the construction of the martingale

$$M_t=E[X\ \vert\ \mathcal{F}_t].$$

&nbsp;

#### Discrete time models

#### One-period time models

The study of this course is the **European call** option (and *put* option). This financial derivative is an agreement between two parties where the holder of the option has the right to *"exercise"* the derivative, at a future time $t=T$. Exercising means buying an asset at a certain agreed opon price-strike $K$. In the case of the put-option: the holder has the right (but not obligation) to sell the asset at the strike price $K$. As such the derivative has the payoff

$$\text{Call}\ \text{option:}\hspace{10pt}\Phi(S_T)=(S_T-K)^+,\hspace{20pt}\text{Put}\ \text{option:}\hspace{10pt}\Phi(S_T)=(K-S_T)^+.$$

Our objective is to understand when an arbitrage exist and to find the fair price of these derivative. The strategy in pricing is finding a replicating portfolio with the same payoff as the option (with probability one) and then price the derivative accordingly.

##### Model description

In the one-period model we consider the simplest possible market. We have two distinct times $t=0$ (today) and $t=1$ (tomorrow) and we may buy any portfolio as a mixture of bonds and one stock. We denote the bonds price by $B_t$ and the stocks price by $S_t$ and we assume the following:

$$
B_0=1,\ B_1=1+R,\hspace{20pt}S_0=s,\ S_1=\left\{\begin{matrix}s\cdot u, & with\ probability\ p_u.\\s\cdot d, & with\ probability\ p_d.\end{matrix}\right.
$$

We may introduce $Z$ as the random variable

$$
Z=u\cdot (I)+d\cdot (1-I),
$$

for an bernoulli variable $I$ with succes probability $p_u$. Naturally, we assume $d\le (1+R)\le u$ (this is imperative to ensure no arbitrage as we will see).

##### Portfolios and arbirtage

We study any portfolio on the $(B,S)$ market as a vector $h=(x,y)$ where $x$ is the amount of bonds and $y$ is the amount of stock held in the portfolio. Notice that we allow for shorting, that is $x<0$ or $y<0$. As such, we have that $h\in \mathbb{R}^2$. In this we have made some unrealistic, but attractable assumptions included in the assumptions:

  * We allow short positions and fractional holding, i.e. $h\in \mathbb{R}^2$,
  * We assume no spread between ask and bids,
  * No transaction costs and
  * A completely liquid market i.e. we may borrow and buy as much stock and bonds as wanted.

Given that we have chosen a portfolio $h$ we may introduce the value process.

<blockquote class = "def">

**Definition 2.1.** The **value process** of the porfolio $h\in\mathbb{R}^2$ is the stochastic process

$$V^h_t=xB_t+yS_t,\ t=0,1.$$

</blockquote>

Given this notation we may define what an arbitrage is.

<blockquote class = "def">

**Definition 2.2.** An **arbitrage** is a portfolio $h$ with the properties: 1) $V^h_0=0$, 2) $P(V^h_1\ge 0)=1$ and 3) $P(V^h_1>0)>0$.

</blockquote>

That is $h$ is an deterministic money-machine where we at least never loose any money. Granted the bonds give a determinictic non-negative return, but an arbitrage does not require any money out of pocket. With the notion of an arbitrage we will show the first proposition regarding the choice of $R,u,d$ as defined above.

<blockquote class = "prop">

**Proposition 2.3.** The one-period binomial model is arbitrage free if and only if the following inequality hold:

$$d\le (1+R)\le u.\tag{2.1}$$

</blockquote>

<details>
<summary>**Proof.**</summary>

The statement is proofed by contradiction. Assume that $d>1+R$ holds. Then by definition $u>d>1+R$. Notice that any portfolio satisfying $V_0^h=0$ must satisfy

$$0=xB_0+yS_0=x+ys\iff x=-ys$$

That is for some choice $y$ the only arbitrage candidate is the portfolio $h=(-ys,y)$. Calculating the value at time $t=1$ we have

$$V_1^h=-ys\cdot(1+R)+y\cdot s\cdot Z=ys(Z-1-R)$$

However since $Z\ge d$ we have $Z-(1+R)\ge 0$ and therefore an arbitrage (for $y>0$). The other inequality $1+R>u$ follows analog steps. Simply choose some $y<0$ and the result follows. $\blacksquare$

</details>

From inequality (2.1) we see that since $1+R$ is between $u$ and $d$ we may find a pair $q_d,q_u\ge 0$ with $q_d+q_u=1$ such that

$$1+R=q_u\cdot u+q_d\cdot d.$$

This yields the important risk neutral valuation formula as summed op in the following definition

<blockquote class = "def">

**Definition 2.4.** A probability measure $Q$ is called a **martingale meausre** if the following condition holds:

$$S_0=\frac{1}{1+R}E^Q[S_1].$$

</blockquote>

The above measure $Q$ is the measure $Q(Z=d)=q_d$ and $Q(Z=u)=q_u$ for the binomial model. This does in fact yield the risk neautral valuation formula:

\begin{align*}
S_0&=\frac{1}{1+R}E^Q[S_1]=\frac{1}{1+R}(Q(Z=d)\cdot d\cdot s+Q(Z=u)\cdot u\cdot s)\\
&=s\frac{1}{1+R}(q_d\cdot d+q_u\cdot u)=s,
\end{align*}

where we simply use $1+R=q_d\cdot d+q_u\cdot u$. We call this the risk neautral valuation formula because it in some sense gives an expected discounted value of the future stock price. We end this endavour with reformulating the arbitrage proposition and determining the values of the $Q$-measure.

<blockquote class = "prop">

**Proposition 2.5.** The one-period binomial model is arbitrage free if and only if there exists a martingale measure $Q$.

</blockquote>

<blockquote class = "prop">

**Proposition 2.6.** The one-period binomial model has martingale probabilities given by:

$$\left\{\begin{matrix}q_u=\frac{(1+R)-d}{u-d},\\ q_u=\frac{u-(1+R)}{u-d}.\end{matrix}\right.$$

</blockquote>

##### Contingent Claims

This chapter revolves around the financial derivative and we start by stating the definition of the financial derivative.

<blockquote class = "def">

**Definition 2.7.** A **contingent claim** (financial derivative) is *any* stochastic variable $X$ of the form $\Phi(Z)$, where $Z$ is the stochastic varible driving the stock price process.

</blockquote>

We may also call the function $\Phi$ the **contract function** as it states how the contract is resolved once the stochastic variable $Z$ has been realised. Our objective is now to study, what a buyer of said contract would have to pay at any given time $t$. We call the fair price of $X$ at time $t$: $\Pi_t[X]$. As such it is easy to see that the fair price at the time of maturity $T$ is simply the payout $X$ i.e. $\Pi_T[X]=X$. Our strategy is to find a replicating portfolio $h$ and determine the price of said portfolio.

<blockquote class = "def">

**Definition 2.8.** A contingent claim $X$ can be **replicated**, or said to be **reachable** if there exist a portfolio $h$ such that

$$
V_1^h=X,
$$

with probability one. In that case, we say that the portfolio $h$ is a **hedging** portfolio or a **replicationg** portfolio. If all claims can be replicated we say that the market is **complete**.

</blockquote>

Our pricing strategy is then to determine the value process of the replicating portfolio and then by the first pricing principle below we say that the price is imply the value of the replicating portfolio.

**Pricing principle 1.** If a clain $X$ is reachable with replicating portfolio $h$, then the only reasonable price process for $X$ is given by

$$
\Pi_t[X]=V_t^h.
$$

Notice, that this assumes that a replicating portfolio exist and even so we have a uniqueness statement to solve. We end this section by writing two important results.

<blockquote class = "prop">

**Proposition 2.9.** Suppose that a claim $X$ is reachable with replicating portfolio $h$. Then any price at time $t\ge 0$ of the claim $X$ other than the value process of $h$ will lead to an arbitrage on the extended market $(B,S,X)$.

</blockquote>

<blockquote class = "prop">

**Proposition 2.10.** If the one-period binomial model is free of arbitrage, then it is also complete.

</blockquote>

The hedging portfolio in the one-period binomial model is given by the portfolio $(x,y)$ below

$$
x=\frac{1}{1+R}\cdot\frac{u\Phi(d)-d\Phi(u)}{u-d},\hspace{20pt}y=\frac{1}{s}\cdot\frac{\Phi(u)-\Phi(d)}{u-d}.
$$

##### Risk Neutral Valuation

We see that since the one-period model is complete we can price any contingent claim and we see that

\begin{align*}
\Pi_0[X]&=\frac{1}{1+R}\cdot\frac{u\Phi(d)-d\Phi(u)}{u-d}+s\frac{1}{s}\cdot\frac{\Phi(u)-\Phi(d)}{u-d}\\
&=\frac{1}{1+R}\left\{\frac{u\Phi(d)-d\Phi(u)}{u-d}+(1+R)\frac{\Phi(u)-\Phi(d)}{u-d}\right\}\\
&=\frac{1}{1+R}\left\{\frac{(1+R)-d}{u-d}\Phi(u)+\frac{u-(1+R)}{u-d}\Phi(d)\right\}\\
&=\frac{1}{1+R}E^Q[X].
\end{align*}

i.e. the price at time $t=0$ should simply be the expected discounted payout according to the martingale measure. This leads to the important pricing proposition:

<blockquote class = "prop">

**Proposition 2.11.** If the one-period binomial model is free of arbitrage, then the arbitrage free price of a contingent claim $X$ is given by

$$
\Pi_0[X]=\frac{1}{1+R}E^Q[X].\tag{2.4}
$$

Here the martingale measure $Q$ is uniquely determined by the relation

$$
S_0=\frac{1}{1+R}E^Q[S_1],\tag{2.5}
$$

and the explicit expressions for $q_u$ and $q_d$ are given in proposition 2.6. Furthermore the claim $X$ can be replicated using the portfolio

\begin{align*}
x&=\frac{1}{1+R}\cdot\frac{u\Phi(d)-d\Phi(u)}{u-d},\tag{2.6}\\
y&=\frac{1}{s}\cdot\frac{\Phi(u)-\Phi(d)}{u-d}.\tag{2.7}
\end{align*}

</blockquote>

#### Multi-period model

The one-period binomial model can easily be extended to a multi-period model, by assuming that the bond and stock pricess evolve by the processes:

$$
t\ge1:\ B_t=(1+R)B_{t-1}\hspace{20pt}\text{and}\hspace{20pt}B_0=1,
$$

$$
t\ge1:\ S_t=Z_{t-1}S_{t-1}\hspace{20pt}\text{and}\hspace{20pt}S_0=s,
$$

where we obviously have that $B_t=(1+R)^t$ for $t\ge 0$. In the above $Z_t$ is $u$ with probability $p_u$ and $d$ with probability $p_d$. In this context, we need to define a portfolio in terms of a strategy.

<blockquote class = "def">

**Definition 2.13.** A **portfolio strategy** is a stochastic process on $\{1,...,T\}$

$$
h=\left\{h_t=(x_t,y_t);\ t=1,...,T\right\}
$$

such that $h_t$ is a function of $S_0,S_1,...,S_{t-1}$. For a given portfolio strategy $h$ we set $h_0=h_1$ by convention. The associated **value process** corresponding to the portfolio $h$ is defined by

$$
V_t^h=x_t(1+R)+y_tS_t.
$$

</blockquote>

Given this notation we may define what an arbitrage is, but first we introduce the notion of a self-financing portfolio. A self-financing portfolio in an intuative sense is a portfolio that is not withdrawn from or deposited into.

<blockquote class = "def">

**Definition 2.14.** A portfolio strategy $h$ is said to be **self-financing** if the following condition holds for all $t=0,...,T-1$:

$$
x_t(1+R)+y_tS_t=x_{t+1}+y_{t+1}S_t.
$$

</blockquote>

The above equation says that the portfolio purchased at time $t$ and helt until $t+1$ $(x_{t+1},y_{t+1})$ can only be financed by the market value of the portfolio held from $[t-1,t)$ i.e. $(x_{t},y_{t})$. We now define an arbitrage.

<blockquote class = "def">

**Definition 2.15.** An **arbitrage** is a self-financing portfolio $h$ with the properties: 1) $V^h_0=0$, 2) $P(V^h_T\ge 0)=1$ and 3) $P(V^h_T>0)>0$.

</blockquote>

The multiperiod binomial model has an just like the oneperiod model a result regarding when an arbitrage exists.

<blockquote class = "lem">

**Lemma 2.16.** If $d\le (1+R)\le u$ then the multiperiod model is arbitrage-free.

</blockquote>

As one can see, the multiperiod model is rather similar to the one period model. We wil in the following summarise equivalent statements for the multiperiod model as the ones in the oneperiod model. 

<blockquote class = "def">

**Definition 2.17.** The martingale probabilities $q_u$ and $q_d$ are defined as the probabilities for which the relation below holds.

$$
s=\frac{1}{1+R}E^Q[S_{t+1}\ \vert\ S_t].
$$

</blockquote>

<blockquote class = "prop">

**Proposition 2.18.** The martingale probabilities $q_u$ and $q_d$ are given by

$$
\left\{\begin{matrix}q_u=\frac{(1+R)-d}{u-d},\\ q_u=\frac{u-(1+R)}{u-d}.\end{matrix}\right.
$$

</blockquote>

<blockquote class = "def">

**Definition 2.19.** A **contingent claim** is a stochastic variable $X$ of the form

$$
X=\Phi(S_T),
$$

where the **contract function** $\mathbf{\Phi}$ is some given real valued function.

</blockquote>

<blockquote class = "def">

**Definition 2.20.** A given contingent claim $X$ is said to be **reachable** if there exists a self-financing portfolio $h$ such that

$$
V_T^h=X,
$$

with probability one. In that case we say that the portfolio $h$ is a **hedging** portfolio or a **replicating** portfolio. If all claims can be replicated we say that the market is *(dynamically)* **complete**.

</blockquote>

**Pricing principle 2.** If a claim $X$ is reachable with replicating portfolio $h$, then the only reasonable price process for $X$ os given by

$$
\Pi_t[X]=V_t^h,\ t=0,1,...,T.
$$

<blockquote class = "prop">

**Proposition 2.21.** Assume $X$ is reachable by $h$, then any price other than $V_t^h$ for some $t\ge 0$ leads to an arbitrage opportunity.

</blockquote>

<blockquote class = "prop">

**Proposition 2.22.** The multiperiod model is complete, i.e. every claim can be replicated by a self-financing portfolio.

</blockquote>

<blockquote class = "prop">

**Proposition 2.24.** **(Binomial algorithm)** Consider a $T$-claim $X=\Phi(S_T)$. Then this claim can be replicated using af self-financing portfolio. If $V_t(k)$ denotes the value of the portfolio at the node $(t,k)$ ($k$ referring to $k$ amount of up-moves for the stock), then $V_t(k)$ can be computed recursively by the scheme

$$
\left\{\begin{matrix}V_t(k)=\frac{1}{1+R}\left\{q_uV_{t+1}(k+1)+q_dV_{t+1}(k)\right\},\\ V_T(k)=\Phi(su^kd^{T-k}).\end{matrix}\right.
$$

where the martingale probabilities $q_u$ and $q_d$ are given by

$$
\left\{\begin{matrix}q_u=\frac{(1+R)-d}{u-d},\\ q_u=\frac{u-(1+R)}{u-d}.\end{matrix}\right.
$$

With the notation as above, the hedging portfolio is given by

$$
\left\{\begin{matrix}x_t(k)=\frac{1}{1+R}\cdot\frac{uV_t(k)-dV_t(k+1)}{u-d},\\ y_t(k)=\frac{1}{S_{t-1}}\cdot\frac{V_t(k+1)-V_t(k)}{u-d}.\end{matrix}\right.
$$

In particular, the arbitrage free price of the claim at $t=0$ is given by $V_0(0)$.

</blockquote>

<details>
<summary>**Example.**</summary>

```{tikz,echo=FALSE}
\tikzstyle{level 1}=[level distance=4cm, sibling distance=3.5cm,->]
\tikzstyle{level 2}=[level distance=4cm, sibling distance=2cm,->]

\tikzstyle{bag} = [text width=2em, text centered]
\tikzstyle{end} = []

\begin{tikzpicture}[grow=right, sloped]
\node[bag] {100}
    child {
        node[bag] {90}        
            child {
                node[end, label=right:
                    {81}] {}
                edge from parent
                node[above] {}
                node[below]  {$p_d^2$}
            }
            child {
                node[end, label=right:
                    {99}] {}
                edge from parent
                node[above] {}
                node[below]  {$p_dp_u$}
            }
            edge from parent 
            node[above] {}
            node[below]  {$p_d$}
    }
    child {
        node[bag] {110}        
        child {
                node[end, label=right:
                    {99}] {}
                edge from parent
                node[above] {$p_dp_u$}
                node[below]  {}
            }
            child {
                node[end, label=right:
                    {121}] {}
                edge from parent
                node[above] {$p_u^2$}
                node[below]  {}
            }
        edge from parent         
            node[above] {$p_u$}
            node[below]  {}
    };
\end{tikzpicture}
```

Consider $R=0.04$, $s=100$, $u=1.1$, $d=0.9$, $p_u=0.6$ and $p_d=0.4$. We consider a model of length $T=2$ and we want to evaluate the price of the european call option with srike $K=90$ that is the contingent claim

$$
X=(S_T-K)^+,\hspace{20pt}\Phi(s)=(s-K)^+.
$$

For each time $t$ we know the replicating portfolio, if we know the payoff the following period. Therefore we start from the leaves of the tree and work towards the root. Since the strike price is $K=90$ the end result will be the following payoffs:

\begin{align*}
u^2:\hspace{20pt}&(121-90)^+=31\\
ud:\hspace{20pt}&(99-90)^+=9\\
du:\hspace{20pt}&(99-90)^+=9\\
d^2:\hspace{20pt}&(81-90)^+=0
\end{align*}

Therefore by the risk neautral valuation formula with $q_u=\frac{(1+R)-d}{u-d}=0.7$ and $q_d=\frac{u-(1+R)}{u-d}=0.3$ we have that the cost of the replicating portfolio at time $t=1$ is respectively

\begin{align*}
u:\hspace{20pt}&\frac{1}{1+R}\left\{31\cdot q_u + 9 \cdot q_d\right\}\approx 23.46\\
d:\hspace{20pt}&\frac{1}{1+R}\left\{9\cdot q_u + 0 \cdot q_d\right\}\approx 6.06
\end{align*}

To replicate this payoff at time $t=1$ we can use the risk neutral valuation formula once more to find the base cost of the replicating portfolio i.e. the price of $X$ at time $t=0$

$$
\frac{1}{1+R}\left\{23.46\cdot q_u + 6.06 \cdot q_d\right\}\approx 17.54.
$$

Working from the root to the leaves we can now calculate the hedging portfolio at time $t=0,1$ for each path. For time $t=0$ we calculate

\begin{align*}
x=&\frac{1}{1+R}\cdot \frac{u\cdot 6.06-d\cdot 23.46}{u-d}\approx -69.46,\\
y=&\frac{1}{s}\cdot\frac{23.46-6.06}{u-d}\approx0.87
\end{align*}

We see by calculations that this does indeed replicate the payoff at time $t=1$:

\begin{align*}
u:\hspace{20pt}&V_1^h=(1+R)\cdot x + 110\cdot y\approx 23.46,\\
d:\hspace{20pt}&V_1^h=(1+R)\cdot x + 90\cdot y\approx 6.06.
\end{align*}

We also see by calculation that the initial portfolio does cost the expected 17.54 as

$$
x\cdot 1+y\cdot100=87-69.46=17.54.
$$

Following these steps at time $t=1$ the portfolios $(-86.54,1)$ (for the up-scenario) and $(-38.94,0.5)$ (for the down-scenario) would arise. Notice when calculating $y$ one has to use the current price $S_1=S_0\cdot Z$ not $S_0$. One should also check by similar calculations as above, that these portfolios does indeed replicate the payoff of the contingent claim $X$. $\square$

</details>

<blockquote class = "prop">

**Proposition 2.25.** The arbitrage free price at $t=0$ of a $T$-claim $X$ is given by

$$
\Pi_0[X]=\frac{1}{(1+R)^T}E^Q[X]
$$

where $Q$ denotes the martingale measure, or more explicitly

$$
\Pi_0[X]=\frac{1}{(1+R)^T}\sum_{k=0}^T\binom{T}{k}q_u^kq_d^{T-k}\Phi(su^kd^{T-k}).
$$

</blockquote>

<details>
<summary>**Example.**</summary>

```{r}
R <- 0.04
s <- 100
u <- 1.1
d <- 0.9
p_u <- 0.6
p_d <- 0.4
q_u <- (1+R-d)/(u-d)
q_d <- (u-1-R)/(u-d)
cap_t <- 2

#Test for K=90
K <- 90
pi_0 <- (1+R)**(-cap_t)*sum(
  choose(cap_t,0:cap_t)*q_u**(0:cap_t)*q_d**(cap_t - 0:cap_t)*pmax(s*u**(0:cap_t)*d**(cap_t - 0:cap_t)-K,0)
) # = 17.53883

pi_0 <- unlist(lapply(0:ceiling(s*u**cap_t), function(K){
  (1+R)**(-cap_t)*sum(
    choose(cap_t,0:cap_t)*q_u**(0:cap_t)*q_d**(cap_t - 0:cap_t)*pmax(s*u**(0:cap_t)*d**(cap_t - 0:cap_t)-K,0)
  )
}))
```
```{r, echo = FALSE, wrapf = TRUE}
library(dplyr)
library(ggplot2)
data.frame(K = 0:ceiling(s*u**cap_t),
           Pi_0 = pi_0) %>%
  ggplot(.) + geom_line(aes(x=K,y=Pi_0)) +
  labs(title = "Price of european call option",
       x = "Strike", y = "Price") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
        title = element_text(size = 18))
```

We follow an analog example as the one after proposition 2.24. Let $K=90$ and we see that

\begin{align*}
&\Pi_0[X]\\
&=\frac{1}{(1+0.04)^2}\sum_{k=0}^2\binom{2}{k}\cdot0.7^k\cdot0.3^{2-k}\cdot\Phi(100\cdot 1.1^k\cdot0.9^{2-k})\\
&=0.9245562\cdot\left(\underbrace{1\cdot 1\cdot0.09\cdot0}_{k=0}+\underbrace{2\cdot 0.7\cdot0.
3\cdot 9}_{k=1}+\underbrace{1\cdot 0.49\cdot1\cdot31}_{k=2}\right)\\
&=0.9245562\cdot\left(0+3.78+15.19\right)\\
&=17.53883
\end{align*}


Since we know that $K$ must meaningfully range in $[0,121]$ we could try to calculate the price of the contingent claim at time $t=0$ for all integers in this interval. We see that the price range between $S_0$ and 0 as expected. One can also see that the price changes slope at the prices 99 and 121 as the function is linear in $\Phi$ and som realisations loose any effect on the price when the strike is higher than the outcome. $\square$

</details>

<blockquote class = "prop">

**Proposition 2.26.** The condition $d<(1+R)<u$ is necessary and sufficient condition for absence of arbitrage.

</blockquote>

#### Generelised one-period model

In the previous we had the simpel model where we only had one stochastic asset $S$ and only one stochastic variable $Z$ determining the future stock price. Now we will generelise this model by introducing $N$ assets and introducing som stochastic behaviour to the system.

##### Model specification

We consider the market consisting of a collection of stochastic prices assets $i=1,...,N$ with $N$-dimensional price process.

$$
S_t=\begin{bmatrix} S_t^1\\
\vdots\\
S_t^N\end{bmatrix}
$$

We now assume that $S_t$ is defined on a background space with finite sample space $\Omega = \{\omega_1,...,\omega_M\}$ with associated probabilities $p_j=P(\omega_j)$, $j=1,...,M$. We can then for eact time $t=1,...,T$ define the $N\times M$ matrix $D_t$ as such

$$
D_t=\begin{bmatrix} S_t^1(\omega_1)&\cdots &S_t^1(\omega_M)\\
\vdots &\ddots & \vdots\\
S_t^N(\omega_1) &\cdots&S_t^M(\omega_M)\end{bmatrix}.
$$

We will assume that $S_0^1>0$ and $S_1^1(\omega_j)>0$, $j=1,...,M$.

##### Absence of Arbitrage

We now define a **portfolio** as an $N$-dimensional row vector

$$
h=\begin{bmatrix} h^1, \dots,h^N\end{bmatrix}
$$

representing the amount of assets held at time $t=0$ and held until $t=1$. The **value process** is then

$$
V^h_t=h\cdot S_t=\sum_{i=1}^N h^iS_t^i,\ t=0,1.
$$

For a given $\omega_j\in\Omega$ we have the realisation

$$
V_t^h=hS_t(\omega_j)=hd_j=(hD)_j.
$$

<blockquote class = "def">

**Definition 3.1.** The portfolio $h$ is an **arbitrage portfolio** fil it satisfies the conditions: $V_0^h=0$, $P(V_1^h\ge 0)=1$ and $P(V_1^h>0)>0$.

</blockquote>

<blockquote class = "lem">

**Lemma 3.2.** **(Farkas' Lemma)** Suppose that $d_0,d_1,...,d_M$ are column vectors in $\mathbb{R}^N$. Then exactly one of the following problems possesses a solution.

  * **Problem 1**: There exist $\lambda_1,...,\lambda_M\ge0$ such that $d_0=\sum_{j=1}^M\lambda_jd_j$.
  * **Problem 2**: There exist $h\in\mathbb{R}^N$ such that $h^\top d_0<0$ and $h^\top d_j\ge 0$ for $j=1,...,M$.

</blockquote>

We now investegate this system for any possible arbitrage portfolios. However first we acknowledge that there exist a nominal price system $S_t$ and a normalised price system $Z_t$. The latter we define as the nominel pricess under the numeraire $S_t^1$ that is

$$
Z_t=\begin{bmatrix} S_t^1/S_t^1\\
S_t^2/S_t^1\\
\vdots\\
S_t^N/S_t^1\end{bmatrix}=\begin{bmatrix} 1\\
S_t^2/S_t^1\\
\vdots\\
S_t^N/S_t^1\end{bmatrix}.
$$

The reason for introducing the normalized price system is that we can without much effort translate results in this system to the nominal system and the normalised system is easier to analize. For this, however, we need af few results.

<blockquote class = "lem">

**Lemma 3.3.** With notation as above, the following hold.

  1. The $Z_t$ value process i related to the $S_t$ value process by
  $$
  V_t^{h,Z}=hZ_t=\frac{1}{S_t^1}V_t^h.
  $$
  2. A portfolio is an arbitrage in the $S_t$ system if and only if there is an arbitrage in the $Z_t$ system.
  3. In the $Z_t$ price system, the numeraie asset $Z^1$ has unit constant prices i.e. $Z_t^1=1$ for all $t\ge 0$.

</blockquote>

One of the reason that the normalised system is attractable is that the numeraire asset is constant i.e. risk free in the normalised system. Let us formulate our first main result.

<blockquote class = "prop">

**Proposition 3.4.** The market is arbitrage free if and only iff there exists strictly positive real numbers $q_1,...,q_M\ge 0$ with $q_1+\cdots + q_M=1$ (probability vector) such that the following vector equality holds

$$
\begin{bmatrix} Z_0^1\\
\vdots\\
Z_N^1\end{bmatrix}=\begin{bmatrix} Z_1^1(\omega_1)\\
\vdots\\
Z_1^N(\omega_1)\end{bmatrix}q_1+\cdots +\begin{bmatrix} Z_1^1(\omega_M)\\
\vdots\\
Z_1^N(\omega_M)\end{bmatrix}q_M.\tag{3.3}
$$

</blockquote>

<details>
<summary>**Proof.**</summary>

</details>

##### Martingale Measures

<blockquote class = "def">

**Definition 3.5.** Given the objective probability measure $P$ on $(\Omega,\mathcal{F},P)$, we say that another probability measure $Q$ defined on $\Omega$ is  **equivalent** to $P$ if

$$
\forall A\in\mathcal{F}:P(A)=0\iff Q(A)=0,
$$

or equivalently

$$
\forall A\in\mathcal{F}:P(A)=1\iff Q(A)=1.
$$

</blockquote>

<blockquote class = "def">

**Definition 3.7.** Consider the market model above and set $S^1$ as the numeraire asset. We say that a probability measure $Q$ defined on $\Omega$ is a **martingale measure** if it satisfies the following conditions:

  1. $Q$ is equivalent to $P$, i.e. $Q\sim P$.
  2. For every $i=1,...,N$, the normalized asset price process
  $$
  Z_t^i=\frac{S_t^i}{S_t^1},
  $$
  is martingale under the measure $Q$.

</blockquote>

<blockquote class = "thm">

**Theorem 3.8.** **(First Fundamental Theorem)** Given a fixed numeraire, ther market is free of arbitrage possibilities if and only if there exists a martingale measure $Q$.

</blockquote>

By assuming that the numeraire asset is risk free (i.e. does not depend on $\omega$) then by scaling we can derive the short interest rate as

$$
1+R=\frac{S_1^1}{S_0^1}.
$$

With this in mind we can formulate theorem 3.8 in its more widely used form.

<blockquote class = "thm">

**Theorem 3.9.** **(First Fundamental Theorem)** Assume that there exist a risk free asset, and denote the corresponding risk free interest rate by $R$. Then the market is arbitrage free if and only if there exist a measure $Q\sim P$ such that

$$
S_0^i=\frac{1}{1+R}E^Q[S_1^i],\hspace{20pt}\text{for all}\ i=1,...,N.\tag{3.9}
$$

</blockquote>

##### Martingale Pricing

Moving forward we will assume that there exist a risk free asset and we will denote it by $B_t$ ($B_t=S^1_t/S^1_0$).

<blockquote class = "def">

**Definition 3.10.** A **contingent claim** is any random variable $X$, defined on the sample space $\Omega$.

</blockquote>

To ensure no arbitrage in the extended market containing the $N$ assets and the contingent claim we can apply the first fundamental pricing theorem on the extended market.

<blockquote class = "prop">

**Proposition 3.11.** Consider a given claim $X$. In order to avoid arbitrage, $X$ must then be priced according to the formula

$$
\Pi_0[X]=\frac{1}{1+R}E^Q[X],\tag{3.10}
$$

where $Q$ is a martingale measure for the underlying market $(\Pi,S^1,...,S^N)$.

</blockquote>

##### Completeness

Given that a market is arbitrage-free we may run into a uniqueness issue when determining the price of a contingent claim. If a martingale measure exist we will very much like it to be unique as this will ensure that the price from the risk neutral valuation formula is unique. To this we need the market to be complete.

<blockquote class = "def">

**Definition 3.12.** Consider a contingent claim $X$. If there exists a portfolio $h$, based on the underlying assets, such that

$$
V_1^h=X,\ \text{with probability 1}\tag{3.11}
$$

i.e.

$$
V_1^h(\omega_j)=X(\omega_j),\ j=1,...,M,\tag{3.12}
$$

then we say that $X$ is **replicated**, or **hedged** by $h$. Such a portfolio $h$ is called a replicating, or hedging portfolio. If every contingent claim can be replicated, we say that the market is **complete**.

</blockquote>

We can now formulate a proposition on when the market is complete in terms of the matrix $D$.

<blockquote class = "prop">

**Proposition 3.13.** The market is complete if and only if the rows of the matrix $D$ span $\mathbb{R}^M$, i.e. if and only if $D$ has rank $M$.

</blockquote>

Now we formulate the second fundamental pricing theorem in terms of the martingale measure $Q$.

<blockquote class = "prop">

**Proposition 3.14.** **(Second Fundamental Theorem)** Assume that the model is arbitrage free i.e. $Q$ exist. Then the market is unique if and only if the martingale measure is unique.

</blockquote>

##### Stochastic Discount Factors

<blockquote class = "def">

**Definition 3.16.** The random variable $L$ on $\Omega$ is defined by

$$
L(\omega_i)=\frac{q_i}{p_i},\hspace{20pt} i=1,...,M.
$$

</blockquote>

<blockquote class = "def">

**Definition 3.17.** Assume the absence of arbitrage, and fix a martingale measure $Q$. With notation as above, the **stochastic discount factor** (or "state price deflator") is the random variable $\Lambda$ on $\Omega$ by

$$
\mathbf{M}(\omega)=\frac{1}{1+R}\cdot L(\omega).\tag{3.19}
$$

</blockquote>

<blockquote class = "prop">

**Proposition 3.18.** The arbitrage free price of any claim $X$ is given by the formula

$$
\Pi_0[X]=E^P[\mathbf{M}\cdot X]
$$

where $\mathbf{M}$ is a stochastic discount factor.

</blockquote>

&nbsp;

 ### Exercises

**Probability exercises**

Let $(W(t))_{t\ge}$ be a Brownian motion (Bjork, Definition 4.1).

**Exercise 1.** Show that the following processes also are Brownian motions.

 i. $(-W(t))_{t\ge 0}$ (symmetry)
 ii. For any $s\ge 0$, $(W(t+s)-W(s))_{t\ge 0}$ (time-homogeneity).
 iii. For every $c>0$, $(cW(t/c^2))_{t\ge 0}$ (scaling).

<details>
<summary>**Solution (i).**</summary>

By assumption $W$ is a Brownian motion and so it follows that

$$-W_0=-1\cdot0=0$$

Furthermore, for $r<s\le t< u$ it holds that $W_u-W_t$ and $W_s-W_r$ is independent. By seperate transformations the independence property is preserved and $-(W_u-W_t)$ and $-(W_s-W_r)$ is independent. Next, for a normal distributed random variable $N\sim\mathcal{N}(\mu,\sigma^2)$ it holds, that for a scaler $c\in\mathbb{R}$ we have $c N\sim\mathcal{N}(c\mu,c^2\sigma ^2)$. Then obviously;

$$-(W_t)=(-1)W_t\stackrel{d}{=}\mathcal{N}((-1)\cdot0,(-1)^2(t-s))\stackrel{d}{=}\mathcal{N}( 0,t-s).$$

Lastly, let $\omega \in \Omega$ and consider the sample path $s\mapsto (-W_s)(\omega)$. Clearly for two continuous functions $f$ and $g$ it holds that $(g\circ f)$ is continuous. Then with $g(f)=-f$ and $f(t)=W_t(\omega)"/>$ it follows that $(-W_t)=(g\circ W)(t)$ is also continuous.

</details>
<details>
<summary>**Solution (ii).**</summary>

Much like the previous exercise we define a new process and show the properties hold. Let $s\ge 0$ be chosen arbitrary. Now define $X_t=W(t+s)-W(s)$.

First, we let $t=0$ and see

$$X_0=W(0+s)-W(s)=W(s)-W(s)=0.$$

Secondly, we have that for $r<u$:

$$X_u-X_r=W(u+s)-W(s)-(W(r+s)-W(s))=W(u+s)-W(r+s)\sim \mathcal{N}(0,u+s-(r+s))=\mathcal{N}(0,u-r).$$

and since for $r<u\le k<l$ the translation $r+s<u+s\le k+s<l+s$ still holds and $X_l-X_k=W(l+s)-W(k+s)$ and $X_u-X_r=W(u+s)-W(k+s)$ are independent. Finally since $W_t(\omega)$ is continuous in $t$ hence the translation $W_{t+s}$ is continouos. Adding a constant yields a function that is also continuous, hence $X_t$ is continuous.

</details>
<details>
<summary>**Solution (iii).**</summary>

Let $c>0$ be given. We show that

$$X_t=cW\left(\frac{t}{c^2}\right)$$

is a Brownian motion. We simply show the four properties. Let $t=0$ and notice

$$X_0=cW\left(\frac{0}{c^2}\right)=cW(0)=0.$$

The second property follows from seperate transformation and that for $r<u\le s<t$ we consider

$$X_u-X_r=c\left(W\left(\frac{u}{c^2}\right)-W\left(\frac{r}{c^2}\right)\right)\hspace{20pt}\text{and}\hspace{20pt}X_t-X_s=c\left(W\left(\frac{t}{c^2}\right)-W\left(\frac{s}{c^2}\right)\right)$$

and since $c,r,u,t,s>0$ we have the same order for the scaled version of $r,u,t,s$ and hence we have two independent RV scaled by $c$. Then by seperate transformations the variables is still independent. Next for the third property:

$$X_t-X_s=c\left(W\left(\frac{t}{c^2}\right)-W\left(\frac{s}{c^2}\right)\right)\sim\mathcal{N}\left(c\cdot 0,c^2\left(\frac{t}{c^2}-\frac{s}{c^2}\right)\right)=\mathcal{N}(0,t-s).$$

Where we use the properties of scaling a normal distributed random variable i.e. for $c>0$ and $N\sim\mathcal{N}(\mu,\sigma ^2)$ it follows that $c N\sim\mathcal{N}(c\mu,c^2\sigma ^2)$. Finally, the forth property follows since $g(f)=cf$ is continuous and $h(t)=t/c^2$ is continuous, then for any continuous function $f(s)$ it follows that $(g \circ f\circ h)=g(f(h(t)))$ is continuous.

</details>

&nbsp;

<blockquote class = "prop">
**Proposition B.37.** Let $(\Omega,\mathcal{F},P)$ be a given probability space, let $\mathcal{G}$ be a sub-sigma-algebra of $\mathcal{F}$, and let $X$ be a square integrable random variable.
Consider the problem of minimizing
$$E\left[(X-Z)^2\right]$$
where $Z$ is allowed to vary over the class of all square integrable $\mathcal{G}$ measurable random variables. The optimal solution $\hat{Z}$ is then given by.
$$\hat{Z}=E[X\vert\mathcal{G}].$$
</blockquote>

**Exercise 2.** *(Bjork, exercise B.11.)* Prove proposition B.37 by going along the following lines.

  a. Prove that the "estimation error" $X-E[X\vert\mathcal{G}]$ is orthogonal to $L^2(\Omega,\mathcal{G},P)$ in the sence that for any $Z\in L^2(\Omega,\mathcal{G},P)$ we have
  $$E[Z\cdot(X-E[X\vert\mathcal{G}])]=0$$
  b. Now prove the proposition by writing
  $$X-Z=(X-E[X\vert\mathcal{G}])+(E[X\vert\mathcal{G}]-Z)$$
  and use the result just proved.

<details>
<summary>**Solution (a).**</summary>

Let $X\in L^2(\Omega,\mathcal{F},P)$ be a random variable. Now consider an arbitrary $Z\in L^2(\Omega,\mathcal{G},P)$. Recall that $\mathcal{G}\subset \mathcal{F}$ and so $X$ is also in $Z\in L^2(\Omega,\mathcal{G},P)$, as it is bothe square integrable and $\mathcal{G}$-measurable. Then

$$E\left[Z\cdot(X-E[X\vert\mathcal{G}])\right]=E\left[Z\cdot X\right]-E\left[Z\cdot E[X\vert\mathcal{G}]\right].$$

Then by using the law of total expectation and secondly that $Z$ is $\mathcal{G}$-measurable we have that

$$E\left[Z\cdot X\right]=E\left[E[Z\cdot X\vert\mathcal{G}]\right]=E\left[Z\cdot E[ X\vert\mathcal{G}]\right].$$

Combining the two equations gives the desired result.

</details>
<details>
<summary>**Solution (b).**</summary>

Obviously, we have that

$$X-Z=X-Z+E[X\vert\mathcal{G}]-E[X\vert\mathcal{G}]=(X-E[X\vert\mathcal{G}])+(E[X\vert\mathcal{G}]-Z).$$

Then squaring the terms gives

$$(X-Z)^2=(X-E[X\vert\mathcal{G}])^2+(E[X\vert\mathcal{G}]-Z)^2+2(X-E[X\vert\mathcal{G}])(E[X\vert\mathcal{G}]-Z)$$

Taking expectation on each side and using linearity of the expectation we have that

$$E[(X-Z)^2]=E\left[(X-E[X\vert\mathcal{G}])^2\right]+E\left[(E[X\vert\mathcal{G}]-Z)^2\right]+2E\left[(X-E[X\vert\mathcal{G}])(E[X\vert\mathcal{G}]-Z)\right].$$

We can now use that $E[X\vert\mathcal{G}]-Z$ is $\mathcal{G}$-measurable with the above result on the last term.

$$E[(X-Z)^2]=E\left[(X-E[X\vert\mathcal{G}])^2\right]+E\left[(E[X\vert\mathcal{G}]-Z)^2\right].$$

Now since $X$ is given the term $E\left[(X-E[X\vert\mathcal{G}])^2\right]$ is simply a constant not depending on the choice og $Z$. The optimal choice of $Z$ is then $E[X\vert\mathcal{G}]$ since this minimizes the second term. The statement is then proved.

</details>

&nbsp;

**Exercise 3.** Discuss the following theory/results of Moment generating functions (Laplace transform).

Let $X$ be a random variable with distribution function $F(x)=P(X\le x)$ and $Y$ be a random variable with distribution function $G(y)=P(Y\le y)$.

<blockquote class = "def">
**Definition.** The moment generating function or Laplace transform of $X$ is

$$\psi_X(\lambda)=E\left[e^{\lambda X}\right]=\int_{-\infty}^\infty e^{\lambda x}dF(x)$$

provided the expectation is finite for $\vert\lambda\vert<h$ for some $h>0$.
</blockquote>

The MGF uniquely determine the distribution of a random variable, due to the following result.

<blockquote class = "thm">
**Theorem 1.** *(Uniqueness)* If $\psi_X(\lambda)=\psi_Y(\lambda)$ when $\vert\lambda\vert<h$ for some $h>0$, then $X$ and $Y$ has the same distribution, that is, $F=G$.
</blockquote>

There is also the following result of independence for Moment generating functions.

<blockquote class = "thm">
**Theorem 1.** *(Independence)* If 

$$E\left[e^{\lambda_1X+\lambda_2Y}\right]=\psi_X(\lambda_1)\psi_Y(\lambda_2)$$

for $\vert\lambda_i\vert<h$ for $i=1,2$ for some $h>0$, then $X$ and $Y$ are independent random variables.
</blockquote>

**Example.** Recall that the Moment generating function of a normal (Gaussian) distribution is given by

$$\psi_X(\lambda)=E\left[e^{\lambda X}\right]=\exp\left(\lambda \mu + \frac{\lambda^2}{2}\sigma^2\right)$$

where $X$ is normally distributed with mean $\mu$ and variance $\sigma^2$ and $\lambda\in\mathbb{R}$ is a constant. Since a Brownian motion $W(t)$ is normally distributed with zero mean and variance $t$, we have that

$$E[\exp(\lambda W(t))]=\exp\left(\frac{\lambda^2}{2}t\right).$$

<details>
<summary>**Discussion.**</summary>



</details>

&nbsp;

**Exercise 4.** *(Bjork, exercise C.8.(a-c))* Let $W$ be a Brownian motion. Notice that for the natural filtration $\mathcal{F}_s=\sigma(W_t\vert t\le s)$ $W_t-W_s$ is independent of $\mathcal{F}_s$

  a. Show that $W_t$ is a martingale.
  b. Show that $W^2_t-t$ is a martingale.
  c. Show that $\exp(\lambda W_t-\frac{\lambda^2}{2}t)$ is a martingale.

<details>
<summary>**Solution (a).**</summary>

We show that for the natural filtration that $W_t$ is a martingale. This include showing integrability and the martingale property. For the first we note that for a normal distributed random variable with mean 0 we have

$$E[\vert N\vert]=\int_{-\infty}^\infty \vert x\vert dF_N(x)=2\int_{0}^\infty xdF_N(x)$$

since the distribution is symmetric. Substituting the distribution function $\Phi(x)=P(N\le x)$ in we see that

$$E[\vert N\vert]=2\int_{0}^\infty xd\Phi(x)=2\int_{0}^\infty x\frac{1}{\sqrt{2\pi\sigma^2}}e^{-x^2/(2\sigma^2)}dx=(*)$$

by substituting $u=x^2/(2\sigma^2)$ ($x=\sqrt{2\sigma^2u}$) we have that

$$\frac{dx}{du}=\frac{1}{2}\sqrt{2\sigma^2u}2\sigma^2=(\sigma^2)^{3/2}\sqrt{2}u\iff dx=(\sigma^2)^{3/2}\sqrt{2}u\ du$$

hence

$$(*)=\frac{2}{\sqrt{2\pi\sigma^2}}\int_0^{\infty}\sqrt{2\sigma^2u}e^{-u}(\sigma^2)^{3/2}\sqrt{2}u\ du=\frac{2\sqrt{2\sigma^2}(\sigma^2)^{3/2}\sqrt{2}}{\sqrt{2\pi\sigma^2}}\int_0^{\infty}\sqrt{u}e^{-u}u\ du.$$

This then simplify to

$$(*)=\frac{(2\sigma^2)^{3/2}}{\sqrt{\pi}}\int_0^{\infty}u^{3/2}e^{-u}\ du=(2\sigma^2)^{1/2}\sqrt{\frac{2\sigma^2}{\pi}}\int_0^{\infty}u^{3/2}e^{-u}\ du=\sqrt{\frac{2\sigma^2}{\pi}}<\infty.$$

(Obviously the above is not derived correctly, but the end expression is valid, source: [link](https://arxiv.org/pdf/1402.3559.pdf)) However since

$$W_t=W_t-0=W_t-W_0\sim\mathcal{N}(0,t)$$

we have that $E\vert W_t\vert<\infty$ as desired.

Next, we have that

$$E[W_t\vert \mathcal{F}_s]=E[W_t-W_s\vert\mathcal{F}_s]+W_s=0+W_s=W_s.$$

In the above we used that $W_t-W_s$ is $\mathcal{F}_s$-measurable with mean 0. Then it follows that $W_t$ is a martingale.

</details>
<details>
<summary>**Solution (b).**</summary>

Let $M_t=W_t^2-t$. First, we observe that two measurable functions composed is still a measurable function. Hence we know that $M_t$ is measurable wrt. the filtration since $W_t$ is measurable and $w\mapsto w^2+t$ is measurable. Secondly, we have that

$$E[\vert W_t^2-t\vert]\le E\vert W_t^2\vert +E\vert t\vert=t+t=2t<\infty$$

where we use the triangle inequality. Thirdly, for the martingale property we have that for $t>s$:

$$E[M_t\vert \mathcal{F}_s]=E[W_t^2-t\vert \mathcal{F}_s]=E[W_t^2+W_s^2-2W_tW_s-W_s^2+2W_tW_s-t\vert \mathcal{F}_s]$$

which by linearity and independence of increments to the filtration gives

$$E[M_t\vert \mathcal{F}_s]=E[(W_t-W_s)^2-W_s^2+2W_tW_s-t\vert \mathcal{F}_s]=t-s-t+E[2W_tW_s-W_s^2\vert \mathcal{F}_s]$$

However since $W_s$ is measurable wrt. the filtration at time $s$ the above is

$$E[M_t\vert \mathcal{F}_s]=2W_sE[W_t\vert \mathcal{F}_s]-W_s^2-s=2W_s^2-W_s^2-s=W_s^2-s=M_s.$$

Since from (a) we know that $W_t$ is a martingale. Then we arrive at the desired result.

</details>
<details>
<summary>**Solution (c).**</summary>

Let $M_t=\exp\left(\lambda W_t-\frac{\lambda^2}{2}t\right)$. First, by composition of measurable functions $M_t$ is $\mathcal{F}_t$-measurable. Secondly, we have using the MGF for a normal distributed random variable:

$$E\vert M_t=E\left(\exp\left(\lambda W_t-\frac{\lambda^2}{2}t\right)\right)\le E\left(\exp\left(\lambda W_t\right)\right)=\exp\left(\frac{\lambda^2}{2}t\right)<\infty.$$

Thirdly, we consider

$$E[M_t\vert\mathcal{F}_s]=E\left.\left[\left(\exp\left(\lambda W_t-\frac{\lambda^2}{2}t\right)\right)\right\vert\mathcal{F}_s\right]=\exp\left(-\frac{\lambda^2}{2}t\right)E\left.\left[\left(\exp\left(\lambda W_t\right)\right)\right\vert\mathcal{F}_s\right].$$

By adding and subtracting $W_s$ in the exponent we get

\begin{align*}
E[M_t\vert\mathcal{F}_s]&=\exp\left(-\frac{\lambda^2}{2}t\right)E\left.\left[\left(\exp\left(\lambda (W_t-W_s)+\lambda W_s\right)\right)\right\vert\mathcal{F}_s\right]\\
&=\exp\left(-\frac{\lambda^2}{2}t\right)\exp\left(\frac{\lambda^2}{2}(t-s)\right)E\left.\left[\left(\exp\left(\lambda W_s\right)\right)\right\vert\mathcal{F}_s\right].
\end{align*}

Using that $E\left.\left[\left(\exp\left(\lambda W_s\right)\right)\right\vert\mathcal{F}_s\right]=\exp\left(\lambda W_s\right)$ and combining the exponents gives the desired:

$$E[M_t\vert\mathcal{F}_s]=\exp\left(\lambda W_s-\frac{\lambda^2}{2}s\right)=M_s.$$

</details>

## Week 2 

### Material

  * Stochastic integrals and Ito formula (Chapter 4 and Appendix C.2)
  * Stochastic differential equations (Chapter 5.1-4)

### Theory

#### Stochastic Integrals

We want to formulate financial markets in continuous time and the most elegant theory is obtained from processes that can be defined in terms of **stochastic differential equations** or in other words by their dynamics. We may call them **diffusion processes**, as they may be approximated by a stochastic difference equation:

$$
X_{t+\Delta t}-X_t=\mu(t,X_t)\Delta t+\sigma(t,X_t)Z_t.\tag{4.1}
$$

Above $Z_t$ is a normally distributed random variable (a disturbance). In this formulation we say that $S_t$ is driven by two forces: on one hand a locally deterministic velocity or drift $\mu(t,X_t)$ and on the other hand a Gaussian term amplified by the deterministic factor $\sigma(t,X_t)$.

##### Information

We consider a primary process $X_t$ and we introduce the notion of information generated by $X_t$ in terms of the natural filtration. The idea can be summed up in the following definition.

<blockquote class = "def">

**Definition 4.3.** The symbol $\mathcal{F}^X_t\subseteq\mathcal{F}$ denotes "the information generated by $X_t$ on the interval $[0,t]$", or alternatively "what has happened to $X_t$ over ther interval $[0,t]$".

  1. If, based upon observations of the trajectory $\{X_s;\ 0\le s\le t\}$, it is possible to decide whether a given event $A$ has occurred or not, then we write this as
  $$
  A\in\mathcal{F}^X_t
  $$
  or say that "$A$ is $\mathcal{F}^X_t$-measurable".
  2. If the value of a given random variable $Z$ can be completely determined given observations of the tragectory $\{X_s;\ 0\le s\le t\}$, then we also write
  $$
  Z\in\mathcal{F}^X_t.\ \text{(}Z\text{ is }\mathcal{F}^X_t\text{-measurable)}
  $$
  3. If $Y$ is a stochastic process such that we have
  $$
  Y_t\in\mathcal{F}^X_t
  $$
  for all $t\ge0$ then we say that $Y_t$ is **adapted** to the **filtration** $\{\mathcal{F}^X_t\}_{t\ge 0}$. For brevity of notation, we will sometimes write the filtration as $\{\mathcal{F}^X_t\}_{t\ge 0}=\mathbf{F}$.

</blockquote>

##### Stochastic Integrals

We will now formulate the theory of stochastic integrals, that is, processes written in terms of stochastic processes with stochastic integrator and/or stochastic integrant. We will consider some given standard Brownian motion $W_t$ and another stochastic process $X_t$. We need som integrability condition on $X_t$ in order to do the calculations. We therefore determine a selection of suitable stochastic processes $X$ must be contained in.

<blockquote class = "def">

**Definition 4.4.** Let $X_t$ be a stochastic process, then

  i. We say that $X_t$ belongs to the class $\mathcal{L}^2[a,b]$ if $X_t$ is adapted to the filtration $\mathcal{F}^X_t$ and the following holds
  $$\int_a^bE[X_s^2]\ ds<\infty$$
  ii. We say that $X_t$ belongs to the class $\mathcal{L}^2$ if $X_t\in\mathcal{L}^2[0,t]$ for all $t>0$.

</blockquote>

We now want to define what we mean by 

$$
\int_a^bX_t\ dW_s
$$

for some process $X_t\in\mathcal{L}^2$. We see that a way to go about this problem is to start by defining the concept for a simple stochastic process $X_t$. By *simple* we mean a process $X_t$ that is constant on between some deterministic points in time $a=t_0<t_1<\cdots<t_n=b$. In that case we may define the integral as

$$
\int_a^bX_s\ dW_s = \sum_{k=0}^{n-1}X_{t_k}[W_{t_{k+1}}-W_{t_k}].
$$

In the more general setting we may follow the following approach:

  1. Approximate $X$ with a sequence $\{X^n\}_{n\in\mathbb{N}}$ of simple processes such that the following convergence criteria hold
  
  $$
  \int_a^bE[(X_s^n-X_s)^2]\ ds\to 0,\ n\to\infty
  $$
  
  2. For each $n$ the integral $\int_a^b X_s^n\ dW_s:=Z^n$ is well defined and it is possible to prove, using DCT, that a variable $Z$ exists such that $Z^n\to Z$ that is in $L^2$.
  3. We now define the stochastic integral by the limit
  
  $$
  \int_a^b X_s\ dW_s=\lim_{n\to \infty}\int_a^b X_s^n\ dW_s.
  $$

Obviously the hardest step is finding the processes $X^n$. This stochastic har some properties we will use.

<blockquote class = "prop">

**Proposition 4.5.** Let $X_t\in\mathcal{L}^2$, then

\begin{align*}
&E\left[\int_a^b X_s\ dW_s\right]=0.\tag{4.11}\\
&E\left[\left(\int_a^b X_s\ dW_s\right)^2\right]=\int_a^b E[ X_s^2]\ dW_s.\tag{4.13}\\
&\int_a^b X_s\ dW_s\ \text{ is }\mathcal{F}_b^W\text{-measurable.}\tag{4.14}
\end{align*}

</blockquote>

##### Martingales

<blockquote class = "def">

**Definition 4.7.** Let $M_t$ be a stochastic process defined on a background space $(\Omega,\mathcal{F},P)$. Let $(\mathcal{F}_t)_{t\ge 0}$ be a filtration. If $M_t$ is adapted to the filtration $\mathcal{F}_t$, $E\vert M_t\vert <\infty$ and

$$E[M_t\vert \mathcal{F}_s]=M_s,\hspace{20pt}P-\text{a.s.}$$

holds for any $t>s$ we say that $M_t$ is a martingale ($\mathbf{F}$-martingale). If the above has $\ge$ or $\le$ we say that $M_t$ is either a **submartingale** or **supermartingale** respectively.

</blockquote>

<blockquote class = "prop">

**Proposition 4.8.** For any process $X_t\in\mathcal{L}^2[s,t]$ the following hold:

$$
E\left[\left.\int_s^t X_s\ dW_s\right\vert\mathcal{F}_s^W\right]=0
$$

</blockquote>

<blockquote class = "prop">

**Corollary 4.9.** For any process $X_t\in\mathcal{L}^2$ the process

$$
M_t=\int_s^t X_s\ dW_s,
$$

is an $(\mathcal{F}_t^W)$-martingale. In other words, modulo an integrability condition, *every stochastic integral is a martingale*.

</blockquote>

<blockquote class = "lem">

**Lemma 4.10.** Let $M_t$ be a stochastic process with stochastic differential, then $M_t$ is a martingale if and only if the stochastic differential has the form $dM_t=X_t\ dW_t$ i.e. $M_t$ as no $dt$-term.

</blockquote>

##### Stochastic Calculus and the Ito Formula

Given this breif introduction to stochastic integrals we may formulate som simple calculus revolving around Ito's formula. We consider the stochastic process $X_t$ and we suppose that there exist a real number $X_0$ and adapted processes $\mu$ and $\sigma$ wrt. $\mathcal{F}_t^W$ such that for all $t\ge0$ we have

$$
X_t=X_0+\int_0^t\mu_s\ ds+\int_0^t\sigma_s\ dW_s,\tag{4.16}
$$

where $W_t$ is a standard Brownian motion. We know from earlier courses that the above may be written in terms of the dynamics (pure notation):

$$
\left\{\begin{matrix}dX_t=\mu_t\ dt+\sigma_t\ dW_t,\\ X_0=X_0.\end{matrix}\right.
$$

Here we intepret the above as $X_t$ has boundary condition $X_0$ and evolves with a drift $\mu_t\ dt$ amplified and distorted by the drift $\sigma_t\ dW_t$. We say that $X_t$ has **stochastic differential** $dX_t$ and initial condition $X_0$.

We want to understand how transformation of such an integral behaves and therefore we introduce some calculus which will tell how for instance $f(t,X_t)$ (for some $C^{1,2}$-function) behaves. This insight is given by the important Ito's formula.

<blockquote class = "thm">

**Thoerem 4.11.** Assume that the process $X$ has a stochastic differential form given by

$$
dX_t=\mu_t\ dt + \sigma_t\ dW_t,\tag{4.28}
$$

where $\mu$ and $\sigma$ are adapted processes, and let $f:\mathbb{R}_+\times\mathbb{R}\to\mathbb{R}$ be a $C^{1,2}$-function. Define the process $Z$ by $Z_t=f(t,X_t)$. Then $Z$ has stochastic differential given by

$$
df(t,X_t)=\left(\frac{\partial f}{\partial t}(t,X_t) + \mu_t\frac{\partial f}{\partial x}(t,X_t) + \frac{1}{2}\sigma^2_t\frac{\partial^2 f}{\partial x^2}(t,X_t)\right)\ dt+\sigma_t\frac{\partial f}{\partial x}(t,X_t)\ dW_t.\tag{4.29}
$$

</blockquote>

<blockquote class = "prop">

**Proposition 4.12.** With assumptions as in theorem 4.11, $df$ is given by

$$
df=\frac{\partial f}{\partial t}\ dt + \frac{\partial f}{\partial x}\ dX_t + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\ (dX_t)^2,\tag{4.31}
$$

where we use the following table

$$
\left\{\begin{matrix}(dt)^2=0,\\ dt\cdot dW_t=0,\\ (dW_t)^2=dt.\end{matrix}\right.
$$

</blockquote>

<blockquote class = "lem">

**Lemma 4.18.** Let $\sigma(t)$ be deterministic function of time and define the process $X$ by

$$
X_t=\int_0^t \sigma(s)\ dW_s.\tag{4.37}
$$

Then

$$
X_t\sim\mathcal{N}\left(0,\int_0^t\sigma^2(s)\ ds\right).
$$

</blockquote>

##### The multidimensional Ito Formula

Consider a vector process $X=(X^1,...,X^n)^\top$ where each component $X^i$ has stochastic differential

$$
d X_t^i=\mu_t^i\ dt+\sum_{j=1}^d\sigma^{ij}_t\ dW_t^j
$$

where $W^1,...,W^d$ is independent Brownian motions. Then we have respectively the drift vector process $\mu_t$ in $n$ dimensions, the vector Brownian motion in $d$ dimensions and a $n\times d$-dimensional **diffusion matrix** $\sigma_t$ given as below

$$
\mu_t=\begin{bmatrix}\mu^1_t\\ \vdots\\ \mu^n_t\end{bmatrix},\hspace{10pt}W_t=\begin{bmatrix}W^1_t\\ \vdots\\ W^d_t\end{bmatrix},\hspace{10pt}\sigma_t=\begin{bmatrix}\sigma^{11}_t & \cdots & \sigma^{1d}_t \\ \vdots & \ddots & \vdots\\ \sigma^{n1}_t &\cdots& \sigma^{nd}_t\end{bmatrix}.
$$

Given this we may write the dynamics of $X$ as

$$
d X_t=\mu_t\ dt+\sigma_t\ dW_t\in\mathbb{R}^n.
$$

Consider now a function $f:\mathbb{R}_+\times \mathbb{R}^n\to\mathbb{R}$ which is a $C^{1,2}$-mapping. We want to study the dynamics of the process

$$
Z_t=f(t,X_t).
$$

The dynamics is given in the multidimensional version of Ito's formula.

<blockquote class = "thm">

**Thoerem 4.19.** Let $X$ be given as above. Then the following holds:

  * The process $f(t,X_t)$ has the stochastisc differential given by
  $$
  df(t,X_t)=\left(\frac{\partial f}{\partial t}(t,X_t) + \sum_{i=1}^n\mu^i_t\frac{\partial f}{\partial x^i}(t,X_t) + \frac{1}{2}\sum_{i,j=1}^nC_t^{ij}\frac{\partial^2 f}{\partial x^i\partial x^j}(t,X_t)\right)\ dt+\sum_{i=1}^n\sigma^i_t\frac{\partial f}{\partial x^i}(t,X_t)\ dW_t.
  $$
  Here the row vector $\sigma^i_t$ is the $i$'th row of the matrix $\sigma_t$ and the matrix $C$ is defined by $C=\sigma\sigma^\top$.
  * Alternatively, the differential is given by the formula
  $$
  df(t,X_t)=\frac{\partial f}{\partial t}(t,X_t)\ dt + \sum_{i=1}^n\frac{\partial f}{\partial x^i}(t,X_t)\ dX^i_t + \frac{1}{2}\sum_{i,j=1}^n\frac{\partial^2 f}{\partial x^i\partial x^j}(t,X_t)\ dX^i_tdX^j_t,
  $$
  with the formal multiplication table
  $$
  \left\{\begin{matrix}(dt)^2=0,\\  dt\cdot dW_t^i=0, & i = 1,...,d,\\ (dW_t^i)^2=dt, & i=1,...,d, \\ dW_t^i\cdot dW_t^i =0, & i\ne j.\end{matrix}\right.
  $$

</blockquote>

Obviously, one can write the differential in Ito's formula in many other ways including a matrix-wise version using the Hessian matrix $H_{ij}=\frac{\partial^2 f}{\partial x^i\partial x^j}$.

##### Correlated Brownian motions

In the previous section the $d$-dimensional Brownian was assumed to have independent Brownian motions. However we may instead consider a variation where we have some dependence between the Brownian motions.

This section has not been finished.

#### Discrete Stochastic Integrals

This section has not been finished.

#### Stochastic Differential Equations

We start the chapter by formalising some used objects. We consider the following objects.

  * $M(n,d)$ denotes the class of $n\times d$-matrices.
  * $W$ is a $d$-dimensional Brownian motion
  * $\mu$ is a $\mathbb{R}^n$-valued function with arguments $(t,X_t)$ with $X_t$ being a $n$-dimensional stochastic process.
  * $\sigma$ a $M(n,d)$-valued function with arguments as in $\mu$.
  * $x_0$ a $\mathbb{R}^n$-valued vector.

We want then to understand when the following has a solution

$$
dX_t=\mu(t,X_t)\ dt + \sigma(t,X_t)\ dW_t,\ \ X_0=x_0.
$$

We call such an equation the **stochastic differential equation** or simply SDE. We know that the above is loosely notation for the integral form as

$$
X_t=x_0+\int_0^t\mu(s,X_s)\ ds +\int_0^t\sigma(s,X_s)\ dW_s,
$$

for all $t\ge 0$. The following proposition tells us when an solution exist to the problem above. In the below $\Vert \cdot \Vert$ is usual euclidian norm

$$
\Vert x\Vert=\sqrt{\sum_{i=1}^nx_i^2}.
$$

<blockquote class = "prop">

**Proposition 5.1.** Suppose that there existis a constant $K$ such that the following conditions are satisfied for all $x,y$ and $t$.

\begin{align*}
\Vert \mu(t,x) - \mu(t,y) \Vert &\le K\Vert x-y\Vert,\\
\Vert \sigma(t,x) - \sigma(t,y) \Vert &\le K\Vert x-y\Vert,\\
\Vert \mu(t,x) \Vert +\Vert \sigma(t,x) \Vert&\le K(1+\Vert x\Vert).
\end{align*}

Then there exists a unique solution to the SDE above. Furthermore, the solution has the properties

  1. $X$ is $\mathcal{F}_t^W$-adapted.
  2. $X$ has continuous trajectories.
  3. $X$ is a Markov process.
  4. There exists a constant $C$ such that
  $$
  E[\Vert X_t\Vert^2]\le Ce^{Ct}(1+\Vert x_0\Vert^2).
  $$

</blockquote>
  
In genereal the solution to an SDE is so complicated, that it in practical terms is unsolvable and may only be approximated on a finely subdividet grid as jumps. There does however exist som nontrivial cases where we may infer a analytical solution. One is the rather important **Geometric Brownian motion**.

<blockquote class = "prop">

**Proposition 5.2.** Consider the SDE

$$
dX_t=\alpha X_t\ dt+\sigma X_t\ dW_t,
$$

with $X_0=x_0$. Then the solution is given as

$$
X_t=x_0\cdot \exp\left\{\left(\alpha- \frac{\sigma^2}{2}\right)t+\sigma W_t\right\}.
$$

The expected value of $X$ is given as $E[X_t]=x_0e^{\alpha t}$.

</blockquote>

One other generalisation that is analytically solvable is the Linear SDE.

<blockquote class = "prop">

**Proposition 5.3.** Consider the SDE

$$
dX_t=(A X_t + b_t)\ dt+ \sigma_t\ dW_t,
$$

with $X_0=x_0$ and $A\in M(n,n)$ and $b_t$ being a real-valued function. Then the solution is given as

$$
X_t=e^{At}x_0+\int_0^te^{A(t-s)}b_s\ ds+\int_0^te^{A(t-s)}\sigma_s\ dW_s.
$$

Where we define the exponential of a matrix as below

$$
e^{At}=\sum_{k=0}^\infty A^k\frac{1}{k!}t^k.
$$

</blockquote>

In general with the SDE we have a partial differential operator $\mathcal{A}$ called the **infinitesimal operator** of $X$ which has some interesting analytical properties regarding $X$.

<blockquote class = "def">

**Definition 5.4.** Consider the SDE

$$
dX_t=\mu(t,X_t)\ dt+\sigma(t,X_t)\ dW_t.
$$

The partial differential operator $\mathcal{A}$ is defined, for any function $h\in C^2(\mathbb{R}^n)$, by

$$
\mathcal{A}h(t,x)=\sum_{i=1}^n\mu_i(t,x)\frac{\partial h}{\partial x_i}(x) + \frac{1}{2}\sum_{i,j=1}^n (\sigma(t,x)\sigma(t,x)^\top)_{ij}\frac{\partial^2h}{\partial x_i\partial x_j}(x).
$$

</blockquote>

We see that in terms of Ito's formula the operator is included as such

$$
df(t,X_t)=\left\{\frac{\partial f}{\partial t}(t,X_t)+\mathcal{A}f(t,x)\right\}\ dt+[\nabla_xf](t,X_t)\sigma(t,X_t)\ dW_t,
$$

where $\nabla_x$ is the gradient for function $h\in C^1(\mathbb{R}^n)$ as

$$
\nabla_xh(x)=\left[\frac{\partial h}{\partial x_1}(x),...,\frac{\partial h}{\partial x_n}(x)\right].
$$

### Exercises

**Exercise 1** *(Bjork 4.1)* Compute the stochastic differential $dZ_t$ when

  a. $Z_t=e^{\alpha t}$.
  b. $Z_t=\int_0^t g_s\ dW_s$, where $g$ is an adapted stochastic process.
  c. $Z_t=e^{\alpha W_t}$.
  d. $Z_t=e^{\alpha X_t}$, where $X$ has stochastic differential $dX_t=\mu\ dt + \sigma\ dW_t$ and $\mu,\sigma$ is constants.
  e. $Z_t=X_t^2$, where $X$ has stochastic differential $dX_t=\alpha X_t\ dt+\sigma X_t\ dW_t$.

<details>
<summary>**Solution (a).**</summary>

Let $Z_t=e^{\alpha t}$, then we see that $f(t,x)=e^{\alpha t}$ and the the following relevant derivatives is

$$
\frac{\partial f}{\partial t}(t,x)=\alpha e^{\alpha t},\hspace{10pt}\frac{\partial f}{\partial x}(t,x) =0,\hspace{10pt}\frac{\partial f}{\partial x^2}(t,x) =0.
$$

Since $Z$ does not depend on any stochastic process, we will content with $X_t=0$, that is $\mu_t=\sigma_t=0$. Then by theorem 4.11 (Ito's formula) we have

$$
dZ_t=\left(\alpha e^{\alpha t} +0+0\right)\ dt + 0=\alpha e^{\alpha t}\ dt,
$$

as expected. $\square$

</details>

<details>
<summary>**Solution (b).**</summary>

Let $Z_t=\int_0^t g_s\ dW_s$, where $g$ is an adapted stochastic process. We see that if we set $X_t=\int_0^t g_s\ dW_s$ then

$$
dX_t=0\ dt+g_t\ dW_t.
$$

Then we have the function $f(t,x)=x$ and the relevant derivatives are:

$$
\frac{\partial f}{\partial t}(t,x)=0,\hspace{10pt}\frac{\partial f}{\partial x}(t,x) =1,\hspace{10pt}\frac{\partial f}{\partial x^2}(t,x) =0.
$$

This then gives 

$$
dZ_t=\left(0+0+\frac{1}{2}g_t\cdot 0\right)\ dt + g_t\cdot 1\ dW_t=g_t\ dW_t,
$$

as expected. $\square$

</details>

<details>
<summary>**Solution (c).**</summary>

Let $Z_t=e^{\alpha W_t}$. Then we may set $X_t=W_t$ and we then have $\mu_t=0$ and $\sigma_t=1$. The function $f(t,x)=e^{\alpha x}$ and the relevant derivatives are:

$$
\frac{\partial f}{\partial t}(t,x)=0,\hspace{10pt}\frac{\partial f}{\partial x}(t,x) =\alpha e^{\alpha x},\hspace{10pt}\frac{\partial f}{\partial x^2}(t,x) =\alpha^2 e^{\alpha x}.
$$

Then the dynamics of $Z_t$ is as follows

\begin{align*}
dZ_t&=\left(0+0+\frac{1}{2}1^2\alpha^2e^{\alpha X_t}\right)\ dt + 1\alpha e^{\alpha X_t}\ dW_t\\
&=\frac{\alpha^2}{2}e^{\alpha X_t}\ dt +\alpha e^{\alpha X_t}\ dW_t\\
&=\frac{\alpha^2}{2}Z_t\ dt +\alpha Z_t\ dW_t.
\end{align*}

As desired. $\square$.

</details>

<details>
<summary>**Solution (d).**</summary>

Let $Z_t=e^{\alpha X_t}$, where $X$ has stochastic differential $dX_t=\mu\ dt + \sigma\ dW_t$ and $\mu,\sigma$ is constants. Then we have been given the definition of $X_t$ and we set $f(t,x)=e^{\alpha x}$. The relevant derivatives are then:

$$
\frac{\partial f}{\partial t}(t,x)=0,\hspace{10pt}\frac{\partial f}{\partial x}(t,x) =\alpha e^{\alpha x},\hspace{10pt}\frac{\partial f}{\partial x^2}(t,x) =\alpha^2 e^{\alpha x}.
$$

We may now derive the dynamics of $Z_t$:

\begin{align*}
dZ_t&=\left(0+\mu \alpha e^{\alpha X_t}+\frac{1}{2} \sigma^2\alpha^2 e^{\alpha X_t}\right)\ dt+\sigma \alpha e^{\alpha X_t}\ dW_t\\
&=\left(\mu+\frac{1}{2}\sigma^2\alpha\right)\alpha e^{\alpha X_t}\ dt+\sigma \alpha e^{\alpha X_t}\ dW_t\\
&=\left(\mu+\frac{1}{2}\sigma^2\alpha\right)\alpha Z_t\ dt+\sigma \alpha Z_t\ dW_t.
\end{align*}


As desired. $\square$.

</details>

<details>
<summary>**Solution (e).**</summary>

Let $Z_t=X_t^2$, where $X$ has stochastic differential $dX_t=\alpha X_t\ dt+\sigma X_t\ dW_t$. Then we set $f(t,x)=x^2$ and the relevant derivatives are:

$$
\frac{\partial f}{\partial t}(t,x)=0,\hspace{10pt}\frac{\partial f}{\partial x}(t,x) =2x,\hspace{10pt}\frac{\partial f}{\partial x^2}(t,x) =2.
$$

Given this we have the dynamics of $Z_t$ as follows

\begin{align*}
dZ_t&=\left(0 + \alpha X_t2X_t+\frac{1}{2}(\sigma X_t)^22\right)\ dt+\sigma X_t 2 X_t\ dW_t\\
&=\left(2\alpha +\sigma^2\right) X_t^2\ dt + 2\sigma X_t^2\ dW_t\\
&=\left(2\alpha +\sigma^2\right) Z_t\ dt + 2\sigma Z_t\ dW_t.
\end{align*}

As desired. $\square$.

</details>

**Exercise 2** *(Bjork 4.2)* Compute the stochastic differential for $Z$ when $Z_t=(X_t)^{-1}$ and $X$ has the stochastic differential

$$
dX_t=\alpha X_t\ dt + \sigma X_t\ dW_t.
$$

Furthermore, by using the definition $Z=X^{-1}$ you can in fact express the right-hand side of $dZ$ entirely in terms of $Z$ itself (rather then in terms of $X$). Thus $Z$ satisfies a stochastic differential equation. Which one?

<details>
<summary>**Solution.**</summary>

We see that $f(t,x)=1/x$ and so the relevant derivatives is

$$
\frac{\partial f}{\partial t}(t,x)=0,\hspace{10pt}\frac{\partial f}{\partial x}(t,x) =-\frac{1}{x^2},\hspace{10pt}\frac{\partial f}{\partial x^2}(t,x) =\frac{2}{x^3}.
$$

Then we by Ito's formula we have

\begin{align*}
dZ_t&=\left(0-\alpha X_t\frac{1}{X_t^2}+\frac{1}{2} \sigma^2 X_t^2\frac{2}{X_t^3}\right)\ dt-\sigma X_t\frac{1}{X_t^2}\ dW_t\\
&=\left(-\alpha \frac{1}{X_t}+ \sigma^2 \frac{1}{X_t}\right)\ dt-\sigma \frac{1}{X_t}\ dW_t\\
&=(\sigma^2-\alpha)Z_t\ dt-\sigma Z_t\ dW_t.
\end{align*}

We also notice that

$$
Z_t=\frac{1}{X_t}\Rightarrow dZ_t=d\left(\frac{1}{X_t}\right)=-\left(\frac{1}{X_t}\right)^2\ dX_t=-Z_t^2(\alpha X_t\ dt+\sigma X_t\ dW_t)
$$

Hence we may insert $X_t=Z_t^{-1}$ and optain

$$
dZ_t=-Z_t^2\left(\alpha\frac{1}{Z_t}\ dt + \sigma \frac{1}{Z_t}\ dW_t\right)=-\alpha Z_t\ dt-\sigma Z_t\ dW_t.
$$

Which clearly is faulty.. $\square$

</details>

**Exercise 3.** *(Bjork 4.3)* Let $\sigma(t)$ be a given deterministic function of time and define the process $X$ by

$$
X_t=\int_0^t\sigma(s)\ dW_s.
$$

Use the technique discribed in example 4.17 in order to show that the characteristic function of $X_t$ (for a fixed $t$) is given by

$$
E[e^{iuX_t}]=\exp\left\{-\frac{u^2}{2}\int_0^t\sigma^2(s)\ ds\right\},\ \ u\in\mathbb{R},
$$

thus showing that $X_t$ is normally distributed with zero mean and a variance given by

$$
Var[X_t]=\int_0^t\sigma^2(s)\ ds.
$$

<details>
<summary>**Solution.**</summary>

We follow along the lines of

  1. Determine the dynamics of $Z_t=e^{iuX_t}$ (for fixed $u$).
  2. Write the integral form of $Z_t$.
  3. Take expectation.
  4. Solve ODE.

"1)" Set $f(t,x)=e^{iuX_t}$ then the relevant derivatives are

$$
\frac{\partial f}{\partial t}(t,x)=0,\hspace{10pt}\frac{\partial f}{\partial x}(t,x) =iue^{iuX_t}=iuZ_t,\hspace{10pt}\frac{\partial f}{\partial x^2}(t,x) =i^2u^2e^{iuX_t}=-u^2Z_t.
$$

Recall that $dX_t=\sigma(t)\ dW_t$, then by Ito's formula we have

$$
dZ_t=\left(-\sigma(t)^2\frac{1}{2}u^2Z_t\right)\ dt+\sigma(t)iuZ_t\ dW_t.\tag{*}
$$

"2)" We can now write (*) on integral form as below

$$
Z_t=Z_0-\frac{u^2}{2}\int_0^t\sigma^2(s)Z_s\ ds+iu\int_0^t\sigma (s)Z_s\ dW_s,
$$

where $Z_0=e^{iuX_0}=1$.

"3)" Taking expectation now yields

$$
E[Z_t]=1-\frac{u^2}{2}\int_0^t\sigma^2(s)E[Z_s]\ ds+iuE\left[\int_0^t \sigma(s)Z_s\ dW_s\right]=1-\frac{u^2}{2}\int_0^t\sigma^2(s)E[Z_s]\ ds,
$$

since any expectaion of an integral wrt. a Brownian motion is 0 (proposition 4.5).

"4)" Now we see that the $t$-derivative gives

$$
dE[Z_t]=-\frac{u^2}{2}\sigma^2(t)E[Z_t]\ dt,\ \ E[Z_0]=1.
$$

This is a ordinary differential equation with solution $y(t)=\exp\{-u^2/2\int_0^t\sigma^2(s)\ ds\}$ (check by differentiating) hence

$$
E[e^{iuX_t}]=E[Z_t]=\exp\left\{-\frac{u^2}{2}\int_0^t\sigma^2(s)\ ds\right\}.
$$

We recognize this as the characteristic function of a normally distributed random variable with variance $\int_0^t\sigma^2(s)\ ds$ as desired. ($X_t$ follows this distributions since characteristic functions determine the distribution) $\square$

</details>

**Exercise 4** *(Bjork 4.4)* Suppose that $X$ has the stochastic differential

$$
dX_t=\alpha X_t\ dt+\sigma_t\ dW_t,
$$

where $\alpha$ is a real number and $\sigma_t$ is a integrable adapted stochastic process. Use the technique in example 4.17 in order to determine the function $m(t)=E[X_t]$.

<details>
<summary>**Solution.**</summary>

We follow the same steps as the previous exercise. We have been given the dynamics of $X$ hence we may write it on integral form.

$$
X_t=X_0+\alpha\int_0^tX_s\ ds+\int_0^t\sigma(s)\ dW_s.
$$

Then taking expectation now gives

$$
E[X_t]=X_0+\alpha\int_0^tE[X_s]\ ds.
$$

Hence $E[X_t]$ follows from the solution to the ODE below

$$
dE[X_t]=\alpha E[X_t]\Rightarrow E[X_t]=C\cdot\exp\{\alpha t\}.
$$

Then obviously $C=X_0$ and we arrive at the solution $E[X_t]=X_0e^{\alpha t}$, where $X_0$ is some deterministic value. $\square$

</details>

**Exercise 5** *(Bjork 4.5)* Suppose that the process $X$ has a stochastic differential

$$
dX_t=\mu_t\ dt+\sigma_t\ dW_t,
$$

and that $\mu_t\ge 0$ with probability one for all $t\ge 0$. Show that this implies that $X$ is a sub-martingale.

<details>
<summary>**Solution.**</summary>

Note that we are (strictly speaking) supposed to show adaptation and integrability, we will however only fokus on the submartingale property.

"$E[X_t\vert \mathcal{F}_s]\ge X_s$" Intuitively speaking, the statement is obvious since we have with probability one a positive upwards drift with Brownian distortion (i.e. martingale). Formally, we will show the statement by first writing $X_t$ on integral form

$$
X_t=x_0+\int_0^t\mu_s\ ds+\int_0^t\sigma_s\ dW_s.
$$

And so

$$
X_t-X_s=\int_s^t\mu_u\ du+\int_s^t\sigma_u\ dW_u.
$$

We then have

\begin{align*}
E[X_t\ \vert\ \mathcal{F}_s]-X_s&=E[X_t-X_s\ \vert\ \mathcal{F}_s]\\
&=E\left[\left.\int_s^t\mu_u\ du+\int_s^t\sigma_u\ dW_u\ \right\vert\ \mathcal{F}_s\right]\\
&=E\left[\left.\int_s^t\mu_u\ du\ \right\vert\ \mathcal{F}_s\right]+E\left[\left.\int_s^t\sigma_u\ dW_u\ \right\vert\ \mathcal{F}_s\right]\\
&=E\left[\left.\int_s^t\mu_u\ du\ \right\vert\ \mathcal{F}_s\right]\ge 0.
\end{align*}

Then adding $X_s$ to the above inequality yields the result. $\square$

</details>

**Exercise 6** *(Bjork 4.7)* The objective of this exercise is to give an argument for the formal identity

$$
dW_1(t)\cdot dW_2(t)=0,
$$

when $W_1$ and $W_2$ are independent Brownian motions. Let us therefore fix a time $t$, and divide the inerval $[0,t]$ into equidistant points $0=t_0<t_1<\cdots < t_n=t$, where $t_i=\frac{i}{n}\cdot t$. We use the notation

$$
\Delta W_i(t_k)=W_i(t_k)-W_i(t_{k-1}),\ i=1,2.
$$

Now define $Q_n$ by

$$
Q_n=\sum_{k=1}^n \Delta W_1(t_k)\cdot \Delta W_2(t_k).
$$

Show that $Q_n\to 0$ in $L^2$, i.e. show that

$$
E[Q_n]=0,\\
Var[Q_n]\to 0.
$$

<details>
<summary>**Solution.**</summary>

We wish to show the statement

$$
E[(Q_n-0)^2]=E[Q_n^2]\to 0,
$$

as $n\to \infty$. Recall that 

$$
Var[Q_n]=E[Q_n^2]-E[Q_n]^2,
$$

hence if $Q_n$ has mean 0, then showing convergence in $L^2$ is equivalent to showing variance going to 0. Let us start by showing the mean is 0.

We have that

\begin{align*}
Q_n&=\sum_{k=1}^n \Delta W_1(t_k)\cdot \Delta W_2(t_k)\\
&=\sum_{k=1}^n(W_1(t_k)-W_1(t_{k-1}))\cdot (W_2(t_k)-W_2(t_{k-1}))\\
&\stackrel{\mathcal{D}}{=}\sum_{k=1}^nXY,
\end{align*}

where $X,Y\sim\mathcal{N}(0,t_k-t_{k-1})=\mathcal{N}(0,1/n)$ and independent random variable. This is justified since the increments of the Brownian motion has mean 0 and variance equal to the increment size. Now this implies, that we need to show that $E[XY]=0$ and that $Var[XY]$ is sufficiently small in terms of $n$ such that it is summable. We see that

$$
E[XY]=E[X]E[Y]=0^2=0.
$$

Here we use independence. We now know that the mean is

$$
E[Q_n]=\sum_{k=1}^nE[XY]=0.
$$

We know from basic properties of variance that

\begin{align*}
Var(Q_n)&=\sum_{k=1}^n Var(XY)=\sum_{k=1}^n E[(XY)^2]\\
&=\sum_{k=1}^n\frac{1}{n^2}=\frac{1}{n^2}n\\
&=\frac{1}{n}\to0,\ n\to\infty.
\end{align*}

And so the result follows. $\square$

</details>

**Exercise 7** *(Bjork 4.8)* Let $X$ and $Y$ be given as the solutions to the following system of stochastic differential equations.

\begin{align*}
&dX_t=\alpha X_t\ dt-Y_t\ dW_t,\ &X_0=x_0,\\
&dY_t=\alpha Y_t\ dt + X_t\ dW_t,\ &Y_0=y_0.
\end{align*}

Note that the initial values $x_0$ and $y_0$ are deterministic constants.

  a. Prove that the process $R$ defined by $R_t=X_t^2+Y_t^2$ is deterministic.
  b. Compute $E[X_t]$.

<details>
<summary>**Solution (a).**</summary>

We see that

$$
dR_t=d(X_t^2+Y_t^2)=d(X_t^2)+d(Y_t^2)
$$

Hence we may start by considering de dynamics of the processes $X_t^2$ and $Y_t^2$. We see that for the process $Z_t=X_t^2$ we may set $f(t,x)=x^2$ and the relevant derivatives are

$$
\frac{\partial f}{\partial t}(t,x)=0,\ \frac{\partial f}{\partial x}(t,x)=2x,\ \frac{\partial^2 f}{\partial x^2}(t,x)=2.
$$

By Ito's formula we have

$$
d(X_t^2)=\left(\alpha X_t2X_t+Y_t^22\right)\ dt-Y_t2X_t\ dW_t=2(\alpha X_t^2+Y_t^2)\ dt-2X_tY_t\ dW_t.
$$

By the same concept we have

$$
d(Y_t^2)=\left(\alpha Y_t2Y_t+X_t^22\right)\ dt+X_t2Y_t\ dW_t=2(\alpha Y_t^2+X_t^2)\ dt+2X_tY_t\ dW_t.
$$

Combining we get the dynamics

\begin{align*}
dR_t&=2(\alpha X_t^2+Y_t^2)\ dt-2X_tY_t\ dW_t\\
&+2(\alpha Y_t^2+X_t^2)\ dt+2X_tY_t\ dW_t\\
&=(2\alpha +1)(X_t^2 + Y_t^2)\ dt\\
&=(2\alpha +1)R_t\ dt
\end{align*}

Hence $R_t$ has deterministic derivative and therefore a deterministic process. In fact, the solution to above is

$$
R_t=R_0\exp\left\{(2\alpha + 1)t\right\}=(x_0^2+y_0^2)e^{(2\alpha + 1)t},
$$

which is clearly deterministic. $\square$

</details>

<details>
<summary>**Solution (b).**</summary>

We start by acknowledging that the differential form of $X$ may be written on integral form:

$$
X_t=x_0+\alpha\int_0^tX_s\ ds-\int_0^tY_s\ dW_s.
$$

Taking expectation we see that

$$
E[X_t]=x_0+\int_0^tE[X_s]\ ds
$$

as the last term has mean 0 according to proposition 4.5. Then the above may be written on the differential form

$$
dE[X_t]=E[X_t]\ dt
$$

Hence we have that

$$
E[X_t]=x_0e^{t}.
$$

Hence $X_t$ has mean not depending on the tragetory of the sister-process $Y_t$. $\square$

</details>

## Week 3

### Material

  * Partial differential equations (Chapter 5.5)
  * Self-financing portfolios (Chapter 6)
  * Black-Scholes PDE (classic approach) and risk neutral valuation (Chapter 7.1-5)

### Theory

#### Partial differential equations

<blockquote class = "prop">

**Proposition 5.5.** **(Feynmann-Kac)** Assume that $F$ is a solution to the boundary value problem

$$
\frac{\partial F}{\partial t}(t,x)+\mu(t,x)\frac{\partial F}{\partial x}(x,t)+\frac{1}{2}\sigma^2(t,x)\frac{\partial^2 F}{\partial x^2}(t,x)=0,
$$

with boundary condition $F(T,x)=\Phi(x)$. Assume furthermore that the process

$$
\sigma(s,X_s)\frac{\partial F}{\partial x}(s,X_s) \in \mathcal{L}^2
$$

as per definition 4.4, where $X$ is defined below. Then $F$ has the representation

$$
F(t,x)=E_{t,x}[\Phi(X_T)]=E[\Phi(X_T)\ \vert\ X_t=x],
$$

where $X$ satisfies the SDE

$$
dX_s=\mu(s,X_s)\ ds+\sigma(s,X_s)\ dW_s,
$$

with boundary condition $X_t=x$.

</blockquote>

<blockquote class = "prop">

**Proposition 5.6.** **(Feynmann-Kac)** Assume that $F$ is a solution to the boundary value problem

$$
\frac{\partial F}{\partial t}(t,x)+\mu(t,x)\frac{\partial F}{\partial x}(x,t)+\frac{1}{2}\sigma^2(t,x)\frac{\partial^2 F}{\partial x^2}(t,x)-rF(t,x)=0,
$$

with boundary condition $F(T,x)=\Phi(x)$. Assume furthermore that the process

$$
e^{-rs}\sigma(s,X_s)\frac{\partial F}{\partial x}(s,X_s) \in \mathcal{L}^2
$$

as per definition 4.4, where $X$ is defined below. Then $F$ has the representation

$$
F(t,x)=e^{-r(T-t)}E_{t,x}[\Phi(X_T)]=e^{-r(T-t)}E[\Phi(X_T)\ \vert\ X_t=x],
$$

where $X$ satisfies the SDE

$$
dX_s=\mu(s,X_s)\ ds+\sigma(s,X_s)\ dW_s,
$$

with boundary condition $X_t=x$.

</blockquote>

<blockquote class = "prop">

**Proposition 5.8.** **(Feynmann-Kac)** Assume that $F$ is a solution to the boundary value problem

$$
\frac{\partial F}{\partial t}(t,x)+\sum_{i=1}^n\mu_i(t,x)\frac{\partial F}{\partial x}(x,t)+\frac{1}{2}\sum_{i,j=1}^n C_{ij}(t,x)\frac{\partial^2 F}{\partial x^2}(t,x)-rF(t,x)=0,
$$

with boundary condition $F(T,x)=\Phi(x)$ and $C_{ij}=\sigma \sigma^\top$. Assume furthermore that the process

$$
e^{-rs}\sum_{i=1}^n\sigma_i(s,X_s)\frac{\partial F}{\partial x}(s,X_s) \in \mathcal{L}^2
$$

as per definition 4.4, where $X$ is defined below. Then $F$ has the representation

$$
F(t,x)=e^{-r(T-t)}E_{t,x}[\Phi(X_T)],
$$

where $X$ satisfies the SDE

$$
dX_s=\mu(s,X_s)\ ds+\sigma(s,X_s)\ dW_s,
$$

with boundary condition $X_t=x$.

</blockquote>

<blockquote class = "prop">

**Proposition 5.9.** Consider as given a vector process $X$ with generator $\mathcal{A}$, and a function $F(t,x)$. Then, modulo some integrability condition, the following hold:

  * The process $F(t,X_t)$ is a martingale relative to the filtration $\mathcal{F}^X$ if and only if $F$ satisfies the PDE
  $$
  \frac{\partial F}{\partial t}+\mathcal{A}F=0.
  $$
  * The process $F(t,X_t)$ is a martingale relative to the filtration $\mathcal{F}^X$ if and only if, for every $(t,x)$ and $T\ge t$, we have
  $$
  F(t,x)=E_{t,x}[F(T,X_T)].
  $$

</blockquote>

#### Self-financing portfolios

We move forward in this chapter by first defining a self-financing portfolio in discrete time and then by letting the step length tend to zero obtain the continuous time analogue.

##### Discrete time SF portfolio

We consider $N$ different adapted price processes $S^1,...,S^N$. We use the following definition.

<blockquote class = "def">

**Definition 6.1.** We use the following definitions.

  * $S_n^i$ is th price of asset $i$ at time $n$,
  * $h_n^i$ is the number of units of asset $i$ held during $[n,n+1)$, that is bought at time $n$,
  * $d_n^i$ is the dividends from asset $i$ in the time-interval $[n-1,n)$, that is recieved at time $n$,
  * $h_n$ is the portfolio $(h_n^1,...,h_n^N)$ held during $[n,n+1)$,
  * $c_n$ is the consumption i.e. withdrawel at time $n$ (negative being deposits/saving),
  * $V_n$ is the value of the portfolio just before time $n$ i.e. of the portfolio $h_{n-1}$ at time $n$.

</blockquote>

We are now ready to define the self-financing portfolio

<blockquote class = "def">

**Definition 6.2.** A **self-financing portfolio supporting the consumption stream** $\mathbf{c}$ is a portfolio adhering to the **budget constraint** given as

$$
h_{n+1}S_{n+1}+c_{n+1}=h_nS_{n+1}+h_nd_{n+1.}
$$

The interpretation being, that we may only use funds obtained from selling the old portfolio $h_n$ and recieved in dividends to buy the new portfolio $h_{n+1}$ and consume the amount $c_{n+1}$.

</blockquote>

Before studying the self-financing portfolio we define the operator $\Delta$ (in definition 6.3) as the increment $\Delta x_n=x_{n+1}-x_n$ of a countable sequence $(x_n)_{n\in\mathbb{N}_0}$. Notice that we define the increment forward so the increment $n$ is the increment over the time period $[n,n+1)$ with the first increment being $[0,1)$. Using this notation we can derive the lemma below.

<blockquote class = "lem">

**Lemma 6.4.** For any pair of sequences of real numbers $(x_n)_{n\in\mathbb{N}_0}$ and $(y_n)_{n\in\mathbb{N}_0}$ we have the relations

\begin{align*}
\Delta(xy)_n&=x_n\Delta y_n+y_{n+1}\Delta x_n,\\
\Delta(xy)_n&=y_n\Delta x_n+x_{n+1}\Delta y_n,\\
\Delta(xy)_n&=x_n\Delta y_n+y_n\Delta x_n+\Delta x_n\Delta y_n.
\end{align*}

This is also valid if the sequances are $N$-dimensional, where we interpret the products above as scalar products ($xy^\top$).

</blockquote>

Using these definitions and the lemma above we see that the dynamics of the self-financing portfolio is given below.

<blockquote class = "prop">

**Proposition 6.6.** The dynamics of any self-financing portfolio supporting the consumption stream $c$ are given by

$$
\Delta V_n=h_n \Delta S_n+h_nd_{n+1}-c_{n+1},
$$

or, in more detail

$$
\Delta V_n=\sum_{i=1}^Nh_n^i(\Delta S_n^i+d^i_{n+1})-c_{n+1}.
$$

</blockquote>

We may rewrite the dividends as accumulating dividends $D^i_n=\sum_{k=1}^nd^i_k$ and see that $d_{n+1}^i=\Delta D^i_n$ and so the above condition is equivalent with.

<blockquote class = "prop">

**Proposition 6.8.** The dynamics of any self-financing portfolio supporting the consumption stream $c$ are given by

$$
\Delta V_n=h_n \Delta S_n+h_n\Delta D_n-c_{n+1},
$$

or, in more detail

$$
\Delta V_n=\sum_{i=1}^Nh_n^i(\Delta S_n^i+\Delta D^i_n)-c_{n+1}.
$$

</blockquote>

##### Continuous time SF portfolio

Formulating the dynamics of the self-financing portfolio in continuous time is easy work given the discrete setup above. However since we now are in continuous time we will change the $n$ with a $t$ and cosider the behavour $V_{t+dt}-V_t$ as we let $dt\to 0$. First we formulate some basic notation.

<blockquote class = "def">

**Definition 6.9.** We use the following definitions.

  * $S_t^i$ is th price of asset $i$ at time $t$,
  * $h_t^i$ is the number of units of asset $i$ held at time $t$,
  * $D_t^i$ is the cumulative dividend processs for asset $i$,
  * $h_t$ is the portfolio $(h_t^1,...,h_t^N)$ held at time $t$,
  * $c_t$ is the consumption rate at time $n$ (negative being deposits/saving),
  * $V_t$ is the value of the portfolio at time $t$ i.e. of the portfolio $h_t$ at time $t$.

</blockquote>

Given these definitions we may define a portfolio strategy that is self-financing.

<blockquote class = "def">

**Definition 6.10.** Let $S$ be and adapted $N$-dimensional price process. We define the following

  1. A **portfolio strategy** is any adapted $N$-dimensional process $h$.
  2. The **value process** $V^h$ corresponding to the portfolio $h$ is given by
  $$
  V_t^h=\sum_{i=1}^N h_t^iS_t^i.
  $$
  3. A **consumption process** is any adapted one-dimensional process $c$.
  4. A portfolio-consumption pair $(h,c)$ is called **self-financing** if the value process $V^h$ satisfies the condition
  $$
  dV_t^h=\sum_{i=1}^N h_t^i(dS_t^i+d D^i_t)-c_t\ dt,
  $$
  i.e. if
  $$
  dV_t^h=h_t\ dS_t + h_t\ dD_t -c_t\ dt.
  $$
  5. The **gain process** $G$ is defined by
  $$
  G_t=S_t+D_t
  $$
  so we can write the self-financing condition as
  $$
  dV_t=h_t\ dG_t-c_t\ dt.
  $$
  6. The portfolio $h$ is said to be **Markovian** if it is of the form
  $$
  h_t=h(t,S_t),
  $$
  for some function $h : \mathbb{R}_+\times \mathbb{R}^N\to\mathbb{R}^N$.

</blockquote>

##### Portfolio weights

<blockquote class = "def">

**Definition 6.11.** For a given portfolio $h$ the corresponding **relative portfolio** or **portfolio weights** $w$ are defined by

$$
w_t^i=\frac{h_t^iS_t^i}{V_t^h},\ i=1,...,N,
$$

so, in particular, we have $\sum_{i=1}^N w_i=1$.

</blockquote>

<blockquote class = "lem">

**Lemma 6.12.** A portfolio-consumption par $(h,c)$ is self-financing if and only if

$$
dV_t^h=V_t^h\sum_{i=1}^N w_t^i\frac{dS_t^i+dD_t^i}{S_t^i}-c_t\ dt.
$$

</blockquote>

<blockquote class = "lem">

**Lemma 6.13.** Consider the case with no dividends. Let $c$ be a consumption process, and assume that there exist a scalar process $Z$ and a vector process $q=(q^1,...,q^N)$ such that

$$
dZ_t=Z_t\sum_{i=1}^N q_t^i\frac{dS_t^i}{S_t^i}-c_t\ dt,
$$

and $\sum_{i=1}^Nqq^i=1$. Now define a portfolio $h$ by

$$
h_t^i=\frac{q_t^iZ_t}{S_t^i}.
$$

Then the value process $V^h$ is given by $V^h=Z$, the pair $(h,c)$ is self-financing, and the corresponding relative portfolio $w$ is given by $w=q$.

</blockquote>

#### Black-Scholes PDE/Risk-neutral valuation

The Black-Scholes model revolves arround SDE's as seen above. In this model we have two assets a risk free asset $B$ and a stochastic priced asset $S$. We therefore start by defining what we mean by a quote-on-qoute *risk free* asset.

<blockquote class = "def">

**Definition 7.1.** The price process $B$ is the price of a **risk free** asset if it has the dynamics

$$
dB_t=r_t B_t\ dt,
$$

where $r$ is any $\mathcal{F}_t$ adapted process.

</blockquote>

We see from this definition that the meaning of "risk free" is the property, that $B$ is priced locally deterministic in the sence that $r$ is adapted and therefore known at time $t$ and we therefore know the yield on a short term basis. This is also why we may call $r$ the **short interest rate**. Given the dynamics above, we know that $B$ in fact is represented by the process

$$
B_t=B_0e^{\int_0^tr_s\ ds},
$$

for some $B_0$ initial value. We will moving forward assume that $B_0=1$. The stochastic asset $S$ has dynamics.

$$
dS_t=\mu(t,S_t)\ dt + \sigma(t,S_t)\ dW_t,
$$

where as usual $\mu$ and $\sigma$ are deterministic functions and $W_t$ is a standard Brownian motion. Note that the risk free asset has a similarly process with $\sigma = 0$. We may now include this in the definition of the Black-Scholes model.

<blockquote class = "def">

**Definition 7.2.** The **Black-Scholes model** consists of two assets with dynamics given by

\begin{align*}
dB_t&=rB_t\ dt,\\
dS_t&=\mu S_t\ dt+\sigma S_t\ dW_t,
\end{align*}

where $r,\mu,\sigma$ are deterministic constants.

</blockquote>

<blockquote class = "def">

**Definition 7.3.** A **zero coupon bond** with maturity $T$ (henceforth "$T$-bond") is an asset which pays the holder the face value 1 dollar at time $T$. The price at time $n$ of a $T$-bond is denoted by $p(n,T)$.

</blockquote>

<blockquote class = "def">

**Definition 7.4.** The (possible stochastic) discrete **short rate** $r_n$, for the period $[n,n+1]$, is defined as

$$
p(n,n+1)=\frac{1}{1+ r_n}.
$$

</blockquote>

From this short rate we may derive the dynamics of the bank account recieving zero-coupon rates for each distinct time interval.

<blockquote class = "def">

**Definition 7.5.** The dynamics of the bank account are given by

$$
\Delta B_n=r_n B_n.
$$

</blockquote>

##### Contingent Claims and Arbitrage

<blockquote class = "def">

**Definition 7.6.** A **European call option** with **exercise price** (or strike price) $K$ and **time of maturity** (exercise date) $T$ on the **underlying asset** $S$ is a contract defined by the following clauses:

  * The holder of the option has, at time $T$, the right to buy one share of the underlying stock at the price $K$ dollars from the underwriter of the option.
  * The holder of the option is in no way obliged to buy the underlying stock.
  * The right to buy the underlying stock at the price $K$ can only be exercised at the precise time $T$.

</blockquote>

Obviously, we also have the **european put** option which gives the owner the right to sell an asset at price $K$ at time $T$. Let os formally define a contingent claim.

<blockquote class = "def">

**Definition 7.7.** Consider a financial market with vector price process $S$. A **contingent claim** with **date of maturity** $T$, also called a $T$-claim, is any random variable $\mathcal{X}\in\mathcal{F}_T^S$. A contingent claim $\mathcal{X}$ is called a **simple** claim if it is of the form $\mathcal{X} = \Phi(S_t)$. The function $\Phi$ is called the **contract function**.

</blockquote>

<blockquote class = "def">

**Definition 7.8.** An **arbitrage** possibility on a financial market is a self-financed portfolio $h$ such that

\begin{align*}
V^h(0)&=0,\\
P(V_T^h\ge0)&=1,\\
P(V_T^h>0)&>0.
\end{align*}

We say that the market is **arbitrage free** if there are no arbitrage possibilities.

</blockquote>

<blockquote class = "def">

**Definition 7.9.** Suppose that there exists a self-financing portfolio $h$, such that the value process $V^h$ has the dynamics

$$
d V_t^h=k_tV_t^h\ dt,
$$

where $k$ is an adapted process. Then it must hold that $k_t=r_t$ for all $t$, ore there exists an arbitrage possibility.

</blockquote>

<blockquote class = "thm">

**Theorem 7.10.** **(Black-Scholes equation)** Assume that the market is specified by the equations

\begin{align*}
dB_t&=rB_t\ dt,\\
dS_t&=\mu(t,S_t) S_t\ dt+\sigma(t,S_t)S_t\ dW_t,
\end{align*}

and that we want to price a contingent claim of the form $\mathcal{X}=\Phi(S_t)$. Then the only pricing function of the form $\Pi_t[\Phi(S_t)]=F(t,S_t)$ which is consistent with the absence of arbitrage in the market $[B_t,S_t,\Pi_t]$ is when $F$ is the solution of the following boundary value problem in the domain $[0,T]\times\mathbb{R}_+$:

\begin{align*}
F_t(t,s)+rsF_s(t,s)+\frac{1}{2}s^2\sigma^2(t,s)F_ss(t,s)-rF(t,s)&=0,\\
F(T,s)&=\Phi(s).
\end{align*}

</blockquote>

##### Risk Neutral Valuation

<blockquote class = "thm">

**Theorem 7.11.** **(Risk Neutral Valuation)** The arbitrage free price of the claim $\Phi(S_t)$ is given by $\Pi_t[\Phi]=F(t,S_t)$, where $F$ is given by the formula

$$
F(t,s)=e^{-r(T-t)}E^Q_{t,s}[\Phi(S_T)],
$$

where the $Q$-dynamics of $S$ are those of

$$
dS_t=rS_t\ dt+S_t\sigma(t,S_t)\ dW_t^Q.
$$

</blockquote>

<blockquote class = "prop">

**Property 7.12.** **(The Martingale Property)** In the Black-Scholes model, the price process $\Pi_t$ for every traded asset, be it the underlying or derivate asset, has the property the the normalized price process

$$
Z_t=\frac{\Pi_t}{B_t},
$$

(including $S_t/B_t$) is a martingale under the measure $Q$.

</blockquote>

### Exercises

**Exercise 1** *(Bjork 5.1)* Show that the scalar SDE

$$
\left\{
\begin{matrix}
dX_t=\alpha X_t\ dt + \sigma\ dW_t,\\
X_0 = x_0,
\end{matrix}\right.
$$

has the solution

$$
X(t)=e^{\alpha t}x_0+ \sigma\int_0^te^{\alpha(t-s)}\ dW_s,
$$

by differentiating $X$ as defined by the equation above and showing that $X$ so defined satisfies the SDE.

<details>
<summary>**Solution.**</summary>

We move forward by rewriting the solution in terms of three processes $Z$, $Y$ and $R$ as

$$
X_t=\underbrace{x_0e^{\alpha t}}_{:=Y_t}+\underbrace{\sigma e^{\alpha t}}_{:=Z_t} \underbrace{\int_0^t e^{-\alpha s}\ dW_s}_{R_t}=Y_t+Z_t\cdot R_t.
$$

We furthermore see easily that the dynamics of the processes individually has dynamics

\begin{align*}
d Y_t&=\alpha x_0e^{\alpha t}\ dt=\alpha Y_t\ dt,\ &Y_0=x_0,\\
d Z_t&=\alpha \sigma^{\alpha t}\ dt=\alpha Z_t\ dt,\ &Z_0=\sigma,\\
d R_t&=e^{-\alpha t}\ dW_s,\ &R_0=0.
\end{align*}

We then have the following function

$$
f\left(t,y,z,r\right)=y+zr.
$$

With the following multi-dimensional process

$$
dM_t=\begin{bmatrix}\alpha Y_t\\ \alpha Z_t\\ 0\end{bmatrix}dt+\begin{bmatrix}0 &0  &0 \\ 0 & 0 &0 \\ 0 & 0 & e^{-\alpha t}\end{bmatrix}\begin{bmatrix}dW_t\\ dW_t\\ dW_t\end{bmatrix},
$$

with 

$$
C=\sigma \sigma^\top =\begin{bmatrix}0 &0  &0 \\ 0 & 0 &0 \\ 0 & 0 & e^{-\alpha t}\end{bmatrix}^2=\begin{bmatrix}0 &0  &0 \\ 0 & 0 &0 \\ 0 & 0 & e^{-2\alpha t}\end{bmatrix}.
$$

That is $X_t=f(t,M_t)$. We can then use the multidimensional version of Ito's formula.

\begin{align*}
dX_t&=df(t,M_t)\\
&=\left(\frac{\partial f}{\partial t}(t,M_t)+\sum_{i=1}^3\mu_i \frac{\partial f}{\partial x^i}(t,M_t)+\frac{1}{2}\sum_{i,j=1}^3 C^{ij}_t\frac{\partial^2 f}{\partial x^i\partial x^j}(t,M_t)\right)\ dt + \sum_{i=1}^3 \frac{\partial f}{\partial x^i}(t,M_t) \sigma^i_t\ dW_t\\
&=\left(0+\alpha Y_t+\alpha Z_t R_t\right)\ dt + Z_te^{-\alpha t}\ dW_t\\
&=\left(\alpha x_0e^{\alpha t}+\alpha \sigma e^{\alpha t} \int_0^t e^{-\alpha \sigma}\ dW_s\right)\ dt + \sigma e^{\alpha t}e^{-\alpha t}\ dW_t\\
&=\left(\alpha x_0e^{\alpha t}+\alpha \sigma  \int_0^t e^{(t-s)\alpha }\ dW_s\right)\ dt + \sigma \ dW_t\\
&=\alpha X_t\ dt + \sigma \ dW_t.
\end{align*}

Then this solution does in fact satisfies the differential form. We furthermore have that $X_0=x_0$ and the desired result follows. $\square$

</details>

**Exercise 2** *(Bjork 5.5)* Suppose that $X$ satisfies the SDE

$$
dX_t = \alpha X_t\ dt + \sigma X_t\ dW_t.
$$

Now define $Y$ by $Y_t = X^\beta_t$, where $\beta$ is a real number. Then $Y$ is also a GBM process. Compute $dY_t$ and find out which SDE $Y$ satisfies.

<details>
<summary>**Solution.**</summary>

If we set $f(t,x)=x^\beta$, we have the relevant derivatives as follows

$$
\frac{\partial f}{\partial t}(t,x)=0,\ \frac{\partial f}{\partial x}(t,x)=\beta x^{\beta -1},\ \frac{\partial^2 f}{\partial x^2}(t,x)=\beta (\beta -1) x^{\beta -2}.
$$

Then by applying Ito's formula we have

\begin{align*}
dY_t&=df(t,X_t)\\
&=\left(0 + \beta X_t^{\beta -1}\alpha X_t+\frac{1}{2}\sigma ^2X_t^2\beta (\beta -1) X_t^{\beta -2}\right)\ dt+\sigma X_t\beta X_t^{\beta -1} \ dW_t\\
&=\left(\alpha \beta+\frac{1}{2}\sigma ^2\beta (\beta -1)\right) X_t^{\beta}\ dt+\sigma \beta X_t^{\beta } \ dW_t\\
&=\left(\alpha \beta+\frac{1}{2}\sigma ^2\beta (\beta -1)\right) Y_t\ dt+\sigma \beta Y_t \ dW_t\\
&= \alpha^Y Y_t\ dt + \sigma^Y Y_t\ dW_t,
\end{align*}

where $\alpha^Y=\left(\alpha \beta+\frac{1}{2}\sigma ^2\beta (\beta -1)\right)$ and $\sigma^Y =\sigma \beta$. Futhermore $Y_0=y_0=x_0^\beta$. Then by definition of GBM we have that $Y_t$ is a GBM as desired. $\square$

</details>

**Exercise 3** *(Bjork 5.6)* Suppose that $X$ satisfies the SDE

$$
dX_t = \alpha X_t\ dt + \sigma X_t\ dW_t,
$$

and $Y$ satisfies

$$
dY_t = \gamma Y_t\ dt+\delta Y_t\ dV_t,
$$

where $V$ is a Brownian motion which is independent of $W$. Define $Z=X/Y$ and derive an SDE for $Z$ by computing $dZ$. If $X$ is nominal income and $Y$ describe inflation then $Z$ describes real income.

<details>
<summary>**Solution.**</summary>

We have that for the function $f(t,x,y)=x/y$ and wish to determine the derivative of the stochastic process $Z_t=f(t,X_t,Y_t)$. We do this by applying Ito's formula in the multidimensional case. That is

\begin{align*}
df(t,X_t,Y_t)&=\frac{\partial f}{\partial t}(t,X_t,Y_t)\ dt + \frac{\partial f}{\partial x}(t,X_t,Y_t)\ dX_t + \frac{\partial f}{\partial y}(t,X_t,Y_t)\ dY_t\\
&+\frac{1}{2}\frac{\partial^2 f}{\partial x^2}(t,X_t,Y_t)(dX_t)^2 + \frac{1}{2}\frac{\partial^2 f}{\partial y^2}(t,X_t,Y_t)(dY_t)^2\\
&+\frac{1}{2}\frac{\partial^2 f}{\partial x\partial y}(t,X_t,Y_t)(dX_t)(dY_t)\\
&=\frac{1}{Y_t}(\alpha X_t\ dt + \sigma X_t\ dW_t)-\frac{X_t}{Y_t^2}(\gamma Y_t\ dt+\delta Y_t\ dV_t)+\frac{1}{2}2\frac{X_t}{Y_t^3}(\gamma Y_t\ dt+\delta Y_t\ dV_t)^2\\
&-\frac{1}{2}\frac{1}{Y_t^2}(\gamma Y_t\ dt+\delta Y_t\ dV_t)(\alpha X_t\ dt + \sigma X_t\ dW_t)
\end{align*}

Calculating further gives

\begin{align*}
(dY_t)^2&=\gamma^2 Y_t^2 (dt)^2+\delta^2Y_t^2 (dV_t)^2+2\gamma Y_t\delta Y_t\ dt\cdot dV_t\\
&=0 +\delta^2Y_t^2 dt + 0=\delta^2Y_t^2 dt\\
(dX_t)(dY_t)&=(\gamma Y_t\ dt+\delta Y_t\ dV_t)(\alpha X_t\ dt + \sigma X_t\ dW_t)\\
&=\gamma Y_t \alpha X_t\ (dt)^2 +\gamma Y_t\sigma X_t\ dt\cdot dW_t\\
&+\delta Y_t \alpha X_t\ dt\cdot dV_t+\gamma Y_t\sigma X_t\ (dW_t)(dV_t)\\
&=0+0+0+0=0
\end{align*}

Hence we conclude that

\begin{align*}
df(t,X_t,Y_t)&=\alpha\frac{X_t}{Y_t}\ dt + \sigma \frac{X_t}{Y_t}\ dW_t-\gamma \frac{X_tY_t}{Y_t^2}\ dt+\delta \frac{X_tY_t}{Y_t^2}\ dV_t\\
&+\frac{1}{2}2\frac{X_t}{Y_t^3}\delta^2Y_t^2 dt\\
&=\left(\alpha Z_t-\gamma Z_t+Z_t\delta^2\right)\ dt+ \sigma Z_t\ dW_t+\delta Z_t\ dV_t\\
&=\left(\alpha -\gamma +\delta^2\right)Z_t\ dt+ \sigma Z_t\ dW_t+\delta Z_t\ dV_t.
\end{align*}

As desired the above is the SDE for the process $Z_t$. $\square$

</details>

**Exercise 4** *(Bjork 5.9)* Use a stochastic representation result in order to solve the following boundary value problem in the domain $[0,T]\times\mathbb{R}$.

$$
\frac{\partial F}{\partial t}+\mu x\frac{\partial F}{\partial x}+\frac{1}{2}\sigma^2x^2\frac{\partial^2F}{\partial x^2}=0,
$$

with $F(T,x)=\log(x^2)$. Here $\mu$ and $\sigma$ are assumed to be know constants.

<details>
<summary>**Solution.**</summary>

We use proposition 5.5 Feymann-Kac with $\mu(t,x)=\mu x$ and $\sigma(t,x)=\sigma x$. We know that, given that the process

$$
\sigma X_t \frac{\partial F}{\partial x}(x,X_t)\in \mathcal{L}^2,
$$

Then $F$ has stochastic representation

$$
F(t,x)=E[\log(X_T^2)\ \vert\ X_t=x],
$$

with stochastic process $X_t$ satisfying the SDE

$$
dX_t=\mu X_t\ dt+\sigma X_t\ dW_t,\hspace{20pt} X_t=x.
$$

Now, since $X_t$ satisfies the above SDE, we see that $X_t$ is a GBM. Then by proposition 5.2 we have

$$
X_T=x\exp\left\{\left(\mu - \frac{1}{2}\sigma^2\right)(T-t)+\sigma(W_T-W_t)\right\}.
$$

Inserting this we find that

\begin{align*}
F(t,x)&=E\left.\left[\log(x^2\exp\left\{\left(2\mu - \sigma^2\right)(T-t)+2\sigma(W_T-W_t)\right\})\ \right\vert\ X_t=x\right]\\
&=E\left.\left[\log(x^2)+\left(2\mu - \sigma^2\right)(T-t)+2\sigma(W_T-W_t)\ \right\vert\ X_t=x\right]\\
&=2\log(x)+\left(2\mu - \sigma^2\right)(T-t)+2\sigma E\left.\left[W_T-W_t\ \right\vert\ X_t=x\right]\\
&=2\log(x)+\left(2\mu - \sigma^2\right)(T-t).
\end{align*}

Using that the Brownian motion has increments with mean 0. $\square$

</details>

**Exercise 5** *(Bjork 5.13)* Solve the boundary value problem

$$
\frac{\partial F}{\partial t}+\frac{1}{2}\sigma^2 \frac{\partial^2F}{\partial x^2}++\frac{1}{2}\delta^2\frac{\partial^2F}{\partial y^2}=0,
$$

with $F(T,x,y)=xy$.

<details>
<summary>**Solution.**</summary>



</details>

**Exercise 6** *(Exam 2017/18, problem 1, question (a)-(b))*

<details>
<summary>**Solution.**</summary>



</details>

**Exercise 7** *(Exam 2019/20, problem 1, question (a))*

<details>
<summary>**Solution.**</summary>



</details>

**Exercise 8** *(Exam 2020/21, problem 1, question (a)-(b))*

<details>
<summary>**Solution.**</summary>



</details>

## Week 4

### Material

  * Black-Scholes formula (Chapter 7.6, see also Remark to Black-Scholes formula)
  * Completeness and hedging (Chapter 8)
  * Put-call parity (Chapter 10.1)
  * The Greeks (Chapter 10.2)
  * Risk neutral valuation formula (Chapter 11.6) 
  * Equivalent probability measures (Appendix A.11, B.6 and C.3) 

### Theory

```{r,echo=FALSE,include=FALSE}
set.seed(10)
mu <- 0.15
sigma <- 0.35
r <- 0.04
N <- 1000 #Observations pr. year
T <- 10
t <- (0:(T*N))/N
deltaW <- rnorm(length(t)-1,mean =0, sd = sqrt(1/N))
S_0 <- 1
K <- 1.2
d_1 <- (log(S_0/K)+(r+0.5*sigma**2)*(T-0))/(sigma*sqrt(T-0))
d_2 <- d_1 - sigma*sqrt(T-0)
Pi_0 <- S_0 * pnorm(d_1)-exp(-r*(T-0))*K*pnorm(d_2)
W_B_0 <- (Pi_0-S_0*pnorm(d_1))/Pi_0
W_S_0 <- 1 - W_B_0
Prices <- data.frame(t = t,
                     S = S_0,
                     B = exp(r*t),
                     Pi = Pi_0,
                     W_B = W_B_0,
                     Value = Pi_0,
                     Phi = max(S_0-K,0))
for (i in 2:length(t)){
  S_t <- Prices[i-1,"S"]
  S_t <- S_t+S_t*mu/N+S_t*sigma*deltaW[i-1]
  Prices[i,"S"] <- S_t
  Prices[i,"Value"] <- Prices[i-1,"Value"]*(1+((S_t/Prices[i-1,"S"]-1)*(1-Prices[i-1,"W_B"])+(Prices[i,"B"]/Prices[i-1,"B"]-1)*Prices[i-1,"W_B"]))
  d_1 <- (log(S_t/K)+(r+0.5*sigma**2)*(T-Prices[i,"t"]))/(sigma*sqrt(T-Prices[i,"t"]))
  d_2 <- d_1 - sigma*sqrt(T-Prices[i,"t"])
  Pi <- S_t * pnorm(d_1)-exp(-r*(T-Prices[i,"t"]))*K*pnorm(d_2)
  Prices[i,"Pi"] <- Pi
  W_B <- (Pi-S_t*pnorm(d_1))/Pi
  Prices[i,"W_B"] <- W_B
  Prices[i,"Phi"] <- max(S_t-K,0)
}
ggplot2::ggplot(data = Prices) + geom_line(aes(x=t,y=S), col = "black") + geom_line(aes(x=t,y=B),col = "blue") + geom_hline(yintercept = K, col = "red") +
  labs(title = "Realisation of a Brownian motion") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
        title = element_text(size = 18))
ggplot2::ggplot(data = Prices)+ geom_line(aes(x=t,y=Pi), col = "red") +geom_line(aes(x=t,y=Value), col = "black") + geom_line(aes(x=t,y=Phi), col = "red") +
  labs(title = "Realisation of a Brownian motion") +
  theme_bw() +
  theme(axis.text = element_text(size = 15),
        title = element_text(size = 18))
```


### Exercises

## Week 5

### Material

  * Girsanov theorem (Chapter 12, see also Levy characterization of Brownian motion and proof of Girsanov)
  * Martingale representation theorem (Chapter 12)

### Theory

### Exercises

## Week 6

### Material

  * Black-Scholes model, martingale approach (Chapter 13)
  * Multidimensional models (Chapter 14)

### Theory

### Exercises

## Week 7

### Material

  * Pricing and proof of fundamental pricing theorem I and II (Chapter 11)
  * Incomplete Markets (Chapter 9)

### Theory

### Exercises
